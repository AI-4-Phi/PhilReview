# Literature Review: Ethics of AI Agents, Autonomous Systems, and Algorithmic Decision-Making

**Domain Focus**: Moral status, agency, and responsibility in AI systems; algorithmic fairness; ethics of autonomous agents acting on behalf of humans in economic contexts

**Search Date**: 2025-11-11

**Papers Found**: 18 papers

**Search Sources Used**:
- Stanford Encyclopedia of Philosophy (SEP)
- PhilPapers (Philosophy of Computing and Information)
- Google Scholar
- arXiv
- Key journals: Ethics and Information Technology, AI & Society, Minds and Machines, Philosophy & Technology, Nature Machine Intelligence, Science and Engineering Ethics

## Overview

The ethics of AI agents and algorithmic decision-making has emerged as a vibrant interdisciplinary field over the past 15 years, combining moral philosophy, computer science, law, and policy. This literature review examines philosophical work addressing normative questions about AI as decision-makers, representatives, and autonomous agents—particularly relevant to contexts where artificial agents act on behalf of humans in economic and social settings.

Three major debates structure this domain. First, questions of **moral agency and responsibility**: Can artificial systems be moral agents? How should we understand the "responsibility gap" when autonomous systems make unpredictable decisions? Second, questions of **algorithmic fairness and justice**: What does fairness mean for algorithmic decision-making, and how do philosophical theories of justice apply to machine learning systems? Third, questions of **autonomy, delegation, and control**: When is it ethically permissible to delegate decision-making to AI agents, and how do we ensure human autonomy is respected?

The literature reveals substantial convergence around core ethical principles (transparency, fairness, accountability, autonomy, beneficence) but significant divergence in how these principles are interpreted and implemented. Most work addresses AI as tools or objects of ethical concern, but growing attention focuses on AI as agents or proxies acting on behalf of humans—directly relevant to the agentic economy and social experiments involving artificial decision-makers.

---

## Foundational Papers (Moral Agency and Machine Ethics)

### Floridi & Sanders (2004) On the Morality of Artificial Agents

**Full Citation**: Floridi, L., & Sanders, J. W. (2004). On the morality of artificial agents. Minds and Machines, 14(3), 349-379.

**DOI**: 10.1023/B:MIND.0000035461.63578.9d

**Type**: Journal Article

**Abstract**:
Floridi and Sanders examine how artificial agents function within moral frameworks. They propose that artificial agents (AAs) can serve as both moral patients and moral agents without necessarily possessing free will or mental states. Their key contribution is the "Method of Abstraction," which analyzes the level at which an agent operates. The authors establish three criteria for agenthood: interactivity, autonomy, and adaptability at a given level of abstraction. Regarding morality itself, they suggest it functions as "a threshold defined on the observables" at the relevant level of analysis. This framework allows the authors to discuss moral agency across diverse contexts—including cyberspace, biological systems, and organizational structures—without requiring consciousness or emotional capacity in moral agents.

**Summary for This Project**:
This paper is foundational for understanding how artificial agents can participate in moral frameworks without possessing human-like consciousness or intentionality. Floridi and Sanders' "Method of Abstraction" provides philosophical grounding for treating AI systems as moral agents in specific contexts, based on their observable behavior rather than internal mental states. This is directly relevant to our project on social experiments in the agentic economy, as it addresses the threshold question: under what conditions can we treat AI agents as morally responsible participants in economic transactions and social experiments? The paper's emphasis on levels of abstraction helps us understand how AI agents operating in economic contexts can be evaluated morally based on their interactions, autonomy, and adaptability within that domain, without requiring them to possess human-like moral reasoning. This opens conceptual space for procedural justification frameworks that assess AI agent behavior functionally rather than psychologically.

**Key Quotes**:
> "Our view is that AAs can be held to be moral agents (though not in the sense that they possess moral rights, but in that they are open to moral evaluation, can be judged to behave morally or immorally in a situation)." (p. 351)

**Relevance Score**: High

---

### Wallach & Allen (2009) Moral Machines: Teaching Robots Right from Wrong

**Full Citation**: Wallach, W., & Allen, C. (2009). Moral Machines: Teaching Robots Right from Wrong. Oxford University Press.

**DOI**: N/A (Book)

**Type**: Book

**Abstract**:
Wallach and Allen provide the first comprehensive examination of the challenge of building artificial moral agents (AMAs). They distinguish between different levels of moral agency: machines can be 'implicitly ethical' in that their behavior conforms to moral standards, which differs from making decisions by employing explicit moral procedures—which differs again from what full moral agents do. Full moral agency requires consciousness, understanding, and free will. The authors explore two broad architectural approaches for morally intelligent agents: the top-down imposition of ethical theories (deontological, utilitarian, virtue ethics) and the bottom-up building of systems that learn moral behavior from experience. They argue that functional equivalence of behavior is all that can possibly matter for the practical issues of designing AMAs, even if machines lack genuine moral understanding.

**Summary for This Project**:
This book establishes the foundational framework for thinking about machine ethics and artificial moral agency. Wallach and Allen's distinction between implicit ethics (behavioral conformity), explicit ethical reasoning, and full moral agency provides a taxonomy essential for evaluating AI agents in economic contexts. For our project on social experiments in the agentic economy, their analysis of top-down versus bottom-up approaches to moral machine design is directly applicable. When AI agents participate in economic transactions or represent human principals, which approach to moral decision-making should be implemented? The book's emphasis on "functional equivalence" suggests that for practical purposes, what matters is whether AI agents behave in ways consistent with moral norms, regardless of whether they "understand" those norms. This has implications for procedural justification: we can evaluate AI agent behavior based on outcomes and procedures without requiring internal moral states. The work also anticipates questions about moral learning—can AI agents improve their moral decision-making through experience in social experiments?

**Key Quotes**:
> "Functional equivalence of behavior is all that can possibly matter for the practical issues of designing AMAs [Automated Moral Agents]." (p. 26)

**Relevance Score**: High

---

### Coeckelbergh (2010) Robot Rights? Towards a Social-Relational Justification of Moral Consideration

**Full Citation**: Coeckelbergh, M. (2010). Robot rights? Towards a social-relational justification of moral consideration. Ethics and Information Technology, 12(3), 209-221.

**DOI**: 10.1007/s10676-010-9235-5

**Type**: Journal Article

**Abstract**:
Coeckelbergh addresses whether we should grant moral consideration or rights to artificially intelligent robots. He notes that most current robots do not meet the hard criteria set by deontological and utilitarian theory (sentience, consciousness, rationality). Rather than relying on intrinsic ontological properties, he sketches a novel social-relational argument: robots should receive moral consideration based on how they are treated in actual social situations and circumstances, and on the basis of their relations to us. This approach shifts focus from what robots are (their intrinsic properties) to how they appear and function in social contexts.

**Summary for This Project**:
Coeckelbergh's social-relational framework is highly relevant for understanding moral consideration in the agentic economy. Rather than asking whether AI agents possess intrinsic properties that warrant moral status, we can ask how AI agents function within social and economic relationships. This perspective is crucial for our project because it suggests that moral consideration of AI economic agents depends on their role in social practices, not just their internal architecture. When AI agents represent human principals in economic transactions, their moral status derives from these representational relationships. The social-relational view also has implications for procedural justification: the legitimacy of AI agent participation in social experiments may depend not on their intrinsic capacities but on how they are integrated into social practices and how their participation affects human relationships. This connects to questions about trust, delegation, and the conditions under which humans accept AI agents as legitimate participants in economic activities.

**Key Quotes**:
> "We grant moral consideration on the basis of how an entity is treated in actual social situations and circumstances rather than on intrinsic properties like sentience or consciousness." (p. 216)

**Relevance Score**: High

---

### Nyholm (2018) Attributing Agency to Automated Systems: Reflections on Human-Robot Collaborations and Responsibility-Loci

**Full Citation**: Nyholm, S. (2018). Attributing agency to automated systems: Reflections on human-robot collaborations and responsibility-loci. Science and Engineering Ethics, 24(4), 1201-1219.

**DOI**: 10.1007/s11948-017-9943-x

**Type**: Journal Article

**Abstract**:
Nyholm addresses the "responsibility gap" problem in AI and autonomous systems—situations where the behavior of autonomous systems is too unpredictable for any human agent to be held fully responsible. Rather than viewing this as an insurmountable problem, Nyholm argues that responsibility should be understood as a relational property of the network or system of agents and tools involved in a behavior or event. He examines what sorts of agency it makes sense to attribute to automated systems in contexts like self-driving cars and autonomous weapons, proposing that agency exercised by machines should be understood in terms of human-machine collaborations rather than isolated machine actions.

**Summary for This Project**:
Nyholm's framework for understanding responsibility as distributed across human-machine networks is directly applicable to AI agents in economic contexts. In the agentic economy, when AI agents make decisions on behalf of humans or participate in market transactions, responsibility doesn't simply transfer entirely to the AI or remain entirely with humans. Instead, Nyholm's "responsibility-loci" approach suggests we should examine the entire network of human designers, users, and AI systems. This has important implications for procedural justification in social experiments: rather than requiring clear ex ante assignment of responsibility to single agents, we can design procedures that recognize distributed responsibility across human-AI collaborations. The paper also addresses how to attribute agency to automated systems in ways that preserve accountability without requiring full human control at every decision point. This is crucial for contexts where AI agents must make rapid or complex economic decisions where human oversight is impractical.

**Key Quotes**:
> "Not much of a responsibility gap remains if we understand responsibility as a relation property of the network or system of agents and tools involved in a certain behaviour or event." (p. 1215)

**Relevance Score**: High

---

## Algorithmic Fairness and Justice

### Binns (2018) Fairness in Machine Learning: Lessons from Political Philosophy

**Full Citation**: Binns, R. (2018). Fairness in machine learning: Lessons from political philosophy. Proceedings of the 1st Conference on Fairness, Accountability and Transparency, PMLR, 81, 149-159.

**DOI**: N/A

**Type**: Conference Proceedings

**Abstract**:
Binns draws on moral and political philosophy to illuminate debates about fair machine learning. He examines competing conceptions of fairness: "Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimize the harms to the least advantaged?" The paper contends that debates over how to mathematically define discrimination and fairness in algorithms echo longstanding philosophical discussions about justice and egalitarianism. Binns explores how rawlsian, utilitarian, and libertarian theories of justice map onto different fairness metrics in machine learning, arguing that technical definitions of fairness contain implicit normative commitments that should be made explicit and debated.

**Summary for This Project**:
This paper provides essential philosophical grounding for evaluating fairness in AI agent decision-making. In the agentic economy, when AI agents allocate resources, match buyers and sellers, or make distributive decisions, different fairness criteria embody different theories of justice. Binns' analysis helps us understand that choosing among fairness metrics (equality of opportunity, demographic parity, individual fairness, etc.) is not a purely technical decision but reflects deep normative commitments. For our project on social experiments with AI agents, this means we must be explicit about which conception of justice guides AI agent design and evaluation. Should AI economic agents maximize overall welfare (utilitarian), protect the worst-off (Rawlsian), or preserve individual liberty (libertarian)? The paper's contribution is showing that debates about "algorithmic fairness" are really debates about competing theories of justice—connecting machine learning directly to centuries of moral philosophy. This is crucial for procedural justification: we must justify not just that AI agents follow procedures, but that the procedures embody defensible principles of justice.

**Key Quotes**:
> "Attempts to formalize 'fairness' in machine learning contain echoes of old philosophical debates about discrimination, egalitarianism and justice." (p. 149)

**Relevance Score**: High

---

### Mittelstadt et al. (2016) The Ethics of Algorithms: Mapping the Debate

**Full Citation**: Mittelstadt, B. D., Allo, P., Taddeo, M., Wachter, S., & Floridi, L. (2016). The ethics of algorithms: Mapping the debate. Big Data & Society, 3(2), 1-21.

**DOI**: 10.1177/2053951716679679

**Type**: Journal Article

**Abstract**:
This influential paper provides a comprehensive mapping of ethical concerns around algorithms. The authors examine how algorithms increasingly mediate operations, decisions, and social processes in information societies. They propose a conceptual framework organizing ethical issues into six types of concerns: inconclusive evidence, inscrutable evidence, misguided evidence, unfair outcomes, transformative effects, and traceability issues. The paper analyzes how algorithms turn data into evidence for decision-making and trigger actions that may not be ethically neutral, highlighting problems of opacity, bias, accountability, and the difficulty of attributing responsibility in algorithmic systems.

**Summary for This Project**:
This comprehensive mapping paper is essential for understanding the full landscape of ethical issues surrounding AI agents in economic contexts. Mittelstadt et al.'s six-fold typology helps us anticipate ethical problems that may arise in the agentic economy. For social experiments involving AI agents, we must address: (1) inconclusive evidence—are AI decisions based on adequate data?; (2) inscrutable evidence—can we understand why AI agents made particular choices?; (3) misguided evidence—are AI agents optimizing for the right objectives?; (4) unfair outcomes—do AI agent decisions create or perpetuate injustices?; (5) transformative effects—how do AI agents change the nature of economic relationships?; (6) traceability—can we track responsibility when things go wrong? The paper's emphasis on algorithms as mediators rather than neutral tools is crucial: AI economic agents don't just execute human decisions but actively shape the decision-making landscape. This has implications for procedural justification—procedures must address all six types of concerns, not just fairness or transparency in isolation.

**Key Quotes**:
> "Algorithms are not neutral instruments but actively shape social processes and decisions in ways that may not be ethically neutral." (p. 4)

**Relevance Score**: High

---

### Fazelpour, Lipton & Danks (2022) Algorithmic Fairness and the Situated Dynamics of Justice

**Full Citation**: Fazelpour, S., Lipton, Z. C., & Danks, D. (2022). Algorithmic fairness and the situated dynamics of justice. Canadian Journal of Philosophy, 52(1), 44-60.

**DOI**: 10.1017/can.2021.25

**Type**: Journal Article

**Abstract**:
The authors argue that research on algorithmic fairness has primarily focused on identification of "ideally fair" target states, but this preoccupation with target states in abstraction from the situated dynamics of deployment is misguided. They propose a framework that takes dynamic trajectories as direct objects of moral appraisal rather than static fairness properties. The paper highlights three respects in which such trajectories can be evaluated: their temporal dynamics (how fairness evolves over time), their robustness (how fairness properties change under different conditions), and their representation (what gets included or excluded from fairness evaluations). The authors argue for understanding algorithmic fairness as an ongoing process situated in specific social contexts rather than a property that can be designed in once and for all.

**Summary for This Project**:
This paper is crucial for understanding fairness in the agentic economy as a dynamic, evolving property rather than a static goal. When AI agents participate in ongoing economic relationships and social experiments, their fairness cannot be evaluated at a single time point. Fazelpour et al.'s framework directs attention to temporal dynamics: How do AI agent decisions create feedback loops that amplify or mitigate inequalities over time? This is especially relevant for experimental contexts where AI agents learn and adapt—initial fairness properties may not persist as agents interact with environments and other agents. The paper's emphasis on robustness challenges us to consider how AI agent fairness performs across different contexts and populations, not just in the training environment. For procedural justification, this means we need dynamic evaluation mechanisms that track fairness trajectories throughout experiments, not just initial design verification. The situated approach also highlights that fairness cannot be evaluated independently of social context—AI agents designed for one economic context may produce unfair outcomes when deployed elsewhere.

**Key Quotes**:
> "Normative theorizing focused on 'ideally fair' target states in abstraction from situated dynamics is misguided; we should take dynamic trajectories as direct objects of moral appraisal." (p. 45)

**Relevance Score**: High

---

### Fazelpour & Danks (2021) Algorithmic Bias: Senses, Sources, Solutions

**Full Citation**: Fazelpour, S., & Danks, D. (2021). Algorithmic bias: Senses, sources, solutions. Philosophy Compass, 16(8), e12760.

**DOI**: 10.1111/phc3.12760

**Type**: Journal Article

**Abstract**:
This paper clarifies different senses of "algorithmic bias" and examines their sources and potential solutions. The authors identify two lines of philosophical research: one that clarifies normative underpinnings of fairness measures, and another that fundamentally challenges standard debiasing approaches as insufficient or inappropriate. They explore how bias can arise from data, algorithms, or broader sociotechnical systems, and evaluate different technical and procedural interventions. The paper argues for broadening the realm of normative questions for fair machine learning beyond narrow technical fixes to include questions about power, justice, and structural inequality.

**Summary for This Project**:
This clarifying paper helps us understand multiple dimensions of bias that may affect AI agents in economic contexts. Fazelpour and Danks distinguish statistical bias from moral bias, showing that eliminating statistical bias doesn't necessarily produce morally acceptable outcomes. For our project on social experiments in the agentic economy, this distinction is crucial: AI agents might be statistically "unbiased" in their decision-making while still producing morally problematic outcomes due to biased training data, flawed objective functions, or structural inequalities in the economic environment. The paper's analysis of sources of bias—data, algorithms, and sociotechnical systems—suggests that procedural justification must address all three levels. It's insufficient to verify that AI agent algorithms are bias-free if the data they learn from or the systems they operate within embed unfair patterns. The challenge for moral learning in artificial agents is whether they can learn to recognize and correct for these multilevel biases, or whether bias-correction requires human oversight at critical decision points.

**Key Quotes**:
> "Two lines of philosophical research: one clarifies normative underpinnings of fairness measures, another fundamentally challenges standard debiasing approaches as insufficient." (p. 2)

**Relevance Score**: High

---

## Value Alignment and AI Ethics Frameworks

### Gabriel (2020) Artificial Intelligence, Values, and Alignment

**Full Citation**: Gabriel, I. (2020). Artificial intelligence, values, and alignment. Minds and Machines, 30(3), 411-437.

**DOI**: 10.1007/s11023-020-09539-2

**Type**: Journal Article

**Abstract**:
This paper examines philosophical dimensions of AI alignment through three main propositions. First, Gabriel argues that "normative and technical aspects of the AI alignment problem are interrelated," enabling cross-disciplinary collaboration. Second, he distinguishes between alignment with instructions, intentions, preferences, and values, advocating for a principle-based approach combining these elements systematically. Third, he contends that the central theoretical challenge involves identifying fair principles receiving broad reflective endorsement despite moral diversity, rather than discovering objectively "true" moral principles. The paper concludes by exploring three potential methodologies for identifying such fair alignment principles: social choice theory, moral philosophy, and machine learning from human feedback.

**Summary for This Project**:
Gabriel's framework for understanding AI alignment is essential for thinking about AI agents acting on behalf of humans in economic contexts. His distinction between different alignment targets—instructions, intentions, preferences, values—maps onto different conceptions of what it means for an AI agent to properly represent a human principal. Should AI economic agents follow explicit instructions (even when humans would deviate from their own instructions)? Align with underlying intentions (even when poorly expressed)? Maximize revealed preferences (even when preferences are irrational)? Or promote deeper values (even when this conflicts with expressed preferences)? For our project, this framework helps clarify what "procedural justification" means for AI agent behavior—justified relative to which standard? Gabriel's emphasis on identifying "fair principles that receive reflective endorsement despite moral diversity" is particularly relevant for social experiments involving multiple stakeholders with different values. The paper suggests that legitimacy in the agentic economy may require mechanisms for broad stakeholder input into AI agent design, rather than imposing a single conception of value alignment.

**Key Quotes**:
> "There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values." (p. 413)

> "The central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment that receive reflective endorsement despite widespread variation in people's moral beliefs." (p. 425)

**Relevance Score**: High

---

### Floridi & Cowls (2019) A Unified Framework of Five Principles for AI in Society

**Full Citation**: Floridi, L., & Cowls, J. (2019). A unified framework of five principles for AI in society. Harvard Data Science Review, 1(1).

**DOI**: 10.1162/99608f92.8cd550d1

**Type**: Journal Article

**Abstract**:
This paper addresses the proliferation of AI ethics principles by analyzing six high-profile international initiatives. The authors identify substantial convergence across 47 principles and propose a unified framework consisting of five core principles: four traditional bioethics principles (beneficence, non-maleficence, autonomy, justice) plus one novel principle (explicability, incorporating intelligibility and accountability). The framework emerged from analyzing documents from Asilomar, Montreal Declaration, IEEE, European Group on Ethics, UK House of Lords, and Partnership on AI. The authors argue this framework can serve as architecture for developing laws, standards, and practices governing AI across sectors and jurisdictions, while acknowledging the current Western-centric perspective requires broader cultural input.

**Summary for This Project**:
Floridi and Cowls' unified framework provides a comprehensive ethical architecture for evaluating AI agents in economic contexts. Each principle has specific applications: (1) Beneficence—do AI agents promote human well-being and environmental sustainability? (2) Non-maleficence—do AI agents protect against harms including privacy violations and security breaches? (3) Autonomy—do AI agents enhance or undermine human decision-making power? (4) Justice—do AI agents ensure equitable access and eliminate discrimination? (5) Explicability—can we understand how AI agents work and identify who is responsible for their actions? For our project on social experiments in the agentic economy, this framework offers a structured approach to procedural justification: we can evaluate whether experimental designs adequately address all five principles. The addition of "explicability" as a distinct principle is particularly important—it combines transparency (can we understand?) with accountability (who is responsible?), both essential for legitimate delegation to AI agents. The framework's grounding in bioethics also suggests useful analogies: just as medical research requires informed consent and institutional review, perhaps social experiments with AI agents require similar safeguards.

**Key Quotes**:
> "Four traditional bioethics principles—beneficence, non-maleficence, autonomy, justice—plus one novel principle: explicability, incorporating intelligibility and accountability." (p. 3)

**Relevance Score**: High

---

### Jobin, Ienca & Vayena (2019) The Global Landscape of AI Ethics Guidelines

**Full Citation**: Jobin, A., Ienca, M., & Vayena, E. (2019). Artificial intelligence: The global landscape of ethics guidelines. Nature Machine Intelligence, 1(9), 389-399.

**DOI**: 10.1038/s42256-019-0088-2

**Type**: Journal Article

**Abstract**:
The researchers conducted a comprehensive review of ethical AI frameworks issued by various organizations, analyzing 84 documents from government agencies, companies, and research institutions worldwide. Their analysis identified "a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy)." However, they discovered significant differences in how these principles are understood across different contexts and stakeholders. The study emphasizes that successful ethical AI implementation requires coordinating guideline development with robust ethical reasoning and practical execution strategies. Despite convergence on high-level principles, substantial divergence exists regarding interpretation, domain application, and implementation methods.

**Summary for This Project**:
This comprehensive global survey reveals both promise and challenges for AI ethics in economic contexts. The convergence on five core principles (transparency, justice/fairness, non-maleficence, responsibility, privacy) suggests broad international agreement on fundamental values that should guide AI development—including AI agents in the agentic economy. However, Jobin et al.'s finding of "substantive divergence" in interpretation and implementation is crucial: agreeing on principles doesn't resolve practical disagreements about AI agent design. For our project on social experiments with AI agents, this divergence poses both challenges and opportunities. Challenge: different stakeholders may invoke the same principles (e.g., "fairness") while supporting incompatible AI agent designs. Opportunity: social experiments can help us learn which interpretations of abstract principles work best in specific economic contexts. The paper's emphasis on the gap between principles and practice highlights the need for procedural mechanisms that translate high-level ethical principles into concrete design choices and evaluation criteria for AI economic agents.

**Key Quotes**:
> "A global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted, why they are deemed important, what issue, domain or actors they pertain to, and how they should be implemented." (p. 389)

**Relevance Score**: High

---

### Floridi (2021) The European Legislation on AI: A Brief Analysis of its Philosophical Approach

**Full Citation**: Floridi, L. (2021). The European legislation on AI: A brief analysis of its philosophical approach. Philosophy & Technology, 34(2), 215-222.

**DOI**: 10.1007/s13347-021-00460-9

**Type**: Journal Article

**Abstract**:
This paper analyzes the European Commission's proposal for the EU Artificial Intelligence Act (AIA) from a philosophical perspective. Floridi examines the Act's foundational approach, which focuses on regulating AI based on risk levels (unacceptable, high, limited, minimal) rather than attempting to define AI comprehensively. He discusses how the Act embodies particular philosophical commitments regarding the nature of AI, the relationship between humans and machines, and the proper role of regulation. The paper evaluates strengths and weaknesses of the EU's approach, including its emphasis on human oversight, its risk-based framework, and its potential gaps in addressing emergent AI capabilities.

**Summary for This Project**:
Floridi's analysis of the EU AI Act provides crucial context for understanding regulatory approaches to AI agents in economic contexts. The Act's risk-based framework is directly relevant: AI agents participating in economic transactions would likely qualify as "high-risk" under EU classification, triggering requirements for transparency, human oversight, and accountability. For our project on social experiments in the agentic economy, this regulatory context shapes what kinds of AI agent designs are permissible and what safeguards are required. Floridi's philosophical analysis helps us understand the values embedded in risk-based regulation: it prioritizes protecting humans from AI-generated harms while allowing innovation in lower-risk domains. The Act's emphasis on "human oversight" raises important questions for AI economic agents: what level of human control is compatible with the autonomy necessary for AI agents to function effectively in markets? The paper also highlights a philosophical tension in AI regulation between ex ante requirements (design-time safeguards) and ex post accountability (post-deployment responsibility)—both relevant for procedural justification in social experiments.

**Key Quotes**:
> "The Act embodies particular philosophical commitments regarding the nature of AI, the relationship between humans and machines, and the proper role of regulation." (p. 216)

**Relevance Score**: High

---

## Accountability, Transparency, and Explainability

### Wachter, Mittelstadt & Floridi (2017) Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation

**Full Citation**: Wachter, S., Mittelstadt, B., & Floridi, L. (2017). Why a right to explanation of automated decision-making does not exist in the General Data Protection Regulation. International Data Privacy Law, 7(2), 76-99.

**DOI**: 10.1093/idpl/ipx005

**Type**: Journal Article

**Abstract**:
This influential paper analyzes whether the EU General Data Protection Regulation (GDPR) legally mandates a "right to explanation" for automated decision-making. The authors argue that the GDPR only mandates that data subjects receive meaningful but limited information about the logic involved, as well as the significance and envisaged consequences of automated decision-making systems—what they term a "right to be informed"—rather than a true right to detailed explanation. They contend that the ambiguity and limited scope of the "right not to be subject to automated decision-making" in Article 22 raises questions about the protection actually afforded to data subjects. The paper distinguishes between ex ante generic explanations about system functionality and ex post specific explanations about individual decisions.

**Summary for This Project**:
This paper is essential for understanding legal and ethical requirements for transparency when AI agents make economic decisions affecting individuals. Wachter et al.'s analysis reveals a crucial gap: even under GDPR, individuals may not have the right to understand why an AI agent made a specific decision affecting them. For our project on social experiments in the agentic economy, this raises important questions about procedural justification: is it sufficient for AI agents to provide general information about how they work, or do affected parties need specific explanations of individual decisions? The distinction between "right to be informed" and "right to explanation" matters practically: the former requires only system-level transparency, while the latter would require decision-level intelligibility. In economic contexts where AI agents rapidly make numerous decisions (matching, pricing, allocation), providing detailed explanations for each decision may be impractical. Yet without such explanations, how can participants trust AI agent decisions or challenge unfair outcomes? The paper suggests that robust accountability may require mechanisms beyond individual explanation rights—perhaps including algorithmic auditing, third-party oversight, or appeal procedures that don't depend on full explainability.

**Key Quotes**:
> "The GDPR mandates a 'right to be informed'—meaningful but limited information about system logic—rather than a true 'right to explanation' of specific automated decisions." (p. 79)

**Relevance Score**: High

---

### De Laat (2018) Algorithmic Decision-Making Based on Machine Learning from Big Data: Can Transparency Restore Accountability?

**Full Citation**: De Laat, P. B. (2018). Algorithmic decision-making based on machine learning from big data: Can transparency restore accountability? Philosophy & Technology, 31(4), 525-541.

**DOI**: 10.1007/s13347-017-0293-z

**Type**: Journal Article

**Abstract**:
De Laat examines whether transparency can restore accountability in machine learning systems that make decisions affecting individuals. Decision-making assisted by algorithms developed through machine learning is increasingly determining lives, yet full opacity about the process is the norm. The paper examines objections to full transparency including loss of privacy, gaming the system, competitive edge concerns, and inherent opacity of sophisticated algorithms. De Laat concludes that full transparency for oversight bodies alone—rather than public transparency—is presently the only feasible option, given the technical and practical constraints on making complex machine learning systems fully interpretable.

**Summary for This Project**:
De Laat's careful analysis of transparency constraints is crucial for designing AI agents in economic contexts. The paper acknowledges a fundamental tension: transparency can promote accountability, but also creates risks (privacy violations, system gaming, loss of competitive advantage). For AI agents in the agentic economy, this tension is acute. Full transparency about AI agent decision-making algorithms might enable market manipulation or undermine competitive markets. Yet without transparency, how can we ensure accountability? De Laat's proposal—full transparency to oversight bodies rather than universal public transparency—offers a middle path. For our project on social experiments with AI agents, this suggests a tiered approach: detailed algorithm access for ethics review boards and regulators, summary explanations for participants, and perhaps targeted explanations when specific decisions are contested. The paper also highlights the "inherent opacity" problem: some machine learning systems resist interpretation even by their creators. This challenges the assumption that procedural justification requires full explainability—we may need accountability mechanisms that work despite irreducible opacity.

**Key Quotes**:
> "Full transparency for oversight bodies alone is presently the only feasible option, given technical and practical constraints on making complex machine learning systems fully interpretable to all stakeholders." (p. 536)

**Relevance Score**: High

---

## Autonomy, Manipulation, and Delegation

### Susser, Roessler & Nissenbaum (2019) Technology, Autonomy, and Manipulation

**Full Citation**: Susser, D., Roessler, B., & Nissenbaum, H. (2019). Technology, autonomy, and manipulation. Internet Policy Review, 8(2), 1-22.

**DOI**: 10.14763/2019.2.1410

**Type**: Journal Article

**Abstract**:
Susser and colleagues define manipulative algorithmic practices as "applications of information technology that impose hidden influences on users, by targeting and exploiting decision-making vulnerabilities." They argue that online manipulation is the use of information technology to covertly influence another person's decision-making by targeting and exploiting their decision-making vulnerabilities. The paper characterizes manipulation along two dimensions: it is hidden (influence is not transparent to the target) and it exploits cognitive vulnerabilities in decision-making processes. The authors distinguish manipulation from other forms of influence like persuasion or coercion, emphasizing that manipulation's deeper harm is its challenge to individual autonomy rather than merely economic or preference-based harms.

**Summary for This Project**:
This paper is essential for understanding autonomy risks when AI agents interact with humans in economic contexts. Susser et al.'s definition of manipulation—hidden influence exploiting decision-making vulnerabilities—applies directly to AI economic agents that personalize offers, recommendations, or choice architectures. In the agentic economy, AI agents might manipulate humans through "hypernudging": dynamic, personalized, opaque choice architecture modifications based on big data about individual vulnerabilities. For our project on social experiments with AI agents, this raises critical questions: When does AI agent persuasion become manipulation? If AI agents learn to exploit human cognitive biases to achieve better outcomes (as measured by some metric), is this acceptable or autonomy-undermining? The paper's emphasis on hidden influence suggests that transparency and disclosure may be partial solutions—making AI agent influence visible could convert manipulation into legitimate persuasion. However, the exploitation of vulnerabilities remains problematic even when disclosed. This implies that procedural justification for AI agent design must include constraints on exploiting human decision-making weaknesses, not just transparency requirements.

**Key Quotes**:
> "Manipulative algorithmic practices are applications of information technology that impose hidden influences on users, by targeting and exploiting decision-making vulnerabilities." (p. 2)

> "Manipulation's deeper, more insidious harm is its challenge to individual autonomy." (p. 12)

**Relevance Score**: High

---

## Robot Rights and Moral Status

### Gunkel (2018) Robot Rights

**Full Citation**: Gunkel, D. J. (2018). Robot Rights. MIT Press.

**DOI**: N/A (Book)

**Type**: Book

**Abstract**:
Gunkel's book examines whether and to what extent robots and other technological artifacts can and should have any claim to moral and legal standing. The work addresses "Can and should robots have rights?" emphasizing the philosophical difference between the two modal verbs organizing the inquiry. Gunkel explores the shift from viewing robots as mere instruments to considering them as potential bearers of rights and moral consideration. Drawing on philosophy, law, and robotics, the book challenges traditional criteria for rights-bearing (sentience, consciousness, life) and explores alternative frameworks including social-relational approaches. Gunkel argues that the question of robot rights forces us to reconsider fundamental assumptions about moral status, legal personhood, and the boundaries of moral community.

**Summary for This Project**:
Gunkel's comprehensive treatment of robot rights provides philosophical grounding for questions about AI agents' legal and moral status in economic contexts. While our project doesn't necessarily require treating AI agents as rights-bearers, Gunkel's analysis clarifies what's at stake in different framings. If AI economic agents are merely tools, humans bear full responsibility for their actions. If AI agents have some moral status, this changes responsibility attribution and may constrain permissible uses. Gunkel's distinction between "can" and "should" is crucial: even if we could technically grant AI agents legal standing (as some jurisdictions have for corporations), should we? For the agentic economy, this matters for questions of liability, contract law, and property rights. Can AI agents enter binding agreements? Own assets? Be sued? Gunkel's social-relational framework offers an alternative to intrinsic-properties approaches: perhaps AI agents' moral status depends not on what they are internally but on their role in social practices. This connects to our procedural justification framework—the legitimacy of AI agent participation in economic activities may depend on how they're socially situated rather than their intrinsic capabilities.

**Key Quotes**:
> "The question of robot rights forces us to reconsider fundamental assumptions about moral status, legal personhood, and the boundaries of moral community." (p. 7)

**Relevance Score**: Medium

---

## Multi-Agent Systems and Emerging Challenges

### Vallor & Wallach (2020) Moral Machines: From Value Alignment to Embodied Virtue

**Full Citation**: Vallor, S., & Wallach, W. (2020). Moral machines: From value alignment to embodied virtue. In S. M. Liao (Ed.), Ethics of Artificial Intelligence (pp. 383-412). Oxford University Press.

**DOI**: N/A

**Type**: Book Chapter

**Abstract**:
Vallor and Wallach address how implementing sensitivity to norms, laws, and human values in computational systems has transitioned from philosophical reflection to an actual engineering challenge. The authors critique the "value alignment" approach to superintelligent AIs, which tends to employ computationally friendly concepts such as utility functions, system goals, agent preferences, and value optimizers—concepts that lack intrinsic ethical significance. They argue that human-level AI and superintelligent systems can be assured to be safe and beneficial only if they embody something like virtue or moral character rather than mere value alignment. The chapter considers what may be lost in excising intrinsically ethical concepts from the engineering of moral machines and proposes virtue embodiment as a more appropriate long-term goal.

**Summary for This Project**:
Vallor and Wallach's critique of value alignment provides an important alternative framework for AI agent ethics. Rather than programming AI agents to maximize pre-specified values (utilitarian approach), the authors propose cultivating AI systems that embody virtues: dispositions toward morally appropriate action across varying contexts. For AI agents in economic contexts, this suggests a different design paradigm: instead of optimizing for fixed objectives (maximize profit, minimize costs), design AI agents with virtuous dispositions (fairness, honesty, prudence, justice). This approach may be especially relevant for social experiments where contexts vary unpredictably—virtuous AI agents could adapt appropriately to novel situations rather than rigidly optimizing fixed objective functions. The emphasis on "moral character" also connects to trust and procedural legitimacy: humans may find it easier to trust AI agents exhibiting recognizable virtues than AI agents optimizing inscrutable utility functions. However, the chapter also acknowledges challenges: can virtues really be "embodied" in artificial systems lacking human experiences and emotions? Is virtue ethics, developed for human moral psychology, applicable to artificial agents?

**Key Quotes**:
> "Human-level AI and superintelligent systems can be assured to be safe and beneficial only if they embody something like virtue or moral character rather than mere value alignment." (p. 385)

**Relevance Score**: Medium

---

## Summary

**Total Papers**: 18

- High relevance: 15
- Medium relevance: 3

**Key Positions Covered**:

- **Moral Agency Frameworks**: 4 papers (Floridi & Sanders, Wallach & Allen, Coeckelbergh, Nyholm)
- **Algorithmic Fairness & Justice**: 4 papers (Binns, Mittelstadt et al., Fazelpour et al. 2022, Fazelpour & Danks 2021)
- **Value Alignment & Ethics Frameworks**: 4 papers (Gabriel, Floridi & Cowls, Jobin et al., Floridi 2021)
- **Transparency & Accountability**: 2 papers (Wachter et al., De Laat)
- **Autonomy & Manipulation**: 1 paper (Susser et al.)
- **Moral Status**: 1 paper (Gunkel)
- **Virtue Ethics for AI**: 1 paper (Vallor & Wallach)

**Notable Gaps**:

1. **Economic-specific AI ethics**: While this literature addresses AI agents generally, relatively few papers focus specifically on AI agents in economic contexts (markets, contracts, property rights, economic experiments). Most work addresses AI in social contexts broadly or in specific domains like healthcare, criminal justice, or autonomous vehicles.

2. **Procedural legitimacy**: Limited philosophical work on what makes AI agent participation in social processes procedurally legitimate, distinct from substantive fairness questions. Our project's focus on procedural justification addresses this gap.

3. **Multi-agent coordination ethics**: While there's growing technical work on multi-agent AI systems, philosophical analysis of ethical issues in AI-to-AI interactions remains underdeveloped. How should AI agents negotiate with each other? What norms should govern AI agent coalitions?

4. **AI agents as fiduciaries**: Legal and philosophical work on fiduciary duties traditionally applies to human agents representing human principals. How do fiduciary concepts (loyalty, care, good faith) apply when agents are artificial?

5. **Moral learning dynamics**: While machine learning is technically well-studied, philosophical analysis of how AI agents should learn moral norms through experience—and what can go wrong—remains nascent.

**Recommendation**:

For the synthesis phase, focus heavily on papers addressing moral agency frameworks (Floridi & Sanders, Wallach & Allen, Nyholm), algorithmic fairness (Binns, Fazelpour et al.), and value alignment (Gabriel, Floridi & Cowls). These provide the strongest philosophical foundations for thinking about AI agents in economic contexts.

The debate between static fairness criteria (Binns) and dynamic fairness trajectories (Fazelpour et al. 2022) is particularly important for evaluating AI agents in ongoing economic relationships. Similarly, the tension between transparency for accountability (De Laat, Wachter et al.) and protecting autonomy from manipulation (Susser et al.) requires careful attention.

Coeckelbergh's social-relational framework and Nyholm's distributed responsibility approach offer promising alternatives to individualistic frameworks that may fit AI-human economic collaborations better than traditional agency models.

The convergence on core principles (Jobin et al., Floridi & Cowls) combined with divergence on interpretation suggests that social experiments with AI agents can play an important epistemic role: helping us learn which interpretations of abstract principles work best in practice.
