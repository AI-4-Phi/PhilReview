# PhilReview

PhilReview is a multi-agent system for generating focused, insight-driven literature reviews with verified bibliographies. Designed for philosophy research proposals where analytical depth, citation integrity, and reference-manager-ready outputs matter.

> **Note:** PhilReview uses large language models to synthesize literature. LLMs can produce errors, misattributions, and distortions of source material. Always verify claims and citations against the original sources before relying on them in your work.

## Highlights

- **6-phase workflow**: Environment → Plan → Research → Outline → Write → Assemble
- **Parallel execution**: Domain researchers and section writers run concurrently
- **Structured API searches**: Semantic Scholar, OpenAlex, CORE, arXiv, SEP, IEP, PhilPapers, CrossRef
- **Citation integrity**: All references sourced from structured APIs (not generated by the LLM), validated via CrossRef, and checked by automated hooks for syntax, required fields, and duplicate keys
- **Content enrichment**: Abstracts resolved from Semantic Scholar, OpenAlex, and CORE; book summaries extracted from Notre Dame Philosophical Reviews; encyclopedia context (SEP, IEP) added for key papers; hallucinated metadata fields automatically stripped
- **Bibliography deduplication**: Entries merged across domains by citation key and DOI, preserving the highest-importance and most-complete version
- **BibTeX-first**: Valid `.bib` files for reference managers, pandoc, or direct use
- **Resumable**: Progress tracked in `task-progress.md`

## How It Works

The `/literature-review` skill orchestrates 6 phases, invoking specialized subagents via the Task tool:

| Phase | Subagent | Output |
|-------|----------|--------|
| 1. Environment | (skill) | Verify setup, choose mode |
| 2. Plan | `literature-review-planner` | `lit-review-plan.md` |
| 3. Research | `domain-literature-researcher` ×N (parallel) | `literature-domain-*.bib` |
| 4. Outline | `synthesis-planner` | `synthesis-outline.md` |
| 5. Write | `synthesis-writer` ×N (parallel) | `synthesis-section-*.md` |
| 6. Assemble | (skill) | `literature-review-final.md`, `literature-all.bib` |

## Cost

A typical review costs roughly **USD 10** in API usage. Start Claude Code with **Sonnet** selected as the model for best cost-efficiency.

## Quick Start

Requires [Claude Code](https://claude.ai/code). See [GETTING_STARTED.md](GETTING_STARTED.md) for setup.

Once set up, provide your research idea:

```
I need a literature review for [topic].

[Your research idea in 2-5 paragraphs]
```

## Output Structure

```
reviews/[topic]/
├── literature-review-final.md      # Complete review
├── literature-all.bib              # Aggregated bibliography
└── intermediate_files/
    ├── json/                       # API response files (archived)
    ├── lit-review-plan.md          # Domain decomposition
    ├── literature-domain-*.bib     # Per-domain BibTeX files
    ├── synthesis-outline.md        # Review structure
    ├── synthesis-section-*.md      # Individual sections
    └── task-progress.md            # Progress tracker
```

## Quality Standards

- No fabricated citations—only papers found via structured APIs
- Insight over coverage—key debates, recent work, concrete gaps
- Chicago author-date citations
- Balanced presentation of positions

## Development

For agent architecture: `.claude/docs/ARCHITECTURE.md`

For Claude instructions: `CLAUDE.md`

## Contact

- **Johannes Himmelreich** — Syracuse University — [jrhimmel@syr.edu](mailto:jrhimmel@syr.edu)
- **Marco Meyer** — University of Hamburg — [marco.meyer@uni-hamburg.de](mailto:marco.meyer@uni-hamburg.de)

---

Inspired by [LiRA](https://arxiv.org/abs/2510.05138) multi-agent patterns.
