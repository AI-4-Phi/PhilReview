{
  "status": "success",
  "source": "openalex",
  "query": "group fairness individual fairness",
  "results": [
    {
      "openalex_id": "W2905029197",
      "doi": "10.1109/icassp.2019.8682620",
      "title": "Bias Mitigation Post-processing for Individual and Group Fairness",
      "authors": [
        {
          "name": "Pranay Lohia",
          "openalex_id": "A5045547703",
          "institutions": [
            "IBM Research - India"
          ]
        },
        {
          "name": "Karthikeyan Natesan Ramamurthy",
          "openalex_id": "A5081874896",
          "orcid": "https://orcid.org/0000-0002-6021-5930",
          "institutions": [
            "IBM (United States)"
          ]
        },
        {
          "name": "Manish Bhide",
          "openalex_id": "A5068967374"
        },
        {
          "name": "Diptikalyan Saha",
          "openalex_id": "A5010368277",
          "orcid": "https://orcid.org/0000-0002-1583-5479",
          "institutions": [
            "IBM Research - India"
          ]
        },
        {
          "name": "Kush R. Varshney",
          "openalex_id": "A5015286159",
          "orcid": "https://orcid.org/0000-0002-7376-5536",
          "institutions": [
            "IBM (United States)"
          ]
        },
        {
          "name": "Ruchir Puri",
          "openalex_id": "A5045722906",
          "orcid": "https://orcid.org/0009-0006-8803-7079",
          "institutions": [
            "IBM (United States)"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-04-17",
      "abstract": "Whereas previous post-processing approaches for increasing the fairness of predictions of biased classifiers address only group fairness, we propose a method for increasing both individual and group fairness. Our novel framework includes an individual bias detector used to prioritize data samples in a bias mitigation algorithm aiming to improve the group fairness measure of disparate impact. We show superior performance to previous work in the combination of classification accuracy, individual fairness and group fairness on several real-world datasets in applications such as credit, employment, and criminal justice.",
      "cited_by_count": 111,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 38,
      "url": "https://openalex.org/W2905029197"
    },
    {
      "openalex_id": "W4307362130",
      "doi": "10.1287/ijds.2022.0022",
      "title": "GIFAIR-FL: A Framework for Group and Individual Fairness in Federated Learning",
      "authors": [
        {
          "name": "Xubo Yue",
          "openalex_id": "A5001018616",
          "orcid": "https://orcid.org/0000-0001-9929-8895",
          "institutions": [
            "University of Michigan\u2013Ann Arbor"
          ]
        },
        {
          "name": "Maher Nouiehed",
          "openalex_id": "A5088345581",
          "orcid": "https://orcid.org/0000-0001-8089-7011",
          "institutions": [
            "American University of Beirut"
          ]
        },
        {
          "name": "Raed Al Kontar",
          "openalex_id": "A5075324117",
          "orcid": "https://orcid.org/0000-0002-4546-324X",
          "institutions": [
            "University of Michigan\u2013Ann Arbor"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-10-27",
      "abstract": "In this paper, we propose GIFAIR-FL, a framework that imposes group and individual fairness (GIFAIR) to federated learning (FL) settings. By adding a regularization term, our algorithm penalizes the spread in the loss of client groups to drive the optimizer to fair solutions. Our framework GIFAIR-FL can accommodate both global and personalized settings. Theoretically, we show convergence in nonconvex and strongly convex settings. Our convergence guarantees hold for both independent and identically distributed (i.i.d.) and non-i.i.d. data. To demonstrate the empirical performance of our algorithm, we apply our method to image classification and text prediction tasks. Compared with existing algorithms, our method shows improved fairness results while retaining superior or similar prediction accuracy. History: Kwok-Leung Tsui served as the senior editor for this article. Funding: This work was supported by NSF CAREER [Grant 2144147]. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/2590027/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.0022 ).",
      "cited_by_count": 31,
      "type": "article",
      "source": {
        "name": "INFORMS Journal on Data Science",
        "type": "journal",
        "issn": [
          "2694-4022",
          "2694-4030"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Privacy-Preserving Technologies in Data",
        "Stochastic Gradient Optimization Techniques",
        "Mobile Crowdsensing and Crowdsourcing"
      ],
      "referenced_works_count": 11,
      "url": "https://openalex.org/W4307362130"
    },
    {
      "openalex_id": "W4290878493",
      "doi": "10.1145/3534678.3539346",
      "title": "GUIDE: Group Equality Informed Individual Fairness in Graph Neural Networks",
      "authors": [
        {
          "name": "Weihao Song",
          "openalex_id": "A5077434718",
          "orcid": "https://orcid.org/0000-0003-3604-3224",
          "institutions": [
            "University of Virginia"
          ]
        },
        {
          "name": "Yushun Dong",
          "openalex_id": "A5047581320",
          "orcid": "https://orcid.org/0000-0001-7504-6159",
          "institutions": [
            "University of Virginia"
          ]
        },
        {
          "name": "Ninghao Liu",
          "openalex_id": "A5007489034",
          "orcid": "https://orcid.org/0000-0002-9170-2424",
          "institutions": [
            "University of Georgia"
          ]
        },
        {
          "name": "Jundong Li",
          "openalex_id": "A5029588473",
          "orcid": "https://orcid.org/0000-0002-1878-817X",
          "institutions": [
            "University of Virginia"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-08-12",
      "abstract": "Graph Neural Networks (GNNs) are playing increasingly important roles in critical decision-making scenarios due to their exceptional performance and end-to-end design. However, concerns have been raised that GNNs could make biased decisions against underprivileged groups or individuals. To remedy this issue, researchers have proposed various fairness notions including individual fairness that gives similar predictions to similar individuals. However, existing methods in individual fairness rely on Lipschitz condition: they only optimize overall individual fairness and disregard equality of individual fairness between groups. This leads to drastically different levels of individual fairness among groups. We tackle this problem by proposing a novel GNN framework GUIDE to achieve group equality informed individual fairness in GNNs. We aim to not only achieve individual fairness but also equalize the levels of individual fairness among groups. Specifically, our framework operates on the similarity matrix of individuals to learn personalized attention to achieve individual fairness without group level disparity. Comprehensive experiments on real-world datasets demonstrate that GUIDE obtains good balance of group equality informed individual fairness and model utility. The open-source implementation of GUIDE can be found here: https://github.com/mikesong724/GUIDE.",
      "cited_by_count": 29,
      "type": "article",
      "source": {
        "name": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Advanced Graph Neural Networks",
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 21,
      "url": "https://openalex.org/W4290878493"
    },
    {
      "openalex_id": "W2996424229",
      "doi": "10.1145/3351095.3372864",
      "title": "On the apparent conflict between individual and group fairness",
      "authors": [
        {
          "name": "Reuben Binns",
          "openalex_id": "A5042396542",
          "orcid": "https://orcid.org/0000-0001-8717-6559",
          "institutions": [
            "University of Oxford"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-22",
      "abstract": "A distinction has been drawn in fair machine learning research between `group' and `individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on theoretical discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artifact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of `unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.",
      "cited_by_count": 21,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/1912.06883"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Free Will and Agency",
        "Neuroethics, Human Enhancement, Biomedical Innovations"
      ],
      "referenced_works_count": 46,
      "url": "https://openalex.org/W2996424229"
    },
    {
      "openalex_id": "W4389284730",
      "doi": "10.1016/j.omega.2023.103015",
      "title": "Group efficiency and individual fairness tradeoff in making wise decisions",
      "authors": [
        {
          "name": "Ming Tang",
          "openalex_id": "A5066098207",
          "orcid": "https://orcid.org/0000-0002-4338-699X",
          "institutions": [
            "Dalian University of Technology"
          ]
        },
        {
          "name": "Huchang Liao",
          "openalex_id": "A5038996322",
          "orcid": "https://orcid.org/0000-0001-8278-3384",
          "institutions": [
            "Sichuan University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-12-03",
      "abstract": null,
      "cited_by_count": 23,
      "type": "article",
      "source": {
        "name": "Omega",
        "type": "journal",
        "issn": [
          "0305-0483",
          "1873-5274"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Opinion Dynamics and Social Influence",
        "Complex Network Analysis Techniques",
        "Mobile Crowdsensing and Crowdsourcing"
      ],
      "referenced_works_count": 82,
      "url": "https://openalex.org/W4389284730"
    },
    {
      "openalex_id": "W4322825128",
      "doi": "10.1145/3544548.3581227",
      "title": "Fairness Evaluation in Text Classification: Machine Learning Practitioner Perspectives of Individual and Group Fairness",
      "authors": [
        {
          "name": "Zahra Ashktorab",
          "openalex_id": "A5003440919",
          "orcid": "https://orcid.org/0000-0002-0686-7911",
          "institutions": [
            "IBM Research - Austin"
          ]
        },
        {
          "name": "Benjamin Hoover",
          "openalex_id": "A5069794837",
          "orcid": "https://orcid.org/0000-0001-5218-3185"
        },
        {
          "name": "Mayank Agarwal",
          "openalex_id": "A5103186038",
          "orcid": "https://orcid.org/0000-0002-0641-2458"
        },
        {
          "name": "Casey Dugan",
          "openalex_id": "A5082995130",
          "orcid": "https://orcid.org/0000-0002-1508-2091"
        },
        {
          "name": "Werner Geyer",
          "openalex_id": "A5022799238",
          "orcid": "https://orcid.org/0000-0003-4699-5026"
        },
        {
          "name": "Hao Yang",
          "openalex_id": "A5101611720",
          "orcid": "https://orcid.org/0000-0002-4332-3905",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Mikhail Yurochkin",
          "openalex_id": "A5026283694",
          "orcid": "https://orcid.org/0000-0003-0153-6811"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-04-19",
      "abstract": "Mitigating algorithmic bias is a critical task in the development and\\ndeployment of machine learning models. While several toolkits exist to aid\\nmachine learning practitioners in addressing fairness issues, little is known\\nabout the strategies practitioners employ to evaluate model fairness and what\\nfactors influence their assessment, particularly in the context of text\\nclassification. Two common approaches of evaluating the fairness of a model are\\ngroup fairness and individual fairness. We run a study with Machine Learning\\npractitioners (n=24) to understand the strategies used to evaluate models.\\nMetrics presented to practitioners (group vs. individual fairness) impact which\\nmodels they consider fair. Participants focused on risks associated with\\nunderpredicting/overpredicting and model sensitivity relative to identity token\\nmanipulations. We discover fairness assessment strategies involving personal\\nexperiences or how users form groups of identity tokens to test model fairness.\\nWe provide recommendations for interactive tools for evaluating fairness in\\ntext classification.\\n",
      "cited_by_count": 12,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3544548.3581227"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Psychology of Moral and Emotional Judgment",
        "Qualitative Comparative Analysis Research"
      ],
      "referenced_works_count": 78,
      "url": "https://openalex.org/W4322825128"
    },
    {
      "openalex_id": "W3187795038",
      "doi": null,
      "title": "GIFAIR-FL: An Approach for Group and Individual Fairness in Federated Learning.",
      "authors": [
        {
          "name": "Xubo Yue",
          "openalex_id": "A5001018616",
          "orcid": "https://orcid.org/0000-0001-9929-8895"
        },
        {
          "name": "Maher Nouiehed",
          "openalex_id": "A5088345581",
          "orcid": "https://orcid.org/0000-0001-8089-7011"
        },
        {
          "name": "Raed Al Kontar",
          "openalex_id": "A5075324117",
          "orcid": "https://orcid.org/0000-0002-4546-324X"
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-08-05",
      "abstract": "In this paper we propose \\texttt{GIFAIR-FL}: an approach that imposes group and individual fairness to federated learning settings. By adding a regularization term, our algorithm penalizes the spread in the loss of client groups to drive the optimizer to fair solutions. Theoretically, we show convergence in non-convex and strongly convex settings. Our convergence guarantees hold for both $i.i.d.$ and non-$i.i.d.$ data. To demonstrate the empirical performance of our algorithm, we apply our method on image classification and text prediction tasks. Compared to existing algorithms, our method shows improved fairness results while retaining superior or similar prediction accuracy.",
      "cited_by_count": 12,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2108.02741.pdf"
      },
      "topics": [
        "Privacy-Preserving Technologies in Data",
        "Stochastic Gradient Optimization Techniques",
        "Mobile Crowdsensing and Crowdsourcing"
      ],
      "referenced_works_count": 43,
      "url": "https://openalex.org/W3187795038"
    },
    {
      "openalex_id": "W3048186973",
      "doi": "10.5334/dsj-2020-043",
      "title": "The CARE Principles for Indigenous Data Governance",
      "authors": [
        {
          "name": "Stephanie Russo Carroll",
          "openalex_id": "A5103001937",
          "orcid": "https://orcid.org/0000-0002-8996-8071",
          "institutions": [
            "University of Arizona"
          ]
        },
        {
          "name": "Ibrahim Garba",
          "openalex_id": "A5053534131",
          "orcid": "https://orcid.org/0000-0001-9127-2842",
          "institutions": [
            "University of Arizona"
          ]
        },
        {
          "name": "\u00d3scar Luis Figueroa Rodr\u00edguez",
          "openalex_id": "A5109812884",
          "institutions": [
            "Colegio de Postgraduados"
          ]
        },
        {
          "name": "Jarita Holbrook",
          "openalex_id": "A5087050214",
          "orcid": "https://orcid.org/0000-0003-4920-6686",
          "institutions": [
            "University of the Western Cape"
          ]
        },
        {
          "name": "Raymond Lovett",
          "openalex_id": "A5018660491",
          "orcid": "https://orcid.org/0000-0002-8905-2135",
          "institutions": [
            "Australian National University"
          ]
        },
        {
          "name": "S. A. Materechera",
          "openalex_id": "A5044298567",
          "orcid": "https://orcid.org/0000-0001-7440-6765",
          "institutions": [
            "North-West University"
          ]
        },
        {
          "name": "M. A. Parsons",
          "openalex_id": "A5076654717",
          "orcid": "https://orcid.org/0000-0002-7723-0950",
          "institutions": [
            "Rensselaer Polytechnic Institute"
          ]
        },
        {
          "name": "Kay Raseroka",
          "openalex_id": "A5045672538"
        },
        {
          "name": "Desi Rodriguez-Lonebear",
          "openalex_id": "A5005942021",
          "institutions": [
            "University of California, Los Angeles"
          ]
        },
        {
          "name": "Robyn Rowe",
          "openalex_id": "A5062220699",
          "orcid": "https://orcid.org/0000-0003-0591-6213",
          "institutions": [
            "Laurentian University"
          ]
        },
        {
          "name": "Rodrigo Sara",
          "openalex_id": "A5009371601",
          "orcid": "https://orcid.org/0000-0002-6341-6981"
        },
        {
          "name": "Jennifer Walker",
          "openalex_id": "A5066813537",
          "orcid": "https://orcid.org/0000-0003-0486-6568",
          "institutions": [
            "Laurentian University"
          ]
        },
        {
          "name": "Jane Anderson",
          "openalex_id": "A5081892017",
          "orcid": "https://orcid.org/0000-0002-3304-0477",
          "institutions": [
            "New York University"
          ]
        },
        {
          "name": "M\u0101ui Hudson",
          "openalex_id": "A5018449054",
          "orcid": "https://orcid.org/0000-0003-3880-4015",
          "institutions": [
            "University of Waikato"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-01",
      "abstract": "Concerns about secondary use of data and limited opportunities for benefit-sharing have focused attention on the tension that Indigenous communities feel between (1) protecting Indigenous rights and interests in Indigenous data (including traditional knowledges) and (2) supporting open data, machine learning, broad data sharing, and big data initiatives. The International Indigenous Data Sovereignty Interest Group (within the Research Data Alliance) is a network of nation-state based Indigenous data sovereignty networks and individuals that developed the \u2018CARE Principles for Indigenous Data Governance\u2019 (Collective Benefit, Authority to Control, Responsibility, and Ethics) in consultation with Indigenous Peoples, scholars, non-profit organizations, and governments. The CARE Principles are people- and purpose-oriented, reflecting the crucial role of data in advancing innovation, governance, and self-determination among Indigenous Peoples. The Principles complement the existing data-centric approach represented in the \u2018FAIR Guiding Principles for scientific data management and stewardship\u2019 (Findable, Accessible, Interoperable, Reusable). The CARE Principles build upon earlier work by the Te Mana Raraunga Maori Data Sovereignty Network, US Indigenous Data Sovereignty Network, Maiam nayri Wingara Aboriginal and Torres Strait Islander Data Sovereignty Collective, and numerous Indigenous Peoples, nations, and communities. The goal is that stewards and other users of Indigenous data will \u2018Be FAIR and CARE.' In this first formal publication of the CARE Principles, we articulate their rationale, describe their relation to the FAIR Principles, and present examples of their application.",
      "cited_by_count": 1010,
      "type": "article",
      "source": {
        "name": "Data Science Journal",
        "type": "journal",
        "issn": [
          "1683-1470"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "http://datascience.codata.org/articles/10.5334/dsj-2020-043/galley/1010/download/"
      },
      "topics": [
        "Research Data Management Practices"
      ],
      "referenced_works_count": 19,
      "url": "https://openalex.org/W3048186973"
    },
    {
      "openalex_id": "W3206437907",
      "doi": "10.1080/14719037.2021.1988270",
      "title": "The differing effects of individual- and group-based pay for performance on employee satisfaction: the role of the perceived fairness of performance evaluations",
      "authors": [
        {
          "name": "Kwang Bin Bae",
          "openalex_id": "A5027331476",
          "orcid": "https://orcid.org/0000-0001-6723-7924",
          "institutions": [
            "Dongguk University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-10-06",
      "abstract": "This study examines the varied effects of individual-based pay for performance and group-based pay for performance on three measures of employee satisfaction: pay satisfaction, organizational satisfaction, and job satisfaction in the public sector. Examining a twelve-year panel data set, this study finds that individual-based pay for performance has a significantly positive relationship with organizational and job satisfaction, but group-based pay for performance has a significantly negative relationship with pay satisfaction. Meanwhile, the results show that the perceived fairness of performance evaluations has significantly positive relationships with pay satisfaction, organizational satisfaction, and job satisfaction.",
      "cited_by_count": 18,
      "type": "article",
      "source": {
        "name": "Public Management Review",
        "type": "journal",
        "issn": [
          "1471-9037",
          "1471-9045"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Customer Service Quality and Loyalty",
        "Job Satisfaction and Organizational Behavior",
        "Public Policy and Administration Research"
      ],
      "referenced_works_count": 72,
      "url": "https://openalex.org/W3206437907"
    },
    {
      "openalex_id": "W4403488220",
      "doi": "10.3233/faia240578",
      "title": "Group Fairness with Individual and Censorship Constraints",
      "authors": [
        {
          "name": "Zichong Wang",
          "openalex_id": "A5011724269",
          "orcid": "https://orcid.org/0000-0001-6091-6609",
          "institutions": [
            "Florida International University"
          ]
        },
        {
          "name": "Wenbin Zhang",
          "openalex_id": "A5100710816",
          "orcid": "https://orcid.org/0000-0003-4746-9570",
          "institutions": [
            "Florida International University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-10-16",
      "abstract": "The widespread use of Artificial Intelligence (AI) based decision-making systems has raised a lot of concerns regarding potential discrimination, particularly in domains with high societal impact. Most existing fairness research focused on tackling bias relies heavily on the presence of class labels, an assumption that often mismatches real-world scenarios, which ignores the ubiquity of censored data. Further, existing works regard group fairness and individual fairness as two disparate goals, overlooking their inherent interconnection, i.e., addressing one can degrade the other. This paper proposes a novel unified method that aims to mitigate group unfairness under censorship while curbing the amplification of individual unfairness when enforcing group fairness constraints. Specifically, our introduced ranking algorithm optimizes individual fairness within the bounds of group fairness, uniquely accounting for censored information. Evaluation across four benchmark tasks confirms the effectiveness of our method in quantifying and mitigating both fairness dimensions in the face of censored data.",
      "cited_by_count": 6,
      "type": "book-chapter",
      "source": {
        "name": "Frontiers in artificial intelligence and applications",
        "type": "journal",
        "issn": [
          "0922-6389",
          "1879-8314"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA240578"
      },
      "topics": [
        "Experimental Behavioral Economics Studies",
        "Game Theory and Applications",
        "Auction Theory and Applications"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4403488220"
    },
    {
      "openalex_id": "W4403487714",
      "doi": "10.3233/faia240679",
      "title": "Individual Fairness with Group Constraints in Graph Neural Networks",
      "authors": [
        {
          "name": "Zichong Wang",
          "openalex_id": "A5011724269",
          "orcid": "https://orcid.org/0000-0001-6091-6609",
          "institutions": [
            "Florida International University"
          ]
        },
        {
          "name": "David Ulloa",
          "openalex_id": "A5114301954",
          "institutions": [
            "Florida International University"
          ]
        },
        {
          "name": "Tongjia Yu",
          "openalex_id": "A5086478069",
          "orcid": "https://orcid.org/0009-0003-4667-8132",
          "institutions": [
            "Goldman Sachs (United States)"
          ]
        },
        {
          "name": "Raju Rangaswami",
          "openalex_id": "A5052356609",
          "orcid": "https://orcid.org/0009-0000-5243-9451",
          "institutions": [
            "Florida International University"
          ]
        },
        {
          "name": "Roland H. C. Yap",
          "openalex_id": "A5028568420",
          "orcid": "https://orcid.org/0000-0002-1188-7474",
          "institutions": [
            "National University of Singapore"
          ]
        },
        {
          "name": "Wenbin Zhang",
          "openalex_id": "A5028376037",
          "orcid": "https://orcid.org/0000-0001-8219-0781",
          "institutions": [
            "Florida International University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-10-16",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable capabilities across various domains. Despite the successes of GNN deployment, their utilization often reflects societal biases, which critically hinder their adoption in high-stake decision-making scenarios such as online clinical diagnosis, financial crediting, etc. Numerous efforts have been made to develop fair GNNs but they typically concentrate on either individual or group fairness, overlooking the intricate interplay between the two, resulting in the enhancement of one, usually at the cost of the other. In addition, existing individual fairness approaches using a ranking perspective fail to identify discrimination in the ranking. This paper introduces two innovative notions dealing with individual graph fairness and group-aware individual graph fairness, aiming to more accurately measure individual and group biases. Our Group Equality Individual Fairness (GEIF) framework is designed to achieve individual fairness while equalizing the level of individual fairness among subgroups. Preliminary experiments on several real-world graph datasets demonstrate that GEIF outperforms state-of-the-art methods by a significant margin in terms of individual fairness, group fairness, and utility performance.",
      "cited_by_count": 7,
      "type": "book-chapter",
      "source": {
        "name": "Frontiers in artificial intelligence and applications",
        "type": "journal",
        "issn": [
          "0922-6389",
          "1879-8314"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://doi.org/10.3233/faia240679"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4403487714"
    },
    {
      "openalex_id": "W4382239417",
      "doi": "10.1609/aaai.v37i5.25698",
      "title": "Rawlsian Fairness in Online Bipartite Matching: Two-Sided, Group, and Individual",
      "authors": [
        {
          "name": "Seyed A. Esmaeili",
          "openalex_id": "A5064453417",
          "orcid": "https://orcid.org/0000-0002-1195-6599",
          "institutions": [
            "University of Maryland, College Park"
          ]
        },
        {
          "name": "Sharmila Duppala",
          "openalex_id": "A5009952954",
          "orcid": "https://orcid.org/0000-0001-6809-8858",
          "institutions": [
            "University of Maryland, College Park"
          ]
        },
        {
          "name": "D.K. Cheng",
          "openalex_id": "A5112946991",
          "institutions": [
            "Colorado College"
          ]
        },
        {
          "name": "Vedant Nanda",
          "openalex_id": "A5048896019",
          "institutions": [
            "University of Maryland, College Park"
          ]
        },
        {
          "name": "Aravind Srinivasan",
          "openalex_id": "A5005118312",
          "orcid": "https://orcid.org/0000-0002-0062-3684",
          "institutions": [
            "University of Maryland, College Park"
          ]
        },
        {
          "name": "John P. Dickerson",
          "openalex_id": "A5071538560",
          "orcid": "https://orcid.org/0000-0003-2231-680X",
          "institutions": [
            "University of Maryland, College Park"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-06-26",
      "abstract": "Online bipartite-matching platforms are ubiquitous and find applications in important areas such as crowdsourcing and ridesharing. In the most general form, the platform consists of three entities: two sides to be matched and a platform operator that decides the matching. The design of algorithms for such platforms has traditionally focused on the operator\u2019s (expected) profit. Since fairness has become an important consideration that was ignored in the existing algorithms a collection of online matching algorithms have been developed that give a fair treatment guarantee for one side of the market at the expense of a drop in the operator\u2019s profit. In this paper, we generalize the existing work to offer fair treatment guarantees to both sides of the market simultaneously, at a calculated worst case drop to operator profit. We consider group and individual Rawlsian fairness criteria. Moreover, our algorithms have theoretical guarantees and have adjustable parameters that can be tuned as desired to balance the trade-off between the utilities of the three sides. We also derive hardness results that give clear upper bounds over the performance of any algorithm.",
      "cited_by_count": 9,
      "type": "article",
      "source": {
        "name": "Proceedings of the AAAI Conference on Artificial Intelligence",
        "type": "conference",
        "issn": [
          "2159-5399",
          "2374-3468"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "https://ojs.aaai.org/index.php/AAAI/article/download/25698/25470"
      },
      "topics": [
        "Transportation and Mobility Innovations",
        "Sharing Economy and Platforms",
        "Privacy-Preserving Technologies in Data"
      ],
      "referenced_works_count": 39,
      "url": "https://openalex.org/W4382239417"
    },
    {
      "openalex_id": "W4402013540",
      "doi": "10.1007/978-3-031-70362-1_6",
      "title": "Individual Fairness with\u00a0Group Awareness Under Uncertainty",
      "authors": [
        {
          "name": "Zichong Wang",
          "openalex_id": "A5011724269",
          "orcid": "https://orcid.org/0000-0001-6091-6609",
          "institutions": [
            "Florida International University"
          ]
        },
        {
          "name": "Jocelyn Dzuong",
          "openalex_id": "A5095740359",
          "institutions": [
            "Florida International University"
          ]
        },
        {
          "name": "Xiaoyong Yuan",
          "openalex_id": "A5113356210",
          "institutions": [
            "Clemson University"
          ]
        },
        {
          "name": "Zhong Chen",
          "openalex_id": "A5100430399",
          "orcid": "https://orcid.org/0000-0001-7518-1414",
          "institutions": [
            "Southern Illinois University Carbondale"
          ]
        },
        {
          "name": "Yanzhao Wu",
          "openalex_id": "A5060093535",
          "orcid": "https://orcid.org/0000-0001-8761-5486",
          "institutions": [
            "Florida International University"
          ]
        },
        {
          "name": "Xin Yao",
          "openalex_id": "A5100584906"
        },
        {
          "name": "Wenbin Zhang",
          "openalex_id": "A5100710814",
          "orcid": "https://orcid.org/0000-0003-3024-5415",
          "institutions": [
            "Florida International University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-01-01",
      "abstract": null,
      "cited_by_count": 10,
      "type": "book-chapter",
      "source": {
        "name": "Lecture notes in computer science",
        "type": "book series",
        "issn": [
          "0302-9743",
          "1611-3349"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Experimental Behavioral Economics Studies",
        "Game Theory and Applications"
      ],
      "referenced_works_count": 47,
      "url": "https://openalex.org/W4402013540"
    },
    {
      "openalex_id": "W4399362672",
      "doi": "10.1145/3630106.3658989",
      "title": "Equalised Odds is not Equal Individual Odds: Post-processing for Group and Individual Fairness",
      "authors": [
        {
          "name": "Edward Small",
          "openalex_id": "A5069301426",
          "orcid": "https://orcid.org/0000-0002-3368-1397",
          "institutions": [
            "RMIT University"
          ]
        },
        {
          "name": "Kacper Sokol",
          "openalex_id": "A5083819794",
          "orcid": "https://orcid.org/0000-0002-9869-5896",
          "institutions": [
            "ETH Zurich"
          ]
        },
        {
          "name": "Daniel Manning",
          "openalex_id": "A5009460814",
          "orcid": "https://orcid.org/0009-0007-1144-3121",
          "institutions": [
            "RMIT University"
          ]
        },
        {
          "name": "Flora D. Salim",
          "openalex_id": "A5090893421",
          "orcid": "https://orcid.org/0000-0002-1237-1664",
          "institutions": [
            "UNSW Sydney"
          ]
        },
        {
          "name": "Jeffrey Chan",
          "openalex_id": "A5071422010",
          "orcid": "https://orcid.org/0000-0002-7865-072X",
          "institutions": [
            "RMIT University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-06-03",
      "abstract": "Group fairness is achieved by equalising prediction distributions between protected sub-populations; individual fairness requires treating similar individuals alike. These two objectives, however, are incompatible when a scoring model is calibrated through discontinuous probability functions, where individuals can be randomly assigned an outcome determined by a fixed probability. This procedure may provide two similar individuals from the same protected group with classification odds that are disparately different - a clear violation of individual fairness. Assigning unique odds to each protected sub-population may also prevent members of one sub-population from ever receiving the chances of a positive outcome available to individuals from another sub-population, which we argue is another type of unfairness called individual odds. We reconcile all this by constructing continuous probability functions between group thresholds that are constrained by their Lipschitz constant. Our solution preserves the model's predictive power, individual fairness and robustness while ensuring group fairness.",
      "cited_by_count": 6,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3630106.3658989"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Qualitative Comparative Analysis Research",
        "Experimental Behavioral Economics Studies"
      ],
      "referenced_works_count": 22,
      "url": "https://openalex.org/W4399362672"
    },
    {
      "openalex_id": "W4321012583",
      "doi": "10.48550/arxiv.2302.06958",
      "title": "For One and All: Individual and Group Fairness in the Allocation of Indivisible Goods",
      "authors": [
        {
          "name": "Jonathan Scarlett",
          "openalex_id": "A5046337437",
          "orcid": "https://orcid.org/0000-0003-1403-9160"
        },
        {
          "name": "Nicholas Teh",
          "openalex_id": "A5037242857",
          "orcid": "https://orcid.org/0000-0002-9698-466X"
        },
        {
          "name": "Yair Zick",
          "openalex_id": "A5076656216",
          "orcid": "https://orcid.org/0000-0002-0635-6230"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-02-14",
      "abstract": "Fair allocation of indivisible goods is a well-explored problem. Traditionally, research focused on individual fairness - are individual agents satisfied with their allotted share? - and group fairness - are groups of agents treated fairly? In this paper, we explore the coexistence of individual envy-freeness (i-EF) and its group counterpart, group weighted envy-freeness (g-WEF), in the allocation of indivisible goods. We propose several polynomial-time algorithms that provably achieve i-EF and g-WEF simultaneously in various degrees of approximation under three different conditions on the agents' (i) when agents have identical additive valuation functions, i-EFX and i-WEF1 can be achieved simultaneously; (ii) when agents within a group share a common valuation function, an allocation satisfying both i-EF1 and g-WEF1 exists; and (iii) when agents' valuations for goods within a group differ, we show that while maintaining i-EF1, we can achieve a 1/3-approximation to ex-ante g-WEF1. Our results thus provide a first step towards connecting individual and group fairness in the allocation of indivisible goods, in hopes of its useful application to domains requiring the reconciliation of diversity with individual demands.",
      "cited_by_count": 6,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2302.06958"
      },
      "topics": [
        "Experimental Behavioral Economics Studies",
        "Game Theory and Voting Systems"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4321012583"
    },
    {
      "openalex_id": "W4367046901",
      "doi": "10.1145/3543507.3583480",
      "title": "DualFair: Fair Representation Learning at Both Group and Individual Levels via Contrastive Self-supervision",
      "authors": [
        {
          "name": "Sungwon Han",
          "openalex_id": "A5030123507",
          "orcid": "https://orcid.org/0000-0002-1129-760X",
          "institutions": [
            "Korea Advanced Institute of Science and Technology"
          ]
        },
        {
          "name": "Seungeon Lee",
          "openalex_id": "A5085149492",
          "orcid": "https://orcid.org/0000-0002-9756-0068",
          "institutions": [
            "Korea Advanced Institute of Science and Technology"
          ]
        },
        {
          "name": "Fangzhao Wu",
          "openalex_id": "A5076423724",
          "orcid": "https://orcid.org/0000-0001-9138-1272",
          "institutions": [
            "Microsoft Research Asia (China)"
          ]
        },
        {
          "name": "Sundong Kim",
          "openalex_id": "A5073596347",
          "orcid": "https://orcid.org/0000-0001-9687-2409",
          "institutions": [
            "Gwangju Institute of Science and Technology"
          ]
        },
        {
          "name": "Chuhan Wu",
          "openalex_id": "A5001967239",
          "orcid": "https://orcid.org/0000-0001-5730-8792",
          "institutions": [
            "Tsinghua University"
          ]
        },
        {
          "name": "Xiting Wang",
          "openalex_id": "A5101524540",
          "orcid": "https://orcid.org/0000-0001-5768-1095",
          "institutions": [
            "Microsoft Research Asia (China)"
          ]
        },
        {
          "name": "Xing Xie",
          "openalex_id": "A5044651577",
          "orcid": "https://orcid.org/0000-0002-8608-8482",
          "institutions": [
            "Microsoft Research Asia (China)"
          ]
        },
        {
          "name": "Meeyoung Cha",
          "openalex_id": "A5061810530",
          "orcid": "https://orcid.org/0000-0003-4085-9648",
          "institutions": [
            "Korea Advanced Institute of Science and Technology",
            "Institute for Basic Science"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-04-26",
      "abstract": "Algorithmic fairness has become an important machine learning problem, especially for mission-critical Web applications. This work presents a self-supervised model, called DualFair, that can debias sensitive attributes like gender and race from learned representations. Unlike existing models that target a single type of fairness, our model jointly optimizes for two fairness criteria\u2014group fairness and counterfactual fairness\u2014and hence makes fairer predictions at both the group and individual levels. Our model uses contrastive loss to generate embeddings that are indistinguishable for each protected group, while forcing the embeddings of counterfactual pairs to be similar. It then uses a self-knowledge distillation method to maintain the quality of representation for the downstream tasks. Extensive analysis over multiple datasets confirms the model's validity and further shows the synergy of jointly addressing two fairness criteria, suggesting the model's potential value in fair intelligent Web applications.",
      "cited_by_count": 8,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3543507.3583480"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Psychology of Moral and Emotional Judgment",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 21,
      "url": "https://openalex.org/W4367046901"
    },
    {
      "openalex_id": "W3081139133",
      "doi": "10.48550/arxiv.2008.09490",
      "title": "Beyond Individual and Group Fairness",
      "authors": [
        {
          "name": "Pranjal Awasthi",
          "openalex_id": "A5056617357"
        },
        {
          "name": "Corinna Cortes",
          "openalex_id": "A5013243115"
        },
        {
          "name": "Yishay Mansour",
          "openalex_id": "A5014637159",
          "orcid": "https://orcid.org/0000-0001-6891-2645"
        },
        {
          "name": "Mehryar Mohri",
          "openalex_id": "A5058849006",
          "orcid": "https://orcid.org/0000-0002-3987-9847"
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-08-21",
      "abstract": "We present a new data-driven model of fairness that, unlike existing static definitions of individual or group fairness is guided by the unfairness complaints received by the system. Our model supports multiple fairness criteria and takes into account their potential incompatibilities. We consider both a stochastic and an adversarial setting of our model. In the stochastic setting, we show that our framework can be naturally cast as a Markov Decision Process with stochastic losses, for which we give efficient vanishing regret algorithmic solutions. In the adversarial setting, we design efficient algorithms with competitive ratio guarantees. We also report the results of experiments with our algorithms and the stochastic framework on artificial datasets, to demonstrate their effectiveness empirically.",
      "cited_by_count": 3,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2008.09490"
      },
      "topics": [
        "Advanced Bandit Algorithms Research",
        "Ethics and Social Impacts of AI",
        "Decision-Making and Behavioral Economics"
      ],
      "referenced_works_count": 45,
      "url": "https://openalex.org/W3081139133"
    },
    {
      "openalex_id": "W4409348617",
      "doi": "10.1609/aaai.v39i27.35071",
      "title": "Fair Graph U-Net: A Fair Graph Learning Framework Integrating Group and Individual Awareness",
      "authors": [
        {
          "name": "Zichong Wang",
          "openalex_id": "A5011724269",
          "orcid": "https://orcid.org/0000-0001-6091-6609",
          "institutions": [
            "Florida International University"
          ]
        },
        {
          "name": "Zhibo Chu",
          "openalex_id": "A5113356979",
          "institutions": [
            "Florida International University"
          ]
        },
        {
          "name": "Thang Viet Doan",
          "openalex_id": "A5106850165",
          "orcid": "https://orcid.org/0009-0009-3072-5532",
          "institutions": [
            "Florida International University"
          ]
        },
        {
          "name": "Shaowei Wang",
          "openalex_id": "A5100664830",
          "orcid": "https://orcid.org/0000-0003-1577-1193",
          "institutions": [
            "University of Manitoba"
          ]
        },
        {
          "name": "Yongkai Wu",
          "openalex_id": "A5100731120",
          "orcid": "https://orcid.org/0000-0002-7313-9439",
          "institutions": [
            "Clemson University"
          ]
        },
        {
          "name": "Vasile Palade",
          "openalex_id": "A5076935774",
          "orcid": "https://orcid.org/0000-0002-6768-8394",
          "institutions": [
            "Coventry University"
          ]
        },
        {
          "name": "Wenbin Zhang",
          "openalex_id": "A5028376037",
          "orcid": "https://orcid.org/0000-0001-8219-0781",
          "institutions": [
            "Florida International University"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-04-11",
      "abstract": "Learning high-level representations for graphs is crucial for tasks like node classification, where graph pooling aggregates node features to provide a holistic view that enhances predictive performance. Despite numerous methods that have been proposed in this promising and rapidly developing research field, most efforts to generalize the pooling operation to graphs are primarily performance-driven, with fairness issues largely overlooked: i) the process of graph pooling could exacerbate disparities in distribution among various subgroups; ii) the resultant graph structure augmentation may inadvertently strengthen intra-group connectivity, leading to unintended inter-group isolation. To this end, this paper extends the initial effort on fair graph pooling to the development of fair graph neural networks, while also providing a unified framework to collectively address group and individual graph fairness. Our experimental evaluations on multiple datasets demonstrate that the proposed method not only outperforms state-of-the-art baselines in terms of fairness but also achieves comparable predictive performance.",
      "cited_by_count": 4,
      "type": "article",
      "source": {
        "name": "Proceedings of the AAAI Conference on Artificial Intelligence",
        "type": "conference",
        "issn": [
          "2159-5399",
          "2374-3468"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "https://ojs.aaai.org/index.php/AAAI/article/download/35071/37226"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Privacy, Security, and Data Protection",
        "Privacy-Preserving Technologies in Data"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4409348617"
    },
    {
      "openalex_id": "W4288617757",
      "doi": "10.1145/3287560.3287600",
      "title": "50 Years of Test (Un)fairness",
      "authors": [
        {
          "name": "Ben Hutchinson",
          "openalex_id": "A5071599724",
          "orcid": "https://orcid.org/0000-0003-2253-6204"
        },
        {
          "name": "Margaret Mitchell",
          "openalex_id": "A5046235098",
          "orcid": "https://orcid.org/0000-0001-7043-6545"
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-01-09",
      "abstract": "Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.",
      "cited_by_count": 274,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Digital Economy and Work Transformation"
      ],
      "referenced_works_count": 70,
      "url": "https://openalex.org/W4288617757"
    },
    {
      "openalex_id": "W4300482433",
      "doi": "10.1145/3209978.3210063",
      "title": "Equity of Attention",
      "authors": [
        {
          "name": "Asia J. Biega",
          "openalex_id": "A5087294637",
          "orcid": "https://orcid.org/0000-0001-8083-0976",
          "institutions": [
            "Max Planck Institute for Informatics"
          ]
        },
        {
          "name": "Krishna P. Gummadi",
          "openalex_id": "A5067688305",
          "orcid": "https://orcid.org/0000-0003-1256-8800",
          "institutions": [
            "Max Planck Institute for Software Systems"
          ]
        },
        {
          "name": "Gerhard Weikum",
          "openalex_id": "A5088135366",
          "orcid": "https://orcid.org/0000-0003-4959-6098",
          "institutions": [
            "Max Planck Institute for Informatics"
          ]
        }
      ],
      "publication_year": 2018,
      "publication_date": "2018-06-27",
      "abstract": "Rankings of people and items are at the heart of selection-making, match-making, and recommender systems, ranging from employment sites to sharing economy platforms. As ranking positions influence the amount of attention the ranked subjects receive, biases in rankings can lead to unfair distribution of opportunities and resources, such as jobs or income. This paper proposes new measures and mechanisms to quantify and mitigate unfairness from a bias inherent to all rankings, namely, the position bias, which leads to disproportionately less attention being paid to low-ranked subjects. Our approach differs from recent fair ranking approaches in two important ways. First, existing works measure unfairness at the level of subject groups while our measures capture unfairness at the level of individual subjects, and as such subsume group unfairness. Second, as no single ranking can achieve individual attention fairness, we propose a novel mechanism that achieves amortized fairness, where attention accumulated across a series of rankings is proportional to accumulated relevance. We formulate the challenge of achieving amortized individual fairness subject to constraints on ranking quality as an online optimization problem and show that it can be solved as an integer linear program. Our experimental evaluation reveals that unfair attention distribution in rankings can be substantial, and demonstrates that our method can improve individual fairness while retaining high ranking quality.",
      "cited_by_count": 290,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Mobile Crowdsensing and Crowdsourcing",
        "Privacy, Security, and Data Protection"
      ],
      "referenced_works_count": 25,
      "url": "https://openalex.org/W4300482433"
    },
    {
      "openalex_id": "W3046113390",
      "doi": "10.1038/s41746-020-0304-9",
      "title": "Predictably unequal: understanding and addressing concerns that algorithmic clinical prediction may increase health disparities",
      "authors": [
        {
          "name": "Jessica K. Paulus",
          "openalex_id": "A5014632945",
          "orcid": "https://orcid.org/0000-0001-7194-2239",
          "institutions": [
            "Tufts Medical Center",
            "Tufts University"
          ]
        },
        {
          "name": "David M. Kent",
          "openalex_id": "A5006559598",
          "orcid": "https://orcid.org/0000-0002-9205-5070",
          "institutions": [
            "Tufts Medical Center",
            "Tufts University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-07-30",
      "abstract": "Abstract The machine learning community has become alert to the ways that predictive algorithms can inadvertently introduce unfairness in decision-making. Herein, we discuss how concepts of algorithmic fairness might apply in healthcare, where predictive algorithms are being increasingly used to support decision-making. Central to our discussion is the distinction between algorithmic fairness and algorithmic bias. Fairness concerns apply specifically when algorithms are used to support polar decisions (i.e., where one pole of prediction leads to decisions that are generally more desired than the other), such as when predictions are used to allocate scarce health care resources to a group of patients that could all benefit. We review different fairness criteria and demonstrate their mutual incompatibility. Even when models are used to balance benefits-harms to make optimal decisions for individuals (i.e., for non-polar decisions)\u2013and fairness concerns are not germane\u2013model, data or sampling issues can lead to biased predictions that support decisions that are differentially harmful/beneficial across groups. We review these potential sources of bias, and also discuss ways to diagnose and remedy algorithmic bias. We note that remedies for algorithmic fairness may be more problematic, since we lack agreed upon definitions of fairness. Finally, we propose a provisional framework for the evaluation of clinical prediction models offered for further elaboration and refinement. Given the proliferation of prediction models used to guide clinical decisions, developing consensus for how these concerns can be addressed should be prioritized.",
      "cited_by_count": 239,
      "type": "review",
      "source": {
        "name": "npj Digital Medicine",
        "type": "journal",
        "issn": [
          "2398-6352"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.nature.com/articles/s41746-020-0304-9.pdf"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Healthcare cost, quality, practices",
        "Health Systems, Economic Evaluations, Quality of Life"
      ],
      "referenced_works_count": 46,
      "url": "https://openalex.org/W3046113390"
    },
    {
      "openalex_id": "W2949876871",
      "doi": "10.48550/arxiv.1812.06135",
      "title": "Bias Mitigation Post-processing for Individual and Group Fairness",
      "authors": [
        {
          "name": "Pranay Lohia",
          "openalex_id": "A5045547703",
          "institutions": [
            "IBM Research - India"
          ]
        },
        {
          "name": "Karthikeyan Natesan Ramamurthy",
          "openalex_id": "A5081874896",
          "orcid": "https://orcid.org/0000-0002-6021-5930",
          "institutions": [
            "IBM (United States)"
          ]
        },
        {
          "name": "Manish Bhide",
          "openalex_id": "A5068967374"
        },
        {
          "name": "Diptikalyan Saha",
          "openalex_id": "A5010368277",
          "orcid": "https://orcid.org/0000-0002-1583-5479",
          "institutions": [
            "IBM Research - India"
          ]
        },
        {
          "name": "Kush R. Varshney",
          "openalex_id": "A5015286159",
          "orcid": "https://orcid.org/0000-0002-7376-5536",
          "institutions": [
            "IBM (United States)"
          ]
        },
        {
          "name": "Ruchir Puri",
          "openalex_id": "A5045722906",
          "orcid": "https://orcid.org/0009-0006-8803-7079",
          "institutions": [
            "IBM (United States)"
          ]
        }
      ],
      "publication_year": 2018,
      "publication_date": "2018-12-14",
      "abstract": "Whereas previous post-processing approaches for increasing the fairness of predictions of biased classifiers address only group fairness, we propose a method for increasing both individual and group fairness. Our novel framework includes an individual bias detector used to prioritize data samples in a bias mitigation algorithm aiming to improve the group fairness measure of disparate impact. We show superior performance to previous work in the combination of classification accuracy, individual fairness and group fairness on several real-world datasets in applications such as credit, employment, and criminal justice.",
      "cited_by_count": 3,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/1812.06135"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Psychology of Moral and Emotional Judgment",
        "Regulation and Compliance Studies"
      ],
      "referenced_works_count": 15,
      "url": "https://openalex.org/W2949876871"
    },
    {
      "openalex_id": "W4402928231",
      "doi": "10.1007/s10489-024-05732-3",
      "title": "A group consensus reaching model balancing individual satisfaction and group fairness for distributed linguistic preference relations",
      "authors": [
        {
          "name": "Yingying Liang",
          "openalex_id": "A5001968972",
          "orcid": "https://orcid.org/0000-0002-7354-5939",
          "institutions": [
            "Hebei University of Technology"
          ]
        },
        {
          "name": "Tianyu Zhang",
          "openalex_id": "A5107954653",
          "institutions": [
            "Beijing Institute of Technology"
          ]
        },
        {
          "name": "Yan Tu",
          "openalex_id": "A5109790998",
          "institutions": [
            "Wuhan University of Technology"
          ]
        },
        {
          "name": "Qian Zhao",
          "openalex_id": "A5111344493",
          "institutions": [
            "University of Modena and Reggio Emilia"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-09-27",
      "abstract": null,
      "cited_by_count": 5,
      "type": "article",
      "source": {
        "name": "Applied Intelligence",
        "type": "journal",
        "issn": [
          "0924-669X",
          "1573-7497"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Multi-Criteria Decision Making",
        "Bayesian Modeling and Causal Inference",
        "Cognitive Science and Mapping"
      ],
      "referenced_works_count": 57,
      "url": "https://openalex.org/W4402928231"
    },
    {
      "openalex_id": "W2795743913",
      "doi": "10.1145/3173574.3174230",
      "title": "A Qualitative Exploration of Perceptions of Algorithmic Fairness",
      "authors": [
        {
          "name": "Allison Woodruff",
          "openalex_id": "A5045188560",
          "orcid": "https://orcid.org/0000-0002-5745-7103",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Sarah Fox",
          "openalex_id": "A5038768773",
          "orcid": "https://orcid.org/0000-0002-7888-2598",
          "institutions": [
            "University of Washington",
            "Google (United States)"
          ]
        },
        {
          "name": "Steven Rousso-Schindler",
          "openalex_id": "A5081697936",
          "orcid": "https://orcid.org/0009-0008-9407-804X",
          "institutions": [
            "California State University, Long Beach"
          ]
        },
        {
          "name": "Jeffrey Warshaw",
          "openalex_id": "A5001800178",
          "institutions": [
            "Google (United States)"
          ]
        }
      ],
      "publication_year": 2018,
      "publication_date": "2018-04-20",
      "abstract": "Algorithmic systems increasingly shape information people are exposed to as well as influence decisions about employment, finances, and other opportunities. In some cases, algorithmic systems may be more or less favorable to certain groups or individuals, sparking substantial discussion of algorithmic fairness in public policy circles, academia, and the press. We broaden this discussion by exploring how members of potentially affected communities feel about algorithmic fairness. We conducted workshops and interviews with 44 participants from several populations traditionally marginalized by categories of race or class in the United States. While the concept of algorithmic fairness was largely unfamiliar, learning about algorithmic (un)fairness elicited negative feelings that connect to current national discussions about racial injustice and economic inequality. In addition to their concerns about potential harms to themselves and society, participants also indicated that algorithmic fairness (or lack thereof) could substantially affect their trust in a company or product.",
      "cited_by_count": 222,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3173574.3174230"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Digital Economy and Work Transformation",
        "Privacy, Security, and Data Protection"
      ],
      "referenced_works_count": 79,
      "url": "https://openalex.org/W2795743913"
    },
    {
      "openalex_id": "W2961989711",
      "doi": "10.1080/14693062.2019.1639490",
      "title": "Perceived fairness and public acceptability of carbon pricing: a review of the literature",
      "authors": [
        {
          "name": "Sara Maestre\u2010Andr\u00e9s",
          "openalex_id": "A5044305652",
          "orcid": "https://orcid.org/0000-0003-3971-5209",
          "institutions": [
            "Universitat Aut\u00f2noma de Barcelona"
          ]
        },
        {
          "name": "Stefan Drews",
          "openalex_id": "A5003540579",
          "orcid": "https://orcid.org/0000-0001-6393-3121",
          "institutions": [
            "Universitat Aut\u00f2noma de Barcelona"
          ]
        },
        {
          "name": "Jeroen C.J.M. van den Bergh",
          "openalex_id": "A5075929639",
          "orcid": "https://orcid.org/0000-0003-3415-3083",
          "institutions": [
            "Universitat Aut\u00f2noma de Barcelona",
            "Vrije Universiteit Amsterdam",
            "Instituci\u00f3 Catalana de Recerca i Estudis Avan\u00e7ats"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-07-10",
      "abstract": "While carbon pricing is widely seen as a crucial element of climate policy and has been implemented in many countries, it also has met with strong resistance. We provide a comprehensive overview of public perceptions of the fairness of carbon pricing and how these affect policy acceptability. To this end, we review evidence from empirical studies on how individuals judge personal, distributional and procedural aspects of carbon taxes and cap-and-trade. In addition, we examine preferences for particular redistributive and other uses of revenues generated by carbon pricing and their role in instrument acceptability. Our results indicate a high concern over distributional effects, particularly in relation to policy impacts on poor people, in turn reducing policy acceptability. In addition, people show little trust in the capacities of governments to put the revenues of carbon pricing to good use. Somewhat surprisingly, most studies do not indicate clear public preferences for using revenues to ensure fairer policy outcomes, notably by reducing its regressive effects. Instead, many people prefer using revenues for \u2018environmental projects\u2019 of various kinds. We end by providing recommendations for improving public acceptability of carbon pricing. One suggestion to increase policy acceptability is combining the redistribution of revenue to vulnerable groups with the funding for environmental projects, such as on renewable energy. Key policy insights If people perceive carbon pricing instruments as fair, this increases policy acceptability and support. People\u2019s satisfaction with information provided by the government about the policy instrument increases acceptability. While people express high concern over uneven distribution of the policy burden, they often prefer using carbon pricing revenues for environmental projects instead of compensation for inequitable outcomes. Recent studies find that people\u2019s preferences shift to using revenues for making policy fairer if they better understand the functioning of carbon pricing, notably that relatively high prices of CO2-intensive goods and services reduce their consumption. Combining the redistribution of revenue to support both vulnerable groups and environmental projects, such as on renewable energy, seems to most increase policy acceptability.",
      "cited_by_count": 398,
      "type": "review",
      "source": {
        "name": "Climate Policy",
        "type": "journal",
        "issn": [
          "1469-3062",
          "1752-7457"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://ddd.uab.cat/record/212942"
      },
      "topics": [
        "Energy, Environment, and Transportation Policies",
        "Climate Change Policy and Economics",
        "Economic and Environmental Valuation"
      ],
      "referenced_works_count": 64,
      "url": "https://openalex.org/W2961989711"
    },
    {
      "openalex_id": "W4287028128",
      "doi": "10.48550/arxiv.2108.02741",
      "title": "GIFAIR-FL: A Framework for Group and Individual Fairness in Federated Learning",
      "authors": [
        {
          "name": "Xubo Yue",
          "openalex_id": "A5001018616",
          "orcid": "https://orcid.org/0000-0001-9929-8895"
        },
        {
          "name": "Maher Nouiehed",
          "openalex_id": "A5088345581",
          "orcid": "https://orcid.org/0000-0001-8089-7011"
        },
        {
          "name": "Raed Al Kontar",
          "openalex_id": "A5075324117",
          "orcid": "https://orcid.org/0000-0002-4546-324X"
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-08-05",
      "abstract": "In this paper we propose \\texttt{GIFAIR-FL}: a framework that imposes \\textbf{G}roup and \\textbf{I}ndividual \\textbf{FAIR}ness to \\textbf{F}ederated \\textbf{L}earning settings. By adding a regularization term, our algorithm penalizes the spread in the loss of client groups to drive the optimizer to fair solutions. Our framework \\texttt{GIFAIR-FL} can accommodate both global and personalized settings. Theoretically, we show convergence in non-convex and strongly convex settings. Our convergence guarantees hold for both $i.i.d.$ and non-$i.i.d.$ data. To demonstrate the empirical performance of our algorithm, we apply our method to image classification and text prediction tasks. Compared to existing algorithms, our method shows improved fairness results while retaining superior or similar prediction accuracy.",
      "cited_by_count": 5,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2108.02741"
      },
      "topics": [
        "Privacy-Preserving Technologies in Data",
        "Stochastic Gradient Optimization Techniques",
        "Cryptography and Data Security"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4287028128"
    },
    {
      "openalex_id": "W2787955716",
      "doi": "10.1609/aaai.v32i1.11296",
      "title": "Beyond Distributive Fairness in Algorithmic Decision Making: Feature Selection for Procedurally Fair Learning",
      "authors": [
        {
          "name": "Nina Grgi\u0107-Hla\u010da",
          "openalex_id": "A5025919676",
          "orcid": "https://orcid.org/0000-0003-3397-2984",
          "institutions": [
            "Max Planck Institute for Software Systems"
          ]
        },
        {
          "name": "Muhammad Bilal Zafar",
          "openalex_id": "A5102901191",
          "orcid": "https://orcid.org/0000-0001-8347-7813",
          "institutions": [
            "Max Planck Institute for Software Systems"
          ]
        },
        {
          "name": "Krishna P. Gummadi",
          "openalex_id": "A5067688305",
          "orcid": "https://orcid.org/0000-0003-1256-8800",
          "institutions": [
            "Max Planck Institute for Software Systems"
          ]
        },
        {
          "name": "Adrian Weller",
          "openalex_id": "A5042278493",
          "orcid": "https://orcid.org/0000-0003-1915-7158",
          "institutions": [
            "University of Cambridge"
          ]
        }
      ],
      "publication_year": 2018,
      "publication_date": "2018-04-25",
      "abstract": "With widespread use of machine learning methods in numerous domains involving humans, several studies have raised questions about the potential for unfairness towards certain individuals or groups. A number of recent works have proposed methods to measure and eliminate unfairness from machine learning models. However, most of this work has focused on only one dimension of fair decision making: distributive fairness, i.e., the fairness of the decision outcomes. In this work, we leverage the rich literature on organizational justice and focus on another dimension of fair decision making: procedural fairness, i.e., the fairness of the decision making process. We propose measures for procedural fairness that consider the input features used in the decision process, and evaluate the moral judgments of humans regarding the use of these features. We operationalize these measures on two real world datasets using human surveys on the Amazon Mechanical Turk (AMT) platform, demonstrating that our measures capture important properties of procedurally fair decision making. We provide fast submodular mechanisms to optimize the tradeoff between procedural fairness and prediction accuracy. On our datasets, we observe empirically that procedural fairness may be achieved with little cost to outcome fairness, but that some loss of accuracy is unavoidable.",
      "cited_by_count": 178,
      "type": "article",
      "source": {
        "name": "Proceedings of the AAAI Conference on Artificial Intelligence",
        "type": "conference",
        "issn": [
          "2159-5399",
          "2374-3468"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "https://ojs.aaai.org/index.php/AAAI/article/download/11296/11155"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Psychology of Moral and Emotional Judgment",
        "Free Will and Agency"
      ],
      "referenced_works_count": 58,
      "url": "https://openalex.org/W2787955716"
    },
    {
      "openalex_id": "W4313527987",
      "doi": "10.1080/10447318.2022.2153320",
      "title": "Six Human-Centered Artificial Intelligence Grand Challenges",
      "authors": [
        {
          "name": "\u00d6zlem \u00d6zmen Garibay",
          "openalex_id": "A5062903281",
          "orcid": "https://orcid.org/0000-0001-9215-694X",
          "institutions": [
            "University of Central Florida"
          ]
        },
        {
          "name": "Brent Winslow",
          "openalex_id": "A5058724676",
          "orcid": "https://orcid.org/0000-0001-7098-6336",
          "institutions": [
            "Biosignatures (United Kingdom)",
            "Design Interactive (United States)"
          ]
        },
        {
          "name": "Salvatore Andolina",
          "openalex_id": "A5051078868",
          "orcid": "https://orcid.org/0000-0001-9804-4009",
          "institutions": [
            "University of Palermo"
          ]
        },
        {
          "name": "Margherita Antona",
          "openalex_id": "A5016653929",
          "orcid": "https://orcid.org/0000-0003-3208-3755",
          "institutions": [
            "FORTH Institute of Computer Science"
          ]
        },
        {
          "name": "Anja Bodenschatz",
          "openalex_id": "A5024970701",
          "orcid": "https://orcid.org/0000-0001-9982-8809",
          "institutions": [
            "Technical University of Munich"
          ]
        },
        {
          "name": "Constantinos K. Coursaris",
          "openalex_id": "A5001589359",
          "orcid": "https://orcid.org/0000-0001-9301-3289",
          "institutions": [
            "HEC Montr\u00e9al"
          ]
        },
        {
          "name": "Gregory Falco",
          "openalex_id": "A5052907259",
          "orcid": "https://orcid.org/0000-0002-6463-7719",
          "institutions": [
            "Johns Hopkins University"
          ]
        },
        {
          "name": "Stephen M. Fiore",
          "openalex_id": "A5040965331",
          "orcid": "https://orcid.org/0000-0003-3529-1322",
          "institutions": [
            "University of Central Florida"
          ]
        },
        {
          "name": "Iv\u00e1n Garibay",
          "openalex_id": "A5001663885",
          "orcid": "https://orcid.org/0000-0002-3302-9382",
          "institutions": [
            "University of Central Florida"
          ]
        },
        {
          "name": "Keri Grieman",
          "openalex_id": "A5007158173",
          "institutions": [
            "University of Oxford"
          ]
        },
        {
          "name": "John C. Havens",
          "openalex_id": "A5006024485",
          "institutions": [
            "Center for Strategic Research"
          ]
        },
        {
          "name": "Marina Jirotka",
          "openalex_id": "A5023741875",
          "orcid": "https://orcid.org/0000-0002-6088-3955",
          "institutions": [
            "University of Oxford"
          ]
        },
        {
          "name": "Hernisa Kacorri",
          "openalex_id": "A5009475920",
          "orcid": "https://orcid.org/0000-0002-7798-308X",
          "institutions": [
            "University of Maryland, College Park"
          ]
        },
        {
          "name": "Waldemar Karwowski",
          "openalex_id": "A5068942141",
          "orcid": "https://orcid.org/0000-0002-9134-3441",
          "institutions": [
            "University of Central Florida"
          ]
        },
        {
          "name": "Joseph T. Kider",
          "openalex_id": "A5009147264",
          "orcid": "https://orcid.org/0000-0002-4818-115X",
          "institutions": [
            "University of Central Florida"
          ]
        },
        {
          "name": "Joseph A. Konstan",
          "openalex_id": "A5037387341",
          "orcid": "https://orcid.org/0000-0002-7788-2748",
          "institutions": [
            "University of Minnesota"
          ]
        },
        {
          "name": "Sean Koon",
          "openalex_id": "A5069557536",
          "orcid": "https://orcid.org/0000-0003-4947-5114",
          "institutions": [
            "Kaiser Permanente"
          ]
        },
        {
          "name": "M\u00f3nica L\u00f3pez-Gonz\u00e1lez",
          "openalex_id": "A5062849275",
          "orcid": "https://orcid.org/0000-0001-9153-6852",
          "institutions": [
            "Artificial Intelligence in Medicine (Canada)"
          ]
        },
        {
          "name": "Iliana Maifeld-Carucci",
          "openalex_id": "A5029972430",
          "institutions": [
            "Johns Hopkins University"
          ]
        },
        {
          "name": "Sean McGregor",
          "openalex_id": "A5030293975",
          "orcid": "https://orcid.org/0000-0001-5803-4981",
          "institutions": [
            "Physicians Committee for Responsible Medicine"
          ]
        },
        {
          "name": "Gavriel Salvendy",
          "openalex_id": "A5084317374",
          "institutions": [
            "University of Central Florida"
          ]
        },
        {
          "name": "Ben Shneiderman",
          "openalex_id": "A5052623072",
          "orcid": "https://orcid.org/0000-0002-8298-1097",
          "institutions": [
            "University of Maryland, College Park"
          ]
        },
        {
          "name": "Constantine Stephanidis",
          "openalex_id": "A5009270487",
          "orcid": "https://orcid.org/0000-0003-3687-4220",
          "institutions": [
            "FORTH Institute of Computer Science",
            "FORTH Institute of Astrophysics",
            "University of Crete"
          ]
        },
        {
          "name": "Christina Strobel",
          "openalex_id": "A5051897790",
          "orcid": "https://orcid.org/0000-0002-1887-860X",
          "institutions": [
            "Universit\u00e4t Hamburg",
            "Hamburg University of Technology"
          ]
        },
        {
          "name": "Carolyn Ten Holter",
          "openalex_id": "A5012821517",
          "orcid": "https://orcid.org/0000-0001-8739-3211",
          "institutions": [
            "University of Oxford"
          ]
        },
        {
          "name": "Wei Xu",
          "openalex_id": "A5065898067",
          "orcid": "https://orcid.org/0000-0002-2228-998X",
          "institutions": [
            "Zhejiang University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-02",
      "abstract": "Widespread adoption of artificial intelligence (AI) technologies is substantially affecting the human condition in ways that are not yet well understood. Negative unintended consequences abound including the perpetuation and exacerbation of societal inequalities and divisions via algorithmic decision making. We present six grand challenges for the scientific community to create AI technologies that are human-centered, that is, ethical, fair, and enhance the human condition. These grand challenges are the result of an international collaboration across academia, industry and government and represent the consensus views of a group of 26 experts in the field of human-centered artificial intelligence (HCAI). In essence, these challenges advocate for a human-centered approach to AI that (1) is centered in human well-being, (2) is designed responsibly, (3) respects privacy, (4) follows human-centered design principles, (5) is subject to appropriate governance and oversight, and (6) interacts with individuals while respecting human\u2019s cognitive capacities. We hope that these challenges and their associated research directions serve as a call for action to conduct research and development in AI that serves as a force multiplier towards more fair, equitable and sustainable societies.",
      "cited_by_count": 382,
      "type": "article",
      "source": {
        "name": "International Journal of Human-Computer Interaction",
        "type": "journal",
        "issn": [
          "1044-7318",
          "1532-7590"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.tandfonline.com/doi/pdf/10.1080/10447318.2022.2153320?needAccess=true&role=button"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Artificial Intelligence in Healthcare and Education",
        "Neuroethics, Human Enhancement, Biomedical Innovations"
      ],
      "referenced_works_count": 255,
      "url": "https://openalex.org/W4313527987"
    },
    {
      "openalex_id": "W3128553449",
      "doi": "10.48550/arxiv.2102.00417",
      "title": "Priority-based Post-Processing Bias Mitigation for Individual and Group Fairness",
      "authors": [
        {
          "name": "Pranay Lohia",
          "openalex_id": "A5045547703"
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-01-31",
      "abstract": "Previous post-processing bias mitigation algorithms on both group and individual fairness don't work on regression models and datasets with multi-class numerical labels. We propose a priority-based post-processing bias mitigation on both group and individual fairness with the notion that similar individuals should get similar outcomes irrespective of socio-economic factors and more the unfairness, more the injustice. We establish this proposition by a case study on tariff allotment in a smart grid. Our novel framework establishes it by using a user segmentation algorithm to capture the consumption strategy better. This process ensures priority-based fair pricing for group and individual facing the maximum injustice. It upholds the notion of fair tariff allotment to the entire population taken into consideration without modifying the in-built process for tariff calculation. We also validate our method and show superior performance to previous work on a real-world dataset in criminal sentencing.",
      "cited_by_count": 3,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2102.00417"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning",
        "Psychology of Moral and Emotional Judgment"
      ],
      "referenced_works_count": 15,
      "url": "https://openalex.org/W3128553449"
    },
    {
      "openalex_id": "W2902850069",
      "doi": "10.1111/bjso.12297",
      "title": "A quarter century of system justification theory: Questions, answers, criticisms, and societal applications",
      "authors": [
        {
          "name": "John T. Jost",
          "openalex_id": "A5077630021",
          "orcid": "https://orcid.org/0000-0002-2844-4645",
          "institutions": [
            "New York University"
          ]
        }
      ],
      "publication_year": 2018,
      "publication_date": "2018-11-28",
      "abstract": "A theory of system justification was proposed 25 years ago by Jost and Banaji (1994, Br. J. Soc. Psychol., 33, 1) in the British Journal of Social Psychology to explain \u2018the participation by disadvantaged individuals and groups in negative stereotypes of themselves' and the phenomenon of outgroup favouritism. The scope of the theory was subsequently expanded to account for a much wider range of outcomes, including appraisals of fairness, justice, legitimacy, deservingness, and entitlement; spontaneous and deliberate social judgements about individuals, groups, and events; and full\u2010fledged political and religious ideologies. According to system justification theory, people are motivated (to varying degrees, depending upon situational and dispositional factors) to defend, bolster, and justify aspects of existing social, economic, and political systems. Engaging in system justification serves the palliative function of increasing satisfaction with the status quo and addresses underlying epistemic, existential, and relational needs to reduce uncertainty, threat, and social discord. This article summarizes the major tenets of system justification theory, reviews some of the empirical evidence supporting it, answers new (and old) questions and criticisms, and highlights areas of societal relevance and directions for future research.",
      "cited_by_count": 504,
      "type": "article",
      "source": {
        "name": "British Journal of Social Psychology",
        "type": "journal",
        "issn": [
          "0144-6665",
          "2044-8309"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Social and Intergroup Psychology",
        "Cultural Differences and Values",
        "Psychology of Moral and Emotional Judgment"
      ],
      "referenced_works_count": 204,
      "url": "https://openalex.org/W2902850069"
    },
    {
      "openalex_id": "W4289751798",
      "doi": "10.1145/3219819.3220046",
      "title": "A Unified Approach to Quantifying Algorithmic Unfairness",
      "authors": [
        {
          "name": "Till Speicher",
          "openalex_id": "A5032165991",
          "institutions": [
            "Max Planck Institute for Software Systems"
          ]
        },
        {
          "name": "Hoda Heidari",
          "openalex_id": "A5037735812",
          "orcid": "https://orcid.org/0000-0003-3710-4076",
          "institutions": [
            "ETH Zurich"
          ]
        },
        {
          "name": "Nina Grgi\u0107-Hla\u010da",
          "openalex_id": "A5025919676",
          "orcid": "https://orcid.org/0000-0003-3397-2984",
          "institutions": [
            "Max Planck Institute for Software Systems"
          ]
        },
        {
          "name": "Krishna P. Gummadi",
          "openalex_id": "A5067688305",
          "orcid": "https://orcid.org/0000-0003-1256-8800",
          "institutions": [
            "Max Planck Institute for Software Systems"
          ]
        },
        {
          "name": "Adish Singla",
          "openalex_id": "A5027711113",
          "orcid": "https://orcid.org/0000-0001-9922-0668",
          "institutions": [
            "Max Planck Institute for Software Systems"
          ]
        },
        {
          "name": "Adrian Weller",
          "openalex_id": "A5042278493",
          "orcid": "https://orcid.org/0000-0003-1915-7158",
          "institutions": [
            "University of Cambridge",
            "The Alan Turing Institute"
          ]
        },
        {
          "name": "Muhammad Bilal Zafar",
          "openalex_id": "A5011745729",
          "orcid": "https://orcid.org/0000-0001-6329-303X",
          "institutions": [
            "Max Planck Institute for Software Systems"
          ]
        }
      ],
      "publication_year": 2018,
      "publication_date": "2018-07-19",
      "abstract": "Discrimination via algorithmic decision making has received considerable\\nattention. Prior work largely focuses on defining conditions for fairness, but\\ndoes not define satisfactory measures of algorithmic unfairness. In this paper,\\nwe focus on the following question: Given two unfair algorithms, how should we\\ndetermine which of the two is more unfair? Our core idea is to use existing\\ninequality indices from economics to measure how unequally the outcomes of an\\nalgorithm benefit different individuals or groups in a population. Our work\\noffers a justified and general framework to compare and contrast the\\n(un)fairness of algorithmic predictors. This unifying approach enables us to\\nquantify unfairness both at the individual and the group level. Further, our\\nwork reveals overlooked tradeoffs between different fairness notions: using our\\nproposed measures, the overall individual-level unfairness of an algorithm can\\nbe decomposed into a between-group and a within-group component. Earlier\\nmethods are typically designed to tackle only between-group unfairness, which\\nmay be justified for legal or other reasons. However, we demonstrate that\\nminimizing exclusively the between-group component may, in fact, increase the\\nwithin-group, and hence the overall unfairness. We characterize and illustrate\\nthe tradeoffs between our measures of (un)fairness and the prediction accuracy.\\n",
      "cited_by_count": 174,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3219819.3220046"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Privacy-Preserving Technologies in Data"
      ],
      "referenced_works_count": 38,
      "url": "https://openalex.org/W4289751798"
    },
    {
      "openalex_id": "W4390961945",
      "doi": "10.48550/arxiv.2401.07174",
      "title": "On the (In)Compatibility between Group Fairness and Individual Fairness",
      "authors": [
        {
          "name": "Shizhou Xu",
          "openalex_id": "A5101885097",
          "orcid": "https://orcid.org/0000-0001-9461-2638"
        },
        {
          "name": "Thomas Strohmer",
          "openalex_id": "A5080565728",
          "orcid": "https://orcid.org/0000-0003-2029-3317"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-01-13",
      "abstract": "We study the compatibility between the optimal statistical parity solutions and individual fairness. While individual fairness seeks to treat similar individuals similarly, optimal statistical parity aims to provide similar treatment to individuals who share relative similarity within their respective sensitive groups. The two fairness perspectives, while both desirable from a fairness perspective, often come into conflict in applications. Our goal in this work is to analyze the existence of this conflict and its potential solution. In particular, we establish sufficient (sharp) conditions for the compatibility between the optimal (post-processing) statistical parity $L^2$ learning and the ($K$-Lipschitz or $(\u03b5,\u03b4)$) individual fairness requirements. Furthermore, when there exists a conflict between the two, we first relax the former to the Pareto frontier (or equivalently the optimal trade-off) between $L^2$ error and statistical disparity, and then analyze the compatibility between the frontier and the individual fairness requirements. Our analysis identifies regions along the Pareto frontier that satisfy individual fairness requirements. (Lastly, we provide individual fairness guarantees for the composition of a trained model and the optimal post-processing step so that one can determine the compatibility of the post-processed model.) This provides practitioners with a valuable approach to attain Pareto optimality for statistical parity while adhering to the constraints of individual fairness.",
      "cited_by_count": 1,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2401.07174"
      },
      "topics": [
        "Social and Intergroup Psychology",
        "Social Power and Status Dynamics"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4390961945"
    },
    {
      "openalex_id": "W3169716967",
      "doi": "10.48550/arxiv.2106.08652",
      "title": "Maxmin-Fair Ranking: Individual Fairness under Group-Fairness Constraints",
      "authors": [
        {
          "name": "David Garc\u00eda-Soriano",
          "openalex_id": "A5010392760",
          "orcid": "https://orcid.org/0000-0002-2869-9330",
          "institutions": [
            "Institute for Scientific Interchange"
          ]
        },
        {
          "name": "Francesco Bonchi",
          "openalex_id": "A5022272025",
          "orcid": "https://orcid.org/0000-0001-9464-8315",
          "institutions": [
            "Institute for Scientific Interchange"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-06-16",
      "abstract": "We study a novel problem of fairness in ranking aimed at minimizing the amount of individual unfairness introduced when enforcing group-fairness constraints. Our proposal is rooted in the distributional maxmin fairness theory, which uses randomization to maximize the expected satisfaction of the worst-off individuals. We devise an exact polynomial-time algorithm to find maxmin-fair distributions of general search problems (including, but not limited to, ranking), and show that our algorithm can produce rankings which, while satisfying the given group-fairness constraints, ensure that the maximum possible value is brought to individuals.",
      "cited_by_count": 1,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2106.08652"
      },
      "topics": [
        "Game Theory and Voting Systems",
        "Auction Theory and Applications",
        "Game Theory and Applications"
      ],
      "referenced_works_count": 31,
      "url": "https://openalex.org/W3169716967"
    },
    {
      "openalex_id": "W4287118042",
      "doi": "10.48550/arxiv.2106.08652",
      "title": "Maxmin-Fair Ranking: Individual Fairness under Group-Fairness\\n Constraints",
      "authors": [
        {
          "name": "David Garc\u00eda-Soriano",
          "openalex_id": "A5010392760",
          "orcid": "https://orcid.org/0000-0002-2869-9330"
        },
        {
          "name": "Francesco Bonchi",
          "openalex_id": "A5022272025",
          "orcid": "https://orcid.org/0000-0001-9464-8315"
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-06-16",
      "abstract": "We study a novel problem of fairness in ranking aimed at minimizing the\\namount of individual unfairness introduced when enforcing group-fairness\\nconstraints. Our proposal is rooted in the distributional maxmin fairness\\ntheory, which uses randomization to maximize the expected satisfaction of the\\nworst-off individuals. We devise an exact polynomial-time algorithm to find\\nmaxmin-fair distributions of general search problems (including, but not\\nlimited to, ranking), and show that our algorithm can produce rankings which,\\nwhile satisfying the given group-fairness constraints, ensure that the maximum\\npossible value is brought to individuals.\\n",
      "cited_by_count": 1,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2106.08652"
      },
      "topics": [
        "Game Theory and Voting Systems",
        "Auction Theory and Applications",
        "Game Theory and Applications"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4287118042"
    },
    {
      "openalex_id": "W3136620301",
      "doi": "10.1177/20539517221115189",
      "title": "Fairness perceptions of algorithmic decision-making: A systematic review of the empirical literature",
      "authors": [
        {
          "name": "Christopher Starke",
          "openalex_id": "A5060936581",
          "orcid": "https://orcid.org/0000-0001-7899-6029",
          "institutions": [
            "University of Amsterdam"
          ]
        },
        {
          "name": "Janine Baleis",
          "openalex_id": "A5073264835",
          "institutions": [
            "Heinrich Heine University D\u00fcsseldorf"
          ]
        },
        {
          "name": "Birte Keller",
          "openalex_id": "A5089967027",
          "orcid": "https://orcid.org/0000-0002-3145-5206",
          "institutions": [
            "Heinrich Heine University D\u00fcsseldorf"
          ]
        },
        {
          "name": "Frank Marcinkowski",
          "openalex_id": "A5009323144",
          "orcid": "https://orcid.org/0000-0001-6497-9324",
          "institutions": [
            "Heinrich Heine University D\u00fcsseldorf"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-07-01",
      "abstract": "Algorithmic decision-making increasingly shapes people's daily lives. Given that such autonomous systems can cause severe harm to individuals and social groups, fairness concerns have arisen. A human-centric approach demanded by scholars and policymakers requires considering people's fairness perceptions when designing and implementing algorithmic decision-making. We provide a comprehensive, systematic literature review synthesizing the existing empirical insights on perceptions of algorithmic fairness from 58 empirical studies spanning multiple domains and scientific disciplines. Through thorough coding, we systemize the current empirical literature along four dimensions: (1) algorithmic predictors, (2) human predictors, (3) comparative effects (human decision-making vs. algorithmic decision-making), and (4) consequences of algorithmic decision-making. While we identify much heterogeneity around the theoretical concepts and empirical measurements of algorithmic fairness, the insights come almost exclusively from Western-democratic contexts. By advocating for more interdisciplinary research adopting a society-in-the-loop framework, we hope our work will contribute to fairer and more responsible algorithmic decision-making.",
      "cited_by_count": 197,
      "type": "review",
      "source": {
        "name": "Big Data & Society",
        "type": "journal",
        "issn": [
          "2053-9517"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://doi.org/10.1177/20539517221115189"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Privacy, Security, and Data Protection",
        "Digital Economy and Work Transformation"
      ],
      "referenced_works_count": 97,
      "url": "https://openalex.org/W3136620301"
    },
    {
      "openalex_id": "W3092995732",
      "doi": null,
      "title": "Ranking for Individual and Group Fairness Simultaneously.",
      "authors": [
        {
          "name": "Sruthi Gorantla",
          "openalex_id": "A5066114051",
          "orcid": "https://orcid.org/0000-0002-0179-9905"
        },
        {
          "name": "Amit Deshpande",
          "openalex_id": "A5029414285",
          "orcid": "https://orcid.org/0000-0001-8638-1120"
        },
        {
          "name": "Anand Louis",
          "openalex_id": "A5059653613",
          "orcid": "https://orcid.org/0000-0002-4727-9219"
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-09-24",
      "abstract": null,
      "cited_by_count": 2,
      "type": "article",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/abs/2010.06986v1"
      },
      "topics": [
        "Experimental Behavioral Economics Studies"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W3092995732"
    },
    {
      "openalex_id": "W4221160121",
      "doi": "10.48550/arxiv.2201.06021",
      "title": "Rawlsian Fairness in Online Bipartite Matching: Two-sided, Group, and Individual",
      "authors": [
        {
          "name": "Seyed A. Esmaeili",
          "openalex_id": "A5064453417",
          "orcid": "https://orcid.org/0000-0002-1195-6599"
        },
        {
          "name": "Sharmila Duppala",
          "openalex_id": "A5009952954",
          "orcid": "https://orcid.org/0000-0001-6809-8858"
        },
        {
          "name": "Vedant Nanda",
          "openalex_id": "A5048896019"
        },
        {
          "name": "Aravind Srinivasan",
          "openalex_id": "A5005118312",
          "orcid": "https://orcid.org/0000-0002-0062-3684"
        },
        {
          "name": "John P. Dickerson",
          "openalex_id": "A5071538560",
          "orcid": "https://orcid.org/0000-0003-2231-680X"
        },
        {
          "name": "Dickerson, John P.",
          "openalex_id": ""
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-01-16",
      "abstract": "Online bipartite-matching platforms are ubiquitous and find applications in important areas such as crowdsourcing and ridesharing. In the most general form, the platform consists of three entities: two sides to be matched and a platform operator that decides the matching. The design of algorithms for such platforms has traditionally focused on the operator's (expected) profit. Since fairness has become an important consideration that was ignored in the existing algorithms a collection of online matching algorithms have been developed that give a fair treatment guarantee for one side of the market at the expense of a drop in the operator's profit. In this paper, we generalize the existing work to offer fair treatment guarantees to both sides of the market simultaneously, at a calculated worst case drop to operator profit. We consider group and individual Rawlsian fairness criteria. Moreover, our algorithms have theoretical guarantees and have adjustable parameters that can be tuned as desired to balance the trade-off between the utilities of the three sides. We also derive hardness results that give clear upper bounds over the performance of any algorithm.",
      "cited_by_count": 2,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2201.06021"
      },
      "topics": [
        "Sharing Economy and Platforms",
        "Transportation and Mobility Innovations",
        "Privacy-Preserving Technologies in Data"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4221160121"
    },
    {
      "openalex_id": "W3029022080",
      "doi": "10.1145/3313831.3376813",
      "title": "Factors Influencing Perceived Fairness in Algorithmic Decision-Making",
      "authors": [
        {
          "name": "Ruotong Wang",
          "openalex_id": "A5102797041",
          "orcid": "https://orcid.org/0000-0003-0964-6943",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "F. Maxwell Harper",
          "openalex_id": "A5029009966",
          "orcid": "https://orcid.org/0000-0003-0552-5773",
          "institutions": [
            "Amazon (United States)"
          ]
        },
        {
          "name": "Haiyi Zhu",
          "openalex_id": "A5051842323",
          "orcid": "https://orcid.org/0000-0001-7271-9100",
          "institutions": [
            "Carnegie Mellon University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-04-21",
      "abstract": "Algorithmic decision-making systems are increasingly used throughout the public and private sectors to make important decisions or assist humans in making these decisions with real social consequences. While there has been substantial research in recent years to build fair decision-making algorithms, there has been less research seeking to understand the factors that affect people's perceptions of fairness in these systems, which we argue is also important for their broader acceptance. In this research, we conduct an online experiment to better understand perceptions of fairness, focusing on three sets of factors: algorithm outcomes, algorithm development and deployment procedures, and individual differences. We find that people rate the algorithm as more fair when the algorithm predicts in their favor, even surpassing the negative effects of describing algorithms that are very biased against particular demographic groups. We find that this effect is moderated by several variables, including participants' education level, gender, and several aspects of the development procedure. Our findings suggest that systems that evaluate algorithmic fairness through users' feedback must consider the possibility of \"outcome favorability\" bias.",
      "cited_by_count": 176,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Mobile Crowdsensing and Crowdsourcing",
        "Privacy, Security, and Data Protection"
      ],
      "referenced_works_count": 60,
      "url": "https://openalex.org/W3029022080"
    },
    {
      "openalex_id": "W4411025495",
      "doi": "10.1145/3727120",
      "title": "Online Allocation with Multi-Class Arrivals: Group Fairness vs Individual Welfare",
      "authors": [
        {
          "name": "Faraz Zargari",
          "openalex_id": "A5062238662",
          "orcid": "https://orcid.org/0009-0007-2680-0494",
          "institutions": [
            "University of Alberta"
          ]
        },
        {
          "name": "Hossein Nekouyan",
          "openalex_id": "A5117828743",
          "institutions": [
            "University of Alberta"
          ]
        },
        {
          "name": "Bo Sun",
          "openalex_id": "A5100642907",
          "orcid": "https://orcid.org/0000-0003-3172-7811",
          "institutions": [
            "University of Waterloo"
          ]
        },
        {
          "name": "Xiaoqi Tan",
          "openalex_id": "A5047753222",
          "orcid": "https://orcid.org/0000-0002-5339-3245",
          "institutions": [
            "University of Alberta"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-05-27",
      "abstract": "We introduce and study a multi-class online resource allocation problem with group fairness guarantees. The problem involves allocating a fixed amount of resources to a sequence of agents, each belonging to a specific group. The primary objective is to ensure fairness across different groups in an online setting. We focus on three fairness notions: one based on quantity and two based on utility. To achieve fair allocations, we develop two threshold-based online algorithms, proving their optimality under two fairness notions and near-optimality for the more challenging one. Additionally, we demonstrate a fundamental trade-off between group fairness and individual welfare using a novel representative function-based approach. To address this trade-off, we propose a set-aside multi-threshold algorithm that reserves a portion of the resource to ensure fairness across groups while utilizing the remaining resource to optimize efficiency under utility-based fairness notions. This algorithm is proven to achieve the Pareto-optimal trade-off. We also demonstrate that our problem can model a wide range of real-world applications, including network caching and cloud computing, and empirically evaluate our proposed algorithms in the network caching problem using real datasets.",
      "cited_by_count": 2,
      "type": "article",
      "source": {
        "name": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "type": "journal",
        "issn": [
          "2476-1249"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Transportation and Mobility Innovations",
        "Optimization and Search Problems",
        "Sharing Economy and Platforms"
      ],
      "referenced_works_count": 46,
      "url": "https://openalex.org/W4411025495"
    },
    {
      "openalex_id": "W4395098175",
      "doi": "10.1007/978-981-97-2242-6_15",
      "title": "Adversarial Learning of\u00a0Group and\u00a0Individual Fair Representations",
      "authors": [
        {
          "name": "Hao Liu",
          "openalex_id": "A5100458882",
          "orcid": "https://orcid.org/0000-0003-2209-117X",
          "institutions": [
            "University of Hong Kong",
            "Hong Kong University of Science and Technology"
          ]
        },
        {
          "name": "Raymond Chi-Wing Wong",
          "openalex_id": "A5049858061",
          "orcid": "https://orcid.org/0000-0001-7045-6503",
          "institutions": [
            "University of Hong Kong",
            "Hong Kong University of Science and Technology"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-01-01",
      "abstract": null,
      "cited_by_count": 1,
      "type": "book-chapter",
      "source": {
        "name": "Lecture notes in computer science",
        "type": "book series",
        "issn": [
          "0302-9743",
          "1611-3349"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 14,
      "url": "https://openalex.org/W4395098175"
    }
  ],
  "count": 40,
  "errors": []
}
