{
  "status": "success",
  "source": "openalex",
  "query": "impossibility fairness metrics",
  "results": [
    {
      "openalex_id": "W3176768410",
      "doi": "10.1016/j.clsr.2021.105567",
      "title": "Why fairness cannot be automated: Bridging the gap between EU non-discrimination law and AI",
      "authors": [
        {
          "name": "Sandra Wachter",
          "openalex_id": "A5075172090",
          "orcid": "https://orcid.org/0000-0003-3800-0113",
          "institutions": [
            "British Library",
            "Harvard University Press",
            "University of Oxford",
            "Turing Institute",
            "Science Oxford",
            "The Alan Turing Institute"
          ]
        },
        {
          "name": "Brent Mittelstadt",
          "openalex_id": "A5081516308",
          "orcid": "https://orcid.org/0000-0002-4709-6404",
          "institutions": [
            "Turing Institute",
            "British Library",
            "The Alan Turing Institute",
            "Harvard University Press"
          ]
        },
        {
          "name": "Chris Russell",
          "openalex_id": "A5008943199",
          "orcid": "https://orcid.org/0000-0003-1665-1759",
          "institutions": [
            "University of Surrey",
            "Turing Institute",
            "British Library",
            "The Alan Turing Institute"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-06-22",
      "abstract": null,
      "cited_by_count": 246,
      "type": "article",
      "source": {
        "name": "Computer law & security review",
        "type": "journal",
        "issn": [
          "2212-473X",
          "2212-4748"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Law, AI, and Intellectual Property",
        "Digital Transformation in Law",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W3176768410"
    },
    {
      "openalex_id": "W3004155201",
      "doi": "10.1145/3351095.3372869",
      "title": "The false promise of risk assessments",
      "authors": [
        {
          "name": "Ben Green",
          "openalex_id": "A5100610716",
          "orcid": "https://orcid.org/0000-0001-7810-908X",
          "institutions": [
            "Harvard University Press"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-27",
      "abstract": "Risk assessments have proliferated in the United States criminal justice system. The theory of change motivating their adoption involves two key assumptions: first, that risk assessments will reduce human biases by making objective decisions, and second, that risk assessments will promote criminal justice reform. In this paper I interrogate both of these assumptions, concluding that risk assessments are an ill-advised tool for challenging the centrality and legitimacy of incarceration within the criminal justice system. First, risk assessments fail to provide objectivity, as their use creates numerous sites of discretion. Second, risk assessments provide no guarantee of reducing incarceration; instead, they risk legitimizing the criminal justice system's structural racism. I then consider, via an \"epistemic reform,\" the path forward for criminal justice reform. I reinterpret recent results regarding the \"impossibility of fairness\" as not simply a tension between mathematical metrics but as evidence of a deeper tension between notions of equality. This expanded frame challenges the formalist, colorblind proceduralism at the heart of the criminal justice system and suggests a more structural approach to reform. Together, this analysis highlights how algorithmic fairness narrows the scope of judgments about justice and how \"fair\" algorithms can reinforce discrimination.",
      "cited_by_count": 79,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3351095.3372869"
      },
      "topics": [
        "Criminal Justice and Corrections Analysis",
        "Free Will and Agency",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 126,
      "url": "https://openalex.org/W3004155201"
    },
    {
      "openalex_id": "W3157867125",
      "doi": "10.1145/3461702.3462621",
      "title": "What's Fair about Individual Fairness?",
      "authors": [
        {
          "name": "Will Fleisher",
          "openalex_id": "A5001864674",
          "orcid": "https://orcid.org/0000-0002-5980-3970",
          "institutions": [
            "Northeastern University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-07-21",
      "abstract": "One of the main lines of research in algorithmic fairness involves individual fairness (IF) methods. Individual fairness is motivated by an intuitive principle, similar treatment, which requires that similar individuals be treated similarly. IF offers a precise account of this principle using distance metrics to evaluate the similarity of individuals. Proponents of individual fairness have argued that it gives the correct definition of algorithmic fairness, and that it should therefore be preferred to other methods for determining fairness. I argue that individual fairness cannot serve as a definition of fairness. Moreover, IF methods should not be given priority over other fairness methods, nor used in isolation from them. To support these conclusions, I describe four in-principle problems for individual fairness as a definition and as a method for ensuring fairness: (1) counterexamples show that similar treatment (and therefore IF) are insufficient to guarantee fairness; (2) IF methods for learning similarity metrics are at risk of encoding human implicit bias; (3) IF requires prior moral judgments, limiting its usefulness as a guide for fairness and undermining its claim to define fairness; and (4) the incommensurability of relevant moral values makes similarity metrics impossible for many tasks. In light of these limitations, I suggest that individual fairness cannot be a definition of fairness, and instead should be seen as one tool among several for ameliorating algorithmic bias.",
      "cited_by_count": 54,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Psychology of Moral and Emotional Judgment",
        "Free Will and Agency"
      ],
      "referenced_works_count": 47,
      "url": "https://openalex.org/W3157867125"
    },
    {
      "openalex_id": "W3023069697",
      "doi": "10.1016/s2589-7500(20)30065-0",
      "title": "Ethical limitations of algorithmic fairness solutions in health care machine learning",
      "authors": [
        {
          "name": "Melissa D. McCradden",
          "openalex_id": "A5063981686",
          "orcid": "https://orcid.org/0000-0002-6476-2165",
          "institutions": [
            "Hospital for Sick Children"
          ]
        },
        {
          "name": "Shalmali Joshi",
          "openalex_id": "A5035149567",
          "institutions": [
            "Vector Institute"
          ]
        },
        {
          "name": "Mjaye Mazwi",
          "openalex_id": "A5080855486",
          "orcid": "https://orcid.org/0000-0003-1345-5429",
          "institutions": [
            "Hospital for Sick Children"
          ]
        },
        {
          "name": "James A. Anderson",
          "openalex_id": "A5103410231",
          "institutions": [
            "Hospital for Sick Children"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-04-28",
      "abstract": "Artificial intelligence has exposed pernicious bias within health data that constitutes substantial ethical threat to the use of machine learning in medicine.1Char DS Shah NH Magnus D Implementing machine learning in health care\u2014addressing ethical challenges.N Engl J Med. 2018; 378: 981-983Crossref PubMed Scopus (445) Google Scholar, 2Obermeyer Z Powers B Vogeli C Mullainathan S Dissecting racial bias in an algorithm used to manage the health of populations.Science. 2019; 366: 447-453Crossref PubMed Scopus (1151) Google Scholar Solutions of algorithmic fairness have been developed to create neutral models: models designed to produce non-discriminatory predictions by constraining bias with respect to predicted outcomes for protected identities, such as race or gender.3Corbett-Davies S Goel S The measure and mismeasure of fairness: a critical review of fair machine learning.https://5harad.com/papers/fair-ml.pdfDate accessed: March 16, 2020Google Scholar These solutions can omit such variables from the model (widely regarded as ineffective and can increase discrimination), constrain it to ensure equal error rates across groups, derive outcomes that are independent of one's identity after controlling for the estimated risk of that outcome, or mathematically balance benefit and harm to all groups. The temptation to engineer ethics into algorithm design is immense and industry is increasingly pushing these solutions. In the health-care space, where the stakes could be higher, clinicians will integrate these models into their care, trusting the issue of bias has been sufficiently managed within the model. However, even if well recognised technical challenges are set aside,3Corbett-Davies S Goel S The measure and mismeasure of fairness: a critical review of fair machine learning.https://5harad.com/papers/fair-ml.pdfDate accessed: March 16, 2020Google Scholar, 4Marmot M Social determinants of health inequalities.Lancet. 2005; 365: 1099-1104Summary Full Text Full Text PDF PubMed Scopus (2858) Google Scholar framing fairness as a purely technical problem solvable by the inclusion of more data or accurate computations is ethically problematic. We highlight challenges to the ethical and empirical efficacy of solutions of algorithmic fairness that show risks of relying too heavily on the so called veneer of technical neutrality,5Benjamin R Assessing risk, automating racism.Science. 2019; 366: 421-422Crossref PubMed Scopus (97) Google Scholar which could exacerbate harms to vulnerable groups. Historically, algorithmic fairness has not accounted for complex causal relationships between biological, environmental, and social factors that give rise to differences in medical conditions across protected identities. Social determinants of health play an important role, particularly for risk models. Social and structural factors affect health across multiple intersecting identities,4Marmot M Social determinants of health inequalities.Lancet. 2005; 365: 1099-1104Summary Full Text Full Text PDF PubMed Scopus (2858) Google Scholar but the mechanism(s) by which social determinants affect health outcomes is not always well understood. Additional complications flow from the reality that difference does not always entail inequity. In some instances, it is appropriate to incorporate differences between identities because there is a reasonable presumption of causation. For example, biological differences between genders can affect the efficacy of pharmacological compounds; incorporating these differences into prescribing practices does not make those prescriptions unjust. However, incorporating non-causative factors into recommendations can propagate unequal treatment by reifying extant inequities and exacerbating their effects. We should not allow models to promote different standards of care according to protected identities that do not have a causative association with the outcome. Nevertheless, in many cases it is difficult to distinguish between acknowledging difference and propagating discrimination. Given the epistemic uncertainty surrounding the association between protected identities and health outcomes, the use of fairness solutions can create empirical challenges. Consider the case of heart attack symptoms among women.6Wenger NK Cardiovascular disease: the female heart is vulnerable. A call to action from the 10Q Report.Clin Cardiol. 2012; 35: 134-135Crossref PubMed Scopus (11) Google Scholar The under-representation of women (particularly women of colour) in research of heart health is now well recognised as problematic and directly affected uneven improvements in treatment of heart attacks between women and men. By tailoring health solutions to the majority (ie, referent) group, we inevitably fall short of helping all patients. Many algorithmic fairness solutions, in effect, replicate this problem by trying to fit the non-referent groups to that of the referent,7Friedler SA Scheidegger C Venkatasubramanian S On the (im)possibility of fairness.https://arxiv.org/pdf/1609.07236.pdfDate: Sept 23, 2019Date accessed: March 16, 2020Google Scholar, 8Barocas S Hardt M Narayanan A Fairness and machine learning: limitations and opportunities; 2018.Fairmlbook.orgDate: 2019Google Scholar ignoring heterogeneity and assuming that the latter represents a true underlying pattern. Another concern is disconnection between the patient's clinical trajectory and the fair prediction. Consider the implications at the point-of-care, a model, corrected for fairness, will predict that a patient will respond to a treatment as a patient in the referent class would. What happens when that patient does not have the predicted response? This difference between an idealised model and non-ideal, real-world behaviour affects metrics of model performance (eg, specificity, sensitivity) and clinical utility in practice. Moreover, the model has made an ineffective recommendation that could have obscured more relevant interventions to help that patient. If clinicians and patients believe that the mode has been rendered neutral, then any discrepancies between model prediction and the patient's true clinical state might be impossible to interpret. The result would be to camouflage persistent health inequalities. As such, fairness, operationalised by output metrics alone, is insufficient; real-world consequences should be carefully considered. Bias and ineffective solutions of algorithmic fairness threaten the ethical obligation to avoid or minimise harms to patients (non-maleficence; panel). Non-maleficence demands that any new clinical tool should be assessed for patient safety. For health-care machine learning, safety should include awareness of model limitations with respect to protected identities and social determinants of health. Considerations of justice requires that implemented models do not exacerbate pernicious bias. There is a movement toward developing guidelines of standardised reporting for machine learning models of health care9Collins GS Reitsma JB Altman DG Moons KGM TRIPOD GroupTransparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): The TRIPOD Statement.Ann Intern Med. 2015; 162: 55-63Crossref PubMed Scopus (1245) Google Scholar and their prospective appraisal through clinical trials.10The CONSORT-AI and SPIRIT-AI Steering GroupReporting guidelines for clinical trials evaluating artificial intelligence interventions are needed.Nat Med. 2019; 25: 1467-1468Crossref PubMed Scopus (69) Google Scholar Appraisal is particularly important in determining the real-world implications for vulnerable patients when machine learning models are integrated into clinical decision making. Clinical trials are essential to providing a sense of the model's performance for clinicians to make informed decisions at the point-of-care through awareness of identity-related model limitations.PanelRecommendations for ethical approaches to issues of bias in health models of machine learningRelying on neutral algorithms is problematicChallenges cast doubt on whether any solution can adequately facilitate ethical goals. Resisting the tendency to view machine learning solutions as objective is essential to remaining patient-centered and preventing unintended harms.Problem formulation can support improved modelsChanging the way the problem is conceptualised and operationalised can reduce the effect of pernicious bias on outputs.6Wenger NK Cardiovascular disease: the female heart is vulnerable. A call to action from the 10Q Report.Clin Cardiol. 2012; 35: 134-135Crossref PubMed Scopus (11) Google Scholar Clinicians have a key role in identifying actionable, clinical problems where the effects of bias are minimized, or causal knowledge exists to support algorithmic fairness solutions. Clinicians in non-medical sciences might have epistemic advantages to support such formulations.Transparency is required surrounding model development and statistical validationStandardisation of reporting for models of machine learning can promote transparency about training data and statistical validation which can help clinicians and health-care decision makers to determine the transferability of the model to their served population. Large discrepancies increase the risk of under-performance in under-represented patients. Group-specific performance metrics (eg, stratification by ethnicity, gender, or socioeconomic status) can inform considerations for patient safety in implementing a given model.Initiating transparency at point-of-careAs the field of explainable or interpretable machine learning evolves, we suggest that, when sensitive attributes are used in problem formulation and could affect predictions, they should be accompanied by visibility of the top predictive features. Where predictions are affected by sensitive variables, prediction and rationale should be documented for the ensuing clinical decision to promote fair and transparent medical decision making. Communication with patients about the rationale is essential to shared decision making.Transparency at the prediction levelRobust auditing processes are increasingly viewed as essential to proper oversight of machine learning tools. When bias is considered, auditing can promote reflexive understanding of how tools of machine learning can affect care management of vulnerable patients. Discrepancies in systematic prediction can become evident and used as evidence of the need for beneficial interventions and highlight large structural barriers that affect health inequalities.Ethical decision making suggests engaging diverse knowledge sourcesEthical analysis should consider real-world consequences for affected groups, weigh benefits and risks of various approaches, and engage stakeholders to come to the most supportable conclusion. Therefore, analysis needs to focus on the downstream effects on patients rather than adopting the presumption that fairness is accomplished solely in the metrics of the system. Relying on neutral algorithms is problematic Challenges cast doubt on whether any solution can adequately facilitate ethical goals. Resisting the tendency to view machine learning solutions as objective is essential to remaining patient-centered and preventing unintended harms. Problem formulation can support improved models Changing the way the problem is conceptualised and operationalised can reduce the effect of pernicious bias on outputs.6Wenger NK Cardiovascular disease: the female heart is vulnerable. A call to action from the 10Q Report.Clin Cardiol. 2012; 35: 134-135Crossref PubMed Scopus (11) Google Scholar Clinicians have a key role in identifying actionable, clinical problems where the effects of bias are minimized, or causal knowledge exists to support algorithmic fairness solutions. Clinicians in non-medical sciences might have epistemic advantages to support such formulations. Transparency is required surrounding model development and statistical validation Standardisation of reporting for models of machine learning can promote transparency about training data and statistical validation which can help clinicians and health-care decision makers to determine the transferability of the model to their served population. Large discrepancies increase the risk of under-performance in under-represented patients. Group-specific performance metrics (eg, stratification by ethnicity, gender, or socioeconomic status) can inform considerations for patient safety in implementing a given model. Initiating transparency at point-of-care As the field of explainable or interpretable machine learning evolves, we suggest that, when sensitive attributes are used in problem formulation and could affect predictions, they should be accompanied by visibility of the top predictive features. Where predictions are affected by sensitive variables, prediction and rationale should be documented for the ensuing clinical decision to promote fair and transparent medical decision making. Communication with patients about the rationale is essential to shared decision making. Transparency at the prediction level Robust auditing processes are increasingly viewed as essential to proper oversight of machine learning tools. When bias is considered, auditing can promote reflexive understanding of how tools of machine learning can affect care management of vulnerable patients. Discrepancies in systematic prediction can become evident and used as evidence of the need for beneficial interventions and highlight large structural barriers that affect health inequalities. Ethical decision making suggests engaging diverse knowledge sources Ethical analysis should consider real-world consequences for affected groups, weigh benefits and risks of various approaches, and engage stakeholders to come to the most supportable conclusion. Therefore, analysis needs to focus on the downstream effects on patients rather than adopting the presumption that fairness is accomplished solely in the metrics of the system. Some computations can promote justice through revealing unfairness and refining problem formulation. Obermeyer and colleagues2Obermeyer Z Powers B Vogeli C Mullainathan S Dissecting racial bias in an algorithm used to manage the health of populations.Science. 2019; 366: 447-453Crossref PubMed Scopus (1151) Google Scholar show how calibration can reveal unfairness in a seemingly neutral task through which choice of label can dictate how heavily bias is incorporated into predictions. It might be that no way exists to define a purely neutral problem; some clinical prediction tasks might be more susceptible to bias than others. Transparency at multiple points in the pipeline of machine learning including development, testing, and implementation stages can support interpretation of model outputs by relevant stakeholders (eg, researchers, clinicians, patients, and auditors). Combined with adequate documentation of outputs and ensuing decisions, these steps support a strong accountability framework for point-of-care machine learning tools with respect to safety and fairness to patients. Problem formulation with respect to bias will often be value-laden and ethically charged. Ethical decision making highlights the importance of converging knowledge sources to inform a given choice. Important stakeholders could include affected communities, cultural anthropologists, social scientists, and race and gender theorists. Computations alone clearly cannot solve the bias problem, but they could be offered a place within a broader approach to addressing fairness aims in healthcare. Algorithmic fairness could be necessary to fix statistical limitations reflective of perniciously biased data, and we encourage this work. The worry is that suggesting these as solutions risks unintended harms.5Benjamin R Assessing risk, automating racism.Science. 2019; 366: 421-422Crossref PubMed Scopus (97) Google Scholar Bias is not new; however, machine learning has potential to reveal bias, motivate change, and support ethical analysis while bringing this crucial conversation to a new audience. We are at a watershed moment in health care. Ethical considerations have rarely been so integral and essential to maximising success of a technology both empirically and clinically. The time is right to partake in thoughtful and collaborative engagement on the challenge of bias to bring about lasting change. We declare no competing interests.",
      "cited_by_count": 236,
      "type": "article",
      "source": {
        "name": "The Lancet Digital Health",
        "type": "journal",
        "issn": [
          "2589-7500"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "http://www.thelancet.com/article/S2589750020300650/pdf"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Healthcare cost, quality, practices",
        "COVID-19 and healthcare impacts"
      ],
      "referenced_works_count": 10,
      "url": "https://openalex.org/W3023069697"
    },
    {
      "openalex_id": "W2774778035",
      "doi": "10.1109/comst.2017.2782753",
      "title": "Datacenter Traffic Control: Understanding Techniques and Tradeoffs",
      "authors": [
        {
          "name": "Mohammad Noormohammadpour",
          "openalex_id": "",
          "orcid": "https://orcid.org/0000-0002-9602-4490",
          "institutions": [
            "University of Southern California"
          ]
        },
        {
          "name": "Cauligi S. Raghavendra",
          "openalex_id": "",
          "institutions": [
            "University of Southern California"
          ]
        }
      ],
      "publication_year": 2017,
      "publication_date": "2017-12-14",
      "abstract": "International audience \u00e2\u0080\u0094Datacenters provide cost-effective and flexible access to scalable compute and storage resources necessary for today's cloud computing needs. A typical datacenter is made up of thousands of servers connected with a large network and usually managed by one operator. To provide quality access to the variety of applications and services hosted on datacenters and maximize performance, it deems necessary to use datacenter networks effectively and efficiently. Datacenter traffic is often a mix of several classes with different priorities and requirements. This includes user-generated interactive traffic, traffic with deadlines, and long-running traffic. To this end, custom transport protocols and traffic management techniques have been developed to improve datacenter network performance. In this tutorial paper, we review the general architecture of datacenter networks, various topologies proposed for them, their traffic properties, general traffic control challenges in datacenters and general traffic control objectives. The purpose of this paper is to bring out the important characteristics of traffic control in datacenters and not to survey all existing solutions (as it is virtually impossible due to massive body of existing research). We hope to provide readers with a wide range of options and factors while considering a variety of traffic control mechanisms. We discuss various characteristics of datacenter traffic control including management schemes, transmission control, traffic shaping, prioritization, load balancing, multipathing, and traffic scheduling. Next, we point to several open challenges as well as new and interesting networking paradigms. At the end of this paper, we briefly review inter-datacenter networks that connect geographically dispersed datacenters which have been receiving increasing attention recently and pose interesting and novel research problems. To measure the performance of datacenter networks, different performance metrics have been used such as flow completion times, deadline miss rate, throughput and fairness. Depending on the application and user requirements, some metrics may need more attention. While investigating different traffic control techniques, we point out the trade-offs involved in terms of costs, complexity and performance. We find that a combination of different traffic control techniques may be necessary at particular entities and layers in the network to improve the variety of performance metrics. We also find that despite significant research efforts, there are still open problems that demand further attention from the research community.",
      "cited_by_count": 143,
      "type": "article",
      "source": {
        "name": "IEEE Communications Surveys & Tutorials",
        "type": "journal",
        "issn": [
          "1553-877X",
          "2373-745X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/1712.03530"
      },
      "topics": [
        "Cloud Computing and Resource Management",
        "Software-Defined Networks and 5G",
        "Advanced Data Storage Technologies"
      ],
      "referenced_works_count": 199,
      "url": "https://openalex.org/W2774778035"
    },
    {
      "openalex_id": "W3098158941",
      "doi": "10.6084/m9.figshare.5835207.v3",
      "title": "Datacenter Traffic Control: Understanding Techniques and Trade-offs",
      "authors": [
        {
          "name": "Mohammad Noormohammadpour",
          "openalex_id": "A5033259065",
          "orcid": "https://orcid.org/0000-0002-9602-4490",
          "institutions": [
            "University of Southern California"
          ]
        },
        {
          "name": "C.S. Raghavendra",
          "openalex_id": "A5112293594",
          "institutions": [
            "University of Southern California"
          ]
        }
      ],
      "publication_year": 2018,
      "publication_date": "2018-01-01",
      "abstract": "Datacenters provide cost-effective and flexible access to scalable compute and storage resources necessary for today\u2019s cloud computing needs. A typical datacenter is made up of thousands of servers connected with a large network and usually managed by one operator. To provide quality access to the variety of applications and services hosted on datacenters and maximize performance, it deems necessary to use datacenter networks effectively and efficiently. Datacenter traffic is often a mix of several classes with different priorities and requirements. This includes user-generated interactive traffic, traffic with deadlines, and long-running traffic. To this end, custom transport protocols and traffic management techniques have been developed to improve datacenter network performance. In this tutorial paper, we review the general architecture of datacenter networks, various topologies proposed for them, their traffic properties, general traffic control challenges in datacenters and general traffic control objectives. The purpose of this paper is to bring out the important characteristics of traffic control in datacenters and not to survey all existing solutions (as it is virtually impossible due to massive body of existing research). We hope to provide readers with a wide range of options and factors while considering a variety of traffic control mechanisms. We discuss various characteristics of datacenter traffic control including management schemes, transmission control, traffic shaping, prioritization, load balancing, multipathing, and traffic scheduling. Next, we point to several open challenges as well as new and interesting networking paradigms. At the end of this paper, we briefly review inter-datacenter networks that connect geographically dispersed datacenters which have been receiving increasing attention recently and pose interesting and novel research problems. To measure the performance of datacenter networks, different performance metrics have been used such as flow completion times, deadline miss rate, throughput and fairness. Depending on the application and user requirements, some metrics may need more attention. While investigating different traffic control techniques, we point out the trade-offs involved in terms of costs, complexity and performance. We find that a combination of different traffic control techniques may be necessary at particular entities and layers in the network to improve the variety of performance metrics. We also find that despite significant research efforts, there are still open problems that demand further attention from the research community.",
      "cited_by_count": 167,
      "type": "article",
      "source": {
        "name": "OPAL (Open@LaTrobe) (La Trobe University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://figshare.com/articles/journal_contribution/Datacenter_Traffic_Control_Understanding_Techniques_and_Trade-offs/5835207"
      },
      "topics": [
        "Cloud Computing and Resource Management",
        "Software-Defined Networks and 5G",
        "Advanced Data Storage Technologies"
      ],
      "referenced_works_count": 224,
      "url": "https://openalex.org/W3098158941"
    },
    {
      "openalex_id": "W3216646432",
      "doi": "10.1163/17455243-20213439",
      "title": "Fairness in Machine Learning: Against False Positive Rate Equality as a Measure of Fairness",
      "authors": [
        {
          "name": "Robert R. Long",
          "openalex_id": "A5009824942",
          "institutions": [
            "New York University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-11-22",
      "abstract": "Abstract As machine learning informs increasingly consequential decisions, different metrics have been proposed for measuring algorithmic bias or unfairness. Two popular \u201cfairness measures\u201d are calibration and equality of false positive rate. Each measure seems intuitively important, but notably, it is usually impossible to satisfy both measures. For this reason, a large literature in machine learning speaks of a \u201cfairness tradeoff\u201d between these two measures. This framing assumes that both measures are, in fact, capturing something important. To date, philosophers have seldom examined this crucial assumption, and examined to what extent each measure actually tracks a normatively important property. This makes this inevitable statistical conflict \u2013 between calibration and false positive rate equality \u2013 an important topic for ethics. In this paper, I give an ethical framework for thinking about these measures and argue that, contrary to initial appearances, false positive rate equality is in fact morally irrelevant and does not measure fairness.",
      "cited_by_count": 40,
      "type": "article",
      "source": {
        "name": "Journal of Moral Philosophy",
        "type": "journal",
        "issn": [
          "1740-4681",
          "1745-5243"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning",
        "Psychology of Moral and Emotional Judgment"
      ],
      "referenced_works_count": 39,
      "url": "https://openalex.org/W3216646432"
    },
    {
      "openalex_id": "W4232235627",
      "doi": "10.31228/osf.io/jq2b6",
      "title": "Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI",
      "authors": [
        {
          "name": "Sandra Wachter",
          "openalex_id": "A5075172090",
          "orcid": "https://orcid.org/0000-0003-3800-0113"
        },
        {
          "name": "Brent Mittelstadt",
          "openalex_id": "A5081516308",
          "orcid": "https://orcid.org/0000-0002-4709-6404"
        },
        {
          "name": "Chris Russell",
          "openalex_id": "A5008943199",
          "orcid": "https://orcid.org/0000-0003-1665-1759"
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-05-12",
      "abstract": "In recent years a substantial literature has emerged concerning bias, discrimination, and fairness in AI and machine learning. Connecting this work to existing legal non-discrimination frameworks is essential to create tools and methods that are practically useful across divergent legal regimes. While much work has been undertaken from an American legal perspective, comparatively little has mapped the effects and requirements of EU law. This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness. Through analysis of EU non-discrimination law and jurisprudence of the European Court of Justice (ECJ) and national courts, we identify a critical incompatibility between European notions of discrimination and existing work on algorithmic and automat-ed fairness. A clear gap exists between statistical measures of fairness as embedded in myriad fairness toolkits and governance mechanisms and the context-sensitive, often intuitive and ambiguous discrimination metrics and evidential requirements used by the ECJ; we refer to this approach as \u201ccontextual equality.\u201dThis Article makes three contributions. First, we review the evidential requirements to bring a claim under EU non-discrimination law. Due to the disparate nature of algorithmic and human discrimination, the EU\u2019s current requirements are too contextual, reliant on intuition, and open to judicial interpretation to be automated. Many of the concepts fundamental to bringing a claim, such as the composition of the disadvantaged and advantaged group, the severity and type of harm suffered, and requirements for the relevance and admissibility of evidence, require normative or political choices to be made by the judiciary on a case-by-case basis. We show that automating fairness or non-discrimination in Europe may be impossible because the law, by design, does not provide a static or homogenous framework suited to testing for discrimination in AI systems.Second, we show how the legal protection offered by non-discrimination law is challenged when AI, not humans, discriminate. Humans discriminate due to negative attitudes (e.g. stereotypes, prejudice) and unintentional biases (e.g. organisational practices or internalised stereotypes) which can act as a signal to victims that discrimination has occurred. Equivalent signalling mechanisms and agency do not exist in algorithmic systems. Compared to traditional forms of discrimination, automated discrimination is more abstract and unintuitive, subtle, intangible, and difficult to detect. The increasing use of algorithms disrupts traditional legal remedies and procedures for detection, investigation, prevention, and correction of discrimination which have predominantly relied upon intuition. Consistent assessment procedures that define a common standard for statistical evidence to detect and assess prima facie automated discrimination are urgently needed to support judges, regulators, system controllers and developers, and claimants.Finally, we examine how existing work on fairness in machine learning lines up with procedures for assessing cases under EU non-discrimination law. A \u2018gold standard\u2019 for assessment of prima facie discrimination has been advanced by the European Court of Justice but not yet translated into standard assessment procedures for automated discrimination. We propose \u2018conditional demographic disparity\u2019 (CDD) as a standard baseline statistical measurement that aligns with the Court\u2019s \u2018gold standard\u2019. Establishing a standard set of statistical evidence for automated discrimination cases can help ensure consistent procedures for assessment, but not judicial interpretation, of cases involving AI and automated systems. Through this proposal for procedural regularity in the identification and assessment of auto-mated discrimination, we clarify how to build considerations of fairness into automated systems as far as possible while still respecting and enabling the contextual approach to judicial interpretation practiced under EU non-discrimination law.",
      "cited_by_count": 62,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Law, AI, and Intellectual Property",
        "Digital Transformation in Law",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4232235627"
    },
    {
      "openalex_id": "W4380366644",
      "doi": "10.1145/3593013.3594007",
      "title": "The Possibility of Fairness: Revisiting the Impossibility Theorem in Practice",
      "authors": [
        {
          "name": "Andrew Bell",
          "openalex_id": "A5101752875",
          "orcid": "https://orcid.org/0000-0001-6010-9030",
          "institutions": [
            "New York University"
          ]
        },
        {
          "name": "Lucius Bynum",
          "openalex_id": "A5038439834",
          "orcid": "https://orcid.org/0000-0002-9247-2595",
          "institutions": [
            "New York University"
          ]
        },
        {
          "name": "Nazarii Drushchak",
          "openalex_id": "A5014987268",
          "orcid": "https://orcid.org/0000-0002-5056-3026",
          "institutions": [
            "Ukrainian Catholic University"
          ]
        },
        {
          "name": "Tetiana Zakharchenko",
          "openalex_id": "A5078740312",
          "orcid": "https://orcid.org/0009-0007-3702-2704",
          "institutions": [
            "Ukrainian Catholic University"
          ]
        },
        {
          "name": "Lucas Rosenblatt",
          "openalex_id": "A5068868453",
          "orcid": "https://orcid.org/0000-0001-6952-4361",
          "institutions": [
            "New York University"
          ]
        },
        {
          "name": "Julia Stoyanovich",
          "openalex_id": "A5082830839",
          "orcid": "https://orcid.org/0000-0002-1587-0450",
          "institutions": [
            "New York University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-06-12",
      "abstract": "The \"impossibility theorem\" \u2014 which is considered foundational in algorithmic fairness literature \u2014 asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a practitioner's perspective of fairness), it becomes possible to identify abundant sets of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when \u2014 and to what degree \u2014 fairness along multiple criteria can be achieved. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed.",
      "cited_by_count": 16,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://doi.org/10.1145/3593013.3594007"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Qualitative Comparative Analysis Research"
      ],
      "referenced_works_count": 23,
      "url": "https://openalex.org/W4380366644"
    },
    {
      "openalex_id": "W4283166367",
      "doi": "10.1145/3531146.3534641",
      "title": "Measuring Fairness of Rankings under Noisy Sensitive Information",
      "authors": [
        {
          "name": "Azin Ghazimatin",
          "openalex_id": "A5041491819",
          "orcid": "https://orcid.org/0000-0002-0282-2425"
        },
        {
          "name": "Matth\u00e4us Kleinde\u00dfner",
          "openalex_id": "A5006408039",
          "orcid": "https://orcid.org/0000-0002-9907-4610",
          "institutions": [
            "Amazon (Germany)"
          ]
        },
        {
          "name": "Chris Russell",
          "openalex_id": "A5008943199",
          "orcid": "https://orcid.org/0000-0003-1665-1759",
          "institutions": [
            "Amazon (Germany)"
          ]
        },
        {
          "name": "Ziawasch Abedjan",
          "openalex_id": "A5009128577",
          "orcid": "https://orcid.org/0000-0002-2846-1373",
          "institutions": [
            "Leibniz University Hannover",
            "Amazon (Germany)"
          ]
        },
        {
          "name": "Jacek Go\u0142\u0119biowski",
          "openalex_id": "A5055649136",
          "orcid": "https://orcid.org/0000-0001-8053-8318"
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-06-20",
      "abstract": "Metrics commonly used to assess group fairness in ranking require the knowledge of group membership labels (e.g., whether a job applicant is male or female). Obtaining accurate group membership labels, however, may be costly, operationally difficult, or even infeasible. Where it is not possible to obtain these labels, one common solution is to use proxy labels in their place, which are typically predicted by machine learning models. Proxy labels are susceptible to systematic biases, and using them for fairness estimation can thus lead to unreliable assessments. We investigate the problem of measuring group fairness in ranking for a suite of divergence-based metrics in the presence of proxy labels. We show that under certain assumptions, fairness of a ranking can reliably be measured from the proxy labels. We formalize two assumptions and provide a theoretical analysis for each showing how the true metric values can be derived from the estimates based on proxy labels. We prove that without such assumptions fairness assessment based on proxy labels is impossible. Through extensive experiments on both synthetic and real datasets, we demonstrate the effectiveness of our proposed methods for recovering reliable fairness assessments in rankings.",
      "cited_by_count": 13,
      "type": "article",
      "source": {
        "name": "2022 ACM Conference on Fairness, Accountability, and Transparency",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "bronze",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3531146.3534641"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Game Theory and Voting Systems",
        "Auction Theory and Applications"
      ],
      "referenced_works_count": 50,
      "url": "https://openalex.org/W4283166367"
    },
    {
      "openalex_id": "W2999309192",
      "doi": "10.1186/s12864-019-6413-7",
      "title": "The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation",
      "authors": [
        {
          "name": "Davide Chicco",
          "openalex_id": "A5011556172",
          "orcid": "https://orcid.org/0000-0001-9655-7142",
          "institutions": [
            "Krembil Research Institute",
            "Krembil Foundation"
          ]
        },
        {
          "name": "Giuseppe Jurman",
          "openalex_id": "A5090829168",
          "orcid": "https://orcid.org/0000-0002-2705-5728",
          "institutions": [
            "Fondazione Bruno Kessler"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-02",
      "abstract": null,
      "cited_by_count": 5186,
      "type": "article",
      "source": {
        "name": "BMC Genomics",
        "type": "journal",
        "issn": [
          "1471-2164"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://bmcgenomics.biomedcentral.com/counter/pdf/10.1186/s12864-019-6413-7"
      },
      "topics": [
        "Imbalanced Data Classification Techniques",
        "Text and Document Classification Technologies",
        "Data Mining Algorithms and Applications"
      ],
      "referenced_works_count": 104,
      "url": "https://openalex.org/W2999309192"
    },
    {
      "openalex_id": "W3049006505",
      "doi": "10.3390/rs12162625",
      "title": "Neural Network Training for the Detection and Classification of Oceanic Mesoscale Eddies",
      "authors": [
        {
          "name": "Oliverio J. Santana",
          "openalex_id": "A5051542107",
          "orcid": "https://orcid.org/0000-0001-7511-5783",
          "institutions": [
            "Universidad de Las Palmas de Gran Canaria"
          ]
        },
        {
          "name": "Daniel Hern\u00e1ndez-Sosa",
          "openalex_id": "A5042704590",
          "orcid": "https://orcid.org/0000-0003-3022-7698",
          "institutions": [
            "Universidad de Las Palmas de Gran Canaria"
          ]
        },
        {
          "name": "Jeffrey Martz",
          "openalex_id": "A5003538387",
          "institutions": [
            "Fort Lewis College"
          ]
        },
        {
          "name": "Ryan N. Smith",
          "openalex_id": "A5102817383",
          "orcid": "https://orcid.org/0000-0003-3361-8109",
          "institutions": [
            "Fort Lewis College",
            "Florida International University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-08-14",
      "abstract": "Recent advances in deep learning have made it possible to use neural networks for the detection and classification of oceanic mesoscale eddies from satellite altimetry data. Various neural network models have been proposed in recent years to address this challenge, but they have been trained using different types of input data and evaluated using different performance metrics, making a comparison between them impossible. In this article, we examine the most common dataset and metric choices, by analyzing the reasons for the divergences between them and pointing out the most appropriate choice to obtain a fair evaluation in this scenario. Based on this comparative study, we have developed several neural network models to detect and classify oceanic eddies from satellite images, showing that our most advanced models perform better than the models previously proposed in the literature.",
      "cited_by_count": 25,
      "type": "article",
      "source": {
        "name": "Remote Sensing",
        "type": "journal",
        "issn": [
          "2072-4292"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2072-4292/12/16/2625/pdf?version=1597663121"
      },
      "topics": [
        "Oceanographic and Atmospheric Processes",
        "Ocean Waves and Remote Sensing",
        "Climate variability and models"
      ],
      "referenced_works_count": 45,
      "url": "https://openalex.org/W3049006505"
    },
    {
      "openalex_id": "W4242707237",
      "doi": "10.2139/ssrn.3819799",
      "title": "What's Fair About Individual Fairness?",
      "authors": [
        {
          "name": "Will Fleisher",
          "openalex_id": "A5001864674",
          "orcid": "https://orcid.org/0000-0002-5980-3970",
          "institutions": [
            "Georgetown University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-01-01",
      "abstract": null,
      "cited_by_count": 14,
      "type": "article",
      "source": {
        "name": "SSRN Electronic Journal",
        "type": "repository",
        "issn": [
          "1556-5068"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://doi.org/10.2139/ssrn.3819799"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Free Will and Agency",
        "Psychology of Moral and Emotional Judgment"
      ],
      "referenced_works_count": 48,
      "url": "https://openalex.org/W4242707237"
    },
    {
      "openalex_id": "W4226225589",
      "doi": "10.48550/arxiv.2112.07447",
      "title": "Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in Pretrained Language Models",
      "authors": [
        {
          "name": "Pieter Delobelle",
          "openalex_id": "A5023938338"
        },
        {
          "name": "Ewoenam Kwaku Tokpo",
          "openalex_id": "A5060626938",
          "orcid": "https://orcid.org/0009-0007-9685-2580"
        },
        {
          "name": "Toon Calders",
          "openalex_id": "A5073211270",
          "orcid": "https://orcid.org/0000-0002-4943-6978"
        },
        {
          "name": "Bettina Berendt",
          "openalex_id": "A5072130052",
          "orcid": "https://orcid.org/0000-0002-8003-3413"
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-12-14",
      "abstract": "An increasing awareness of biased patterns in natural language processing resources, like BERT, has motivated many metrics to quantify `bias' and `fairness'. But comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the existing literature on fairness metrics for pretrained language models and experimentally evaluate compatibility, including both biases in language models as in their downstream tasks. We do this by a mixture of traditional literature survey and correlation analysis, as well as by running empirical evaluations. We find that many metrics are not compatible and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, if not at least highly subjective. To improve future comparisons and fairness evaluations, we recommend avoiding embedding-based metrics and focusing on fairness evaluations in downstream tasks.",
      "cited_by_count": 12,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2112.07447"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4226225589"
    },
    {
      "openalex_id": "W4312770475",
      "doi": "10.1109/cvpr52688.2022.00998",
      "title": "Towards Better Understanding Attribution Methods",
      "authors": [
        {
          "name": "Sukrut Rao",
          "openalex_id": "A5089764662",
          "orcid": "https://orcid.org/0000-0001-8896-7619",
          "institutions": [
            "Max Planck Institute for Informatics"
          ]
        },
        {
          "name": "Moritz B\u00f6hle",
          "openalex_id": "A5059074320",
          "orcid": "https://orcid.org/0000-0002-5479-3769",
          "institutions": [
            "Max Planck Institute for Informatics"
          ]
        },
        {
          "name": "Bernt Schiele",
          "openalex_id": "A5051534545",
          "orcid": "https://orcid.org/0000-0001-9683-5237",
          "institutions": [
            "Max Planck Institute for Informatics"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-06-01",
      "abstract": "Deep neural networks are very successful on many vision tasks, but hard to interpret due to their black box nature. To overcome this, various post-hoc attribution methods have been proposed to identify image regions most influential to the models' decisions. Evaluating such methods is challenging since no ground truth attributions exist. We thus propose three novel evaluation schemes to more reliably measure the faithfulness of those methods, to make comparisons between them more fair, and to make visual inspection more systematic. To address faithfulness, we propose a novel evaluation setting (DiFull) in which we carefully control which parts of the input can influence the output in order to distinguish possible from impossible attributions. To address fairness, we note that different methods are applied at different layers, which skews any comparison, and so evaluate all methods on the same layers (ML-Att) and discuss how this impacts their performance on quantitative metrics. For more systematic visualizations, we propose a scheme (AggAtt) to qualitatively evaluate the methods on complete datasets. We use these evaluation schemes to study strengths and shortcomings of some widely used attribution methods. Finally, we propose a post-processing smoothing step that significantly improves the performance of some attribution methods, and discuss its applicability.",
      "cited_by_count": 31,
      "type": "article",
      "source": {
        "name": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Adversarial Robustness in Machine Learning",
        "Advanced Neural Network Applications"
      ],
      "referenced_works_count": 56,
      "url": "https://openalex.org/W4312770475"
    },
    {
      "openalex_id": "W4384524075",
      "doi": "10.1007/s43681-023-00318-0",
      "title": "Navigating fairness measures and trade-offs",
      "authors": [
        {
          "name": "Stefan Buijsman",
          "openalex_id": "A5056736933",
          "orcid": "https://orcid.org/0000-0002-0004-0681",
          "institutions": [
            "Delft University of Technology"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-07-17",
      "abstract": "Abstract To monitor and prevent bias in AI systems, we can use a wide range of (statistical) fairness measures. However, it is mathematically impossible to optimize all of these measures at the same time. In addition, optimizing a fairness measure often greatly reduces the accuracy of the system (Kozodoi et al., Eur J Oper Res 297:1083\u20131094, 2022). As a result, we need a substantive theory that informs us how to make these decisions and for what reasons. I show that by using Rawls\u2019 notion of justice as fairness, we can create a basis for navigating fairness measures and the accuracy trade-off. In particular, this leads to a principled choice focusing on both the most vulnerable groups and the type of fairness measure that has the biggest impact on that group. This also helps to close part of the gap between philosophical accounts of distributive justice and the fairness literature that has been observed by (Kuppler et al. Distributive justice and fairness metrics in automated decision-making: How much overlap is there? arXiv preprint arXiv:2105.01441 , 2021), and to operationalise the value of fairness.",
      "cited_by_count": 13,
      "type": "article",
      "source": {
        "name": "AI and Ethics",
        "type": "journal",
        "issn": [
          "2730-5953",
          "2730-5961"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s43681-023-00318-0.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Neuroethics, Human Enhancement, Biomedical Innovations",
        "Psychology of Moral and Emotional Judgment"
      ],
      "referenced_works_count": 26,
      "url": "https://openalex.org/W4384524075"
    },
    {
      "openalex_id": "W2277195237",
      "doi": "10.1007/s11263-016-0981-7",
      "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
      "authors": [
        {
          "name": "Ranjay Krishna",
          "openalex_id": "A5032451496",
          "orcid": "https://orcid.org/0000-0001-8784-2531",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Yuke Zhu",
          "openalex_id": "A5030826237",
          "orcid": "https://orcid.org/0000-0002-9198-2227",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Oliver Groth",
          "openalex_id": "A5056401069"
        },
        {
          "name": "Justin Johnson",
          "openalex_id": "A5101526625",
          "orcid": "https://orcid.org/0000-0002-1251-088X",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Kenji Hata",
          "openalex_id": "A5028445962",
          "orcid": "https://orcid.org/0000-0003-4367-9343",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Joshua Kravitz",
          "openalex_id": "A5081523557",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Stephanie Chen",
          "openalex_id": "A5100753354",
          "orcid": "https://orcid.org/0000-0002-1244-7462",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Yannis Kalantidis",
          "openalex_id": "A5042126127",
          "institutions": [
            "Yahoo (United States)"
          ]
        },
        {
          "name": "Li-Jia Li",
          "openalex_id": "A5009049500",
          "orcid": "https://orcid.org/0000-0001-5850-7013",
          "institutions": [
            "Snap (United States)"
          ]
        },
        {
          "name": "David A. Shamma",
          "openalex_id": "A5001270504",
          "orcid": "https://orcid.org/0000-0003-2399-9374",
          "institutions": [
            "Centrum Wiskunde & Informatica"
          ]
        },
        {
          "name": "Michael S. Bernstein",
          "openalex_id": "A5076189854",
          "orcid": "https://orcid.org/0000-0001-8020-9434",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Li Fei-Fei",
          "openalex_id": "A5100450462",
          "orcid": "https://orcid.org/0000-0002-7481-0810",
          "institutions": [
            "Stanford University"
          ]
        }
      ],
      "publication_year": 2017,
      "publication_date": "2017-02-06",
      "abstract": null,
      "cited_by_count": 4966,
      "type": "article",
      "source": {
        "name": "International Journal of Computer Vision",
        "type": "journal",
        "issn": [
          "0920-5691",
          "1573-1405"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007%2Fs11263-016-0981-7.pdf"
      },
      "topics": [
        "Multimodal Machine Learning Applications",
        "Image Retrieval and Classification Techniques",
        "Advanced Image and Video Retrieval Techniques"
      ],
      "referenced_works_count": 132,
      "url": "https://openalex.org/W2277195237"
    },
    {
      "openalex_id": "W2296283641",
      "doi": "10.18653/v1/n16-1030",
      "title": "Neural Architectures for Named Entity Recognition",
      "authors": [
        {
          "name": "Guillaume Lample",
          "openalex_id": "A5054371148",
          "institutions": [
            "Carnegie Mellon University",
            "Pompeu Fabra University"
          ]
        },
        {
          "name": "Miguel Ballesteros",
          "openalex_id": "A5101731788",
          "orcid": "https://orcid.org/0000-0003-3949-0361",
          "institutions": [
            "Carnegie Mellon University",
            "Pompeu Fabra University"
          ]
        },
        {
          "name": "Sandeep Subramanian",
          "openalex_id": "A5013965909",
          "orcid": "https://orcid.org/0000-0002-5972-1588",
          "institutions": [
            "Carnegie Mellon University",
            "Pompeu Fabra University"
          ]
        },
        {
          "name": "Kazuya Kawakami",
          "openalex_id": "A5060303231",
          "institutions": [
            "Carnegie Mellon University",
            "Pompeu Fabra University"
          ]
        },
        {
          "name": "Chris Dyer",
          "openalex_id": "A5111222692",
          "institutions": [
            "Carnegie Mellon University",
            "Pompeu Fabra University"
          ]
        }
      ],
      "publication_year": 2016,
      "publication_date": "2016-01-01",
      "abstract": "Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",
      "cited_by_count": 4325,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.aclweb.org/anthology/N16-1030.pdf"
      },
      "topics": [
        "Topic Modeling",
        "Natural Language Processing Techniques",
        "Text Readability and Simplification"
      ],
      "referenced_works_count": 48,
      "url": "https://openalex.org/W2296283641"
    },
    {
      "openalex_id": "W4302307280",
      "doi": "10.48550/arxiv.2210.01725",
      "title": "MEDFAIR: Benchmarking Fairness for Medical Imaging",
      "authors": [
        {
          "name": "Yongshuo Zong",
          "openalex_id": "A5033350433",
          "orcid": "https://orcid.org/0000-0002-3267-753X"
        },
        {
          "name": "Yongxin Yang",
          "openalex_id": "A5101445001",
          "orcid": "https://orcid.org/0000-0003-4134-8559"
        },
        {
          "name": "Timothy M. Hospedales",
          "openalex_id": "A5087823932",
          "orcid": "https://orcid.org/0000-0003-4867-7486"
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-10-04",
      "abstract": "A multitude of work has shown that machine learning-based medical diagnosis systems can be biased against certain subgroups of people. This has motivated a growing number of bias mitigation algorithms that aim to address fairness issues in machine learning. However, it is difficult to compare their effectiveness in medical imaging for two reasons. First, there is little consensus on the criteria to assess fairness. Second, existing bias mitigation algorithms are developed under different settings, e.g., datasets, model selection strategies, backbones, and fairness metrics, making a direct comparison and evaluation based on existing results impossible. In this work, we introduce MEDFAIR, a framework to benchmark the fairness of machine learning models for medical imaging. MEDFAIR covers eleven algorithms from various categories, nine datasets from different imaging modalities, and three model selection criteria. Through extensive experiments, we find that the under-studied issue of model selection criterion can have a significant impact on fairness outcomes; while in contrast, state-of-the-art bias mitigation algorithms do not significantly improve fairness outcomes over empirical risk minimization (ERM) in both in-distribution and out-of-distribution settings. We evaluate fairness from various perspectives and make recommendations for different medical application scenarios that require different ethical principles. Our framework provides a reproducible and easy-to-use entry point for the development and evaluation of future bias mitigation algorithms in deep learning. Code is available at https://github.com/ys-zong/MEDFAIR.",
      "cited_by_count": 14,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2210.01725"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Health Systems, Economic Evaluations, Quality of Life",
        "Healthcare cost, quality, practices"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4302307280"
    },
    {
      "openalex_id": "W4394650867",
      "doi": "10.48550/arxiv.2007.06024",
      "title": "The Impossibility Theorem of Machine Fairness -- A Causal Perspective",
      "authors": [
        {
          "name": "Kailash Karthik Saravanakumar",
          "openalex_id": "A5103967108"
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-07-12",
      "abstract": "With the increasing pervasive use of machine learning in social and economic settings, there has been an interest in the notion of machine bias in the AI community. Models trained on historic data reflect biases that exist in society and propagated them to the future through their decisions. There are three prominent metrics of machine fairness used in the community, and it has been shown statistically that it is impossible to satisfy them all at the same time. This has led to an ambiguity with regards to the definition of fairness. In this report, a causal perspective to the impossibility theorem of fairness is presented along with a causal goal for machine fairness.",
      "cited_by_count": 6,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2007.06024"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Blockchain Technology Applications and Security"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4394650867"
    },
    {
      "openalex_id": "W2804047627",
      "doi": "10.1109/tmi.2018.2837502",
      "title": "Deep Learning Techniques for Automatic MRI Cardiac Multi-Structures Segmentation and Diagnosis: Is the Problem Solved?",
      "authors": [
        {
          "name": "Olivier Bernard",
          "openalex_id": "A5037356492",
          "orcid": "https://orcid.org/0000-0003-0752-9946",
          "institutions": [
            "Lyon College",
            "Inserm",
            "Centre National de la Recherche Scientifique",
            "Institut National des Sciences Appliqu\u00e9es de Lyon",
            "Universit\u00e9 Claude Bernard Lyon 1"
          ]
        },
        {
          "name": "Alain Lalande",
          "openalex_id": "A5014239609",
          "orcid": "https://orcid.org/0000-0002-7970-366X",
          "institutions": [
            "Centre National de la Recherche Scientifique",
            "Laboratoire d\u2019\u00c9lectronique, Informatique et Image"
          ]
        },
        {
          "name": "Cl\u00e9ment Zotti",
          "openalex_id": "A5046041670",
          "orcid": "https://orcid.org/0000-0002-0713-9924",
          "institutions": [
            "Universit\u00e9 de Sherbrooke"
          ]
        },
        {
          "name": "Frederick Cervenansky",
          "openalex_id": "A5049515606",
          "institutions": [
            "Inserm",
            "Centre National de la Recherche Scientifique",
            "Lyon College",
            "Universit\u00e9 Claude Bernard Lyon 1",
            "D\u00e9partement d'Informatique",
            "Institut National des Sciences Appliqu\u00e9es de Lyon"
          ]
        },
        {
          "name": "Xin Yang",
          "openalex_id": "A5017262410",
          "orcid": "https://orcid.org/0000-0003-4653-6524",
          "institutions": [
            "Chinese University of Hong Kong"
          ]
        },
        {
          "name": "Pheng\u2010Ann Heng",
          "openalex_id": "A5032708386",
          "orcid": "https://orcid.org/0000-0003-3055-5034",
          "institutions": [
            "Chinese University of Hong Kong"
          ]
        },
        {
          "name": "Irem Cetin",
          "openalex_id": "A5064494071",
          "orcid": "https://orcid.org/0000-0002-6911-2858",
          "institutions": [
            "Pompeu Fabra University"
          ]
        },
        {
          "name": "Karim Lekadir",
          "openalex_id": "A5078391768",
          "orcid": "https://orcid.org/0000-0002-9456-1612",
          "institutions": [
            "Pompeu Fabra University"
          ]
        },
        {
          "name": "\u00d3scar C\u00e1mara",
          "openalex_id": "A5077843387",
          "orcid": "https://orcid.org/0000-0002-5125-6132",
          "institutions": [
            "Pompeu Fabra University"
          ]
        },
        {
          "name": "Miguel \u00c1. Gonz\u00e1lez Ballester",
          "openalex_id": "A5021143022",
          "orcid": "https://orcid.org/0000-0002-9227-6826",
          "institutions": [
            "Instituci\u00f3 Catalana de Recerca i Estudis Avan\u00e7ats",
            "Pompeu Fabra University"
          ]
        },
        {
          "name": "Gerard Sanrom\u00e0",
          "openalex_id": "A5054220631",
          "orcid": "https://orcid.org/0000-0002-4450-8311",
          "institutions": [
            "Pompeu Fabra University"
          ]
        },
        {
          "name": "Sandy Napel",
          "openalex_id": "A5056555647",
          "orcid": "https://orcid.org/0000-0002-6876-5507",
          "institutions": [
            "Stanford University",
            "Stanford Medicine"
          ]
        },
        {
          "name": "Steffen E. Petersen",
          "openalex_id": "A5076800816",
          "orcid": "https://orcid.org/0000-0003-4622-5160",
          "institutions": [
            "William Harvey Research Institute",
            "Queen Mary University of London"
          ]
        },
        {
          "name": "Georgios Tziritas",
          "openalex_id": "A5076710234",
          "orcid": "https://orcid.org/0000-0002-1802-1825",
          "institutions": [
            "University of Crete"
          ]
        },
        {
          "name": "Elias Grinias",
          "openalex_id": "A5001638626",
          "institutions": [
            "University of Crete"
          ]
        },
        {
          "name": "Mahendra Khened",
          "openalex_id": "A5066090465",
          "orcid": "https://orcid.org/0000-0003-0841-6628",
          "institutions": [
            "University of Madras",
            "Indian Institute of Technology Madras"
          ]
        },
        {
          "name": "Varghese Alex Kollerathu",
          "openalex_id": "A5005933752",
          "orcid": "https://orcid.org/0000-0001-5095-2358",
          "institutions": [
            "University of Madras",
            "Indian Institute of Technology Madras"
          ]
        },
        {
          "name": "Ganapathy Krishnamurthi",
          "openalex_id": "A5048838919",
          "orcid": "https://orcid.org/0000-0002-9262-7569",
          "institutions": [
            "University of Madras",
            "Indian Institute of Technology Madras"
          ]
        },
        {
          "name": "Marc-Michel Roh\u00e9",
          "openalex_id": "A5024644493",
          "institutions": [
            "Institut national de recherche en informatique et en automatique"
          ]
        },
        {
          "name": "Xavier Pennec",
          "openalex_id": "A5003253553",
          "orcid": "https://orcid.org/0000-0002-6617-7664",
          "institutions": [
            "Institut national de recherche en informatique et en automatique"
          ]
        },
        {
          "name": "Maxime Sermesant",
          "openalex_id": "A5070013751",
          "orcid": "https://orcid.org/0000-0002-6256-8350",
          "institutions": [
            "Institut national de recherche en informatique et en automatique"
          ]
        },
        {
          "name": "Fabian Isensee",
          "openalex_id": "A5072647800",
          "orcid": "https://orcid.org/0000-0002-3519-5886",
          "institutions": [
            "Heidelberg University",
            "German Cancer Research Center"
          ]
        },
        {
          "name": "Paul F. J\u00e4ger",
          "openalex_id": "A5012942347",
          "orcid": "https://orcid.org/0000-0002-6243-2568",
          "institutions": [
            "German Cancer Research Center",
            "Heidelberg University"
          ]
        },
        {
          "name": "Klaus Maier\u2010Hein",
          "openalex_id": "A5027292126",
          "orcid": "https://orcid.org/0000-0002-6626-2463",
          "institutions": [
            "German Cancer Research Center",
            "Heidelberg University"
          ]
        },
        {
          "name": "Peter M. Full",
          "openalex_id": "A5041176403",
          "orcid": "https://orcid.org/0000-0003-4326-8026",
          "institutions": [
            "University Hospital Heidelberg",
            "Heidelberg University",
            "Mannheim University of Applied Sciences"
          ]
        },
        {
          "name": "Ivo Wolf",
          "openalex_id": "A5029228546",
          "orcid": "https://orcid.org/0000-0002-6519-6484",
          "institutions": [
            "Mannheim University of Applied Sciences"
          ]
        },
        {
          "name": "Sandy Engelhardt",
          "openalex_id": "A5088137293",
          "orcid": "https://orcid.org/0000-0001-8816-7654",
          "institutions": [
            "Mannheim University of Applied Sciences"
          ]
        },
        {
          "name": "Christian F. Baumgartner",
          "openalex_id": "A5006688931",
          "orcid": "https://orcid.org/0000-0002-3629-4384",
          "institutions": [
            "ETH Zurich"
          ]
        },
        {
          "name": "Lisa M. Koch",
          "openalex_id": "A5056603838",
          "orcid": "https://orcid.org/0000-0003-4377-7074",
          "institutions": [
            "Board of the Swiss Federal Institutes of Technology",
            "ZTE (United States)",
            "ETH Zurich"
          ]
        },
        {
          "name": "Jelmer M. Wolterink",
          "openalex_id": "A5028639654",
          "orcid": "https://orcid.org/0000-0001-5505-475X",
          "institutions": [
            "University Hospital Heidelberg",
            "Heidelberg University",
            "University Medical Center Utrecht"
          ]
        },
        {
          "name": "Ivana I\u0161gum",
          "openalex_id": "A5084070018",
          "orcid": "https://orcid.org/0000-0003-1869-5034",
          "institutions": [
            "University Hospital Heidelberg",
            "Heidelberg University",
            "University Medical Center Utrecht"
          ]
        },
        {
          "name": "Yeonggul Jang",
          "openalex_id": "A5080061451",
          "orcid": "https://orcid.org/0000-0002-5805-7494",
          "institutions": [
            "Yonsei University"
          ]
        },
        {
          "name": "Yoonmi Hong",
          "openalex_id": "A5003629260",
          "orcid": "https://orcid.org/0000-0003-2416-8249",
          "institutions": [
            "Yonsei University"
          ]
        },
        {
          "name": "Jay Patravali",
          "openalex_id": "A5079728784",
          "institutions": [
            "Advanced Centre for Treatment, Research and Education in Cancer"
          ]
        },
        {
          "name": "Shubham Jain",
          "openalex_id": "A5001915349",
          "orcid": "https://orcid.org/0000-0002-7978-9504",
          "institutions": [
            "Advanced Centre for Treatment, Research and Education in Cancer"
          ]
        },
        {
          "name": "Olivier Humbert",
          "openalex_id": "A5053536156",
          "orcid": "https://orcid.org/0000-0003-4440-5416",
          "institutions": [
            "Universit\u00e9 C\u00f4te d'Azur"
          ]
        },
        {
          "name": "Pierre\u2010Marc Jodoin",
          "openalex_id": "A5086581005",
          "orcid": "https://orcid.org/0000-0002-6038-5753",
          "institutions": [
            "Universit\u00e9 de Sherbrooke"
          ]
        }
      ],
      "publication_year": 2018,
      "publication_date": "2018-05-17",
      "abstract": "Delineation of the left ventricular cavity, myocardium, and right ventricle from cardiac magnetic resonance images (multi-slice 2-D cine MRI) is a common clinical task to establish diagnosis. The automation of the corresponding tasks has thus been the subject of intense research over the past decades. In this paper, we introduce the \"Automatic Cardiac Diagnosis Challenge\" dataset (ACDC), the largest publicly available and fully annotated dataset for the purpose of cardiac MRI (CMR) assessment. The dataset contains data from 150 multi-equipments CMRI recordings with reference measurements and classification from two medical experts. The overarching objective of this paper is to measure how far state-of-the-art deep learning methods can go at assessing CMRI, i.e., segmenting the myocardium and the two ventricles as well as classifying pathologies. In the wake of the 2017 MICCAI-ACDC challenge, we report results from deep learning methods provided by nine research groups for the segmentation task and four groups for the classification task. Results show that the best methods faithfully reproduce the expert analysis, leading to a mean value of 0.97 correlation score for the automatic extraction of clinical indices and an accuracy of 0.96 for automatic diagnosis. These results clearly open the door to highly accurate and fully automatic analysis of cardiac CMRI. We also identify scenarios for which deep learning methods are still failing. Both the dataset and detailed results are publicly available online, while the platform will remain open for new submissions.",
      "cited_by_count": 1982,
      "type": "article",
      "source": {
        "name": "IEEE Transactions on Medical Imaging",
        "type": "journal",
        "issn": [
          "0278-0062",
          "1558-254X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/42264/1/Bernard%20et%20al_Deep%20learning%20techniques%20automatic%20MRI_2018_Accepted.pdf"
      },
      "topics": [
        "Cardiac Imaging and Diagnostics",
        "Cardiac Valve Diseases and Treatments",
        "Cardiac Structural Anomalies and Repair"
      ],
      "referenced_works_count": 76,
      "url": "https://openalex.org/W2804047627"
    },
    {
      "openalex_id": "W2969625533",
      "doi": "10.1016/j.ijinfomgt.2019.08.002",
      "title": "Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy",
      "authors": [
        {
          "name": "Yogesh K. Dwivedi",
          "openalex_id": "A5048622877",
          "orcid": "https://orcid.org/0000-0002-5547-9990",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Laurie Hughes",
          "openalex_id": "A5072512285",
          "orcid": "https://orcid.org/0000-0002-0956-0608",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Elvira Ismagilova",
          "openalex_id": "A5033472055",
          "orcid": "https://orcid.org/0000-0001-9634-194X",
          "institutions": [
            "University of Bradford"
          ]
        },
        {
          "name": "Gert Aarts",
          "openalex_id": "A5027512483",
          "orcid": "https://orcid.org/0000-0002-6038-3782",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Crispin Coombs",
          "openalex_id": "A5018731131",
          "orcid": "https://orcid.org/0000-0002-4203-9291",
          "institutions": [
            "Loughborough University"
          ]
        },
        {
          "name": "Tom Crick",
          "openalex_id": "A5015452463",
          "orcid": "https://orcid.org/0000-0001-5196-9389",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Yanqing Duan",
          "openalex_id": "A5032994640",
          "orcid": "https://orcid.org/0000-0003-4205-7403",
          "institutions": [
            "University of Bedfordshire"
          ]
        },
        {
          "name": "Rohita Dwivedi",
          "openalex_id": "A5070017829",
          "orcid": "https://orcid.org/0000-0003-3801-3635",
          "institutions": [
            "Prin. L. N. Welingkar Institute of Management Development and Research"
          ]
        },
        {
          "name": "John S. Edwards",
          "openalex_id": "A5007695310",
          "orcid": "https://orcid.org/0000-0003-3979-017X",
          "institutions": [
            "Aston University"
          ]
        },
        {
          "name": "Aled Eirug",
          "openalex_id": "A5000411910",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Vassilis Galanos",
          "openalex_id": "A5015685463",
          "orcid": "https://orcid.org/0000-0002-8363-4855",
          "institutions": [
            "University of Edinburgh"
          ]
        },
        {
          "name": "P. Vigneswara Ilavarasan",
          "openalex_id": "A5004125306",
          "orcid": "https://orcid.org/0000-0002-9431-3520",
          "institutions": [
            "Indian Institute of Technology Delhi"
          ]
        },
        {
          "name": "Marijn Janssen",
          "openalex_id": "A5062073470",
          "orcid": "https://orcid.org/0000-0001-6211-8790",
          "institutions": [
            "Delft University of Technology"
          ]
        },
        {
          "name": "Paul Jones",
          "openalex_id": "A5080152304",
          "orcid": "https://orcid.org/0000-0003-0417-9143",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Arpan Kumar Kar",
          "openalex_id": "A5061235109",
          "orcid": "https://orcid.org/0000-0003-4186-4887",
          "institutions": [
            "Indian Institute of Technology Delhi"
          ]
        },
        {
          "name": "Hatice Kizgin",
          "openalex_id": "A5037522059",
          "orcid": "https://orcid.org/0000-0003-0841-8973",
          "institutions": [
            "University of Bradford"
          ]
        },
        {
          "name": "Bianca Kronemann",
          "openalex_id": "A5037724665",
          "orcid": "https://orcid.org/0009-0002-5146-537X",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Banita Lal",
          "openalex_id": "A5103045218",
          "orcid": "https://orcid.org/0000-0003-1340-1746",
          "institutions": [
            "University of Bedfordshire"
          ]
        },
        {
          "name": "Biagio Lucini",
          "openalex_id": "A5003777616",
          "orcid": "https://orcid.org/0000-0001-8974-8266",
          "institutions": [
            "Swansea University",
            "Foundry (United Kingdom)"
          ]
        },
        {
          "name": "Rony Medaglia",
          "openalex_id": "A5003475331",
          "orcid": "https://orcid.org/0000-0001-7292-5895",
          "institutions": [
            "Copenhagen Business School"
          ]
        },
        {
          "name": "Kenneth Le Meunier\u2010FitzHugh",
          "openalex_id": "A5059234800",
          "institutions": [
            "University of East Anglia"
          ]
        },
        {
          "name": "Leslie Caroline Le Meunier-FitzHugh",
          "openalex_id": "A5076965699"
        },
        {
          "name": "Santosh K. Misra",
          "openalex_id": "A5018760782",
          "orcid": "https://orcid.org/0000-0002-3313-4895",
          "institutions": [
            "University of East Anglia",
            "Government of Tamil Nadu"
          ]
        },
        {
          "name": "Emmanuel Mogaji",
          "openalex_id": "A5082000265",
          "orcid": "https://orcid.org/0000-0003-0544-4842",
          "institutions": [
            "University of Greenwich"
          ]
        },
        {
          "name": "Sujeet Kumar Sharma",
          "openalex_id": "A5037836435",
          "orcid": "https://orcid.org/0000-0003-3614-9053"
        },
        {
          "name": "Jang Bahadur Singh",
          "openalex_id": "A5040405430",
          "orcid": "https://orcid.org/0000-0001-7017-1989",
          "institutions": [
            "Indian Institute of Management Tiruchirappalli"
          ]
        },
        {
          "name": "Vishnupriya Raghavan",
          "openalex_id": "A5050603102"
        },
        {
          "name": "Ramakrishnan Raman",
          "openalex_id": "A5072261829",
          "orcid": "https://orcid.org/0000-0003-3642-6989",
          "institutions": [
            "Symbiosis International University"
          ]
        },
        {
          "name": "Nripendra P. Rana",
          "openalex_id": "A5034248834",
          "orcid": "https://orcid.org/0000-0003-1105-8729",
          "institutions": [
            "University of Bradford"
          ]
        },
        {
          "name": "Spyridon Samothrakis",
          "openalex_id": "A5033216620",
          "orcid": "https://orcid.org/0000-0003-1902-9690",
          "institutions": [
            "University of Essex"
          ]
        },
        {
          "name": "Jak Spencer",
          "openalex_id": "A5016998336",
          "institutions": [
            "Training Programs in Epidemiology and Public Health Interventions Network"
          ]
        },
        {
          "name": "Kuttimani Tamilmani",
          "openalex_id": "A5008267619",
          "orcid": "https://orcid.org/0000-0002-9615-1465",
          "institutions": [
            "University of Bradford"
          ]
        },
        {
          "name": "Annie Tubadji",
          "openalex_id": "A5070089201",
          "orcid": "https://orcid.org/0000-0002-6134-3520",
          "institutions": [
            "University of the West of England"
          ]
        },
        {
          "name": "Paul Walton",
          "openalex_id": "A5109157215",
          "institutions": [
            "Capgemini (United Kingdom)",
            "Swansea University"
          ]
        },
        {
          "name": "Michael D. Williams",
          "openalex_id": "A5002569790",
          "orcid": "https://orcid.org/0000-0002-3047-0332",
          "institutions": [
            "Swansea University"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-08-27",
      "abstract": null,
      "cited_by_count": 3558,
      "type": "article",
      "source": {
        "name": "International Journal of Information Management",
        "type": "journal",
        "issn": [
          "0268-4012",
          "1873-4707"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.sciencedirect.com/science/article/pii/S026840121930917X"
      },
      "topics": [
        "Big Data and Business Intelligence",
        "Economic and Technological Systems Analysis",
        "Digital Transformation in Industry"
      ],
      "referenced_works_count": 338,
      "url": "https://openalex.org/W2969625533"
    },
    {
      "openalex_id": "W4404182570",
      "doi": "10.1016/j.patter.2024.101080",
      "title": "Benchmark suites instead of leaderboards for evaluating AI fairness",
      "authors": [
        {
          "name": "Angelina Wang",
          "openalex_id": "A5002575847",
          "orcid": "https://orcid.org/0000-0001-9140-3523",
          "institutions": [
            "Stanford University",
            "Princeton University"
          ]
        },
        {
          "name": "Aaron Hertzmann",
          "openalex_id": "A5061237128",
          "orcid": "https://orcid.org/0000-0001-9667-0292",
          "institutions": [
            "Adobe Systems (United States)"
          ]
        },
        {
          "name": "Olga Russakovsky",
          "openalex_id": "A5022811687",
          "orcid": "https://orcid.org/0000-0001-5272-3241",
          "institutions": [
            "Princeton University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-11-01",
      "abstract": null,
      "cited_by_count": 10,
      "type": "review",
      "source": {
        "name": "Patterns",
        "type": "journal",
        "issn": [
          "2666-3899"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://doi.org/10.1016/j.patter.2024.101080"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Psychology of Moral and Emotional Judgment",
        "Neuroethics, Human Enhancement, Biomedical Innovations"
      ],
      "referenced_works_count": 96,
      "url": "https://openalex.org/W4404182570"
    },
    {
      "openalex_id": "W2897154134",
      "doi": "10.1145/3287560.3287598",
      "title": "Fairness and Abstraction in Sociotechnical Systems",
      "authors": [
        {
          "name": "Andrew D. Selbst",
          "openalex_id": "A5088974481",
          "institutions": [
            "Data & Society Research Institute"
          ]
        },
        {
          "name": "danah boyd",
          "openalex_id": "A5009029178",
          "orcid": "https://orcid.org/0000-0002-7722-7778",
          "institutions": [
            "Microsoft (United States)",
            "Data & Society Research Institute"
          ]
        },
        {
          "name": "Sorelle A. Friedler",
          "openalex_id": "A5086384189",
          "orcid": "https://orcid.org/0000-0001-6023-1597",
          "institutions": [
            "Haverford College"
          ]
        },
        {
          "name": "Suresh Venkatasubramanian",
          "openalex_id": "A5061790878",
          "orcid": "https://orcid.org/0000-0001-7679-7130",
          "institutions": [
            "University of Utah"
          ]
        },
        {
          "name": "Janet Vertesi",
          "openalex_id": "A5003703208",
          "orcid": "https://orcid.org/0000-0003-4579-6252",
          "institutions": [
            "Princeton University"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-01-09",
      "abstract": "A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce \"fair\" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five \"traps\" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.",
      "cited_by_count": 1097,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3287560.3287598"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Innovative Human-Technology Interaction",
        "Open Source Software Innovations"
      ],
      "referenced_works_count": 73,
      "url": "https://openalex.org/W2897154134"
    },
    {
      "openalex_id": "W4386852346",
      "doi": "10.1101/2023.09.18.23295660",
      "title": "Measuring and Reducing Racial Bias in a Pediatric Urinary Tract Infection Model",
      "authors": [
        {
          "name": "J. W. Anderson",
          "openalex_id": "A5068174974",
          "orcid": "https://orcid.org/0000-0003-1543-6491",
          "institutions": [
            "University of Pittsburgh",
            "Intelligent Systems Research (United States)"
          ]
        },
        {
          "name": "Nader Shaikh",
          "openalex_id": "A5090998275",
          "orcid": "https://orcid.org/0000-0002-1602-343X",
          "institutions": [
            "University of Pittsburgh"
          ]
        },
        {
          "name": "Shyam Visweswaran",
          "openalex_id": "A5036362467",
          "orcid": "https://orcid.org/0000-0002-2079-8684",
          "institutions": [
            "University of Pittsburgh"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-09-18",
      "abstract": "Abstract Clinical predictive models that include race as a predictor have the potential to exacerbate disparities in healthcare. Such models can be respecified to exclude race or optimized to reduce racial bias. We investigated the impact of such respecifications in a predictive model \u2013 UTICalc \u2013 which was designed to reduce catheterizations in young children with suspected urinary tract infections. To reduce racial bias, race was removed from the UTICalc logistic regression model and replaced with two new features. We compared the two versions of UTICalc using fairness and predictive performance metrics to understand the effects on racial bias. In addition, we derived three new models for UTICalc to specifically improve racial fairness. Our results show that, as predicted by previously described impossibility results, fairness cannot be simultaneously improved on all fairness metrics, and model respecification may improve racial fairness but decrease overall predictive performance.",
      "cited_by_count": 4,
      "type": "preprint",
      "source": {
        "name": "bioRxiv (Cold Spring Harbor Laboratory)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://www.medrxiv.org/content/medrxiv/early/2023/09/18/2023.09.18.23295660.full.pdf"
      },
      "topics": [
        "Healthcare Policy and Management",
        "Healthcare cost, quality, practices",
        "Health and Conflict Studies"
      ],
      "referenced_works_count": 23,
      "url": "https://openalex.org/W4386852346"
    },
    {
      "openalex_id": "W2770598799",
      "doi": "10.1371/journal.pgen.1007081",
      "title": "Orienting the causal relationship between imprecisely measured traits using GWAS summary data",
      "authors": [
        {
          "name": "Gibran Hemani",
          "openalex_id": "A5016515762",
          "orcid": "https://orcid.org/0000-0003-0920-1055",
          "institutions": [
            "University of Bristol"
          ]
        },
        {
          "name": "Kate Tilling",
          "openalex_id": "A5022945304",
          "orcid": "https://orcid.org/0000-0002-1010-8926",
          "institutions": [
            "University of Bristol"
          ]
        },
        {
          "name": "George Davey Smith",
          "openalex_id": "A5013184354",
          "orcid": "https://orcid.org/0000-0002-1407-8314",
          "institutions": [
            "University of Bristol"
          ]
        }
      ],
      "publication_year": 2017,
      "publication_date": "2017-11-17",
      "abstract": "Inference about the causal structure that induces correlations between two traits can be achieved by combining genetic associations with a mediation-based approach, as is done in the causal inference test (CIT). However, we show that measurement error in the phenotypes can lead to the CIT inferring the wrong causal direction, and that increasing sample sizes has the adverse effect of increasing confidence in the wrong answer. This problem is likely to be general to other mediation-based approaches. Here we introduce an extension to Mendelian randomisation, a method that uses genetic associations in an instrumentation framework, that enables inference of the causal direction between traits, with some advantages. First, it can be performed using only summary level data from genome-wide association studies; second, it is less susceptible to bias in the presence of measurement error or unmeasured confounding. We apply the method to infer the causal direction between DNA methylation and gene expression levels. Our results demonstrate that, in general, DNA methylation is more likely to be the causal factor, but this result is highly susceptible to bias induced by systematic differences in measurement error between the platforms, and by horizontal pleiotropy. We emphasise that, where possible, implementing MR and appropriate sensitivity analyses alongside other approaches such as CIT is important to triangulate reliable conclusions about causality.",
      "cited_by_count": 2591,
      "type": "article",
      "source": {
        "name": "PLoS Genetics",
        "type": "journal",
        "issn": [
          "1553-7390",
          "1553-7404"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://journals.plos.org/plosgenetics/article/file?id=10.1371/journal.pgen.1007081&type=printable"
      },
      "topics": [
        "Genetic Associations and Epidemiology",
        "Gene expression and cancer classification",
        "Genetic Mapping and Diversity in Plants and Animals"
      ],
      "referenced_works_count": 59,
      "url": "https://openalex.org/W2770598799"
    },
    {
      "openalex_id": "W2982580298",
      "doi": "10.1186/s12916-019-1426-2",
      "title": "Key challenges for delivering clinical impact with artificial intelligence",
      "authors": [
        {
          "name": "Christopher Kelly",
          "openalex_id": "A5026540467",
          "orcid": "https://orcid.org/0000-0002-1246-844X",
          "institutions": [
            "Google (United Kingdom)"
          ]
        },
        {
          "name": "Alan Karthikesalingam",
          "openalex_id": "A5003509342",
          "orcid": "https://orcid.org/0000-0001-5074-898X",
          "institutions": [
            "Google (United Kingdom)"
          ]
        },
        {
          "name": "Mustafa Suleyman",
          "openalex_id": "A5001712647",
          "orcid": "https://orcid.org/0000-0002-5415-4457",
          "institutions": [
            "DeepMind (United Kingdom)"
          ]
        },
        {
          "name": "Greg S. Corrado",
          "openalex_id": "A5068955381",
          "orcid": "https://orcid.org/0000-0001-8817-0992",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Dominic King",
          "openalex_id": "A5009550607",
          "orcid": "https://orcid.org/0000-0002-1898-842X",
          "institutions": [
            "Google (United Kingdom)"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-10-29",
      "abstract": null,
      "cited_by_count": 2035,
      "type": "article",
      "source": {
        "name": "BMC Medicine",
        "type": "journal",
        "issn": [
          "1741-7015"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://bmcmedicine.biomedcentral.com/track/pdf/10.1186/s12916-019-1426-2"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Healthcare cost, quality, practices",
        "Machine Learning in Healthcare"
      ],
      "referenced_works_count": 98,
      "url": "https://openalex.org/W2982580298"
    },
    {
      "openalex_id": "W3038747585",
      "doi": "10.48550/arxiv.2007.02890",
      "title": "Fairness in machine learning: against false positive rate equality as a measure of fairness",
      "authors": [
        {
          "name": "Robert R. Long",
          "openalex_id": "A5009824942"
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-07-06",
      "abstract": "As machine learning informs increasingly consequential decisions, different metrics have been proposed for measuring algorithmic bias or unfairness. Two popular fairness measures are calibration and equality of false positive rate. Each measure seems intuitively important, but notably, it is usually impossible to satisfy both measures. For this reason, a large literature in machine learning speaks of a fairness tradeoff between these two measures. This framing assumes that both measures are, in fact, capturing something important. To date, philosophers have not examined this crucial assumption, and examined to what extent each measure actually tracks a normatively important property. This makes this inevitable statistical conflict, between calibration and false positive rate equality, an important topic for ethics. In this paper, I give an ethical framework for thinking about these measures and argue that, contrary to initial appearances, false positive rate equality does not track anything about fairness, and thus sets an incoherent standard for evaluating the fairness of algorithms.",
      "cited_by_count": 6,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2007.02890"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 23,
      "url": "https://openalex.org/W3038747585"
    },
    {
      "openalex_id": "W4248654407",
      "doi": "10.31219/osf.io/6qzxc",
      "title": "Datacenter Traffic Control: Understanding Techniques and Trade-offs",
      "authors": [
        {
          "name": "Mohammad Noormohammadpour",
          "openalex_id": "A5033259065",
          "orcid": "https://orcid.org/0000-0002-9602-4490",
          "institutions": [
            "University of Southern California"
          ]
        },
        {
          "name": "C.S. Raghavendra",
          "openalex_id": "A5112293594",
          "institutions": [
            "University of Southern California"
          ]
        }
      ],
      "publication_year": 2017,
      "publication_date": "2017-12-08",
      "abstract": "Datacenters provide cost-effective and flexible access to scalable compute and storage resources necessary for today\u2019s cloud computing needs. A typical datacenter is made up of thousands of servers connected with a large network and usually managed by one operator. To provide quality access to the variety of applications and services hosted on datacenters and maximize performance, it deems necessary to use datacenter networks effectively and efficiently. Datacenter traffic is often a mix of several classes with different priorities and requirements. This includes user-generated interactive traffic, traffic with deadlines, and long-running traffic. To this end, custom transport protocols and traffic management techniques have been developed to improve datacenter network performance. In this tutorial paper, we review the general architecture of datacenter networks, various topologies proposed for them, their traffic properties, general traffic control challenges in datacenters and general traffic control objectives. The purpose of this paper is to bring out the important characteristics of traffic control in datacenters and not to survey all existing solutions (as it is virtually impossible due to massive body of existing research). We hope to provide readers with a wide range of options and factors while considering a variety of traffic control mechanisms. We discuss various characteristics of datacenter traffic control, including management schemes, transmission control, traffic shaping, prioritization, load balancing, multipathing, and traffic scheduling. Next, we point to several open challenges as well as new and interesting networking paradigms. At the end of this paper, we briefly review inter-datacenter networks that connect geographically dispersed datacenters, which have been receiving increasing attention recently and pose interesting and novel research problems. To measure the performance of datacenter networks, different performance metrics have been used, such as flow completion times, deadline miss rate, throughput, and fairness. Depending on the application and user requirements, some metrics may need more attention. While investigating different traffic control techniques, we point out the tradeoffs involved in terms of costs, complexity, and performance. We find that a combination of different traffic control techniques may be necessary at particular entities and layers in the network to improve the variety of performance metrics. We also find that despite significant research efforts, there are still open problems that demand further attention from the research community.",
      "cited_by_count": 19,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Cloud Computing and Resource Management",
        "Software-Defined Networks and 5G",
        "Software System Performance and Reliability"
      ],
      "referenced_works_count": 168,
      "url": "https://openalex.org/W4248654407"
    },
    {
      "openalex_id": "W4291201565",
      "doi": "10.1016/j.egyr.2022.07.175",
      "title": "Comparative meta-analysis of desalination and atmospheric water harvesting technologies based on the minimum energy of separation",
      "authors": [
        {
          "name": "Trevor Hocksun Kwan",
          "openalex_id": "A5029305782",
          "orcid": "https://orcid.org/0000-0003-2239-2650",
          "institutions": [
            "Sun Yat-sen University"
          ]
        },
        {
          "name": "Shuang Yuan",
          "openalex_id": "A5101517273",
          "orcid": "https://orcid.org/0000-0003-0379-5198",
          "institutions": [
            "Hong Kong Polytechnic University"
          ]
        },
        {
          "name": "Yongting Shen",
          "openalex_id": "A5108735518",
          "institutions": [
            "University of Science and Technology of China"
          ]
        },
        {
          "name": "Gang Pei",
          "openalex_id": "A5019671286",
          "orcid": "https://orcid.org/0000-0003-4107-5159",
          "institutions": [
            "University of Science and Technology of China"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-08-13",
      "abstract": null,
      "cited_by_count": 13,
      "type": "article",
      "source": {
        "name": "Energy Reports",
        "type": "journal",
        "issn": [
          "2352-4847"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://doi.org/10.1016/j.egyr.2022.07.175"
      },
      "topics": [
        "Solar-Powered Water Purification Methods",
        "Membrane Separation Technologies",
        "Energy Harvesting in Wireless Networks"
      ],
      "referenced_works_count": 110,
      "url": "https://openalex.org/W4291201565"
    },
    {
      "openalex_id": "W2968923792",
      "doi": "10.1038/s41524-019-0221-0",
      "title": "Recent advances and applications of machine learning in solid-state materials science",
      "authors": [
        {
          "name": "Jonathan Schmidt",
          "openalex_id": "A5057207447",
          "orcid": "https://orcid.org/0000-0001-5685-6404",
          "institutions": [
            "Martin Luther University Halle-Wittenberg"
          ]
        },
        {
          "name": "M\u00e1rio R. G. Marques",
          "openalex_id": "A5013807218",
          "orcid": "https://orcid.org/0000-0002-7420-5098",
          "institutions": [
            "Martin Luther University Halle-Wittenberg"
          ]
        },
        {
          "name": "Silvana Botti",
          "openalex_id": "A5074073454",
          "orcid": "https://orcid.org/0000-0002-4920-2370",
          "institutions": [
            "Friedrich Schiller University Jena"
          ]
        },
        {
          "name": "Miguel A. L. Marques",
          "openalex_id": "A5052107166",
          "orcid": "https://orcid.org/0000-0003-0170-8222",
          "institutions": [
            "Martin Luther University Halle-Wittenberg"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-08-08",
      "abstract": "Abstract One of the most exciting tools that have entered the material science toolbox in recent years is machine learning. This collection of statistical methods has already proved to be capable of considerably speeding up both fundamental and applied research. At present, we are witnessing an explosion of works that develop and apply machine learning to solid-state systems. We provide a comprehensive overview and analysis of the most recent research in this topic. As a starting point, we introduce machine learning principles, algorithms, descriptors, and databases in materials science. We continue with the description of different machine learning approaches for the discovery of stable materials and the prediction of their crystal structure. Then we discuss research in numerous quantitative structure\u2013property relationships and various approaches for the replacement of first-principle methods by machine learning. We review how active learning and surrogate-based optimization can be applied to improve the rational design process and related examples of applications. Two major questions are always the interpretability of and the physical understanding gained from machine learning models. We consider therefore the different facets of interpretability and their importance in materials science. Finally, we propose solutions and future research paths for various challenges in computational materials science.",
      "cited_by_count": 2200,
      "type": "article",
      "source": {
        "name": "npj Computational Materials",
        "type": "journal",
        "issn": [
          "2057-3960"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.nature.com/articles/s41524-019-0221-0.pdf"
      },
      "topics": [
        "Machine Learning in Materials Science",
        "X-ray Diffraction in Crystallography",
        "Computational Drug Discovery Methods"
      ],
      "referenced_works_count": 518,
      "url": "https://openalex.org/W2968923792"
    },
    {
      "openalex_id": "W4396509834",
      "doi": "10.1007/978-3-031-58734-4_2",
      "title": "Ordering Transactions with\u00a0Bounded Unfairness: Definitions, Complexity and\u00a0Constructions",
      "authors": [
        {
          "name": "Aggelos Kiayias",
          "openalex_id": "A5073753740",
          "orcid": "https://orcid.org/0000-0003-2451-1430",
          "institutions": [
            "University of Edinburgh"
          ]
        },
        {
          "name": "Nikos Leonardos",
          "openalex_id": "A5044456685",
          "orcid": "https://orcid.org/0000-0003-3909-7914",
          "institutions": [
            "National and Kapodistrian University of Athens"
          ]
        },
        {
          "name": "Yu Shen",
          "openalex_id": "A5012242973",
          "orcid": "https://orcid.org/0000-0002-4316-8116",
          "institutions": [
            "University of Edinburgh"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-01-01",
      "abstract": null,
      "cited_by_count": 6,
      "type": "book-chapter",
      "source": {
        "name": "Lecture notes in computer science",
        "type": "book series",
        "issn": [
          "0302-9743",
          "1611-3349"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://www.research.ed.ac.uk/en/publications/2af1c202-22ef-4e86-a7b0-b484832d5a71"
      },
      "topics": [
        "Law, Economics, and Judicial Systems",
        "Blockchain Technology Applications and Security",
        "Corporate Insolvency and Governance"
      ],
      "referenced_works_count": 32,
      "url": "https://openalex.org/W4396509834"
    },
    {
      "openalex_id": "W3041909682",
      "doi": null,
      "title": "The Impossibility Theorem of Machine Fairness - A Causal Perspective.",
      "authors": [
        {
          "name": "Kailash Karthik S",
          "openalex_id": "A5051871163",
          "institutions": [
            "Columbia University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-07-12",
      "abstract": "With the increasing pervasive use of machine learning in social and economic settings, there has been an interest in the notion of machine bias in the AI community. Models trained on historic data reflect the biases that exist in society and are propagated to the future through their decisions. A recent study conducted by ProPublica revealed that the COMPAS recidivism prediction tool was biased against the African-American community. There are three prominent metrics of fairness used in the community, and it has been statistically proved that it is impossible to satisfy them at the same time -- which has led to ambiguity about the definition of fairness. In this report, causal perspective to the impossibility theorem of fairness is presented along with a causal goal for machine fairness.",
      "cited_by_count": 3,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2007.06024.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 6,
      "url": "https://openalex.org/W3041909682"
    },
    {
      "openalex_id": "W4363625553",
      "doi": "10.1111/papa.12233",
      "title": "Reconciling Algorithmic Fairness Criteria",
      "authors": [
        {
          "name": "Fabian Beigang",
          "openalex_id": "A5074663886",
          "orcid": "https://orcid.org/0000-0001-7448-8465"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-04-01",
      "abstract": "When the discussion about algorithmic fairness first started to receive academic attention, much of the debate was about criteria that use aggregate statistics of observed data to determine whether a predictive model is fair. At the center of the debate were the two criteria equalized odds and predictive parity. Equalized odds requires that false positive and false negative error rates be equal for different protected groups (e.g., men and women) in order for a predictive model to be considered fair. Predictive parity, on the other hand, stipulates that a model must have equal predictive values for different protected groups. At first glance, both criteria seem like reasonable and intuitive conditions for algorithmic fairness. As it turned out, however, the two criteria are (under realistic circumstances) mutually incompatible.1 This means, it is in most cases impossible to satisfy both\u2014when one is satisfied, the other must be violated. This was a frustrating result since both conditions have some intuitive appeal. At the same time, these impossibility results inadvertently provided a justification for companies, governments, and other organizations to use predictive models which violate one of the fairness criteria: they could simply argue that the model cannot but violate the criterion since it satisfies the other. To illustrate the impossibility of satisfying both equalized odds and predictive parity in a practical setting, consider a predictive model that estimates a person's risk of defaulting on a loan. Let us say this model is used by a bank to renegotiate the loan terms. To retrospectively compare the model's performance for two different demographic groups, we could examine how many people in each group who defaulted on their loan were predicted to do so. Equalized odds requires that this proportion be the same for both groups. Alternatively, we could check how many people in each group who were predicted to default actually did default. Predictive parity requires that this proportion be the same for both groups. However, if one group has a lower default rate than the other, it is impossible to satisfy both equalized odds and predictive parity simultaneously. If equalized odds is satisfied, the proportion of those who do in fact default among those predicted to default will differ between the two groups, violating predictive parity. Conversely, if predictive parity is satisfied, equalized odds will be violated. The bank could exploit this fact to justify the use of a discriminatory predictive model. Various approaches have been proposed to address the issue of the impossibility theorem. Some authors have suggested abandoning one of the fairness criteria,2 while others have proposed that the choice of criterion should depend on the context.3 Still, others have argued for the abandonment of both equalized odds and predictive parity in favor of an alternative criterion.4 However, given the intuitive appeal of these two criteria, it is hard to accept any of these options. As a result, there remains a lack of consensus on how best to deal with the impossibility theorem. In this article, I will argue that both criteria can be modified in a way that retains their intuitive appeal and renders them universally compatible. Instead of requiring that error rates or predictive values be equal across protected groups, the modified equalized odds and predictive parity criteria require that the protected characteristic does not cause discrepancies in these metrics across groups. To formalize these modified versions of equalized odds and predictive parity, I will use a method called matching, which is typically used for causal inference in observational studies. The remainder of this article is organized as follows. In Section II, I introduce the two statistical fairness criteria, equalized odds and predictive parity, and present the Kleinberg-Chouldechova impossibility theorem that involves them. I then present criticisms of the two criteria. In Section III, I turn away from fairness for a moment to introduce the method of matching. In Section IV, I utilize this method to define versions of equalized odds and predictive parity that better capture their intended interpretation. As will be shown, the two modified criteria are universally compatible. Section V concludes the article. Let us begin by providing more precise definitions of the two statistical fairness criteria equalized odds and predictive parity.5 We will here focus on binary predictions (or classifications) and take the predictive model to be a function from a set of input variables to the prediction. We will denote the variable representing the protected characteristic with A (which will also be assumed to be binary), the prediction with Y \u0302 , and the target variable that is to be predicted with Y. Let P be a joint probability distribution function of A, Y \u0302 , and Y, such that (i) P(E) \u2265 0 for all events E in the event space defined on the sample space; (ii) P(\u03a9) = 1, where \u03a9 is the sample space, which is the set of all possible outcomes; and (ii) P(E1\u222aE2) = P(E1) + P(E2) for all events E1 and E2 that are mutually exclusive in the event space, meaning they cannot both occur simultaneously. The event space is a collection of subsets of the sample space that represent different possible events. We will also denote conditional probability (i.e., the probability of an event occurring given another event) by P E 1 E 2 , which is defined as P E 1 | E 2 = P E 1 \u2229 E 2 P E 2 . The probability of a variable V taking on the value v is denoted as P(V = v), abbreviated as P(v) when there is no ambiguity. The probability of two events occurring simultaneously, V = v and U = u, will be abbreviated as P(v, u). We can now formally define the two criteria as follows: Definition 1.(Equalized odds). A predictive model satisfies equalized odds (relative to protected characteristics a1, a2\u2208DA) if and only if for all y \u0302 \u2208 D Y \u0302 and y\u2208DY, P y \u0302 a 1 y = P y \u0302 a 2 y . Definition 2.(Predictive parity). A predictive model satisfies predictive parity (relative to protected characteristics a1, a2\u2208DA) if and only if for all y \u0302 \u2208 D Y \u0302 and y\u2208DY, P y a 1 y \u0302 = P y a 2 y \u0302 . In most contexts, equalized odds and predictive parity cannot be satisfied simultaneously. This follows from a theorem of which two versions were simultaneously and independently proved by Chouldechova6 and Kleinberg.7 More precisely, the theorem states that whenever the prevalence, that is, the relative frequency of an occurrence of the event represented by the target variable, is different for different protected groups, a predictive model which satisfies equalized odds must violate predictive parity, and vice versa.8 For example, a predictive model intended to predict whether a defendant will reoffend (i.e., commit a future crime) cannot at the same time produce equal error rates and have equal predictive values for different ethnic groups, if the prevalence of reoffence (i.e., the relative frequency of defendants committing another crime in the future) differs across these groups. To state this more concisely, the theorem can be formulated as follows: Theorem 1.(The Kleinberg-Chouldechova impossibility). If the prevalence differs across protected groups, no (imperfect) predictive model can satisfy both equalized odds and predictive parity. If both equalized odds and predictive parity were deemed universally necessary conditions for predictive fairness, this impossibility theorem would suggest that truly fair predictive models are an unattainable goal. There is, however, another possible interpretation of this impossibility: namely that it highlights a flaw in our formalization of what constitutes fair predictive models. The impossibility result can then be seen as an indicator that we have to rethink the definitions of the two fairness criteria and reevaluate whether they actually formalize the more intuitive ideas they are intended to formalize. The argument pursued here is along those lines. I will argue that while the intuitive appeal of both fairness criteria is undeniable, they impose stronger requirements than what is necessary in order to avoid certain types of unfairness in predictive models. Before we examine both criteria, let us first consider how we can cash out the broader idea of fairness for predictive models. Algorithmic fairness is, in the context of such models, generally taken to mean the absence of discrimination. Discrimination, in turn, can be understood as the wrongful disadvantageous treatment of an individual on the basis of a sensitive characteristic like ethnicity, gender, or religion.9 There is disagreement on what constitutes wrongful disadvantageous treatment in this context. However, the most widely accepted definitions are based on the ideas that the sensitive characteristic is irrelevant in most situations10 and that a person should be treated as an individual.11 Wrongfulness then arises from basing disadvantageous treatment on an irrelevant sensitive characteristic, or from treating someone disadvantageously based on presumed statistical patterns that associate their sensitive characteristic with some other trait in a way that disregards their individuality. Predictive models, by definition, are mathematical models used to predict the value of a specific variable. The output of such a model can be seen as representing a belief-like propositional attitude. If the possession of a certain sensitive characteristic leads to disadvantageous predictions, such as in terms of accuracy or their impact on the decision for which the model is used, this process can be considered discriminatory. We can consequently understand equalized odds and predictive parity as criteria that prevent discriminatory outcomes in predictive models. Equalized odds can be interpreted as a criterion that prevents discriminatory outcomes by preventing systematic cognitive bias with regard to a sensitive characteristic. By systematic cognitive bias, we here mean misjudging how informative a certain trait is in predicting another trait.12 Assume, for example, that in making predictions about whether someone will get lung cancer, we overestimate how informative it is that the person smokes. More precisely, if someone is a smoker, we predict that they will get lung cancer, and if not, we predict that they will not get lung cancer. We are clearly biased with regard to smoking in predicting lung cancer: not everyone who smokes gets lung cancer, and some people get it without ever having touched a cigarette. It is easy to see that this will result in different error rates for the group of smokers and the group of non-smokers. The smokers will have a false negative rate of 0 (simply for the fact that no smoker was predicted to not get lung cancer) but a false positive error rate above 0 (some smokers do not get lung cancer). Vice versa, the non-smokers will have a false negative rate above 0 (there are some who get lung cancer, but we never predict a non-smoker to get lung cancer) but a false positive rate of 0 (because no non-smoker is predicted to get lung cancer). One could say that this model is systematically biased with regard to smoking in predicting lung cancer. If, however, instead of using the predictive model just described we used a predictive model which guarantees that the error rates across smokers and non-smokers are equal, then we could be sure that the predictor contains no such bias. While being a smoker is typically not considered a sensitive characteristic, overestimating the informativeness of a sensitive characteristic like gender could lead to disadvantageous predictions on the basis of an irrelevant (or less relevant than warranted) sensitive characteristic.13 Yet, it is important to note that a violation of equalized odds across protected groups is an indicator and not a definition of systematic cognitive bias. To see this, note that the statement relating error rates to bias is a conditional: if there is bias with regard to trait A, there will be disparities in error rates between those with trait A and those without. By simple logic, this implies that whenever there are no disparities in error rates, there is no bias. Yet, it does not imply that whenever we observe disparities in error rates between those with trait A and those without, we can conclude that the predictor is biased with regard to A. In other words, equalized odds relative to A is a sufficient condition for the absence of bias with regard to A, but not a necessary one. Hence, trying to deduce that a predictor is biased from the observation that error rates among groups differ amounts to committing the well-known fallacy of affirming the consequent. At best, observing disparities in error rates allows one to make an inference to the best explanation: when disparities in error rates between two groups are observed, and there is no other plausible explanation, then one is justified in suspecting that this is due to bias with regard to the trait that distinguishes the groups. While this inference may seem plausible in many cases, it is important to note that it is a fallible inference, and that is what matters for our purposes (Figure 1). To illustrate this with an example, imagine a health insurance company that tries to predict the healthcare costs an individual incurs in a given year in order to decide how to set their customers' premiums. To simplify things, imagine the company is trying to predict only whether an individual's annual costs are above a certain threshold. This allows us to represent the target variable and the prediction as binary variables. Now imagine that in country C, citizens of religion R1 are, on average, younger than citizens of religion R2 (we can imagine that this is due to the fact that many people of religion R1 in C have recently immigrated, and that people generally tend to immigrate when they are somewhat younger). Suppose that, upon examination, the predictions turn out to have a higher false positive rate for people of religion R1 than for people of religion R2. Can we conclude that the predictive model the insurance company used has a discriminatory bias against people of religion R1? The observation of different error rates does not conclusively establish this. Different explanations for this discrepancy are conceivable. Imagine first a scenario in which the insurance company uses a predictive model which solely takes the individual's age into account. Now imagine further that the predictor is biased with regard to age, in that it overestimates how informative young age is of risky behavior, and hence of increased health costs. This, as we have shown above, will obviously lead to higher false positive rates for predictions of high health costs among young people. Because, on average, people of religion R1 are younger, and the predictive model is biased with regard to age, it will produce predictions with a higher false positive rate for people of religion R1. However, it can be argued that the outcomes of this predictive model do not discriminate against people of religion R1. To see this, consider the following. Imagine that instead of C, the health insurance company operated in a different country D, where citizens of religion R2 are, on average, younger than citizens of religion R1. Again, we can assume that this is because in this context people of religion R2 are mostly recent immigrants to D. It is easy to see that here, the predictive model would produce predictions with a higher false positive rate for people of religion R2. Remember that this model is exactly the same as the one above and that the higher false positive rate for R2 would not occur in C, where the reverse was the case. If we define bias as disparities in observed error rates, we would come to the somewhat contradictory conclusion that the predictive model is biased against people of religion R1, but that, had the insurance company applied the exact same predictive model in a different country, the model would be biased against people of religion R2. We can see that which religion a person has does not, in any sense, influence the predictions (or, for that matter, the error rates). It only happens to be the case that, in the given context, the predictive model works on average less well for one religious group than for another. While this might be worrisome in its own right, it can hardly be considered discrimination on the basis of religion, as there is no explanatory relation between a customer's religion and the predicted health costs. Compare this with a second scenario, depicted in 1(b), in which the insurance's predictor takes a person's religion into account in order to make a healthcare cost prediction. From an observational point of view, the two predictors' performances might be indistinguishable, as they could both produce the same discrepancies in error rates between different religious groups. Yet, on a narrow understanding of systematic cognitive bias, only the latter can be said to be biased against people of religion R1. This is of course not to say that disparities in error rates among different protected groups are of no moral concern by themselves. Disparities in error rates can, if tied to decision-making, lead to unjust distributions of resources and burdens. Yet, we are here following Eidelson14 in claiming that (direct and structural) discrimination is conceptually distinct from matters of distributive justice, and trying to subsume one under the other will get in the way of a clear analysis of each. Especially in the context of algorithmic decision-making, it seems that distinguishing between discriminatory predictions and unjust decisions is necessary in order to determine an appropriate method of mitigating the unfairness without introducing other unanticipated biases into the decision-making process.15 We are here solely concerned with fairness criteria for predictions. A discussion of distributive justice arising from algorithmic decision-making, although important, is beyond the scope of this article. Let us now turn to predictive parity. Predictive parity is often interpreted as a criterion that prohibits the meaning of a prediction from depending on a person's sensitive characteristic, as doing so could incentivize discriminatory behavior.16 Here, a similar observation can be made. Imagine a medical device that tests for a specific disease. Given a person has the disease, there is a 95 percent probability that the test turns out positive. When applied to a person who is healthy, there is a 5 percent probability that the test nonetheless turns out positive. This, we can imagine, can be shown to robustly hold across genders. There is no difference whatsoever in the likelihood of receiving an erroneous result, no matter whether a patient is male or female. Intuitively, it seems, there is no difference in meaning of the prediction for men and for women. But now imagine that the disease happens to occur more frequently in men. More specifically, we can imagine that one in every 10 men has the disease, but only one in every 100 women does. Then the positive predictive value, that is, the probability of actually having the disease given that one receives a positive test result, is different for men and women. For men it is roughly 68 percent, whereas for women it is only about 16 percent.17 This means, in this intuitively fair case, predictive parity is not satisfied. But it seems that this is not due to bias in the testing device, but just to the prevalence of the disease, which differs across genders. In other words, it is not gender which causes the difference in predictive value (since the testing device works, by assumption, equally well for a randomly chosen man as for a randomly chosen woman). So it seems that here, too, we want to distinguish between discrepancies in predictive value which are (causally) explained by gender, and discrepancies in predictive value which are due to external factors, such as differences in the prevalence of a disease. In light of these criticisms, it seems that the definitions of both equalized odds and predictive parity do not adequately explicate the underlying moral intuitions they were designed to capture.18 This, in turn, could mean that the Kleinberg-Chouldechova impossibility result is not so disastrous after all. If neither equalized odds nor predictive parity are, as they are currently defined, necessary conditions for fairness, the impossibility loses its bite. There is a chance that the impossibility theorem is just an artifact of the way the criteria are defined. The remainder of this article will examine this possibility by trying to modified definitions of equalized odds and predictive parity that all the intuitively plausible of the definitions but avoid the We can use method for causal inference on the basis of observational define modified versions of equalized odds and predictive parity. In this I will the The from the following In many it would be to be to whether and to which a given variable has a causal on some other variable. The for causal is the specific of however, it is impossible or to or the only data is observational for about the health of smoking on It would be to a group of to Yet, there might be observational data on the health of that in a where at one of the smokes. to the of a for observational data as best as to for the of causal in cases like the above, where data is causal by the results of an on one group of randomly treatment to a group who do not receive the This that any observed differences between the groups are due to the and not other For example, in a to test the of an could be randomly to receive the or a and the of the could be by the frequency of in the two groups. A of observational is that they can for that may both who receives the treatment and the being this with a without If the were to to take and the results were to those who did not receive the the results could be by One such is the age of the people are more to from and might hence be more to take At the same time, age obviously also whether and with which frequency someone This that the treatment group may have a higher average frequency of than the which could lead to the conclusion that the is not In a however, the age distributions of the treatment and groups would be roughly equal, for a more of the When we only have observational we can to matching. Instead of randomly to the treatment or observational data can be used to a group that does not systematically differ from the treatment group on any observed or variables other than the treatment (i.e., variable, This is as follows. that the data of on the causal variable, the variable, and a of other in this context, will be called the For each individual in the treatment we the individual from the group values are as similar as possible to the values of the individual from the treatment the We with a treatment and a group that have or similar distributions the and certain that we will get to in a for the conclusion that any difference between the groups with regard to the variable is by the difference in the causal variable. To illustrate this, we can to the of the of the on the of To a we would to in the group with that are as as possible to the of the in the treatment as well as similar of By a group with these we would that the treatment and groups have similar distributions of age and us to conclude that any differences in the of between the groups is due to the treatment than age or that the is we would the frequency of to be lower in the group than in the treatment there may be other that could the but allows us to for observed and a more of the Let us now address a namely how to In order for to be as a method for causal as it is that the set of contains all the variables that influence both the causal and the variable (i.e., all This is important because it that there are no differences between the and treatment group conditional on the observed The that this is the case is typically called In our above, is most not there could be other variables that influence both treatment and like for and so we have some by on age and we could get a better of the if we had the data on these other important for is not to any variables that are by the causal variable. This might lead to an of the causal and the let us that there is an of The of the group on the used, whether one uses or the data point is the average of the most similar data and whether is with or without (i.e., whether a data point can be more than The specific choice of however, is of no to the argument Let us now to the to equalized odds and predictive parity to avoid the Kleinberg-Chouldechova We will consider both fairness criteria in Let us begin by out the intended interpretation of the fairness criterion which is to equalized We will this criterion equalized equalized odds requires that the protected characteristic has no on the predictor in a way that its error For if a predictor satisfies equalized it that the fact that a defendant is does not the probability of receiving a false positive can we determine if a predictor satisfies equalized We can this as a causal inference by the causal of the protected characteristic on a error This as explained in the can be using matching. We the protected characteristic as the treatment variable and an appropriate set of We then a such that the treatment and group (i.e., the two protected no systematic differences other than in their protected characteristics and the predictions they we compare the error rates of the two groups. If only the error rates are equal, equalized odds is satisfied. Some more are to be said about the choice of The first point is that we can, of only determine whether a predictor satisfies equalized odds if the satisfy the of a fairness hence",
      "cited_by_count": 19,
      "type": "article",
      "source": {
        "name": "Philosophy &amp Public Affairs",
        "type": "journal",
        "issn": [
          "0048-3915",
          "1088-4963"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/papa.12233"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Impact of AI and Big Data on Business and Society"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4363625553"
    },
    {
      "openalex_id": "W4320854675",
      "doi": "10.48550/arxiv.2302.06347",
      "title": "The Possibility of Fairness: Revisiting the Impossibility Theorem in Practice",
      "authors": [
        {
          "name": "Andrew Bell",
          "openalex_id": "A5101752875",
          "orcid": "https://orcid.org/0000-0001-6010-9030"
        },
        {
          "name": "Lucius Bynum",
          "openalex_id": "A5038439834",
          "orcid": "https://orcid.org/0000-0002-9247-2595"
        },
        {
          "name": "Nazarii Drushchak",
          "openalex_id": "A5014987268",
          "orcid": "https://orcid.org/0000-0002-5056-3026"
        },
        {
          "name": "Tetiana Herasymova",
          "openalex_id": "A5110960620"
        },
        {
          "name": "Lucas Rosenblatt",
          "openalex_id": "A5068868453",
          "orcid": "https://orcid.org/0000-0001-6952-4361"
        },
        {
          "name": "Julia Stoyanovich",
          "openalex_id": "A5082830839",
          "orcid": "https://orcid.org/0000-0002-1587-0450"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-02-13",
      "abstract": "The ``impossibility theorem'' -- which is considered foundational in algorithmic fairness literature -- asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a \\textit{practitioner's} perspective of fairness), it becomes possible to identify a large set of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when -- and to what degree -- fairness along multiple criteria can be achieved. For example, if one allows only a small margin-of-error between metrics, there exists a large set of models simultaneously satisfying \\emph{False Negative Rate Parity}, \\emph{False Positive Rate Parity}, and \\emph{Positive Predictive Value Parity}, even when there is a moderate prevalence difference between groups. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed.",
      "cited_by_count": 2,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2302.06347"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4320854675"
    },
    {
      "openalex_id": "W4281649751",
      "doi": "10.48550/arxiv.2206.00074",
      "title": "To the Fairness Frontier and Beyond: Identifying, Quantifying, and Optimizing the Fairness-Accuracy Pareto Frontier",
      "authors": [
        {
          "name": "Camille Olivia Little",
          "openalex_id": "A5045297865"
        },
        {
          "name": "Michael Weylandt",
          "openalex_id": "A5006633856",
          "orcid": "https://orcid.org/0000-0002-0950-3881"
        },
        {
          "name": "Genevera I. Allen",
          "openalex_id": "A5048412925"
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-05-31",
      "abstract": "Algorithmic fairness has emerged as an important consideration when using machine learning to make high-stakes societal decisions. Yet, improved fairness often comes at the expense of model accuracy. While aspects of the fairness-accuracy tradeoff have been studied, most work reports the fairness and accuracy of various models separately; this makes model comparisons nearly impossible without a model-agnostic metric that reflects the balance of the two desiderata. We seek to identify, quantify, and optimize the empirical Pareto frontier of the fairness-accuracy tradeoff. Specifically, we identify and outline the empirical Pareto frontier through Tradeoff-between-Fairness-and-Accuracy (TAF) Curves; we then develop a metric to quantify this Pareto frontier through the weighted area under the TAF Curve which we term the Fairness-Area-Under-the-Curve (FAUC). TAF Curves provide the first empirical, model-agnostic characterization of the Pareto frontier, while FAUC provides the first metric to impartially compare model families on both fairness and accuracy. Both TAF Curves and FAUC can be employed with all group fairness definitions and accuracy measures. Next, we ask: Is it possible to expand the empirical Pareto frontier and thus improve the FAUC for a given collection of fitted models? We answer affirmately by developing a novel fair model stacking framework, FairStacks, that solves a convex program to maximize the accuracy of model ensemble subject to a score-bias constraint. We show that optimizing with FairStacks always expands the empirical Pareto frontier and improves the FAUC; we additionally study other theoretical properties of our proposed approach. Finally, we empirically validate TAF, FAUC, and FairStacks through studies on several real benchmark data sets, showing that FairStacks leads to major improvements in FAUC that outperform existing algorithmic fairness approaches.",
      "cited_by_count": 4,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2206.00074"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4281649751"
    },
    {
      "openalex_id": "W4407223457",
      "doi": "10.3390/en18030742",
      "title": "Few-Shot Load Forecasting Under Data Scarcity in Smart Grids: A Meta-Learning Approach",
      "authors": [
        {
          "name": "Georgios Tsoumplekas",
          "openalex_id": "A5075357628",
          "orcid": "https://orcid.org/0009-0004-4943-3381",
          "institutions": [
            "Aristotle University of Thessaloniki"
          ]
        },
        {
          "name": "Christos L. Athanasiadis",
          "openalex_id": "A5003433886",
          "orcid": "https://orcid.org/0000-0002-3747-5443",
          "institutions": [
            "Democritus University of Thrace"
          ]
        },
        {
          "name": "Dimitrios I. Doukas",
          "openalex_id": "A5080874126",
          "orcid": "https://orcid.org/0000-0002-7612-6123"
        },
        {
          "name": "Antonios Chrysopoulos",
          "openalex_id": "A5113816042"
        },
        {
          "name": "Pericles A. Mitkas",
          "openalex_id": "A5014906615",
          "orcid": "https://orcid.org/0000-0003-2090-4202",
          "institutions": [
            "Aristotle University of Thessaloniki"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-02-06",
      "abstract": "Despite the rapid expansion of smart grids and large volumes of data at the individual consumer level, there are still various cases where adequate data collection to train accurate load forecasting models is challenging or even impossible. This paper proposes adapting an established Model-Agnostic Meta-Learning algorithm for short-term load forecasting in the context of few-shot learning. Specifically, the proposed method can rapidly adapt and generalize within any unknown load time series of arbitrary length using only minimal training samples. In this context, the meta-learning model learns an optimal set of initial parameters for a base-level learner recurrent neural network. The proposed model is evaluated using a dataset of historical load consumption data from real-world consumers. Despite the examined load series\u2019 short length, it produces accurate forecasts outperforming transfer learning and task-specific machine learning methods by 12.5%. To enhance robustness and fairness during model evaluation, a novel metric, mean average log percentage error, is proposed that alleviates the bias introduced by the commonly used MAPE metric. Finally, a series of studies to evaluate the model\u2019s robustness under different hyperparameters and time series lengths is also conducted, demonstrating that the proposed approach consistently outperforms all other models.",
      "cited_by_count": 9,
      "type": "article",
      "source": {
        "name": "Energies",
        "type": "journal",
        "issn": [
          "1996-1073"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://doi.org/10.3390/en18030742"
      },
      "topics": [
        "Energy Load and Power Forecasting",
        "Traffic Prediction and Management Techniques",
        "Hydrological Forecasting Using AI"
      ],
      "referenced_works_count": 62,
      "url": "https://openalex.org/W4407223457"
    },
    {
      "openalex_id": "W4386142022",
      "doi": "10.1007/s12559-023-10179-8",
      "title": "Interpreting Black-Box Models: A Review on Explainable Artificial Intelligence",
      "authors": [
        {
          "name": "Vikas Hassija",
          "openalex_id": "A5058693080",
          "orcid": "https://orcid.org/0000-0002-3199-8753",
          "institutions": [
            "KIIT University"
          ]
        },
        {
          "name": "Vinay Chamola",
          "openalex_id": "A5005020243",
          "orcid": "https://orcid.org/0000-0002-6730-3060",
          "institutions": [
            "Birla Institute of Technology and Science, Pilani"
          ]
        },
        {
          "name": "A. Mahapatra",
          "openalex_id": "A5114077165",
          "institutions": [
            "Birla Institute of Technology and Science, Pilani"
          ]
        },
        {
          "name": "Abhinandan Singal",
          "openalex_id": "A5058997114",
          "institutions": [
            "Jaypee Institute of Information Technology"
          ]
        },
        {
          "name": "Divyansh Goel",
          "openalex_id": "A5026248849",
          "institutions": [
            "Jaypee Institute of Information Technology"
          ]
        },
        {
          "name": "Kaizhu Huang",
          "openalex_id": "A5026022035",
          "orcid": "https://orcid.org/0000-0002-3034-9639",
          "institutions": [
            "Duke Kunshan University"
          ]
        },
        {
          "name": "Simone Scardapane",
          "openalex_id": "A5022153057",
          "orcid": "https://orcid.org/0000-0003-0881-8344",
          "institutions": [
            "Sapienza University of Rome"
          ]
        },
        {
          "name": "Indro Spinelli",
          "openalex_id": "A5019615617",
          "orcid": "https://orcid.org/0000-0003-1963-3548",
          "institutions": [
            "Istituto Nazionale di Fisica Nucleare, Sezione di Roma I"
          ]
        },
        {
          "name": "Mufti Mahmud",
          "openalex_id": "A5027525633",
          "orcid": "https://orcid.org/0000-0002-2037-8348",
          "institutions": [
            "Nottingham Trent University"
          ]
        },
        {
          "name": "Amir Hussain",
          "openalex_id": "A5062211930",
          "orcid": "https://orcid.org/0000-0002-8080-082X",
          "institutions": [
            "Edinburgh Napier University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-08-24",
      "abstract": "Abstract Recent years have seen a tremendous growth in Artificial Intelligence (AI)-based methodological development in a broad range of domains. In this rapidly evolving field, large number of methods are being reported using machine learning (ML) and Deep Learning (DL) models. Majority of these models are inherently complex and lacks explanations of the decision making process causing these models to be termed as 'Black-Box'. One of the major bottlenecks to adopt such models in mission-critical application domains, such as banking, e-commerce, healthcare, and public services and safety, is the difficulty in interpreting them. Due to the rapid proleferation of these AI models, explaining their learning and decision making process are getting harder which require transparency and easy predictability. Aiming to collate the current state-of-the-art in interpreting the black-box models, this study provides a comprehensive analysis of the explainable AI (XAI) models. To reduce false negative and false positive outcomes of these back-box models, finding flaws in them is still difficult and inefficient. In this paper, the development of XAI is reviewed meticulously through careful selection and analysis of the current state-of-the-art of XAI research. It also provides a comprehensive and in-depth evaluation of the XAI frameworks and their efficacy to serve as a starting point of XAI for applied and theoretical researchers. Towards the end, it highlights emerging and critical issues pertaining to XAI research to showcase major, model-specific trends for better explanation, enhanced transparency, and improved prediction accuracy.",
      "cited_by_count": 1194,
      "type": "review",
      "source": {
        "name": "Cognitive Computation",
        "type": "journal",
        "issn": [
          "1866-9956",
          "1866-9964"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s12559-023-10179-8.pdf"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Imbalanced Data Classification Techniques",
        "Machine Learning in Healthcare"
      ],
      "referenced_works_count": 135,
      "url": "https://openalex.org/W4386142022"
    },
    {
      "openalex_id": "W2791170418",
      "doi": "10.1145/3278721.3278729",
      "title": "Measuring and Mitigating Unintended Bias in Text Classification",
      "authors": [
        {
          "name": "Lucas Dixon",
          "openalex_id": "A5103159534",
          "orcid": "https://orcid.org/0000-0003-1094-1675",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "John Li",
          "openalex_id": "A5101919661",
          "orcid": "https://orcid.org/0000-0002-3730-3713",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Jeffrey Sorensen",
          "openalex_id": "A5110191415",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Nithum Thain",
          "openalex_id": "A5084311372",
          "orcid": "https://orcid.org/0000-0002-7367-0916",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Lucy Vasserman",
          "openalex_id": "A5030217791",
          "institutions": [
            "Google (United States)"
          ]
        }
      ],
      "publication_year": 2018,
      "publication_date": "2018-12-27",
      "abstract": "We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.",
      "cited_by_count": 635,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3278721.3278729"
      },
      "topics": [
        "Hate Speech and Cyberbullying Detection",
        "Topic Modeling",
        "Text Readability and Simplification"
      ],
      "referenced_works_count": 11,
      "url": "https://openalex.org/W2791170418"
    },
    {
      "openalex_id": "W2953356739",
      "doi": "10.18653/v1/p19-1139",
      "title": "ERNIE: Enhanced Language Representation with Informative Entities",
      "authors": [
        {
          "name": "Zhengyan Zhang",
          "openalex_id": "A5101584823",
          "orcid": "https://orcid.org/0000-0003-2988-0083",
          "institutions": [
            "Beijing Academy of Artificial Intelligence",
            "Tsinghua University"
          ]
        },
        {
          "name": "Xu Han",
          "openalex_id": "A5100668503",
          "orcid": "https://orcid.org/0000-0002-4726-7621",
          "institutions": [
            "Tsinghua University"
          ]
        },
        {
          "name": "Zhiyuan Liu",
          "openalex_id": "A5100320723",
          "orcid": "https://orcid.org/0000-0002-7709-2543",
          "institutions": [
            "Tsinghua University",
            "Beijing Academy of Artificial Intelligence"
          ]
        },
        {
          "name": "Xin Jiang",
          "openalex_id": "A5086603207",
          "orcid": "https://orcid.org/0000-0002-9117-8247",
          "institutions": [
            "Huawei Technologies (Sweden)",
            "Beijing Academy of Artificial Intelligence",
            "Tsinghua University"
          ]
        },
        {
          "name": "Maosong Sun",
          "openalex_id": "A5046448314",
          "orcid": "https://orcid.org/0000-0002-6011-6115",
          "institutions": [
            "Beijing Academy of Artificial Intelligence",
            "Tsinghua University"
          ]
        },
        {
          "name": "Qun Liu",
          "openalex_id": "A5100426170",
          "orcid": "https://orcid.org/0000-0002-7000-1792",
          "institutions": [
            "Huawei Technologies (Sweden)"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-01-01",
      "abstract": "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.",
      "cited_by_count": 1362,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.aclweb.org/anthology/P19-1139.pdf"
      },
      "topics": [
        "Natural Language Processing Techniques",
        "Topic Modeling",
        "Text Readability and Simplification"
      ],
      "referenced_works_count": 59,
      "url": "https://openalex.org/W2953356739"
    }
  ],
  "count": 40,
  "errors": []
}
