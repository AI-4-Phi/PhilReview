{
  "status": "success",
  "source": "openalex",
  "query": "epistemic uncertainty fairness",
  "results": [
    {
      "openalex_id": "W2991542819",
      "doi": "10.1136/medethics-2019-105586",
      "title": "On the ethics of algorithmic decision-making in healthcare",
      "authors": [
        {
          "name": "Thomas Grote",
          "openalex_id": "A5067459884",
          "orcid": "https://orcid.org/0000-0002-9832-6046",
          "institutions": [
            "University of T\u00fcbingen"
          ]
        },
        {
          "name": "Philipp Berens",
          "openalex_id": "A5043130208",
          "orcid": "https://orcid.org/0000-0002-0199-4727",
          "institutions": [
            "University of T\u00fcbingen"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-11-20",
      "abstract": "In recent years, a plethora of high-profile scientific publications has been reporting about machine learning algorithms outperforming clinicians in medical diagnosis or treatment recommendations. This has spiked interest in deploying relevant algorithms with the aim of enhancing decision-making in healthcare. In this paper, we argue that instead of straightforwardly enhancing the decision-making capabilities of clinicians and healthcare institutions, deploying machines learning algorithms entails trade-offs at the epistemic and the normative level. Whereas involving machine learning might improve the accuracy of medical diagnosis, it comes at the expense of opacity when trying to assess the reliability of given diagnosis. Drawing on literature in social epistemology and moral responsibility, we argue that the uncertainty in question potentially undermines the epistemic authority of clinicians. Furthermore, we elucidate potential pitfalls of involving machine learning in healthcare with respect to paternalism, moral responsibility and fairness. At last, we discuss how the deployment of machine learning algorithms might shift the evidentiary norms of medical diagnosis. In this regard, we hope to lay the grounds for further ethical reflection of the opportunities and pitfalls of machine learning for enhancing decision-making in healthcare.",
      "cited_by_count": 471,
      "type": "article",
      "source": {
        "name": "Journal of Medical Ethics",
        "type": "journal",
        "issn": [
          "0306-6800",
          "1473-4257"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://jme.bmj.com/content/medethics/46/3/205.full.pdf"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Ethics in Clinical Research",
        "Healthcare cost, quality, practices"
      ],
      "referenced_works_count": 43,
      "url": "https://openalex.org/W2991542819"
    },
    {
      "openalex_id": "W2902850069",
      "doi": "10.1111/bjso.12297",
      "title": "A quarter century of system justification theory: Questions, answers, criticisms, and societal applications",
      "authors": [
        {
          "name": "John T. Jost",
          "openalex_id": "A5077630021",
          "orcid": "https://orcid.org/0000-0002-2844-4645",
          "institutions": [
            "New York University"
          ]
        }
      ],
      "publication_year": 2018,
      "publication_date": "2018-11-28",
      "abstract": "A theory of system justification was proposed 25 years ago by Jost and Banaji (1994, Br. J. Soc. Psychol., 33, 1) in the British Journal of Social Psychology to explain \u2018the participation by disadvantaged individuals and groups in negative stereotypes of themselves' and the phenomenon of outgroup favouritism. The scope of the theory was subsequently expanded to account for a much wider range of outcomes, including appraisals of fairness, justice, legitimacy, deservingness, and entitlement; spontaneous and deliberate social judgements about individuals, groups, and events; and full\u2010fledged political and religious ideologies. According to system justification theory, people are motivated (to varying degrees, depending upon situational and dispositional factors) to defend, bolster, and justify aspects of existing social, economic, and political systems. Engaging in system justification serves the palliative function of increasing satisfaction with the status quo and addresses underlying epistemic, existential, and relational needs to reduce uncertainty, threat, and social discord. This article summarizes the major tenets of system justification theory, reviews some of the empirical evidence supporting it, answers new (and old) questions and criticisms, and highlights areas of societal relevance and directions for future research.",
      "cited_by_count": 504,
      "type": "article",
      "source": {
        "name": "British Journal of Social Psychology",
        "type": "journal",
        "issn": [
          "0144-6665",
          "2044-8309"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Social and Intergroup Psychology",
        "Cultural Differences and Values",
        "Psychology of Moral and Emotional Judgment"
      ],
      "referenced_works_count": 204,
      "url": "https://openalex.org/W2902850069"
    },
    {
      "openalex_id": "W3104001696",
      "doi": "10.1371/journal.pone.0241144",
      "title": "Moral \u201cfoundations\u201d as the product of motivated social cognition: Empathy and other psychological underpinnings of ideological divergence in \u201cindividualizing\u201d and \u201cbinding\u201d concerns",
      "authors": [
        {
          "name": "Michael Strupp-Levitsky",
          "openalex_id": "A5057917010",
          "institutions": [
            "Long Island University"
          ]
        },
        {
          "name": "Sharareh Noorbaloochi",
          "openalex_id": "A5023371191",
          "institutions": [
            "Goldman Sachs (United States)"
          ]
        },
        {
          "name": "Andrew Shipley",
          "openalex_id": "A5020047331"
        },
        {
          "name": "John T. Jost",
          "openalex_id": "A5077630021",
          "orcid": "https://orcid.org/0000-0002-2844-4645",
          "institutions": [
            "New York University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-11-10",
      "abstract": "According to moral foundations theory, there are five distinct sources of moral intuition on which political liberals and conservatives differ. The present research program seeks to contextualize this taxonomy within the broader research literature on political ideology as motivated social cognition, including the observation that conservative judgments often serve system-justifying functions. In two studies, a combination of regression and path modeling techniques were used to explore the motivational underpinnings of ideological differences in moral intuitions. Consistent with our integrative model, the \u201cbinding\u201d foundations (in-group loyalty, respect for authority, and purity) were associated with epistemic and existential needs to reduce uncertainty and threat and system justification tendencies, whereas the so-called \u201cindividualizing\u201d foundations (fairness and avoidance of harm) were generally unrelated to epistemic and existential motives and were instead linked to empathic motivation. Taken as a whole, these results are consistent with the position taken by Hatemi, Crabtree, and Smith that moral \u201cfoundations\u201d are themselves the product of motivated social cognition.",
      "cited_by_count": 51,
      "type": "article",
      "source": {
        "name": "PLoS ONE",
        "type": "journal",
        "issn": [
          "1932-6203"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0241144&type=printable"
      },
      "topics": [
        "Social and Intergroup Psychology",
        "Psychology of Moral and Emotional Judgment",
        "Cultural Differences and Values"
      ],
      "referenced_works_count": 88,
      "url": "https://openalex.org/W3104001696"
    },
    {
      "openalex_id": "W3023069697",
      "doi": "10.1016/s2589-7500(20)30065-0",
      "title": "Ethical limitations of algorithmic fairness solutions in health care machine learning",
      "authors": [
        {
          "name": "Melissa D. McCradden",
          "openalex_id": "A5063981686",
          "orcid": "https://orcid.org/0000-0002-6476-2165",
          "institutions": [
            "Hospital for Sick Children"
          ]
        },
        {
          "name": "Shalmali Joshi",
          "openalex_id": "A5035149567",
          "institutions": [
            "Vector Institute"
          ]
        },
        {
          "name": "Mjaye Mazwi",
          "openalex_id": "A5080855486",
          "orcid": "https://orcid.org/0000-0003-1345-5429",
          "institutions": [
            "Hospital for Sick Children"
          ]
        },
        {
          "name": "James A. Anderson",
          "openalex_id": "A5103410231",
          "institutions": [
            "Hospital for Sick Children"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-04-28",
      "abstract": "Artificial intelligence has exposed pernicious bias within health data that constitutes substantial ethical threat to the use of machine learning in medicine.1Char DS Shah NH Magnus D Implementing machine learning in health care\u2014addressing ethical challenges.N Engl J Med. 2018; 378: 981-983Crossref PubMed Scopus (445) Google Scholar, 2Obermeyer Z Powers B Vogeli C Mullainathan S Dissecting racial bias in an algorithm used to manage the health of populations.Science. 2019; 366: 447-453Crossref PubMed Scopus (1151) Google Scholar Solutions of algorithmic fairness have been developed to create neutral models: models designed to produce non-discriminatory predictions by constraining bias with respect to predicted outcomes for protected identities, such as race or gender.3Corbett-Davies S Goel S The measure and mismeasure of fairness: a critical review of fair machine learning.https://5harad.com/papers/fair-ml.pdfDate accessed: March 16, 2020Google Scholar These solutions can omit such variables from the model (widely regarded as ineffective and can increase discrimination), constrain it to ensure equal error rates across groups, derive outcomes that are independent of one's identity after controlling for the estimated risk of that outcome, or mathematically balance benefit and harm to all groups. The temptation to engineer ethics into algorithm design is immense and industry is increasingly pushing these solutions. In the health-care space, where the stakes could be higher, clinicians will integrate these models into their care, trusting the issue of bias has been sufficiently managed within the model. However, even if well recognised technical challenges are set aside,3Corbett-Davies S Goel S The measure and mismeasure of fairness: a critical review of fair machine learning.https://5harad.com/papers/fair-ml.pdfDate accessed: March 16, 2020Google Scholar, 4Marmot M Social determinants of health inequalities.Lancet. 2005; 365: 1099-1104Summary Full Text Full Text PDF PubMed Scopus (2858) Google Scholar framing fairness as a purely technical problem solvable by the inclusion of more data or accurate computations is ethically problematic. We highlight challenges to the ethical and empirical efficacy of solutions of algorithmic fairness that show risks of relying too heavily on the so called veneer of technical neutrality,5Benjamin R Assessing risk, automating racism.Science. 2019; 366: 421-422Crossref PubMed Scopus (97) Google Scholar which could exacerbate harms to vulnerable groups. Historically, algorithmic fairness has not accounted for complex causal relationships between biological, environmental, and social factors that give rise to differences in medical conditions across protected identities. Social determinants of health play an important role, particularly for risk models. Social and structural factors affect health across multiple intersecting identities,4Marmot M Social determinants of health inequalities.Lancet. 2005; 365: 1099-1104Summary Full Text Full Text PDF PubMed Scopus (2858) Google Scholar but the mechanism(s) by which social determinants affect health outcomes is not always well understood. Additional complications flow from the reality that difference does not always entail inequity. In some instances, it is appropriate to incorporate differences between identities because there is a reasonable presumption of causation. For example, biological differences between genders can affect the efficacy of pharmacological compounds; incorporating these differences into prescribing practices does not make those prescriptions unjust. However, incorporating non-causative factors into recommendations can propagate unequal treatment by reifying extant inequities and exacerbating their effects. We should not allow models to promote different standards of care according to protected identities that do not have a causative association with the outcome. Nevertheless, in many cases it is difficult to distinguish between acknowledging difference and propagating discrimination. Given the epistemic uncertainty surrounding the association between protected identities and health outcomes, the use of fairness solutions can create empirical challenges. Consider the case of heart attack symptoms among women.6Wenger NK Cardiovascular disease: the female heart is vulnerable. A call to action from the 10Q Report.Clin Cardiol. 2012; 35: 134-135Crossref PubMed Scopus (11) Google Scholar The under-representation of women (particularly women of colour) in research of heart health is now well recognised as problematic and directly affected uneven improvements in treatment of heart attacks between women and men. By tailoring health solutions to the majority (ie, referent) group, we inevitably fall short of helping all patients. Many algorithmic fairness solutions, in effect, replicate this problem by trying to fit the non-referent groups to that of the referent,7Friedler SA Scheidegger C Venkatasubramanian S On the (im)possibility of fairness.https://arxiv.org/pdf/1609.07236.pdfDate: Sept 23, 2019Date accessed: March 16, 2020Google Scholar, 8Barocas S Hardt M Narayanan A Fairness and machine learning: limitations and opportunities; 2018.Fairmlbook.orgDate: 2019Google Scholar ignoring heterogeneity and assuming that the latter represents a true underlying pattern. Another concern is disconnection between the patient's clinical trajectory and the fair prediction. Consider the implications at the point-of-care, a model, corrected for fairness, will predict that a patient will respond to a treatment as a patient in the referent class would. What happens when that patient does not have the predicted response? This difference between an idealised model and non-ideal, real-world behaviour affects metrics of model performance (eg, specificity, sensitivity) and clinical utility in practice. Moreover, the model has made an ineffective recommendation that could have obscured more relevant interventions to help that patient. If clinicians and patients believe that the mode has been rendered neutral, then any discrepancies between model prediction and the patient's true clinical state might be impossible to interpret. The result would be to camouflage persistent health inequalities. As such, fairness, operationalised by output metrics alone, is insufficient; real-world consequences should be carefully considered. Bias and ineffective solutions of algorithmic fairness threaten the ethical obligation to avoid or minimise harms to patients (non-maleficence; panel). Non-maleficence demands that any new clinical tool should be assessed for patient safety. For health-care machine learning, safety should include awareness of model limitations with respect to protected identities and social determinants of health. Considerations of justice requires that implemented models do not exacerbate pernicious bias. There is a movement toward developing guidelines of standardised reporting for machine learning models of health care9Collins GS Reitsma JB Altman DG Moons KGM TRIPOD GroupTransparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): The TRIPOD Statement.Ann Intern Med. 2015; 162: 55-63Crossref PubMed Scopus (1245) Google Scholar and their prospective appraisal through clinical trials.10The CONSORT-AI and SPIRIT-AI Steering GroupReporting guidelines for clinical trials evaluating artificial intelligence interventions are needed.Nat Med. 2019; 25: 1467-1468Crossref PubMed Scopus (69) Google Scholar Appraisal is particularly important in determining the real-world implications for vulnerable patients when machine learning models are integrated into clinical decision making. Clinical trials are essential to providing a sense of the model's performance for clinicians to make informed decisions at the point-of-care through awareness of identity-related model limitations.PanelRecommendations for ethical approaches to issues of bias in health models of machine learningRelying on neutral algorithms is problematicChallenges cast doubt on whether any solution can adequately facilitate ethical goals. Resisting the tendency to view machine learning solutions as objective is essential to remaining patient-centered and preventing unintended harms.Problem formulation can support improved modelsChanging the way the problem is conceptualised and operationalised can reduce the effect of pernicious bias on outputs.6Wenger NK Cardiovascular disease: the female heart is vulnerable. A call to action from the 10Q Report.Clin Cardiol. 2012; 35: 134-135Crossref PubMed Scopus (11) Google Scholar Clinicians have a key role in identifying actionable, clinical problems where the effects of bias are minimized, or causal knowledge exists to support algorithmic fairness solutions. Clinicians in non-medical sciences might have epistemic advantages to support such formulations.Transparency is required surrounding model development and statistical validationStandardisation of reporting for models of machine learning can promote transparency about training data and statistical validation which can help clinicians and health-care decision makers to determine the transferability of the model to their served population. Large discrepancies increase the risk of under-performance in under-represented patients. Group-specific performance metrics (eg, stratification by ethnicity, gender, or socioeconomic status) can inform considerations for patient safety in implementing a given model.Initiating transparency at point-of-careAs the field of explainable or interpretable machine learning evolves, we suggest that, when sensitive attributes are used in problem formulation and could affect predictions, they should be accompanied by visibility of the top predictive features. Where predictions are affected by sensitive variables, prediction and rationale should be documented for the ensuing clinical decision to promote fair and transparent medical decision making. Communication with patients about the rationale is essential to shared decision making.Transparency at the prediction levelRobust auditing processes are increasingly viewed as essential to proper oversight of machine learning tools. When bias is considered, auditing can promote reflexive understanding of how tools of machine learning can affect care management of vulnerable patients. Discrepancies in systematic prediction can become evident and used as evidence of the need for beneficial interventions and highlight large structural barriers that affect health inequalities.Ethical decision making suggests engaging diverse knowledge sourcesEthical analysis should consider real-world consequences for affected groups, weigh benefits and risks of various approaches, and engage stakeholders to come to the most supportable conclusion. Therefore, analysis needs to focus on the downstream effects on patients rather than adopting the presumption that fairness is accomplished solely in the metrics of the system. Relying on neutral algorithms is problematic Challenges cast doubt on whether any solution can adequately facilitate ethical goals. Resisting the tendency to view machine learning solutions as objective is essential to remaining patient-centered and preventing unintended harms. Problem formulation can support improved models Changing the way the problem is conceptualised and operationalised can reduce the effect of pernicious bias on outputs.6Wenger NK Cardiovascular disease: the female heart is vulnerable. A call to action from the 10Q Report.Clin Cardiol. 2012; 35: 134-135Crossref PubMed Scopus (11) Google Scholar Clinicians have a key role in identifying actionable, clinical problems where the effects of bias are minimized, or causal knowledge exists to support algorithmic fairness solutions. Clinicians in non-medical sciences might have epistemic advantages to support such formulations. Transparency is required surrounding model development and statistical validation Standardisation of reporting for models of machine learning can promote transparency about training data and statistical validation which can help clinicians and health-care decision makers to determine the transferability of the model to their served population. Large discrepancies increase the risk of under-performance in under-represented patients. Group-specific performance metrics (eg, stratification by ethnicity, gender, or socioeconomic status) can inform considerations for patient safety in implementing a given model. Initiating transparency at point-of-care As the field of explainable or interpretable machine learning evolves, we suggest that, when sensitive attributes are used in problem formulation and could affect predictions, they should be accompanied by visibility of the top predictive features. Where predictions are affected by sensitive variables, prediction and rationale should be documented for the ensuing clinical decision to promote fair and transparent medical decision making. Communication with patients about the rationale is essential to shared decision making. Transparency at the prediction level Robust auditing processes are increasingly viewed as essential to proper oversight of machine learning tools. When bias is considered, auditing can promote reflexive understanding of how tools of machine learning can affect care management of vulnerable patients. Discrepancies in systematic prediction can become evident and used as evidence of the need for beneficial interventions and highlight large structural barriers that affect health inequalities. Ethical decision making suggests engaging diverse knowledge sources Ethical analysis should consider real-world consequences for affected groups, weigh benefits and risks of various approaches, and engage stakeholders to come to the most supportable conclusion. Therefore, analysis needs to focus on the downstream effects on patients rather than adopting the presumption that fairness is accomplished solely in the metrics of the system. Some computations can promote justice through revealing unfairness and refining problem formulation. Obermeyer and colleagues2Obermeyer Z Powers B Vogeli C Mullainathan S Dissecting racial bias in an algorithm used to manage the health of populations.Science. 2019; 366: 447-453Crossref PubMed Scopus (1151) Google Scholar show how calibration can reveal unfairness in a seemingly neutral task through which choice of label can dictate how heavily bias is incorporated into predictions. It might be that no way exists to define a purely neutral problem; some clinical prediction tasks might be more susceptible to bias than others. Transparency at multiple points in the pipeline of machine learning including development, testing, and implementation stages can support interpretation of model outputs by relevant stakeholders (eg, researchers, clinicians, patients, and auditors). Combined with adequate documentation of outputs and ensuing decisions, these steps support a strong accountability framework for point-of-care machine learning tools with respect to safety and fairness to patients. Problem formulation with respect to bias will often be value-laden and ethically charged. Ethical decision making highlights the importance of converging knowledge sources to inform a given choice. Important stakeholders could include affected communities, cultural anthropologists, social scientists, and race and gender theorists. Computations alone clearly cannot solve the bias problem, but they could be offered a place within a broader approach to addressing fairness aims in healthcare. Algorithmic fairness could be necessary to fix statistical limitations reflective of perniciously biased data, and we encourage this work. The worry is that suggesting these as solutions risks unintended harms.5Benjamin R Assessing risk, automating racism.Science. 2019; 366: 421-422Crossref PubMed Scopus (97) Google Scholar Bias is not new; however, machine learning has potential to reveal bias, motivate change, and support ethical analysis while bringing this crucial conversation to a new audience. We are at a watershed moment in health care. Ethical considerations have rarely been so integral and essential to maximising success of a technology both empirically and clinically. The time is right to partake in thoughtful and collaborative engagement on the challenge of bias to bring about lasting change. We declare no competing interests.",
      "cited_by_count": 236,
      "type": "article",
      "source": {
        "name": "The Lancet Digital Health",
        "type": "journal",
        "issn": [
          "2589-7500"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "http://www.thelancet.com/article/S2589750020300650/pdf"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Healthcare cost, quality, practices",
        "COVID-19 and healthcare impacts"
      ],
      "referenced_works_count": 10,
      "url": "https://openalex.org/W3023069697"
    },
    {
      "openalex_id": "W3000112922",
      "doi": "10.1785/0220190186",
      "title": "Seismic Hazard Assessment in Australia: Can Structured Expert Elicitation Achieve Consensus in the \u201cLand of the Fair Go\u201d?",
      "authors": [
        {
          "name": "Jonathan Griffin",
          "openalex_id": "A5082409179",
          "orcid": "https://orcid.org/0000-0002-5197-1742",
          "institutions": [
            "Geoscience Australia",
            "University of Otago"
          ]
        },
        {
          "name": "Trevor I. Allen",
          "openalex_id": "A5006104045",
          "orcid": "https://orcid.org/0000-0003-3420-547X",
          "institutions": [
            "Geoscience Australia"
          ]
        },
        {
          "name": "Matthew C. Gerstenberger",
          "openalex_id": "A5016721953",
          "orcid": "https://orcid.org/0000-0002-0392-7114",
          "institutions": [
            "GNS Science"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-02",
      "abstract": "Abstract The 2018 National Seismic Hazard Assessment of Australia incorporated 19 alternative seismic-source models developed by members of the Australian seismological community. The diversity of these models demonstrates the deep epistemic uncertainty that exists with regards to how best to characterize seismicity in stable continental regions. In the face of similarly high uncertainty, a diverse range of ground-motion models was proposed for use. A complex logic tree was developed to incorporate the alternative component models into a single hazard model. Expert opinion was drawn upon to weight the alternative logic-tree branches through a structured expert elicitation process. Expert elicitation aims to transparently and reproducibly characterize the community distribution of expert estimates for uncertain quantities and thereby quantify the epistemic uncertainty around estimates of seismic hazard in Australia. We achieve a multimodel rational consensus in which each model, and each expert, is, in accordance with the Australian cultural myth of egalitarianism, given a \u201cfair go\u201d\u2014that is, judged on their merits rather than their status. Yet despite this process, we find that the results are not universally accepted. A key issue is a contested boundary between what is scientifically reducible and what remains epistemologically uncertain, with a particular focus on the earthquake catalog. Furthermore, a reduction, on average, of 72% for the 10% in 50 yr probability of exceedance peak ground acceleration levels compared with those underpinning existing building design standards, challenges the choice of metrics upon which design codes are based. Both quantification of the bounds of epistemic uncertainties through expert elicitation and reduction of epistemic uncertainties through scientific advances have changed our understanding of how the hazard behaves. Dialog between scientists, engineers, and policy makers is required to ensure that as our understanding of the hazard evolves, the hazard metrics used to underpin risk management decisions are re-evaluated to ensure societal aims are achieved.",
      "cited_by_count": 23,
      "type": "article",
      "source": {
        "name": "Seismological Research Letters",
        "type": "journal",
        "issn": [
          "0895-0695",
          "1938-2057"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Seismology and Earthquake Studies",
        "earthquake and tectonic studies",
        "Seismic Performance and Analysis"
      ],
      "referenced_works_count": 58,
      "url": "https://openalex.org/W3000112922"
    },
    {
      "openalex_id": "W3014596384",
      "doi": "10.1007/s10994-021-05946-3",
      "title": "Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods",
      "authors": [
        {
          "name": "Eyke H\u00fcllermeier",
          "openalex_id": "",
          "orcid": "https://orcid.org/0000-0002-9944-4108",
          "institutions": [
            "Paderborn University"
          ]
        },
        {
          "name": "Willem Waegeman",
          "openalex_id": "",
          "institutions": [
            "Ghent University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-03-01",
      "abstract": "Abstract The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic . In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.",
      "cited_by_count": 1275,
      "type": "article",
      "source": {
        "name": "Machine Learning",
        "type": "journal",
        "issn": [
          "0885-6125",
          "1573-0565"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s10994-021-05946-3.pdf"
      },
      "topics": [
        "Anomaly Detection Techniques and Applications",
        "Machine Learning and Data Classification",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 111,
      "url": "https://openalex.org/W3014596384"
    },
    {
      "openalex_id": "W2985145919",
      "doi": "10.1016/j.neucom.2019.09.101",
      "title": "Bayesian capsule networks for 3D human pose estimation from single 2D images",
      "authors": [
        {
          "name": "Iv\u00e1n Ram\u00edrez",
          "openalex_id": "A5012720340",
          "orcid": "https://orcid.org/0000-0001-5985-5271",
          "institutions": [
            "Universidad Rey Juan Carlos"
          ]
        },
        {
          "name": "Alfredo Cuesta\u2010Infante",
          "openalex_id": "A5069990831",
          "orcid": "https://orcid.org/0000-0002-3328-501X",
          "institutions": [
            "Universidad Rey Juan Carlos"
          ]
        },
        {
          "name": "Emanuele Schiavi",
          "openalex_id": "A5012713912",
          "orcid": "https://orcid.org/0000-0002-2790-307X",
          "institutions": [
            "Universidad Rey Juan Carlos"
          ]
        },
        {
          "name": "Juan Jos\u00e9 Pantrigo",
          "openalex_id": "A5016277811",
          "orcid": "https://orcid.org/0000-0002-7175-3371",
          "institutions": [
            "Universidad Rey Juan Carlos"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-11-02",
      "abstract": null,
      "cited_by_count": 27,
      "type": "article",
      "source": {
        "name": "Neurocomputing",
        "type": "journal",
        "issn": [
          "0925-2312",
          "1872-8286"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Human Pose and Action Recognition",
        "Anomaly Detection Techniques and Applications",
        "Advanced Neural Network Applications"
      ],
      "referenced_works_count": 66,
      "url": "https://openalex.org/W2985145919"
    },
    {
      "openalex_id": "W3201189722",
      "doi": "10.1007/s11673-021-10126-y",
      "title": "Unmasking the Ethics of Public Health Messaging in a Pandemic",
      "authors": [
        {
          "name": "Anita Ho",
          "openalex_id": "A5030165385",
          "orcid": "https://orcid.org/0000-0002-9797-1326",
          "institutions": [
            "University of British Columbia",
            "University of California, San Francisco"
          ]
        },
        {
          "name": "Vivian Huang",
          "openalex_id": "A5101955363",
          "orcid": "https://orcid.org/0000-0002-6255-5955",
          "institutions": [
            "University of British Columbia"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-09-24",
      "abstract": null,
      "cited_by_count": 18,
      "type": "article",
      "source": {
        "name": "Journal of Bioethical Inquiry",
        "type": "journal",
        "issn": [
          "1176-7529",
          "1872-4353"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "bronze",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s11673-021-10126-y.pdf"
      },
      "topics": [
        "Risk Perception and Management",
        "Misinformation and Its Impacts",
        "Vaccine Coverage and Hesitancy"
      ],
      "referenced_works_count": 56,
      "url": "https://openalex.org/W3201189722"
    },
    {
      "openalex_id": "W4385891197",
      "doi": "10.1145/3583780.3614875",
      "title": "Fairness through Aleatoric Uncertainty",
      "authors": [
        {
          "name": "Anique Tahir",
          "openalex_id": "A5016332375",
          "orcid": "https://orcid.org/0000-0002-2838-147X",
          "institutions": [
            "Arizona State University"
          ]
        },
        {
          "name": "Lu Cheng",
          "openalex_id": "A5040552536",
          "orcid": "https://orcid.org/0000-0002-2503-2522",
          "institutions": [
            "University of Illinois Chicago"
          ]
        },
        {
          "name": "Huan Liu",
          "openalex_id": "A5100338946",
          "orcid": "https://orcid.org/0000-0002-3264-7904",
          "institutions": [
            "Arizona State University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-10-21",
      "abstract": "We propose a simple yet effective solution to tackle the often-competing goals of fairness and utility in classification tasks. While fairness ensures that the model's predictions are unbiased and do not discriminate against any particular group or individual, utility focuses on maximizing the model's predictive performance. This work introduces the idea of leveraging aleatoric uncertainty (e.g., data ambiguity) to improve the fairness-utility trade-off. Our central hypothesis is that aleatoric uncertainty is a key factor for algorithmic fairness and samples with low aleatoric uncertainty are modeled more accurately and fairly than those with high aleatoric uncertainty. We then propose a principled model to improve fairness when aleatoric uncertainty is high and improve utility elsewhere. Our approach first intervenes in the data distribution to better decouple aleatoric uncertainty and epistemic uncertainty. It then introduces a fairness-utility bi-objective loss defined based on the estimated aleatoric uncertainty. Our approach is theoretically guaranteed to improve the fairness-utility trade-off. Experimental results on both tabular and image datasets show that the proposed approach outperforms state-of-the-art methods w.r.t. the fairness-utility trade-off and w.r.t. both group and individual fairness metrics. This work presents a fresh perspective on the trade-off between utility and algorithmic fairness and opens a key avenue for the potential of using prediction uncertainty in fair machine learning.",
      "cited_by_count": 10,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 28,
      "url": "https://openalex.org/W4385891197"
    },
    {
      "openalex_id": "W4380355461",
      "doi": "10.1145/3593013.3594045",
      "title": "On (assessing) the fairness of risk score models",
      "authors": [
        {
          "name": "Eike Petersen",
          "openalex_id": "A5064219363",
          "orcid": "https://orcid.org/0000-0003-0097-3868",
          "institutions": [
            "Technical University of Denmark"
          ]
        },
        {
          "name": "Melanie Ganz",
          "openalex_id": "A5030345952",
          "orcid": "https://orcid.org/0000-0002-9120-8098",
          "institutions": [
            "Rigshospitalet",
            "University of Copenhagen",
            "Copenhagen University Hospital"
          ]
        },
        {
          "name": "Sune Holm",
          "openalex_id": "A5017052411",
          "orcid": "https://orcid.org/0000-0002-3812-7942",
          "institutions": [
            "University of Copenhagen"
          ]
        },
        {
          "name": "Aasa Feragen",
          "openalex_id": "A5041988622",
          "orcid": "https://orcid.org/0000-0002-9945-981X",
          "institutions": [
            "Technical University of Denmark"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-06-12",
      "abstract": "Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such decisions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and meaningful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology \u2013 which is widely applicable in many other settings \u2013 in two case studies, one in recidivism risk prediction, and one in risk of major depressive disorder (MDD) prediction.",
      "cited_by_count": 17,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Health Systems, Economic Evaluations, Quality of Life",
        "Advanced Causal Inference Techniques",
        "Ethics in Clinical Research"
      ],
      "referenced_works_count": 81,
      "url": "https://openalex.org/W4380355461"
    },
    {
      "openalex_id": "W3134960678",
      "doi": "10.4324/9780429326769-44",
      "title": "An epistemic argument for democracy",
      "authors": [
        {
          "name": "H\u00e9l\u00e8ne Landemore",
          "openalex_id": "A5033903696",
          "orcid": "https://orcid.org/0000-0002-0726-4944"
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-03-02",
      "abstract": "Traditional justifications for democracy emphasize procedural or intrinsic arguments such as those based on the ideas of freedom, equality, justice, and fairness. There also exist, however, more instrumental arguments for democracy, which justify it on the basis of the nature of the outcomes that its inclusive and egalitarian procedures are supposed to generate. A specific variety of instrumental argument is epistemic, namely oriented toward the knowledge generated and aggregated by democratic procedures as well as, at least in some versions of the argument, the ability of democratic outcomes informed by such knowledge to track a form of truth about the common good (generally defined in pragmatic terms, if only as the ability of the polity to thrive in the face of unpredictable political events). In this chapter I rehearse my own, strong epistemic argument for democracy, whereby democracy is epistemically superior to all forms of oligarchy (including rule by the few best) because it is the regime form that, in the face of political uncertainty, best taps the collective intelligence of its people.",
      "cited_by_count": 11,
      "type": "book-chapter",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Political Philosophy and Ethics"
      ],
      "referenced_works_count": 1,
      "url": "https://openalex.org/W3134960678"
    },
    {
      "openalex_id": "W2987558718",
      "doi": "10.1080/13563467.2019.1685959",
      "title": "Money and the \u2018Level Playing Field\u2019: The Epistemic Problem of European Financial Market Integration",
      "authors": [
        {
          "name": "Troels Krarup",
          "openalex_id": "A5053601719",
          "orcid": "https://orcid.org/0000-0002-7239-2221",
          "institutions": [
            "University of Copenhagen"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-11-04",
      "abstract": "Financial market integration processes in the European Union (EU) are characterised by an epistemic problem of economic theory. This problem encompasses what \u2018the market\u2019 is, how it is to be \u2018integrated\u2019, and the nature and role of \u2018money\u2019 as infrastructure of the fully integrated market. The EU\u2019s legal framework has imported this epistemic problem along with the competitive conception of the market as described in economic theory \u2013 as a \u2018level playing field\u2019 for private exchange, under free, fair and ideally unrestrained competition. It manifests itself in European financial market integration processes, as exemplified in the article, via two otherwise disconnected areas of European Central Bank (ECB) activity: (a) the provision of central bank credit for the purpose of financial transaction settlement in the Eurozone; and (b) the conduct of ordinary monetary policy in the Eurozone. While the problem can be stabilised through legal, technical and other means, it remains latent, and may manifest itself again in unexpected ways, as happened in the wake of the 2008 financial crisis. Thus, contrary to ideologies that are widely understood as more or less coherent systems of doctrines, epistemic problems are characterised by specific tensions, contradictions and conceptual uncertainties.",
      "cited_by_count": 9,
      "type": "article",
      "source": {
        "name": "New Political Economy",
        "type": "journal",
        "issn": [
          "1356-3467",
          "1469-9923"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://vbn.aau.dk/da/publications/7c151c04-78b6-4d0b-a799-37b04dec9616"
      },
      "topics": [
        "Banking stability, regulation, efficiency",
        "Economic Theory and Policy",
        "European Monetary and Fiscal Policies"
      ],
      "referenced_works_count": 55,
      "url": "https://openalex.org/W2987558718"
    },
    {
      "openalex_id": "W4403386218",
      "doi": "10.1613/jair.1.16041",
      "title": "Uncertainty as a Fairness Measure",
      "authors": [
        {
          "name": "Selim Kuzucu",
          "openalex_id": "A5002410692",
          "orcid": "https://orcid.org/0000-0003-2304-9192"
        },
        {
          "name": "Jiaee Cheong",
          "openalex_id": "A5036204035",
          "institutions": [
            "University of Cambridge",
            "Bridge University"
          ]
        },
        {
          "name": "Hatice G\u00fcne\u015f",
          "openalex_id": "A5060090893",
          "orcid": "https://orcid.org/0000-0003-2407-3012"
        },
        {
          "name": "Sinan Kalkan",
          "openalex_id": "A5032424779",
          "orcid": "https://orcid.org/0000-0003-0915-5917"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-10-13",
      "abstract": "Unfair predictions of machine learning (ML) models impede their broad acceptance in real-world settings. Tackling this arduous challenge first necessitates defining what it means for an ML model to be fair. This has been addressed by the ML community with various measures of fairness that depend on the prediction outcomes of the ML models, either at the group-level or the individual-level. These fairness measures are limited in that they utilize point predictions, neglecting their variances, or uncertainties, making them susceptible to noise, missingness and shifts in data. In this paper, we first show that a ML model may appear to be fair with existing point-based fairness measures but biased against a demographic group in terms of prediction uncertainties. Then, we introduce new fairness measures based on different types of uncertainties, namely, aleatoric uncertainty and epistemic uncertainty. We demonstrate on many datasets that (i) our uncertaintybased measures are complementary to existing measures of fairness, and (ii) they provide more insights about the underlying issues leading to bias.",
      "cited_by_count": 4,
      "type": "article",
      "source": {
        "name": "Journal of Artificial Intelligence Research",
        "type": "journal",
        "issn": [
          "1076-9757",
          "1943-5037"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "https://www.jair.org/index.php/jair/article/download/16041/27089"
      },
      "topics": [
        "Epistemology, Ethics, and Metaphysics",
        "Risk Perception and Management"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4403386218"
    },
    {
      "openalex_id": "W2760944227",
      "doi": "10.1016/j.jarmac.2017.07.008",
      "title": "Beyond misinformation: Understanding and coping with the \u201cpost-truth\u201d era.",
      "authors": [
        {
          "name": "Stephan Lewandowsky",
          "openalex_id": "A5017914184",
          "orcid": "https://orcid.org/0000-0003-1655-2013",
          "institutions": [
            "University of Western Australia"
          ]
        },
        {
          "name": "Ullrich K. H. Ecker",
          "openalex_id": "A5043657584",
          "orcid": "https://orcid.org/0000-0003-4743-313X",
          "institutions": [
            "University of Western Australia"
          ]
        },
        {
          "name": "John Cook",
          "openalex_id": "A5085938112",
          "orcid": "https://orcid.org/0000-0001-8488-6766",
          "institutions": [
            "Bridge University"
          ]
        }
      ],
      "publication_year": 2017,
      "publication_date": "2017-10-12",
      "abstract": "The terms \"post-truth\" and \"fake news\" have become increasingly prevalent in public discourse over the last year. This article explores the growing abundance of misinformation, how it influences people, and how to counter it. We examine the ways in which misinformation can have an adverse impact on society. We summarize how people respond to corrections of misinformation, and what kinds of corrections are most effective. We argue that to be effective, scientific research into misinformation must be considered within a larger political, technological, and societal context. The post-truth world emerged as a result of societal mega-trends such as a decline in social capital, growing economic inequality, increased polarization, declining trust in science, and an increasingly fractionated media landscape. We suggest that responses to this malaise must involve technological solutions incorporating psychological principles, an interdisciplinary approach that we describe as \"technocognition.\" We outline a number of recommendations to counter misinformation in a post-truth world.",
      "cited_by_count": 1766,
      "type": "article",
      "source": {
        "name": "Journal of Applied Research in Memory and Cognition",
        "type": "journal",
        "issn": [
          "2211-3681",
          "2211-369X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://research-repository.uwa.edu.au/en/publications/d1dffe1f-9d10-4f59-a0e9-fa6679f7b18b"
      },
      "topics": [
        "Misinformation and Its Impacts",
        "Media Influence and Health",
        "Social Media and Politics"
      ],
      "referenced_works_count": 141,
      "url": "https://openalex.org/W2760944227"
    },
    {
      "openalex_id": "W3131457744",
      "doi": "10.1016/j.artint.2021.103473",
      "title": "What do we want from Explainable Artificial Intelligence (XAI)? \u2013 A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research",
      "authors": [
        {
          "name": "Markus Langer",
          "openalex_id": "A5038410639",
          "orcid": "https://orcid.org/0000-0002-8165-1803",
          "institutions": [
            "Saarland University"
          ]
        },
        {
          "name": "Daniel Oster",
          "openalex_id": "A5052038529",
          "orcid": "https://orcid.org/0000-0003-2761-6262",
          "institutions": [
            "Saarland University"
          ]
        },
        {
          "name": "Timo Speith",
          "openalex_id": "A5007325624",
          "orcid": "https://orcid.org/0000-0002-6675-154X",
          "institutions": [
            "Saarland University"
          ]
        },
        {
          "name": "Holger Hermanns",
          "openalex_id": "A5028747794",
          "orcid": "https://orcid.org/0000-0002-2766-9615",
          "institutions": [
            "Saarland University",
            "Institute of Software"
          ]
        },
        {
          "name": "Lena K\u00e4stner",
          "openalex_id": "A5034952055",
          "orcid": "https://orcid.org/0000-0002-8747-6911",
          "institutions": [
            "Saarland University"
          ]
        },
        {
          "name": "Eva Schmidt",
          "openalex_id": "A5075670033",
          "orcid": "https://orcid.org/0000-0002-7305-7126",
          "institutions": [
            "TU Dortmund University"
          ]
        },
        {
          "name": "Andreas Sesing-Wagenpfeil",
          "openalex_id": "A5027323776",
          "orcid": "https://orcid.org/0000-0003-0894-5407",
          "institutions": [
            "Saarland University"
          ]
        },
        {
          "name": "Kevin Baum",
          "openalex_id": "A5012518399",
          "orcid": "https://orcid.org/0000-0002-6893-573X",
          "institutions": [
            "Saarland University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-02-15",
      "abstract": null,
      "cited_by_count": 517,
      "type": "article",
      "source": {
        "name": "Artificial Intelligence",
        "type": "journal",
        "issn": [
          "0004-3702",
          "1872-7921"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "bronze",
        "oa_url": "https://www.sciencedirect.com/science/article/pii/S0004370221000242"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Artificial Intelligence in Healthcare and Education",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 325,
      "url": "https://openalex.org/W3131457744"
    },
    {
      "openalex_id": "W3021029826",
      "doi": "10.1109/ciss48834.2020.1570627767",
      "title": "On Mismatched Detection and Safe, Trustworthy Machine Learning",
      "authors": [
        {
          "name": "Kush R. Varshney",
          "openalex_id": "A5015286159",
          "orcid": "https://orcid.org/0000-0002-7376-5536",
          "institutions": [
            "IBM (United States)"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-03-01",
      "abstract": "Instilling trust in high-stakes applications of machine learning is becoming essential. Trust may be decomposed into four dimensions: basic accuracy, reliability, human interaction, and aligned purpose. The first two of these also constitute the properties of safe machine learning systems. The second dimension, reliability, is mainly concerned with being robust to epistemic uncertainty and model mismatch. It arises in the machine learning paradigms of distribution shift, data poisoning attacks, and algorithmic fairness. All of these problems can be abstractly modeled using the theory of mismatched hypothesis testing from statistical signal processing. By doing so, we can take advantage of performance characterizations in that literature to better understand the various machine learning issues.",
      "cited_by_count": 7,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Distributed Sensor Networks and Detection Algorithms",
        "Advanced Statistical Process Monitoring"
      ],
      "referenced_works_count": 38,
      "url": "https://openalex.org/W3021029826"
    },
    {
      "openalex_id": "W4309973150",
      "doi": "10.1109/acii55700.2022.9953850",
      "title": "Bias Reducing Multitask Learning on Mental Health Prediction",
      "authors": [
        {
          "name": "Khadija Zanna",
          "openalex_id": "A5040203552",
          "orcid": "https://orcid.org/0000-0002-3553-303X",
          "institutions": [
            "Rice University"
          ]
        },
        {
          "name": "Kusha Sridhar",
          "openalex_id": "A5113916280",
          "institutions": [
            "Rice University"
          ]
        },
        {
          "name": "Han Yu",
          "openalex_id": "A5101836669",
          "orcid": "https://orcid.org/0000-0003-1944-4173",
          "institutions": [
            "Rice University"
          ]
        },
        {
          "name": "Akane Sano",
          "openalex_id": "A5061041118",
          "orcid": "https://orcid.org/0000-0003-4484-8946",
          "institutions": [
            "Rice University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-10-18",
      "abstract": "There has been an increase in research in developing machine learning models for mental health detection or prediction in recent years due to increased mental health issues in society. Effective use of mental health prediction or detection models can help mental health practitioners re-define mental illnesses more objectively than currently done, and identify illnesses at an earlier stage when interventions may be more effective. However, there is still a lack of standard in evaluating bias in such machine learning models in the field, which leads to challenges in providing reliable predictions and in addressing disparities. This lack of standards persists due to factors such as technical difficulties, complexities of high dimensional clinical health data, etc., which are especially true for physiological signals. This along with prior evidence of relations between some physiological signals with certain demographic identities restates the importance of exploring bias in mental health prediction models that utilize physiological signals. In this work, we aim to perform a fairness analysis and implement a multi-task learning based bias mitigation method on anxiety prediction models using ECG data. Our method is based on the idea of epistemic uncertainty and its relationship with model weights and feature space representation. Our analysis showed that our anxiety prediction base model introduced some bias with regards to age, income, ethnicity, and whether a participant is born in the U.S. or not, and our bias mitigation method performed better at reducing the bias in the model, when compared to the reweighting mitigation technique. Our analysis on feature importance also helped identify relationships between heart rate variability and multiple demographic groupings.",
      "cited_by_count": 17,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Mental Health Research Topics",
        "Health, Environment, Cognitive Aging",
        "Functional Brain Connectivity Studies"
      ],
      "referenced_works_count": 51,
      "url": "https://openalex.org/W4309973150"
    },
    {
      "openalex_id": "W2927351257",
      "doi": "10.1002/widm.1312",
      "title": "Causability and explainability of artificial intelligence in medicine",
      "authors": [
        {
          "name": "Andreas Holzinger",
          "openalex_id": "A5034657358",
          "orcid": "https://orcid.org/0000-0002-6786-5194",
          "institutions": [
            "Medical University of Graz"
          ]
        },
        {
          "name": "Georg Langs",
          "openalex_id": "A5060814361",
          "orcid": "https://orcid.org/0000-0002-5536-6873",
          "institutions": [
            "Medical University of Vienna"
          ]
        },
        {
          "name": "Helmut Denk",
          "openalex_id": "A5072719639",
          "orcid": "https://orcid.org/0000-0001-6959-0828",
          "institutions": [
            "Medical University of Graz"
          ]
        },
        {
          "name": "Kurt Zatloukal",
          "openalex_id": "A5032058761",
          "orcid": "https://orcid.org/0000-0001-5299-7218",
          "institutions": [
            "Medical University of Graz"
          ]
        },
        {
          "name": "Heimo M\u00fcller",
          "openalex_id": "A5087013593",
          "orcid": "https://orcid.org/0000-0002-9691-4872",
          "institutions": [
            "Medical University of Graz"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-04-02",
      "abstract": "Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black\u2010box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use\u2010case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system This article is categorized under: Fundamental Concepts of Data and Knowledge &gt; Human Centricity and User Interaction",
      "cited_by_count": 1499,
      "type": "review",
      "source": {
        "name": "Wiley Interdisciplinary Reviews Data Mining and Knowledge Discovery",
        "type": "journal",
        "issn": [
          "1942-4787",
          "1942-4795"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/widm.1312"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Machine Learning in Healthcare",
        "AI in cancer detection"
      ],
      "referenced_works_count": 88,
      "url": "https://openalex.org/W2927351257"
    },
    {
      "openalex_id": "W4280546417",
      "doi": "10.14324/rfa.06.1.12",
      "title": "A case study from Guyana of adapting engaged research design to promote \u2018fairness in knowing\u2019",
      "authors": [
        {
          "name": "Richard Holliman",
          "openalex_id": "A5024934384",
          "orcid": "https://orcid.org/0000-0002-4587-0929"
        },
        {
          "name": "Alessandra Marino",
          "openalex_id": "A5033190530",
          "orcid": "https://orcid.org/0000-0001-7626-0675"
        },
        {
          "name": "Ann Grand",
          "openalex_id": "A5062278994",
          "orcid": "https://orcid.org/0000-0003-4384-5997"
        },
        {
          "name": "Andrea Berardi",
          "openalex_id": "A5070600830",
          "orcid": "https://orcid.org/0000-0003-3546-4916"
        },
        {
          "name": "Jayalaxshmi Mistry",
          "openalex_id": "A5021368316",
          "orcid": "https://orcid.org/0000-0001-7582-3739"
        },
        {
          "name": "Deirdre Jafferally",
          "openalex_id": "A5018749357",
          "orcid": "https://orcid.org/0000-0003-0532-9246"
        },
        {
          "name": "Raquel Thomas",
          "openalex_id": "A5106634074"
        },
        {
          "name": "G.T. Roberts",
          "openalex_id": "A5101694022",
          "orcid": "https://orcid.org/0000-0003-2427-624X"
        },
        {
          "name": "Carol-Ann Marcus",
          "openalex_id": "A5018875423"
        },
        {
          "name": "Indranee Roopsind",
          "openalex_id": "A5026404452"
        },
        {
          "name": "A. J. Roberts",
          "openalex_id": "A5042609947",
          "orcid": "https://orcid.org/0000-0001-5277-6730"
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-01-01",
      "abstract": "In this paper, we have combined ideas drawn from philosophy (epistemic injustice), critical theory (epistemicide) and practical approaches (engaged research design) with Indigenous knowledge to promote \u2018fairness in knowing\u2019 in a project called DETECT (Integrate d Spac e T echnology V e ctor C on t rol for Enhancing community health and resilience against escalating climatic disruptions), an early warning system to support communities in identifying mosquito breeding sites using satellite, drone and ground-sensing technologies. DETECT used engaged research design to inform pre-award planning. We document how the project team, comprising Indigenous and other researchers, re-imagined the plans in the light of the COVID-19 pandemic to allow project participants to meet safely and equitably, and reflect on some of the key challenges in engaging across borders and cultures in the context of rapidly changing conditions characterised by vulnerability, risk, complexity and uncertainty.",
      "cited_by_count": 6,
      "type": "article",
      "source": {
        "name": "Research for All",
        "type": "journal",
        "issn": [
          "2399-8121"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "https://www.scienceopen.com/document_file/dd298907-e09e-432a-ab59-9b782f7f0bfc/ScienceOpen/rfa06010012.pdf"
      },
      "topics": [
        "Community Development and Social Impact"
      ],
      "referenced_works_count": 36,
      "url": "https://openalex.org/W4280546417"
    },
    {
      "openalex_id": "W3047855151",
      "doi": "10.1038/s41467-020-17591-w",
      "title": "Earthquake transformer\u2014an attentive deep-learning model for simultaneous earthquake detection and phase picking",
      "authors": [
        {
          "name": "S. Mostafa Mousavi",
          "openalex_id": "A5027330844",
          "orcid": "https://orcid.org/0000-0001-5091-5370",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "William L. Ellsworth",
          "openalex_id": "A5042043834",
          "orcid": "https://orcid.org/0000-0001-8378-4979",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Weiqiang Zhu",
          "openalex_id": "A5070811279",
          "orcid": "https://orcid.org/0000-0003-2889-1493",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Lindsay Chuang",
          "openalex_id": "A5070377208",
          "orcid": "https://orcid.org/0000-0002-7349-1862",
          "institutions": [
            "Georgia Institute of Technology"
          ]
        },
        {
          "name": "Gregory C. Beroza",
          "openalex_id": "A5084849254",
          "orcid": "https://orcid.org/0000-0002-8667-1838",
          "institutions": [
            "Stanford University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-08-07",
      "abstract": "Abstract Earthquake signal detection and seismic phase picking are challenging tasks in the processing of noisy data and the monitoring of microearthquakes. Here we present a global deep-learning model for simultaneous earthquake detection and phase picking. Performing these two related tasks in tandem improves model performance in each individual task by combining information in phases and in the full waveform of earthquake signals by using a hierarchical attention mechanism. We show that our model outperforms previous deep-learning and traditional phase-picking and detection algorithms. Applying our model to 5 weeks of continuous data recorded during 2000 Tottori earthquakes in Japan, we were able to detect and locate two times more earthquakes using only a portion (less than 1/3) of seismic stations. Our model picks P and S phases with precision close to manual picks by human analysts; however, its high efficiency and higher sensitivity can result in detecting and characterizing more and smaller events.",
      "cited_by_count": 935,
      "type": "article",
      "source": {
        "name": "Nature Communications",
        "type": "journal",
        "issn": [
          "2041-1723"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.nature.com/articles/s41467-020-17591-w.pdf"
      },
      "topics": [
        "Seismology and Earthquake Studies",
        "Seismic Waves and Analysis",
        "earthquake and tectonic studies"
      ],
      "referenced_works_count": 61,
      "url": "https://openalex.org/W3047855151"
    },
    {
      "openalex_id": "W4221045317",
      "doi": "10.6028/nist.sp.1270",
      "title": "Towards a standard for identifying and managing bias in artificial intelligence",
      "authors": [
        {
          "name": "Reva Schwartz",
          "openalex_id": "A5026537470",
          "orcid": "https://orcid.org/0000-0002-9012-6306",
          "institutions": [
            "National Institute of Standards and Technology",
            "Information Technology Laboratory"
          ]
        },
        {
          "name": "Apostol Vassilev",
          "openalex_id": "A5013116110",
          "orcid": "https://orcid.org/0000-0002-9081-3042",
          "institutions": [
            "National Institute of Standards and Technology",
            "Information Technology Laboratory"
          ]
        },
        {
          "name": "Kristen Greene",
          "openalex_id": "A5073638056",
          "orcid": "https://orcid.org/0000-0001-7034-3672",
          "institutions": [
            "National Institute of Standards and Technology",
            "Information Technology Laboratory"
          ]
        },
        {
          "name": "Lori Perine",
          "openalex_id": "A5030817406",
          "institutions": [
            "Information Technology Laboratory",
            "National Institute of Standards and Technology"
          ]
        },
        {
          "name": "Andrew Burt",
          "openalex_id": "A5060327845",
          "orcid": "https://orcid.org/0000-0002-4209-8101",
          "institutions": [
            "National Institute of Standards and Technology",
            "National Institute of Standards"
          ]
        },
        {
          "name": "Patrick Hall",
          "openalex_id": "A5102849740",
          "orcid": "https://orcid.org/0000-0003-1167-5188",
          "institutions": [
            "National Institute of Standards",
            "National Institute of Standards and Technology"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-03-15",
      "abstract": "As individuals and communities interact in and with an environment that is increasingly virtual they are often vulnerable to the commodification of their digital exhaust. Concepts and behavior that are ambiguous in nature are captured in this environment, quantified, and used to categorize, sort, recommend, or make decisions about people's lives. While many organizations seek to utilize this information in a responsible manner, biases remain endemic across technology processes and can lead to harmful impacts regardless of intent. These harmful outcomes, even if inadvertent, create significant challenges for cultivating public trust in artificial intelligence (AI). SP 1270 is a NIST Artificial Intelligence publication and should be read in conjunction with all publications in the NIST AI Series, which was established in January 2023.",
      "cited_by_count": 450,
      "type": "report",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://doi.org/10.6028/nist.sp.1270"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Artificial Intelligence in Healthcare and Education",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 250,
      "url": "https://openalex.org/W4221045317"
    },
    {
      "openalex_id": "W4294052458",
      "doi": "10.1038/s41586-022-05224-9",
      "title": "Comprehensive evidence implies a higher social cost of CO2",
      "authors": [
        {
          "name": "Kevin Rennert",
          "openalex_id": "A5052502716",
          "institutions": [
            "Resources For The Future"
          ]
        },
        {
          "name": "Frank Errickson",
          "openalex_id": "A5017670288",
          "orcid": "https://orcid.org/0000-0003-2195-9424",
          "institutions": [
            "Princeton Public Schools",
            "Princeton University"
          ]
        },
        {
          "name": "Brian Prest",
          "openalex_id": "A5080500442",
          "orcid": "https://orcid.org/0000-0002-9251-3393",
          "institutions": [
            "Resources For The Future"
          ]
        },
        {
          "name": "Lisa Rennels",
          "openalex_id": "A5070403772",
          "orcid": "https://orcid.org/0000-0002-5307-9336",
          "institutions": [
            "University of California, Berkeley"
          ]
        },
        {
          "name": "Richard G. Newell",
          "openalex_id": "A5062158201",
          "orcid": "https://orcid.org/0000-0002-3205-5562",
          "institutions": [
            "Resources For The Future"
          ]
        },
        {
          "name": "William A. Pizer",
          "openalex_id": "A5062454052",
          "orcid": "https://orcid.org/0000-0003-1498-1148",
          "institutions": [
            "Resources For The Future"
          ]
        },
        {
          "name": "Cora Kingdon",
          "openalex_id": "A5007633697",
          "orcid": "https://orcid.org/0000-0001-6207-1048",
          "institutions": [
            "University of California, Berkeley"
          ]
        },
        {
          "name": "Jordan Wingenroth",
          "openalex_id": "A5019110635",
          "orcid": "https://orcid.org/0000-0002-7970-841X",
          "institutions": [
            "Resources For The Future"
          ]
        },
        {
          "name": "Roger Cooke",
          "openalex_id": "A5023863881",
          "orcid": "https://orcid.org/0000-0003-0643-1971",
          "institutions": [
            "Resources For The Future"
          ]
        },
        {
          "name": "Bryan Parthum",
          "openalex_id": "A5040479412",
          "orcid": "https://orcid.org/0000-0002-9996-2183",
          "institutions": [
            "Environmental Protection Agency",
            "Environmental Protection Agency"
          ]
        },
        {
          "name": "David J. Smith",
          "openalex_id": "A5077806163",
          "orcid": "https://orcid.org/0000-0003-1886-8193",
          "institutions": [
            "Environmental Protection Agency",
            "Environmental Protection Agency"
          ]
        },
        {
          "name": "Kevin Cromar",
          "openalex_id": "A5057100016",
          "orcid": "https://orcid.org/0000-0002-7745-2780",
          "institutions": [
            "University of Akron",
            "New York University"
          ]
        },
        {
          "name": "Delavane Diaz",
          "openalex_id": "A5081461621",
          "orcid": "https://orcid.org/0000-0001-8289-0980",
          "institutions": [
            "Electric Power Research Institute"
          ]
        },
        {
          "name": "Frances C. Moore",
          "openalex_id": "A5057261816",
          "orcid": "https://orcid.org/0000-0003-3866-9642",
          "institutions": [
            "University of California, Davis"
          ]
        },
        {
          "name": "Ulrich K. M\u00fcller",
          "openalex_id": "A5030971614",
          "institutions": [
            "Princeton University"
          ]
        },
        {
          "name": "Richard J. Plevin",
          "openalex_id": "A5040148242",
          "orcid": "https://orcid.org/0000-0003-3919-0836",
          "institutions": [
            "Oldham Council",
            "University of Portland"
          ]
        },
        {
          "name": "Adrian E. Raftery",
          "openalex_id": "A5064414755",
          "orcid": "https://orcid.org/0000-0002-6589-301X",
          "institutions": [
            "University of Washington"
          ]
        },
        {
          "name": "Hana \u0160ev\u010d\u00edkov\u00e1",
          "openalex_id": "A5077243781",
          "orcid": "https://orcid.org/0000-0001-8674-0410",
          "institutions": [
            "University of Washington"
          ]
        },
        {
          "name": "Hannah Sheets",
          "openalex_id": "A5023418623",
          "institutions": [
            "Rochester Institute of Technology"
          ]
        },
        {
          "name": "James H. Stock",
          "openalex_id": "A5113502697",
          "institutions": [
            "Harvard University"
          ]
        },
        {
          "name": "Tammy Tan",
          "openalex_id": "A5028724681",
          "orcid": "https://orcid.org/0000-0002-3236-5421",
          "institutions": [
            "Environmental Protection Agency",
            "Environmental Protection Agency"
          ]
        },
        {
          "name": "Mark W. Watson",
          "openalex_id": "A5103128342",
          "orcid": "https://orcid.org/0009-0009-9986-3466",
          "institutions": [
            "Princeton University"
          ]
        },
        {
          "name": "Tony E. Wong",
          "openalex_id": "A5034481835",
          "orcid": "https://orcid.org/0000-0002-7304-3883",
          "institutions": [
            "Rochester Institute of Technology"
          ]
        },
        {
          "name": "David Anthoff",
          "openalex_id": "A5010225282",
          "orcid": "https://orcid.org/0000-0001-9319-2109",
          "institutions": [
            "University of California, Berkeley"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-09-01",
      "abstract": "Abstract The social cost of carbon dioxide (SC-CO 2 ) measures the monetized value of the damages to society caused by an incremental metric tonne of CO 2 emissions and is a key metric informing climate policy. Used by governments and other decision-makers in benefit\u2013cost analysis for over a decade, SC-CO 2 estimates draw on climate science, economics, demography and other disciplines. However, a 2017 report by the US National Academies of Sciences, Engineering, and Medicine 1 (NASEM) highlighted that current SC-CO 2 estimates no longer reflect the latest research. The report provided a series of recommendations for improving the scientific basis, transparency and uncertainty characterization of SC-CO 2 estimates. Here we show that improved probabilistic socioeconomic projections, climate models, damage functions, and discounting methods that collectively reflect theoretically consistent valuation of risk, substantially increase estimates of the SC-CO 2 . Our preferred mean SC-CO 2 estimate is $185 per tonne of CO 2 ($44\u2013$413 per tCO 2 : 5%\u201395% range, 2020 US dollars) at a near-term risk-free discount rate of 2%, a value 3.6 times higher than the US government\u2019s current value of $51 per tCO 2 . Our estimates incorporate updated scientific understanding throughout all components of SC-CO 2 estimation in the new open-source Greenhouse Gas Impact Value Estimator (GIVE) model, in a manner fully responsive to the near-term NASEM recommendations. Our higher SC-CO 2 values, compared with estimates currently used in policy evaluation, substantially increase the estimated benefits of greenhouse gas mitigation and thereby increase the expected net benefits of more stringent climate policies.",
      "cited_by_count": 865,
      "type": "article",
      "source": {
        "name": "Nature",
        "type": "journal",
        "issn": [
          "0028-0836",
          "1476-4687"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.nature.com/articles/s41586-022-05224-9.pdf"
      },
      "topics": [
        "Climate Change Policy and Economics",
        "Atmospheric and Environmental Gas Dynamics",
        "Energy, Environment, and Transportation Policies"
      ],
      "referenced_works_count": 96,
      "url": "https://openalex.org/W4294052458"
    },
    {
      "openalex_id": "W4399355294",
      "doi": "10.48550/arxiv.2406.00332",
      "title": "A Structured Review of Literature on Uncertainty in Machine Learning &amp; Deep Learning",
      "authors": [
        {
          "name": "Fahimeh Fakour",
          "openalex_id": "A5020609353"
        },
        {
          "name": "Ali Mosleh",
          "openalex_id": "A5084581830",
          "orcid": "https://orcid.org/0000-0002-4628-1050"
        },
        {
          "name": "Ramin Ramezani",
          "openalex_id": "A5022329872",
          "orcid": "https://orcid.org/0000-0001-9276-6932"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-06-01",
      "abstract": "The adaptation and use of Machine Learning (ML) in our daily lives has led to concerns in lack of transparency, privacy, reliability, among others. As a result, we are seeing research in niche areas such as interpretability, causality, bias and fairness, and reliability. In this survey paper, we focus on a critical concern for adaptation of ML in risk-sensitive applications, namely understanding and quantifying uncertainty. Our paper approaches this topic in a structured way, providing a review of the literature in the various facets that uncertainty is enveloped in the ML process. We begin by defining uncertainty and its categories (e.g., aleatoric and epistemic), understanding sources of uncertainty (e.g., data and model), and how uncertainty can be assessed in terms of uncertainty quantification techniques (Ensembles, Bayesian Neural Networks, etc.). As part of our assessment and understanding of uncertainty in the ML realm, we cover metrics for uncertainty quantification for a single sample, dataset, and metrics for accuracy of the uncertainty estimation itself. This is followed by discussions on calibration (model and uncertainty), and decision making under uncertainty. Thus, we provide a more complete treatment of uncertainty: from the sources of uncertainty to the decision-making process. We have focused the review of uncertainty quantification methods on Deep Learning (DL), while providing the necessary background for uncertainty discussion within ML in general. Key contributions in this review are broadening the scope of uncertainty discussion, as well as an updated review of uncertainty quantification methods in DL.",
      "cited_by_count": 5,
      "type": "review",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2406.00332"
      },
      "topics": [
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4399355294"
    },
    {
      "openalex_id": "W3088209892",
      "doi": "10.3389/fpubh.2020.00457",
      "title": "Delphi Technique in Health Sciences: A Map",
      "authors": [
        {
          "name": "Marlen Niederberger",
          "openalex_id": "A5000902875",
          "orcid": "https://orcid.org/0000-0002-4892-7222",
          "institutions": [
            "University of Education Schwaebisch Gmuend"
          ]
        },
        {
          "name": "Julia Spranger",
          "openalex_id": "A5019982287",
          "orcid": "https://orcid.org/0000-0002-1802-8138",
          "institutions": [
            "University of Education Schwaebisch Gmuend"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-09-22",
      "abstract": "<b>Objectives:</b> In health sciences, the Delphi technique is primarily used by researchers when the available knowledge is incomplete or subject to uncertainty and other methods that provide higher levels of evidence cannot be used. The aim is to collect expert-based judgments and often to use them to identify consensus. In this map, we provide an overview of the fields of application for Delphi techniques in health sciences in this map and discuss the processes used and the quality of the findings. We use systematic reviews of Delphi techniques for the map, summarize their findings and examine them from a methodological perspective. <b>Methods:</b> Twelve systematic reviews of Delphi techniques from different sectors of the health sciences were identified and systematically analyzed. <b>Results:</b> The 12 systematic reviews show, that Delphi studies are typically carried out in two to three rounds with a deliberately selected panel of experts. A large number of modifications to the Delphi technique have now been developed. Significant weaknesses exist in the quality of the reporting. <b>Conclusion:</b> Based on the results, there is a need for clarification with regard to the methodological approaches of Delphi techniques, also with respect to any modification. Criteria for evaluating the quality of their execution and reporting also appear to be necessary. However, it should be noted that we cannot make any statements about the quality of execution of the Delphi studies but rather our results are exclusively based on the reported findings of the systematic reviews.",
      "cited_by_count": 925,
      "type": "article",
      "source": {
        "name": "Frontiers in Public Health",
        "type": "journal",
        "issn": [
          "2296-2565"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.frontiersin.org/articles/10.3389/fpubh.2020.00457/pdf"
      },
      "topics": [
        "Delphi Technique in Research"
      ],
      "referenced_works_count": 53,
      "url": "https://openalex.org/W3088209892"
    },
    {
      "openalex_id": "W3092026988",
      "doi": "10.1029/2020wr028091",
      "title": "What Role Does Hydrological Science Play in the Age of Machine Learning?",
      "authors": [
        {
          "name": "Grey Nearing",
          "openalex_id": "A5056778863",
          "orcid": "https://orcid.org/0000-0001-7031-6770",
          "institutions": [
            "University of California, Davis"
          ]
        },
        {
          "name": "Frederik Kratzert",
          "openalex_id": "A5005882134",
          "orcid": "https://orcid.org/0000-0002-8897-7689",
          "institutions": [
            "Johannes Kepler University of Linz"
          ]
        },
        {
          "name": "Alden Keefe Sampson",
          "openalex_id": "A5025379780",
          "orcid": "https://orcid.org/0000-0003-1853-3713",
          "institutions": [
            "Natel Energy (United States)"
          ]
        },
        {
          "name": "Craig Pelissier",
          "openalex_id": "A5025372662",
          "orcid": "https://orcid.org/0000-0003-0466-3517",
          "institutions": [
            "Goddard Space Flight Center"
          ]
        },
        {
          "name": "Daniel Klotz",
          "openalex_id": "A5053159516",
          "orcid": "https://orcid.org/0000-0002-9843-6798",
          "institutions": [
            "Johannes Kepler University of Linz"
          ]
        },
        {
          "name": "Jonathan Frame",
          "openalex_id": "A5032528778",
          "orcid": "https://orcid.org/0000-0002-2533-3843",
          "institutions": [
            "University of California, Davis"
          ]
        },
        {
          "name": "Cristina Prieto",
          "openalex_id": "A5045612183",
          "orcid": "https://orcid.org/0000-0002-6693-0396",
          "institutions": [
            "Instituto de F\u00edsica de Cantabria",
            "Universidad de Cantabria"
          ]
        },
        {
          "name": "Hoshin V. Gupta",
          "openalex_id": "A5045129894",
          "institutions": [
            "University of Arizona"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-11-14",
      "abstract": "Abstract This paper is derived from a keynote talk given at the Google's 2020 Flood Forecasting Meets Machine Learning Workshop. Recent experiments applying deep learning to rainfall\u2010runoff simulation indicate that there is significantly more information in large\u2010scale hydrological data sets than hydrologists have been able to translate into theory or models. While there is a growing interest in machine learning in the hydrological sciences community, in many ways, our community still holds deeply subjective and nonevidence\u2010based preferences for models based on a certain type of \u201cprocess understanding\u201d that has historically not translated into accurate theory, models, or predictions. This commentary is a call to action for the hydrology community to focus on developing a quantitative understanding of where and when hydrological process understanding is valuable in a modeling discipline increasingly dominated by machine learning. We offer some potential perspectives and preliminary examples about how this might be accomplished.",
      "cited_by_count": 670,
      "type": "article",
      "source": {
        "name": "Water Resources Research",
        "type": "journal",
        "issn": [
          "0043-1397",
          "1944-7973"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "bronze",
        "oa_url": "https://agupubs.onlinelibrary.wiley.com/doi/pdfdirect/10.1029/2020WR028091"
      },
      "topics": [
        "Hydrology and Watershed Management Studies",
        "Flood Risk Assessment and Management",
        "Hydrological Forecasting Using AI"
      ],
      "referenced_works_count": 116,
      "url": "https://openalex.org/W3092026988"
    },
    {
      "openalex_id": "W4388160307",
      "doi": "10.1145/3617694.3623262",
      "title": "Policy Fairness and Unknown Bias Dynamics in Sequential Allocations",
      "authors": [
        {
          "name": "Meirav Segal",
          "openalex_id": "A5044429722",
          "orcid": "https://orcid.org/0000-0001-7145-9159",
          "institutions": [
            "University of Oslo"
          ]
        },
        {
          "name": "Anne-Marie George",
          "openalex_id": "A5004569366",
          "orcid": "https://orcid.org/0000-0001-9232-8211",
          "institutions": [
            "University of Oslo"
          ]
        },
        {
          "name": "Christos Dimitrakakis",
          "openalex_id": "A5017471254",
          "orcid": "https://orcid.org/0000-0002-5367-5189",
          "institutions": [
            "University of Neuch\u00e2tel"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-10-29",
      "abstract": "This work considers a dynamic decision making framework for allocating opportunities over time to advantaged and disadvantaged individuals, focusing on the example of college admissions. Here, individuals in the disadvantaged group are assumed to experience a societal bias that limits their success probability. Bias dynamics dictate how the societal bias changes based on the current allocation of opportunities. We model this environment as a Markov Decision Process (MDP) and empirically examine the purely utility maximising policy in terms of fairness. We demonstrate the influence of the bias dynamics on long-term fairness of allocations, and analyse the interplay between utility and policy-fairness for different dynamics under different optimisation parameters. We consider the cases of known and unknown bias dynamics. For known dynamics, we show that a short horizon view presents fairness as a trade-off for utility, but a long horizon view reveals that the two are aligned. Moreover, we suggest that when the dynamics are unknown, the approach towards epistemic uncertainty may also affect fairness, and should be considered when designing fair decision making models.",
      "cited_by_count": 2,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3617694.3623262"
      },
      "topics": [
        "Advanced Causal Inference Techniques",
        "Experimental Behavioral Economics Studies",
        "Economic Policies and Impacts"
      ],
      "referenced_works_count": 28,
      "url": "https://openalex.org/W4388160307"
    },
    {
      "openalex_id": "W3136000444",
      "doi": "10.1136/medethics-2020-106820",
      "title": "Who is afraid of black box algorithms? On the epistemological and ethical basis of trust in medical AI",
      "authors": [
        {
          "name": "Juan M. Dur\u00e1n",
          "openalex_id": "A5049888291",
          "orcid": "https://orcid.org/0000-0001-6482-0399",
          "institutions": [
            "Delft University of Technology"
          ]
        },
        {
          "name": "Karin Jongsma",
          "openalex_id": "A5051588251",
          "orcid": "https://orcid.org/0000-0001-8135-6786",
          "institutions": [
            "Utrecht University",
            "University Medical Center Utrecht"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-03-18",
      "abstract": "The use of black box algorithms in medicine has raised scholarly concerns due to their opaqueness and lack of trustworthiness. Concerns about potential bias, accountability and responsibility, patient autonomy and compromised trust transpire with black box algorithms. These worries connect epistemic concerns with normative issues. In this paper, we outline that black box algorithms are less problematic for epistemic reasons than many scholars seem to believe. By outlining that more transparency in algorithms is not always necessary, and by explaining that computational processes are indeed methodologically opaque to humans, we argue that the reliability of algorithms provides reasons for trusting the outcomes of medical artificial intelligence (AI). To this end, we explain how computational reliabilism , which does not require transparency and supports the reliability of algorithms, justifies the belief that results of medical AI are to be trusted. We also argue that several ethical concerns remain with black box algorithms, even when the results are trustworthy. Having justified knowledge from reliable indicators is, therefore, necessary but not sufficient for normatively justifying physicians to act. This means that deliberation about the results of reliable algorithms is required to find out what is a desirable action. Thus understood, we argue that such challenges should not dismiss the use of black box algorithms altogether but should inform the way in which these algorithms are designed and implemented. When physicians are trained to acquire the necessary skills and expertise, and collaborate with medical informatics and data scientists, black box algorithms can contribute to improving medical care.",
      "cited_by_count": 401,
      "type": "article",
      "source": {
        "name": "Journal of Medical Ethics",
        "type": "journal",
        "issn": [
          "0306-6800",
          "1473-4257"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "bronze",
        "oa_url": "https://jme.bmj.com/content/medethics/47/5/329.full.pdf"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Ethics in Clinical Research",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 62,
      "url": "https://openalex.org/W3136000444"
    },
    {
      "openalex_id": "W4320342651",
      "doi": "10.48550/arxiv.2302.04525",
      "title": "An Epistemic and Aleatoric Decomposition of Arbitrariness to Constrain the Set of Good Models",
      "authors": [
        {
          "name": "Falaah Arif Khan",
          "openalex_id": "A5038710725",
          "orcid": "https://orcid.org/0000-0002-4678-5929"
        },
        {
          "name": "Denys Herasymuk",
          "openalex_id": "A5040712501"
        },
        {
          "name": "Julia Stoyanovich",
          "openalex_id": "A5082830839",
          "orcid": "https://orcid.org/0000-0002-1587-0450"
        },
        {
          "name": "Stoyanovich, Julia",
          "openalex_id": ""
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-02-09",
      "abstract": "Recent research reveals that machine learning (ML) models are highly sensitive to minor changes in their training procedure, such as the inclusion or exclusion of a single data point, leading to conflicting predictions on individual data points; a property termed as arbitrariness or instability in ML pipelines in prior work. Drawing from the uncertainty literature, we show that stability decomposes into epistemic and aleatoric components, capturing the consistency and confidence in prediction, respectively. We use this decomposition to provide two main contributions. Our first contribution is an extensive empirical evaluation. We find that (i) epistemic instability can be reduced with more training data whereas aleatoric instability cannot; (ii) state-of-the-art ML models have aleatoric instability as high as 79% and aleatoric instability disparities among demographic groups as high as 29% in popular fairness benchmarks; and (iii) fairness pre-processing interventions generally increase aleatoric instability more than in-processing interventions, and both epistemic and aleatoric instability are highly sensitive to data-processing interventions and model architecture. Our second contribution is a practical solution to the problem of systematic arbitrariness. We propose a model selection procedure that includes epistemic and aleatoric criteria alongside existing accuracy and fairness criteria, and show that it successfully narrows down a large set of good models (50-100 on our datasets) to a handful of stable, fair and accurate ones. We built and publicly released a python library to measure epistemic and aleatoric multiplicity in any ML pipeline alongside existing confusion-matrix-based metrics, providing practitioners with a rich suite of evaluation metrics to use to define a more precise criterion during model selection.",
      "cited_by_count": 2,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2302.04525"
      },
      "topics": [
        "Psychological Well-being and Life Satisfaction"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4320342651"
    },
    {
      "openalex_id": "W4404551929",
      "doi": "10.1016/j.cma.2024.117524",
      "title": "Parallel active learning reliability analysis: A multi-point look-ahead paradigm",
      "authors": [
        {
          "name": "Tong Zhou",
          "openalex_id": "A5073429227",
          "orcid": "https://orcid.org/0000-0002-3693-2569"
        },
        {
          "name": "Tong Guo",
          "openalex_id": "A5108145566",
          "orcid": "https://orcid.org/0000-0003-3608-1810"
        },
        {
          "name": "Chao Dang",
          "openalex_id": "A5005062211",
          "orcid": "https://orcid.org/0000-0001-7412-6309"
        },
        {
          "name": "Lei Jia",
          "openalex_id": "A5022255906",
          "orcid": "https://orcid.org/0000-0002-4405-7274"
        },
        {
          "name": "You Dong",
          "openalex_id": "A5040725315",
          "orcid": "https://orcid.org/0000-0002-2499-0999"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-11-20",
      "abstract": null,
      "cited_by_count": 10,
      "type": "article",
      "source": {
        "name": "Computer Methods in Applied Mechanics and Engineering",
        "type": "journal",
        "issn": [
          "0045-7825",
          "1879-2138"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Probabilistic and Robust Engineering Design",
        "Reliability and Maintenance Optimization",
        "VLSI and Analog Circuit Testing"
      ],
      "referenced_works_count": 60,
      "url": "https://openalex.org/W4404551929"
    },
    {
      "openalex_id": "W4398219649",
      "doi": "10.1007/s10098-024-02828-9",
      "title": "Proposal of a multi-expert multi-criteria model for the sustainability assessment of industrial systems under uncertainty",
      "authors": [
        {
          "name": "Di Xu",
          "openalex_id": "A5080306653",
          "orcid": "https://orcid.org/0000-0003-1441-0337",
          "institutions": [
            "Chongqing University of Science and Technology"
          ]
        },
        {
          "name": "Jinhai Yuan",
          "openalex_id": "A5100963926",
          "institutions": [
            "Chongqing University of Science and Technology"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-05-22",
      "abstract": null,
      "cited_by_count": 3,
      "type": "article",
      "source": {
        "name": "Clean Technologies and Environmental Policy",
        "type": "journal",
        "issn": [
          "1618-954X",
          "1618-9558"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Multi-Criteria Decision Making",
        "Environmental Impact and Sustainability",
        "Sustainable Development and Environmental Policy"
      ],
      "referenced_works_count": 33,
      "url": "https://openalex.org/W4398219649"
    },
    {
      "openalex_id": "W3208385200",
      "doi": "10.1109/itsc48978.2021.9564433",
      "title": "Efficient Uncertainty Estimation for Monocular 3D Object Detection in Autonomous Driving",
      "authors": [
        {
          "name": "Zechen Liu",
          "openalex_id": "A5102752870"
        },
        {
          "name": "Zhihua Han",
          "openalex_id": "A5100556157"
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-09-19",
      "abstract": "Monocular 3D object detection is an ill-posed problem due to the loss of geometry information in 2D images. To improve the reliability of monocular vision system, we propose to replace the deterministic estimation with a probabilistic prediction by computing both aleatoric and epistemic uncertainty in 3D detection task. We further design a continuous focal loss that can improve location accuracy and when combining with aleatoric uncertainty estimation, also reduce the effect of noise data during training. Our method is capable of estimating uncertainty in a single forward pass while achieving nearly real-time performance with 0.1s per image on a modern platform. Additionally, to make fair comparison between point and probabilistic prediction, we present a novel probabilistic localization evaluation metric which computes the average error over the predicted location distribution. Our experiments indicate that we achieve competitive result on the official KITTI 3D detection leaderboard, among other deterministic methods. Furthermore, our method gains more accurate localization accuracy under different localization evaluation metrics.",
      "cited_by_count": 3,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Advanced Neural Network Applications",
        "Robotics and Sensor-Based Localization",
        "Visual Attention and Saliency Detection"
      ],
      "referenced_works_count": 75,
      "url": "https://openalex.org/W3208385200"
    },
    {
      "openalex_id": "W4391095001",
      "doi": "10.1109/bigdata59044.2023.10386790",
      "title": "Leveraging Uncertainty Quantification for Reducing Data for Recommender Systems",
      "authors": [
        {
          "name": "Xi Niu",
          "openalex_id": "A5069430572",
          "orcid": "https://orcid.org/0000-0002-5418-6969",
          "institutions": [
            "University of North Carolina at Charlotte"
          ]
        },
        {
          "name": "Ruhani Rahman",
          "openalex_id": "A5055851173",
          "orcid": "https://orcid.org/0000-0003-3492-7843",
          "institutions": [
            "University of North Carolina at Charlotte"
          ]
        },
        {
          "name": "Xiangcheng Wu",
          "openalex_id": "A5091844793",
          "orcid": "https://orcid.org/0000-0001-8795-0487",
          "institutions": [
            "University of North Carolina at Charlotte"
          ]
        },
        {
          "name": "Zhe Fu",
          "openalex_id": "A5042363363",
          "orcid": "https://orcid.org/0000-0002-3097-8451",
          "institutions": [
            "University of North Carolina at Charlotte"
          ]
        },
        {
          "name": "Depeng Xu",
          "openalex_id": "A5100730268",
          "orcid": "https://orcid.org/0000-0002-0371-1815",
          "institutions": [
            "University of North Carolina at Charlotte"
          ]
        },
        {
          "name": "Riyi Qiu",
          "openalex_id": "A5040015512",
          "institutions": [
            "Freddie Mac (United States)"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-12-15",
      "abstract": "The recent California Consumer Privacy Act (CCPA) requires that personal data shall be limited to what is necessary for business purposes. Business services shall \"implement technical safeguards that prohibit re-identification of the consumer to whom the information may pertain\". For recommender systems, we believe the legal concepts of limitation and technical safeguard are not specific enough to operationalize in practice. This study makes efforts to map the legislative challenges to practice of reducing personal data. More importantly, we borrowed the notion of uncertainty from the machine learning community, and added it as another aspect of recommendation utility, in addition to recommendation accuracy, to guide the data reduction process. The benefit of using uncertainty is that we have more comprehensive consideration while reducing the personal data. In addition, two major types of uncertainty in machine learning models: aleatoric uncertainty and epistemic uncertainty, helped us formulate two groups of data reduction strategies: within-user and between-user. We conducted a series of analyses regarding uncertainty change and accuracy loss caused by different data reduction strategies. We found that at the aggregate level, data reduction is feasible with certain data reduction strategies. At the individual level, the recommendation utility (both uncertainty and accuracy) loss incurred by data reduction disparately impacts different users \u2014 a finding which has implications for fairness and transparency of AI models. Our results reveal the difficulty and intricacy of the data reduction problem in the context of recommender systems.",
      "cited_by_count": 3,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Recommender Systems and Techniques",
        "Advanced Bandit Algorithms Research",
        "Topic Modeling"
      ],
      "referenced_works_count": 15,
      "url": "https://openalex.org/W4391095001"
    },
    {
      "openalex_id": "W2574767810",
      "doi": "10.1515/jisys-2015-0105",
      "title": "Can Fuzzy Relational Calculus Bring Complex Issues in Selection of Examiners into Focus?",
      "authors": [
        {
          "name": "Satish S. Salunkhe",
          "openalex_id": "A5029345575",
          "orcid": "https://orcid.org/0000-0002-8395-7101",
          "institutions": [
            "Narsee Monjee Institute of Management Studies"
          ]
        },
        {
          "name": "Yashwant Joshi",
          "openalex_id": "A5055228132",
          "orcid": "https://orcid.org/0000-0002-0508-7585",
          "institutions": [
            "Swami Ramanand Teerth Marathwada University"
          ]
        },
        {
          "name": "Ashok Deshpande",
          "openalex_id": "A5108258212",
          "institutions": [
            "Berkeley College"
          ]
        }
      ],
      "publication_year": 2015,
      "publication_date": "2015-12-17",
      "abstract": "Abstract The examinee and the examiner play pivotal roles in the educational grading system. Students\u2019 academic performance evaluation by multiple experts involves epistemic uncertainty, which can be modeled using a fuzzy set theory. How many evaluators/experts are almost similar in their perceptual subjective evaluation of the students answer paper? In other words, how many experts are reliable for a particular evaluation task with a defined possibility level? In this paper, the focus is on object\u2019s features ( students\u2019 marks ) as a basis in the subjective evaluation process to identify the degree of similarity among the domain experts. The case study reveals that 11 out of 20 evaluators are similar in their decision making of students\u2019 academic performance with possibility ( \u03b1 -level cut, 0.98). The inter-rater reliability ( \u03ba -coefficient) among the selected 11 teachers is 0.41, which signifies a fair/moderate agreement in the evaluation process. This paper proposes an approach that is useful for the selection of experts having similar perceptions in judgment. This paper demonstrates a case study showing how it is useful to educational policy makers in the selection of examiners.",
      "cited_by_count": 3,
      "type": "article",
      "source": {
        "name": "Journal of Intelligent Systems",
        "type": "journal",
        "issn": [
          "0334-1860",
          "2191-026X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.degruyter.com/downloadpdf/journals/jisys/25/2/article-p85.pdf"
      },
      "topics": [
        "Educational Technology and Assessment"
      ],
      "referenced_works_count": 48,
      "url": "https://openalex.org/W2574767810"
    },
    {
      "openalex_id": "W1969607224",
      "doi": "10.2514/1.i010359",
      "title": "Special Edition on Uncertainty Quantification of the AIAA <i>Journal of Aerospace Computing, Information, and Communication</i>",
      "authors": [
        {
          "name": "Luis G. Crespo",
          "openalex_id": "A5112043830",
          "institutions": [
            "Langley Research Center"
          ]
        },
        {
          "name": "Sean P. Kenny",
          "openalex_id": "A5104359919",
          "institutions": [
            "Langley Research Center"
          ]
        }
      ],
      "publication_year": 2015,
      "publication_date": "2015-01-01",
      "abstract": "NASA missions often involve the development of new vehicles and systems that must operate in harsh domains with a wide array of uncertainties and operating conditions. These missions involve high-consequence safety-critical systems for which experimental data are either very sparse or prohibitively expensive to collect. Limited heritage data may exist but, typically, are also sparse and may not be directly applicable to the system of interest, making uncertainty quantification (UQ) andmodel validation extremely challenging. Furthermore, NASAmodeling and simulation standards require estimates of uncertainty and descriptions of any processes used to obtain these estimates. The NASALangley Research Center developed a UQ challenge problem in an effort to focus a community of researchers toward key technical challenges common to many practical applications. The problem statement can be found in work by Crespo et al. (\u201cThe NASA Langley Multidisciplinary Uncertainty Quantification Challenge,\u201d 16th AIAA Non-Deterministic Approaches Conference, AIAA Paper 2014-1347, Jan. 2014), and the computational models for it are available at http://uqtools.larc.nasa.gov/nda-uq-challenge-problem-2014/. The challenge problem features key issues in model calibration, uncertainty quantification, global sensitivity analysis, and robust design using a disciplineindependent formulation. The problem formulation is indeed discipline independent, but the underlying model, as well as the requirements imposed upon it, describe a realistic aeronautics application. A key aspect of the problem that makes it novel and challenging is the presence of both aleatory and epistemic uncertainties in a setting requiring consistency between their qualitative and quantitative prescriptions. As such, some uncertain parameters aremodeled as randomvariables, others as unknown constants lyingwithin known intervals, and others as probability boxes. This invited edition compiles the responses of 11 research teams to the challenge problem. The responses were generated by key discipline experts from U.S. research laboratories, industry, and academia. Participants included Sandia National Laboratories, Los Alamos National Laboratory, University of Southern California, University of Florida, Vanderbilt University, Supelec, University of Liverpool, Southwest Research Institute, GE Global Research, NASAAmes Research Center, and the Swiss Federal Institute of Technology. The breadth and depth of the solution strategies proposed constitute a survey of the state of the practice on uncertainty quantification from a practical engineering-like perspective. The usage of alternative and often dissimilar UQmethodologies for tackling the very same problem enables assessing their strengths and limitations on a unifying and fair setting.We hope that the scope of the challenge problem, as well as the responses to it presented herein, will keep inspiring scientists and engineers to develop more effective and efficient UQ technologies for improving the credibility and consistency of simulation-based analyses and designs.",
      "cited_by_count": 5,
      "type": "article",
      "source": {
        "name": "Journal of Aerospace Information Systems",
        "type": "journal",
        "issn": [
          "2327-3097"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Probabilistic and Robust Engineering Design",
        "Distributed Sensor Networks and Detection Algorithms"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W1969607224"
    },
    {
      "openalex_id": "W3183398589",
      "doi": "10.1145/3461702.3462571",
      "title": "Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty",
      "authors": [
        {
          "name": "Umang Bhatt",
          "openalex_id": "A5016469734",
          "orcid": "https://orcid.org/0000-0002-4611-1668",
          "institutions": [
            "University of Cambridge"
          ]
        },
        {
          "name": "Javier Antor\u00e1n",
          "openalex_id": "A5051888078",
          "institutions": [
            "University of Cambridge"
          ]
        },
        {
          "name": "Yunfeng Zhang",
          "openalex_id": "A5100410998",
          "orcid": "https://orcid.org/0000-0002-1237-6035",
          "institutions": [
            "IBM (United States)"
          ]
        },
        {
          "name": "Q. Vera Liao",
          "openalex_id": "A5024320659",
          "orcid": "https://orcid.org/0000-0003-4543-7196",
          "institutions": [
            "IBM (United States)"
          ]
        },
        {
          "name": "Prasanna Sattigeri",
          "openalex_id": "A5060534465",
          "orcid": "https://orcid.org/0000-0003-4435-0486",
          "institutions": [
            "IBM (United States)"
          ]
        },
        {
          "name": "Riccardo Fogliato",
          "openalex_id": "A5008759018",
          "orcid": "https://orcid.org/0000-0002-8636-9639",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Gabrielle Gauthier Melan\u00e7on",
          "openalex_id": "A5066476855",
          "orcid": "https://orcid.org/0000-0003-3240-685X"
        },
        {
          "name": "Ranganath Krishnan",
          "openalex_id": "A5032673732",
          "orcid": "https://orcid.org/0000-0001-5364-6647",
          "institutions": [
            "Intel (United States)"
          ]
        },
        {
          "name": "Jason Stanley",
          "openalex_id": "A5113666379"
        },
        {
          "name": "Omesh Tickoo",
          "openalex_id": "A5084996515",
          "institutions": [
            "Intel (United States)"
          ]
        },
        {
          "name": "Lama Nachman",
          "openalex_id": "A5024793931",
          "orcid": "https://orcid.org/0000-0002-5824-242X",
          "institutions": [
            "Intel (United States)"
          ]
        },
        {
          "name": "Rumi Chunara",
          "openalex_id": "A5005061793",
          "orcid": "https://orcid.org/0000-0002-5346-7259",
          "institutions": [
            "New York University"
          ]
        },
        {
          "name": "Madhulika Srikumar",
          "openalex_id": "A5028241019"
        },
        {
          "name": "Adrian Weller",
          "openalex_id": "A5042278493",
          "orcid": "https://orcid.org/0000-0003-1915-7158",
          "institutions": [
            "University of Cambridge"
          ]
        },
        {
          "name": "Alice Xiang",
          "openalex_id": "A5035786206",
          "orcid": "https://orcid.org/0000-0001-7907-9353",
          "institutions": [
            "Sony Corporation (United States)"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-07-21",
      "abstract": "Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.",
      "cited_by_count": 182,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3461702.3462571"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 120,
      "url": "https://openalex.org/W3183398589"
    }
  ],
  "count": 35,
  "errors": []
}
