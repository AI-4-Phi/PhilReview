{
  "status": "success",
  "source": "semantic_scholar",
  "query": "algorithmic fairness normative framework",
  "results": [
    {
      "paperId": "adcbe825c68362da2a7128792b2a4fab258ce41e",
      "title": "Algorithmic Justice in Financial Markets: A Normative Framework for Fair Trading in the Age of Artificial Intelligence",
      "authors": [
        {
          "name": "Yetong An",
          "authorId": "2380642696"
        }
      ],
      "year": 2025,
      "abstract": "This study explores the issue of fairness in AI-driven algorithmic trading through an ethical lens that synthesizes Rawlsian justice theory, utilitarianism, and virtue ethics. By analyzing key events such as the 2010 Flash Crash and the GameStop incident, this study reveals how algorithmic trading helps professional institutions disadvantage retail investors. We will begin by reviewing the use of trading algorithms such as HFT in today's markets, as well as traditional theories and principles of market ethics. This paper will then articulate the theoretical framework, discuss the fairness challenges posed by algorithmic trading, and incorporate case studies. The article will conclude with recommendations, including mandatory transparency protocols for trading algorithms, latency standardization for achieving equal access, AI-driven real-time market monitoring, and independent certification systems that enforce the principle of \"algorithmic justice.\" By redefining fairness as a necessary consideration during algorithm design rather than an afterthought to avoid market regulation, this study argues for integrating ethical responsibility into code architectures and regulatory infrastructures, balancing technological innovation with fair market participation.",
      "citationCount": 0,
      "doi": "10.54254/2754-1169/2025.ld24640",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/adcbe825c68362da2a7128792b2a4fab258ce41e",
      "venue": "Advances in Economics, Management and Political Sciences",
      "journal": {
        "name": "Advances in Economics, Management and Political Sciences"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "8aabaf82a141f9f8b768c177922bb6ef6d70977b",
      "title": "Developing a Philosophical Framework for Fair Machine Learning: The Case of Algorithmic Collusion and Market Fairness",
      "authors": [
        {
          "name": "James D. Michelson",
          "authorId": "2066480559"
        }
      ],
      "year": 2022,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.48550/arXiv.2208.06308",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/8aabaf82a141f9f8b768c177922bb6ef6d70977b",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2208.06308"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d4e596e00208e4a329d094528128eb4b58c348ec",
      "title": "Algorithmic Fairness and the Situated Dynamics of Justice",
      "authors": [
        {
          "name": "S. Fazelpour",
          "authorId": "4876895"
        },
        {
          "name": "Zachary C. Lipton",
          "authorId": "2266863961"
        },
        {
          "name": "D. Danks",
          "authorId": "143752796"
        }
      ],
      "year": 2021,
      "abstract": "Abstract Machine learning algorithms are increasingly used to shape high-stake allocations, sparking research efforts to orient algorithm design towards ideals of justice and fairness. In this research on algorithmic fairness, normative theorizing has primarily focused on identification of \u201cideally fair\u201d target states. In this paper, we argue that this preoccupation with target states in abstraction from the situated dynamics of deployment is misguided. We propose a framework that takes dynamic trajectories as direct objects of moral appraisal, highlighting three respects in which such trajectories can be subject to evaluation in relation to their (i) temporal dynamics, (ii) robustness, and (iii) representation.",
      "citationCount": 31,
      "doi": "10.1017/can.2021.24",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/d4e596e00208e4a329d094528128eb4b58c348ec",
      "venue": "Canadian Journal of Philosophy",
      "journal": {
        "name": "Canadian Journal of Philosophy",
        "pages": "44 - 60",
        "volume": "52"
      },
      "publicationTypes": null
    },
    {
      "paperId": "f567790f7b2dcef8b500556191602a95a22063b6",
      "title": "The Perils of Objectivity: Towards a Normative Framework for Fair Judicial Decision-Making",
      "authors": [
        {
          "name": "Andi Peng",
          "authorId": "100953200"
        },
        {
          "name": "Malina Simard-Halm",
          "authorId": "1491516207"
        }
      ],
      "year": 2020,
      "abstract": "Fair decision-making in criminal justice relies on the recognition and incorporation of infinite shades of grey. In this paper, we detail how algorithmic risk assessment tools are counteractive to fair legal proceedings in social institutions where desired states of the world are contested ethically and practically. We provide a normative framework for assessing fair judicial decision-making, one that does not seek the elimination of human bias from decision-making as algorithmic fairness efforts currently focus on, but instead centers on sophisticating the incorporation of individualized or discretionary bias--a process that is requisitely human. Through analysis of a case study on social disadvantage, we use this framework to provide an assessment of potential features of consideration, such as political disempowerment and demographic exclusion, that are irreconcilable by current algorithmic efforts and recommend their incorporation in future reform.",
      "citationCount": 0,
      "doi": "10.1145/3375627.3375869",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f567790f7b2dcef8b500556191602a95a22063b6",
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle"
      ]
    },
    {
      "paperId": "e5001b85af276c3cee4babd93747c5c2a0630d14",
      "title": "Fairness as Equality of Opportunity: Normative Guidance from Political Philosophy",
      "authors": [
        {
          "name": "Falaah Arif Khan",
          "authorId": "1410017982"
        },
        {
          "name": "Eleni Manis",
          "authorId": "119219282"
        },
        {
          "name": "J. Stoyanovich",
          "authorId": "1682824"
        }
      ],
      "year": 2021,
      "abstract": "Recent interest in codifying fairness in Automated Decision Systems (ADS) has resulted in a wide range of formulations of what it means for an algorithmic system to be fair. Most of these propositions are inspired by, but inadequately grounded in, political philosophy scholarship. This paper aims to correct that deficit. We introduce a taxonomy of fairness ideals using doctrines of Equality of Opportunity (EOP) from political philosophy, clarifying their conceptions in philosophy and the proposed codification in fair machine learning. We arrange these fairness ideals onto an EOP spectrum, which serves as a useful frame to guide the design of a fair ADS in a given context. \nWe use our fairness-as-EOP framework to re-interpret the impossibility results from a philosophical perspective, as the in-compatibility between different value systems, and demonstrate the utility of the framework with several real-world and hypothetical examples. Through our EOP-framework we hope to answer what it means for an ADS to be fair from a moral and political philosophy standpoint, and to pave the way for similar scholarship from ethics and legal experts.",
      "citationCount": 10,
      "doi": null,
      "arxivId": "2106.08259",
      "url": "https://www.semanticscholar.org/paper/e5001b85af276c3cee4babd93747c5c2a0630d14",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2106.08259"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "70acc3e25bcb64b936c9b242d719ba814857cbcd",
      "title": "Developing a Philosophical Framework for Fair Machine Learning: Lessons From The Case of Algorithmic Collusion",
      "authors": [
        {
          "name": "James D. Michelson",
          "authorId": "2284906610"
        }
      ],
      "year": 2022,
      "abstract": "Fair machine learning research has been primarily concerned with classification tasks that result in discrimination. However, as machine learning algorithms are applied in new contexts the harms and injustices that result are qualitatively different than those presently studied. The existing research paradigm in machine learning which develops metrics and definitions of fairness cannot account for these qualitatively different types of injustice. One example of this is the problem of algorithmic collusion and market fairness. The negative consequences of algorithmic collusion affect all consumers, not only particular members of a protected class. Drawing on this case study, I propose an ethical framework for researchers and practitioners in machine learning seeking to develop and apply fairness metrics that extends to new domains. This contribution ties the development of formal metrics of fairness to specifically scoped normative principles. This enables fairness metrics to reflect different concerns from discrimination. I conclude with the limitations of my proposal and discuss promising avenues for future research.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2208.06308",
      "url": "https://www.semanticscholar.org/paper/70acc3e25bcb64b936c9b242d719ba814857cbcd",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "321f5ac30e17a0af0cf88db4f7467998f5a4ed58",
      "title": "Human Dignity, the Right to be Heard, and Algorithmic Judges",
      "authors": [
        {
          "name": "Andr\u00e9 Dao",
          "authorId": "72870097"
        }
      ],
      "year": 2020,
      "abstract": "\n This article examines the requirements of the right to a fair trial in the context of the use of machine-learning algorithms (MLAs) in judicial proceedings, with a focus on a core component of this right, the right to be heard. Though NGOs and scholars have begun to note that the right to a fair trial may be the best framework to address the challenges raised by MLAs, the actual requirements of the right in this novel context are underdeveloped. This article evaluates two normative approaches to filling this gap. The first approach, the argument from fairness, produces three broad categories of measures for ensuring fairness: measures for increasing the transparency and accountability of MLAs, measures for ensuring the participation of litigants, and measures for securing the impartiality of the human judge. However, this article argues that the argument from fairness cannot provide the necessary normative grounding for the right to a fair trial in the context of MLAs, as it collapses into the concept of \u2018algorithmic fairness\u2019. The second approach is based on the concept of human dignity as a status. The primary argument of this article is that the concept of human dignity as a status can provide better normative grounding for the right to a fair trial because it offers an account of human personhood that resists the de-humanization of data subjectification. That richer account of human personhood allows us to think of the trial not only as a vehicle for accurate outcomes, but also as a forum for the expression of human dignity.",
      "citationCount": 1,
      "doi": "10.1093/bybil/braa009",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/321f5ac30e17a0af0cf88db4f7467998f5a4ed58",
      "venue": "",
      "journal": {
        "name": "British Yearbook of International Law",
        "volume": ""
      },
      "publicationTypes": null
    },
    {
      "paperId": "ee640925afc0f677dd275d897d7e64f71bf2080e",
      "title": "Automating epistemology: how AI reconfigures truth, authority, and verification",
      "authors": [
        {
          "name": "Donghee Shin",
          "authorId": "2377966558"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 3,
      "doi": "10.1007/s00146-025-02560-y",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/ee640925afc0f677dd275d897d7e64f71bf2080e",
      "venue": "AI &amp; SOCIETY",
      "journal": {
        "name": "AI &amp; SOCIETY"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2307e8e7abe90380b1efc7a46873e13e3d1f67dc",
      "title": "Diversity as Ethical Infrastructure: Reimagining AI Governance for Justice and Accountability",
      "authors": [
        {
          "name": "Achi Iseko",
          "authorId": "2378234619"
        }
      ],
      "year": 2025,
      "abstract": "Algorithmic bias remains a persistent ethical challenge in the deployment of artificial intelligence (AI) systems, particularly where opaque decision-making intersects with entrenched social inequities. While technical solutions such as fairness-aware algorithms and explainability tools have proliferated, the governance dimensions of AI ethics, especially the role of diversity in shaping oversight structures, remain undertheorized. This article introduces the Diversity-Centric AI Governance Framework (DCAIGF), a novel model that integrates cognitive diversity, intersectionality ethics, and cross-cultural regulatory alignment as foundational elements of inclusive AI oversight. Grounded in 65 semi-structured expert interviews, comparative case studies (Google and IBM), and policy analysis of key global frameworks (e.g., EU AI Act, UNESCO Recommendation on AI Ethics, OECD AI Principles), this study finds that homogenous governance structures often reproduce epistemic blind spots and normative monocultures. In contrast, diverse institutional architectures foster reflexivity, accountability, and ethical robustness across contexts. By conceptualizing diversity as ethical infrastructure rather than symbolic representation, DCAIGF advances four innovations: mandated cognitive pluralism, embedded intersectionality, hybrid legal adaptability, and modular implementation pathways. These features enable practical translation across public, private, and multilateral governance ecosystems. The paper contributes to AI ethics by offering a socio-technical, globally relevant, and empirically grounded model for institutional reform. It further proposes a policy agenda that links epistemic justice to regulatory legitimacy offering a pluralistic roadmap for addressing algorithmic bias beyond the limits of technical mitigation alone.\n",
      "citationCount": 0,
      "doi": "10.11648/j.ijsts.20251305.13",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/2307e8e7abe90380b1efc7a46873e13e3d1f67dc",
      "venue": "International Journal of Science Technology & Society",
      "journal": {
        "name": "International Journal of Science, Technology and Society"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "bf3e886576eed97cef85dda38fe20659d0310b90",
      "title": "Ethical Frameworks for AI Deployment in Financial Decision-Making: Balancing Profitability and Social Responsibility",
      "authors": [
        {
          "name": "Jeffrey Chidera Ogeawuchi",
          "authorId": "2359484951"
        },
        {
          "name": "Aadit Sharma",
          "authorId": "2362747767"
        },
        {
          "name": "Bolaji Iyanu Adekunle",
          "authorId": "2362280171"
        },
        {
          "name": "Abraham Ayodeji Abayomi",
          "authorId": "2359485894"
        },
        {
          "name": "Omoniyi Onifade",
          "authorId": "2362748000"
        }
      ],
      "year": 2024,
      "abstract": "The integration of artificial intelligence into financial decision-making has introduced transformative efficiencies and competitive advantages across domains such as algorithmic trading, credit risk assessment, fraud detection, and customer profiling. However, these advancements raise profound ethical concerns, particularly in high-stakes environments where opaque models and biased algorithms can perpetuate discrimination, reduce accountability, and compromise consumer trust. This paper critically investigates the ethical implications of AI-driven financial systems, emphasizing the risks of algorithmic bias, the challenge of model interpretability, and the urgency of safeguarding data privacy. By drawing on normative ethical principles\u2014fairness, accountability, transparency, and human oversight\u2014the study proposes a comprehensive governance framework to guide the ethical lifecycle of AI deployment in finance. It evaluates the role of financial institutions, regulatory bodies, and central banks in setting enforceable standards, while offering a practical model for integrating ethics from design to audit. The paper concludes by reflecting on the responsibilities of key stakeholders and outlining future research and policy directions to ensure that AI innovations support not only profitability but also inclusive and socially responsible financial ecosystems.",
      "citationCount": 1,
      "doi": "10.32628/cseit24102141",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/bf3e886576eed97cef85dda38fe20659d0310b90",
      "venue": "International Journal of Scientific Research in Computer Science Engineering and Information Technology",
      "journal": {
        "name": "International Journal of Scientific Research in Computer Science, Engineering and Information Technology"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "087ae574a1713686318644eade32ecd1d33644a2",
      "title": "Towards Substantive Conceptions of Algorithmic Fairness: Normative Guidance from Equal Opportunity Doctrines",
      "authors": [
        {
          "name": "Falaah Arif Khan",
          "authorId": "1410017982"
        },
        {
          "name": "Eleni Manis",
          "authorId": "119219282"
        },
        {
          "name": "J. Stoyanovich",
          "authorId": "1682824"
        }
      ],
      "year": 2022,
      "abstract": "In this work we use Equal Opportunity (EO) doctrines from political philosophy to make explicit the normative judgements embedded in different conceptions of algorithmic fairness. We contrast formal EO approaches that narrowly focus on fair contests at discrete decision points, with substantive EO doctrines that look at people\u2019s fair life chances more holistically over the course of a lifetime. We use this taxonomy to provide a moral interpretation of the impossibility results as the incompatibility between different conceptions of a fair contest \u2014 foward-facing versus backward-facing \u2014 when people do not have fair life chances. We use this result to motivate substantive conceptions of algorithmic fairness and outline two plausible fair decision procedures based on the luck egalitarian doctrine of EO, and Rawls\u2019s principle of fair equality of opportunity.",
      "citationCount": 20,
      "doi": "10.1145/3551624.3555303",
      "arxivId": "2207.02912",
      "url": "https://www.semanticscholar.org/paper/087ae574a1713686318644eade32ecd1d33644a2",
      "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",
      "journal": {
        "name": "Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "7ca4bffc22d6c792ff4d27ac3c568bddb719cbc0",
      "title": "Utility on the brain: an empirical investigation of fairness perceptions of algorithmic decisions under a utility-based ethical evaluation framework",
      "authors": [
        {
          "name": "Serhiy Kandul",
          "authorId": "114625641"
        },
        {
          "name": "Corinna Hertweck",
          "authorId": "2395546033"
        },
        {
          "name": "Ulrich Leicht-Deobald",
          "authorId": "2310740912"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/s43681-025-00854-x",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7ca4bffc22d6c792ff4d27ac3c568bddb719cbc0",
      "venue": "AI and Ethics",
      "journal": {
        "name": "AI and Ethics",
        "volume": "6"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "767edcb0adf9c3cf165476d7c0ef936e5d97f6f6",
      "title": "Theoretical Foundations of Algorithmic Fairness: A Unified Framework Through Pinsker's Inequality",
      "authors": [
        {
          "name": "Agus Sudjianto",
          "authorId": "2379811869"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.2139/ssrn.5402916",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/767edcb0adf9c3cf165476d7c0ef936e5d97f6f6",
      "venue": "Social Science Research Network",
      "journal": {
        "name": "SSRN Electronic Journal"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a0584af4c029ecd8410a5cafadcea5ca9366386e",
      "title": "A Framework for Defining Algorithmic Fairness in the Context of Information Access",
      "authors": [
        {
          "name": "E. Udoh",
          "authorId": "40354671"
        },
        {
          "name": "Xiaojun (Jenny) Yuan",
          "authorId": "1410318900"
        },
        {
          "name": "Abebe Rorissa",
          "authorId": "2594683"
        }
      ],
      "year": 2024,
      "abstract": "As technologies powered by Artificial Intelligence (AI) and Machine Learning (ML) algorithms increasingly take over personal computing online and public sector domains, they simultaneously raise the promise of an extensively productive and sustainable future, as well as fears of widening inequalities, information and content divide, and a more complex information\u2010seeking landscape. Thus, the hopes of improved accuracy, efficiency, productivity, reduced human bias in decision\u2010making, and access to information are fast giving way to a trove of ethical and human rights issues with far\u2010reaching consequences for accountability, privacy, social justice, equity, inclusion, and informed consent, and public participation in decision\u2010making. Since no technology is entirely free of bias, this paper identifies algorithmic fairness as a more realistic threshold and goal. Building on findings from a previous PRISMA review of relevant literature, the paper proposes a comprehensive framework for defining algorithmic fairness in the context of information access.",
      "citationCount": 1,
      "doi": "10.1002/pra2.1077",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a0584af4c029ecd8410a5cafadcea5ca9366386e",
      "venue": "Proceedings of the Association for Information Science and Technology",
      "journal": {
        "name": "Proceedings of the Association for Information Science and Technology",
        "volume": "61"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "d644a1098f8096186322ba2b277b127d1c5d1596",
      "title": "Algorithmic bias, fairness, and inclusivity: a multilevel framework for justice-oriented AI",
      "authors": [
        {
          "name": "Paola Panarese",
          "authorId": "2373299523"
        },
        {
          "name": "Marta Margherita Grasso",
          "authorId": "2372783656"
        },
        {
          "name": "Claudia Solinas",
          "authorId": "2373307407"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.1007/s00146-025-02451-2",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/d644a1098f8096186322ba2b277b127d1c5d1596",
      "venue": "AI &amp; SOCIETY",
      "journal": {
        "name": "AI &amp; SOCIETY"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "99e78e297653b9c1af7fdac7c7634deabcaf3523",
      "title": "Why causal inference is necessary for algorithmic fairness",
      "authors": [
        {
          "name": "Alexander Williams Tolbert",
          "authorId": "2380249150"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/s11229-025-05044-0",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/99e78e297653b9c1af7fdac7c7634deabcaf3523",
      "venue": "Synthese",
      "journal": {
        "name": "Synthese",
        "volume": "206"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "75a30b455370caf8b156df1a8cabca8c8ab5c4e3",
      "title": "Algorithmic Fairness in AI Surrogates for End-of-Life Decision-Making",
      "authors": [
        {
          "name": "Muhammad Aurangzeb Ahmad",
          "authorId": "2387129914"
        }
      ],
      "year": 2025,
      "abstract": "Artificial intelligence surrogates are systems designed to infer preferences when individuals lose decision-making capacity. Fairness in such systems is a domain that has been insufficiently explored. Traditional algorithmic fairness frameworks are insufficient for contexts where decisions are relational, existential, and culturally diverse. This paper explores an ethical framework for algorithmic fairness in AI surrogates by mapping major fairness notions onto potential real-world end-of-life scenarios. It then examines fairness across moral traditions. The authors argue that fairness in this domain extends beyond parity of outcomes to encompass moral representation, fidelity to the patient's values, relationships, and worldview.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.16056",
      "arxivId": "2510.16056",
      "url": "https://www.semanticscholar.org/paper/75a30b455370caf8b156df1a8cabca8c8ab5c4e3",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.16056"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8a4992cb4924a2a5dd17c637c27c8933d78b6c76",
      "title": "The ideals program in algorithmic fairness",
      "authors": [
        {
          "name": "Rush T. Stewart",
          "authorId": "2324363804"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 2,
      "doi": "10.1007/s00146-024-02106-8",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/8a4992cb4924a2a5dd17c637c27c8933d78b6c76",
      "venue": "Ai & Society",
      "journal": {
        "name": "AI & SOCIETY",
        "pages": "2273 - 2283",
        "volume": "40"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "37a051949dced9529ebeae245bf4122092b3c4ef",
      "title": "What\u2019s Impossible about Algorithmic Fairness?",
      "authors": [
        {
          "name": "Otto Sahlgren",
          "authorId": "2320675295"
        }
      ],
      "year": 2024,
      "abstract": "The now well-known impossibility results of algorithmic fairness demonstrate that an error-prone predictive model cannot simultaneously satisfy two plausible conditions for group fairness apart from exceptional circumstances where groups exhibit equal base rates. The results sparked, and continue to shape, lively debates surrounding algorithmic fairness conditions and the very possibility of building fair predictive models. This article, first, highlights three underlying points of disagreement in these debates, which have led to diverging assessments of the feasibility of fairness in prediction-based decision-making. Second, the article explores whether and in what sense fairness as defined by the conjunction of the implicated fairness conditions is (un)attainable. Drawing on philosophical literature on the concept of feasibility and the role of feasibility in normative theory, I outline a cautiously optimistic argument for the diachronic feasibility of fairness. In line with recent works on the topic, I argue that fairness can be made possible through collective efforts to eliminate inequalities that feed into local decision-making procedures.",
      "citationCount": 5,
      "doi": "10.1007/s13347-024-00814-z",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/37a051949dced9529ebeae245bf4122092b3c4ef",
      "venue": "Philosophy & Technology",
      "journal": {
        "name": "Philosophy & Technology",
        "volume": "37"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "94eb358a2ebc53fecf94d7bf27b6021793587dd8",
      "title": "Beyond \u201cFairness\u201d: Rethinking the Use of Algorithmic Predictions in Criminal Justice",
      "authors": [
        {
          "name": "Tashmia Sabera",
          "authorId": "2129654317"
        }
      ],
      "year": 2025,
      "abstract": "This paper critiques the widespread use of predictive\nalgorithmic tools in criminal justice, such as COMPAS,\narguing that concerns about fairness and accuracy, while\nimportant, fail to address a deeper ethical issue: the\ninfringement of the right to be treated as an individual.\nDrawing on Renee Jorgensen\u2019s work, I argue that\nfairness-based reforms are insufficient because predictive\npunishment is incompatible with the demands of negative\nretributivism, the most compatible theory of punishment\nwith the demands of the right to be treated as an\nindividual. Given the high stakes of criminal law and the\ninherent trade-off in algorithmic fairness metrics, I\ncontend that algorithmic predictions should not be used to\njustify punishment or policing decisions. However, I\npropose that algorithmic tools can be ethically employed in\ndeveloping policies aimed at crime reduction, provided they\nare used to identify causal factors rather than to predict\nindividual behavior. To this end, I advocate a pluralist\nframework: negative retributivism should govern punishment\nand policing, while rights-based consequentialism should\ninform long-term policy goals. This approach aims to\nclarify when the use of algorithms in criminal justice is\nunjustified, and when it may be justified with critical\nrevision.",
      "citationCount": 0,
      "doi": "10.1609/aies.v8i3.36709",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/94eb358a2ebc53fecf94d7bf27b6021793587dd8",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "23ba83b51b994c7b985ed13d90bf2135136e96e4",
      "title": "REVISITING THE GOLDEN RULE IN ACCOUNTING: A NORMATIVE INQUIRY INTO JUSTICE, FAIRNESS, AND EQUITY",
      "authors": [
        {
          "name": "Bamidele Vincent",
          "authorId": "2401212866"
        },
        {
          "name": "Olawale",
          "authorId": "2401211135"
        },
        {
          "name": "Abdulsallam Dauda",
          "authorId": "2401214310"
        }
      ],
      "year": 2025,
      "abstract": "This paper examines the application of the ethical principle of the Golden Rule, \u201cTreat others as you wish to be treated,\u201d in a framework of accounting professions. It is notable that justice and fairness, or equity, are accepted elements of accounting\u2019s legitimacy, value, and social worth; nevertheless, a full application of the Golden Rule in accounting theory and practice has not been realized. This work aims at establishing a normative model rooted in the Golden Rule to account for distributive, procedural, and corrective injustice in accounting, drawing from differing philosophical, religious, and ethical works. This model demonstrates how the Golden Rule can help resolve ethics-based dilemmas related to financial reporting, auditing, taxation, and professional practices by cultivating empathy and reciprocal responsibility, which can be termed attitudinal accountability among professionals. This paper also outlines actionable recommendations for educators as well as setters of standards and practitioners within prescribed professional networks. Policy initiatives researched are on the implementation forefront. Ultimately this paper argues that the profession of accounting advocating moral culture with the Golden Rule will restore public confidence while providing better opportunities for all stakeholders.",
      "citationCount": 0,
      "doi": "10.61990/a8s2vd58",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/23ba83b51b994c7b985ed13d90bf2135136e96e4",
      "venue": "International Journal of Economics, Education, Law and Social Sciences (IJEELSC)",
      "journal": {
        "name": "International Journal of Economics, Education, Law and Social Sciences (IJEELSC)"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6dbc9de94aaa3e1f57cf22e398104071d96b3f9c",
      "title": "Framework for developing algorithmic fairness",
      "authors": [
        {
          "name": "Dedy Prasetya Kristiadi",
          "authorId": "30428427"
        },
        {
          "name": "Po Abas Sunarya",
          "authorId": "104202665"
        },
        {
          "name": "Melvin Ismanto",
          "authorId": "1658929264"
        },
        {
          "name": "Joshua Dylan",
          "authorId": "1658932890"
        },
        {
          "name": "Ignasius Raffael Santoso",
          "authorId": "1658929513"
        },
        {
          "name": "Harco L. H. S. Warnars",
          "authorId": "2116279333"
        }
      ],
      "year": 2020,
      "abstract": "In a world where the algorithm can control the lives of society, it is not surprising that specific complications in determining the fairness in the algorithmic decision will arise at some point. Machine learning has been the de facto tool to forecast a problem that humans cannot reliably predict without injecting some amount of subjectivity in it (i.e., eliminating the \u201cirrational\u201d nature of humans). In this paper, we proposed a framework for defining a fair algorithm metric by compiling information and propositions from various papers into a single summarized list of fairness requirements (guideline alike). The researcher can then adopt it as a foundation or reference to aid them in developing their interpretation of algorithmic fairness. Therefore, future work for this domain would have a more straightforward development process. We also found while structuring this framework that to develop a concept of fairness that everyone can accept, it would require collaboration with other domain expertise (e.g., social science, law, etc.) to avoid any misinformation or naivety that might occur from that particular subject. That is because this field of algorithmic fairness is far broader than one would think initially; various problems from the multiple points of view could come by unnoticed to the novice\u2019s eye. In the real world, using active discriminator attributes such as religion, race, nation, tribe, religion, and gender become the problems, but in the algorithm, it becomes the fairness reason.",
      "citationCount": 1,
      "doi": "10.11591/EEI.V9I4.2028",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6dbc9de94aaa3e1f57cf22e398104071d96b3f9c",
      "venue": "Bulletin of Electrical Engineering and Informatics",
      "journal": {
        "name": "Bulletin of Electrical Engineering and Informatics"
      },
      "publicationTypes": null
    },
    {
      "paperId": "8aa4b150507384ef529dbd16da32525adec4b18b",
      "title": "Scales of ignorance: an ethical normative framework to account for relative risk of harm in sport categorization",
      "authors": [
        {
          "name": "Alan C. Oldham",
          "authorId": "2218184813"
        }
      ],
      "year": 2024,
      "abstract": "ABSTRACT Sport categorization is often justified by benefits such as increased fairness or inclusion. Taking inspiration from John Rawls, Sigmund Loland\u2019s fair equality of opportunity principle in sport (FEOPs) is a tool for determining whether the existence of an inequality ethically justifies the institution of a new category in any given sport. It is an elegant ethical normative framework, but since FEOPs does not account explicitly for athlete safety (i.e. athlete physical and mental wellbeing), we are left in an ethically dubious situation where the risk of harm associated with a categorization regime might in fact prove to be greater than the risk of harm present within the sport before its introduction. To address this critical gap, I propose the \u2018scales of ignorance\u2019 ethical normative framework to weigh the relative risk of harm within a sport, crucially inserting athlete safety into the discourse surrounding ethical justification for categorization in sport. The current paper is the first explicit formulation of assessment and ethical justification of risk of harm in the familiar logic of FEOPs. The scales of ignorance framework can also be used independently of Loland\u2019s approach. Two new concepts are also proposed: \u2018insidious risk of harm\u2019 and \u2018pernicious risk of harm\u2019.",
      "citationCount": 0,
      "doi": "10.1080/00948705.2024.2330066",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/8aa4b150507384ef529dbd16da32525adec4b18b",
      "venue": "Journal of the Philosophy of Sport",
      "journal": {
        "name": "Journal of the Philosophy of Sport",
        "pages": "496 - 514",
        "volume": "51"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "08d3c96608cae70e5714e449e608aa5c2362c3f8",
      "title": "Beyond Algorithmic Spirituality: An Islamic Ontological Framework for Digital Religious Practice",
      "authors": [
        {
          "name": "M. Febrianto",
          "authorId": "2402964307"
        },
        {
          "name": "Firman Wijaya",
          "authorId": "2357902458"
        },
        {
          "name": "Wahid Nur Tualeka",
          "authorId": "2402809449"
        },
        {
          "name": "Mahmud Muhsinin",
          "authorId": "2097499849"
        },
        {
          "name": "Ahmad Ghozi",
          "authorId": "2357902309"
        },
        {
          "name": "Al Afnan",
          "authorId": "2357902851"
        },
        {
          "name": "Febrianto Firman",
          "authorId": "2402970001"
        },
        {
          "name": "Wahid Nur Wijaya",
          "authorId": "2402970048"
        },
        {
          "name": "Mahmud Tualeka",
          "authorId": "2402964267"
        },
        {
          "name": "Ahmad Muhsinin",
          "authorId": "2402964301"
        },
        {
          "name": "Ghozi Al",
          "authorId": "2402970096"
        },
        {
          "name": "Afnan",
          "authorId": "2402968288"
        }
      ],
      "year": 2025,
      "abstract": "Purpose: This study aims to develop an Islamic ontological framework for digital religious practice as a response to the reductionism and fragmentation characterizing digital New Age spirituality. The research addresses a critical gap in contemporary scholarship, where digital spirituality is often examined descriptively or pragmatically without sufficient engagement with underlying ontological foundations, particularly within Islamic philosophical discourse. Methodology: This research employs a qualitative conceptual\u2013philosophical approach using systematic conceptual synthesis. The study analyzes twenty-five peer-reviewed scholarly articles published between 2018 and 2025 on digital religiosity, New Age spirituality, and Islamic ontology, alongside classical Islamic sources. Following Jaakkola\u2019s framework for conceptual research, the analysis proceeds through thematic categorization, comparative ontological analysis across six dimensions, and abductive theoretical synthesis to construct a normative framework of Islamic cyberspirituality. Findings: The study identifies four constitutive patterns of digital New Age spirituality\u2014individualism, syncretism, aesthetic spirituality, and commodification\u2014which collectively generate three ontological ruptures: displacement of transcendence by subjective emotionalism, replacement of religious authority by algorithmic validation, and transformation of spiritual discipline into consumable experience. In contrast, Islamic cyberspirituality is shown to rest on two interlocking principles: tawhid as the ontological axis preserving the Creator\u2013creation distinction and revelation-based truth, and tazkiyah al-nafs as a transformative discipline orienting spiritual growth vertically toward God. This framework repositions technology as was\u012blah (instrumental means) rather than gh\u0101yah (end), and generates concrete implications across five domains: digital da\u2018wah, online Sufism, authority structures, attention governance, and religious identity formation. Implications: The findings offer actionable guidance for Muslim content creators, educators, religious institutions, technologists, and individual believers seeking to cultivate authentic digital spirituality. The framework provides evaluative criteria for digital religious content, ethical principles for technology design, and practical strategies for resisting commodification and algorithmic domination in religious practice. Originality/Value: This study contributes original value by moving beyond critique toward constructive ontological reconstruction. It is among the first to systematically integrate classical Islamic metaphysics with contemporary digital practice theory, demonstrating that tawhid and tazkiyah al-nafs provide robust conceptual resources for addressing algorithmic authority, attention commodification, and spiritual fragmentation in digital environments.",
      "citationCount": 0,
      "doi": "10.15575/kt.v7i1.49492",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/08d3c96608cae70e5714e449e608aa5c2362c3f8",
      "venue": "Khazanah Theologia",
      "journal": {
        "name": "Khazanah Theologia"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "2e549c836da5c2e73f58a4c2c7932b101099021e",
      "title": "Am I Being Treated Fairly? A Conceptual Framework for Individuals to Ascertain Fairness",
      "authors": [
        {
          "name": "Juliett Su\u00e1rez Ferreira",
          "authorId": "2353600704"
        },
        {
          "name": "Marija Slavkovik",
          "authorId": "2345547209"
        },
        {
          "name": "Jorge Casillas",
          "authorId": "2345547318"
        }
      ],
      "year": 2025,
      "abstract": "Current fairness metrics and mitigation techniques provide tools for practitioners to asses how non-discriminatory Automatic Decision Making (ADM) systems are. What if I, as an individual facing a decision taken by an ADM system, would like to know: Am I being treated fairly? We explore how to create the affordance for users to be able to ask this question of ADM. In this paper, we argue for the reification of fairness not only as a property of ADM, but also as an epistemic right of an individual to acquire information about the decisions that affect them and use that information to contest and seek effective redress against those decisions, in case they are proven to be discriminatory. We examine key concepts from existing research not only in algorithmic fairness but also in explainable artificial intelligence, accountability, and contestability. Integrating notions from these domains, we propose a conceptual framework to ascertain fairness by combining different tools that empower the end-users of ADM systems. Our framework shifts the focus from technical solutions aimed at practitioners to mechanisms that enable individuals to understand, challenge, and verify the fairness of decisions, and also serves as a blueprint for organizations and policymakers, bridging the gap between technical requirements and practical, user-centered accountability.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2504.02461",
      "arxivId": "2504.02461",
      "url": "https://www.semanticscholar.org/paper/2e549c836da5c2e73f58a4c2c7932b101099021e",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.02461"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e10ffcbe4545b6d11a66320f4df149e001e32312",
      "title": "HUMAN DIGNITY AGAINST AI DOMINATION: IN SEARCH OF A LEGAL AND ETHICAL FRAMEWORK IN THE AGE OF DIGITALIZATION, AUTONOMOUS WARFARE, AND ALGORITHMIC DISCRIMINATION",
      "authors": [
        {
          "name": "Valeria Cruz",
          "authorId": "2383390924"
        }
      ],
      "year": 2025,
      "abstract": "This article examines the unprecedented challenges that artificial intelligence (AI), digitalization, autonomous weapons, and algorithmic discrimination pose to the principle of human dignity in international law and ethics. Human dignity, traditionally regarded as the cornerstone of human rights and humanitarian law, is increasingly undermined by technological practices that erode autonomy, weaken accountability, and institutionalize inequality. The study develops its central thesis by analyzing three interrelated axes of threat: the erosion of personal autonomy under surveillance capitalism, the accountability crisis in the deployment of lethal autonomous weapon systems, and the entrenchment of social inequality through algorithmic bias. Each axis demonstrates the inadequacy of existing legal frameworks, ethical guidelines, and governance structures to address systemic risks, revealing a normative vacuum in which dignity is exposed to structural erosion. To fill this gap, the article proposes a comprehensive framework for safeguarding dignity that integrates legal enforceability, ethical orientation, and governance mechanisms. Legally, the study emphasizes the adaptation of international human rights and humanitarian law to AI contexts, advocating risk-based regulation, prohibitions on dignity-incompatible practices, and the establishment of binding international instruments. Ethically, it draws upon Kantian, Habermasian, and informational ethics traditions to articulate principles of transparency, fairness, human oversight, and accountability. In terms of governance, it argues for multi-stakeholder participation, corporate accountability, standardization, and international coordination, highlighting the indispensable role of civil society and the Global South in shaping inclusive AI governance. The article concludes that preserving human dignity in the digital age requires proactive regulation, interdisciplinary collaboration, and cultural transformation. Binding international treaties, mandatory impact assessments, and ethical education are identified as urgent measures. Ultimately, the study underscores that technological innovation must not be allowed to dictate the conditions of human existence; rather, it must be directed to serve humanity, ensuring a digital future that is just, humane, and respectful of the intrinsic worth of every person.",
      "citationCount": 1,
      "doi": "10.55843/icl2025cong347c",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e10ffcbe4545b6d11a66320f4df149e001e32312",
      "venue": "Congress Proceedings",
      "journal": {
        "name": "Congress Proceedings"
      },
      "publicationTypes": null
    },
    {
      "paperId": "6af49c78b7ac6993c4ada741dc1239d6105d292e",
      "title": "Rules for Radical AI: A Counter-Framework for Algorithmic Contestation",
      "authors": [
        {
          "name": "Victor Frimpong",
          "authorId": "2353545782"
        }
      ],
      "year": 2025,
      "abstract": "AI ethics frameworks\u2014grounded in principles of transparency, fairness, and accountability\u2014have become the dominant mechanism for governing algorithmic systems. However, by treating AI as a neutral technology amenable to technical fixes, they obscure its status as a political project embedded in power, inequality, and historical injustice. In this paper, we introduce the concept of algorithmic contestation\u2014the use of collective, tactical interventions to challenge and reshape the socio-political forces that drive the design and deployment of AI. Drawing on Saul Alinsky\u2019s Rules for Radicals, we derive ten \u201cRules for Radical AI\u201d that activists and researchers can deploy, including \u201cReveal the Code,\u201d \u201cOrganize a Data Strike,\u201d and \u201cSeize Policy Windows.\u201d We first map mainstream ethics principles onto Alinsky\u2019s tactical logic, then illustrate these dynamics through two case studies\u2014the San Francisco facial recognition ban and a New York City gig worker data strike. Finally, we discuss methodological limits, ethical risks, and future research avenues and issue a manifesto: proper AI accountability arises not from compliance alone but from collective contestation of algorithmic power.",
      "citationCount": 0,
      "doi": "10.2139/ssrn.5344455",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6af49c78b7ac6993c4ada741dc1239d6105d292e",
      "venue": "Social Science Research Network",
      "journal": {
        "name": "SSRN Electronic Journal"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "38f3c3caf305daa1c00c847084c69c98f45c92c8",
      "title": "A Harm-Reduction Framework for Algorithmic Fairness",
      "authors": [
        {
          "name": "Micah Altman",
          "authorId": "145918992"
        },
        {
          "name": "Alexandra Wood",
          "authorId": "39328757"
        },
        {
          "name": "E. Vayena",
          "authorId": "145104974"
        }
      ],
      "year": 2018,
      "abstract": "In this article, we recognize the profound effects that algorithmic decision making can have on people\u2019s lives and propose a harm-reduction framework for algorithmic fairness. We argue that any evaluation of algorithmic fairness must take into account the foreseeable effects that algorithmic design, implementation, and use have on the well-being of individuals. We further demonstrate how counterfactual frameworks for causal inference developed in statistics and computer science can be used as the basis for defining and estimating the foreseeable effects of algorithmic decisions. Finally, we argue that certain patterns of foreseeable harms are unfair. An algorithmic decision is unfair if it imposes predictable harms on sets of individuals that are unconscionably disproportionate to the benefits these same decisions produce elsewhere. Also, an algorithmic decision is unfair when it is regressive, that is, when members of disadvantaged groups pay a higher cost for the social benefits of that decision.",
      "citationCount": 40,
      "doi": "10.1109/MSP.2018.2701149",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/38f3c3caf305daa1c00c847084c69c98f45c92c8",
      "venue": "IEEE Security and Privacy",
      "journal": {
        "name": "IEEE Security & Privacy",
        "pages": "34-45",
        "volume": "16"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "39bf05cf1e3047d57b19d016dfd4a53e39e2b71f",
      "title": "Rawls\u2019s Justice as Fairness and Indonesian Health Policy: A Doctrinal Framework for Equity-Oriented Reform",
      "authors": [
        {
          "name": "Tony Richard Alexander Samosir",
          "authorId": "2393313075"
        },
        {
          "name": "Mei Susanto",
          "authorId": "2393310593"
        }
      ],
      "year": 2025,
      "abstract": "The study aims to analyze the application of John Rawls' Theory of Justice in Indonesia's health system policy, emphasizing how principles of justice can promote fairness and equality in the provision of health services, especially for disadvantaged populations.The method used was a judicial normative (doctrinal) approach. This study critically reviewed legal and philosophical texts relevant to the Indonesian health policy framework, integrating Rawls' theory as an analytical lens.The novelty of this research lies in the operationalization of Rawls' abstract philosophical principles into a structured evaluation framework (Rawlsian grid) and its application to the Indonesian socio-political and cultural context. This allows for a systematic assessment of inequalities in the health system and proposals for equitable policy solutions.The findings show that programs such as the National Health Insurance Scheme (JKN) have contributed to broader access to health care, but disparities remain in remote and underdeveloped areas. Using the Rawlsian grid, the study highlights challenges in budget allocation, health worker distribution, and infrastructure access. Recommended strategies include reallocating health budgets, fair distribution of health workers, expansion of telemedicine, and participatory planning.The concludes that integrating Rawlsian principles of justice through a structured evaluation framework can support a more inclusive and sustainable health system in Indonesia, ensuring fair and equal access to quality health services for all citizens.",
      "citationCount": 0,
      "doi": "10.33506/js.v12i1.4556",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/39bf05cf1e3047d57b19d016dfd4a53e39e2b71f",
      "venue": "JUSTISI",
      "journal": {
        "name": "JUSTISI"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "ba1a77c3a93854461be2fe642b1952eaf3e27bd7",
      "title": "Fairness in Algorithmic Recourse Through the Lens of Substantive Equality of Opportunity",
      "authors": [
        {
          "name": "Andrew Bell",
          "authorId": "2239195821"
        },
        {
          "name": "Jo\u00e3o Fonseca",
          "authorId": "2239195588"
        },
        {
          "name": "Carlo Abrate",
          "authorId": "89449460"
        },
        {
          "name": "Francesco Bonchi",
          "authorId": "2179558887"
        },
        {
          "name": "J. Stoyanovich",
          "authorId": "1682824"
        }
      ],
      "year": 2024,
      "abstract": "Algorithmic recourse -- providing recommendations to those affected negatively by the outcome of an algorithmic system on how they can take action and change that outcome -- has gained attention as a means of giving persons agency in their interactions with artificial intelligence (AI) systems. Recent work has shown that even if an AI decision-making classifier is ``fair'' (according to some reasonable criteria), recourse itself may be unfair due to differences in the initial circumstances of individuals, compounding disparities for marginalized populations and requiring them to exert more effort than others. There is a need to define more methods and metrics for evaluating fairness in recourse that span a range of normative views of the world, and specifically those that take into account time. Time is a critical element in recourse because the longer it takes an individual to act, the more the setting may change due to model or data drift. This paper seeks to close this research gap by proposing two notions of fairness in recourse that are in normative alignment with substantive equality of opportunity, and that consider time. The first considers the (often repeated) effort individuals exert per successful recourse event, and the second considers time per successful recourse event. Building upon an agent-based framework for simulating recourse, this paper demonstrates how much effort is needed to overcome disparities in initial circumstances. We then proposes an intervention to improve the fairness of recourse by rewarding effort, and compare it to existing strategies.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2401.16088",
      "arxivId": "2401.16088",
      "url": "https://www.semanticscholar.org/paper/ba1a77c3a93854461be2fe642b1952eaf3e27bd7",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2401.16088"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a3cc3a49bfce756a4164c20887e76a45ae9a7dfa",
      "title": "The Price of Fairness - A Framework to Explore Trade-Offs in Algorithmic Fairness",
      "authors": [
        {
          "name": "C. Haas",
          "authorId": "152864672"
        }
      ],
      "year": 2019,
      "abstract": null,
      "citationCount": 38,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a3cc3a49bfce756a4164c20887e76a45ae9a7dfa",
      "venue": "International Conference on Interaction Sciences",
      "journal": {
        "name": "",
        "volume": ""
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "85152ab48a237708c63a385645861b095d6420a9",
      "title": "Moving from Fairness to Justice: Intentional Algorithmic Solutions Through an Intersectional Lens",
      "authors": [
        {
          "name": "Kenya S. Andrews",
          "authorId": "2377852790"
        }
      ],
      "year": 2025,
      "abstract": "In this forum we explore different perspectives for how to apply intersectionality as a critical framework for design across multiple contexts. --- Yolanda A. Rankin and Jakita O. Thomas, Editors",
      "citationCount": 0,
      "doi": "10.1145/3760549",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/85152ab48a237708c63a385645861b095d6420a9",
      "venue": "Interactions",
      "journal": {
        "name": "Interactions",
        "pages": "58 - 60",
        "volume": "32"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "30a0328de79319c3363aa3ac7f14be4c4fdaa954",
      "title": "An Epistemic Lens on Algorithmic Fairness",
      "authors": [
        {
          "name": "Elizabeth Edenberg",
          "authorId": "15737123"
        },
        {
          "name": "Alexandra Wood",
          "authorId": "2262222390"
        }
      ],
      "year": 2023,
      "abstract": "In this position paper, we introduce a new epistemic lens for analyzing algorithmic harm. We argue that the epistemic lens we propose herein has two key contributions to help reframe and address some of the assumptions underlying inquiries into algorithmic fairness. First, we argue that using the framework of epistemic injustice helps to identify the root causes of harms currently framed as instances of representational harm. We suggest that the epistemic lens offers a theoretical foundation for expanding approaches to algorithmic fairness in order to address a wider range of harms not recognized by existing technical or legal definitions. Second, we argue that the epistemic lens helps to identify the epistemic goals of inquiries into algorithmic fairness. There are two distinct contexts within which we examine algorithmic harm: at times, we seek to understand and describe the world as it is, and, at other times, we seek to build a more just future. The epistemic lens can serve to direct our attention to the epistemic frameworks that shape our interpretations of the world as it is and the ways we envision possible futures. Clarity with respect to which epistemic context is relevant in a given inquiry can further help inform choices among the different ways of measuring and addressing algorithmic harms. We introduce this framework with the goal of initiating new research directions bridging philosophical, legal, and technical approaches to understanding and mitigating algorithmic harms.",
      "citationCount": 5,
      "doi": "10.1145/3617694.3623248",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/30a0328de79319c3363aa3ac7f14be4c4fdaa954",
      "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",
      "journal": {
        "name": "Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "fc7f91d95b88689314617e08b784044061c45e3d",
      "title": "Algorithmic Fairness and Feasibility",
      "authors": [
        {
          "name": "Eva Erman",
          "authorId": "2338612784"
        },
        {
          "name": "Markus Furendal",
          "authorId": "113631409"
        },
        {
          "name": "Niklas M\u00f6ller",
          "authorId": "145497600"
        }
      ],
      "year": 2025,
      "abstract": "The \u201cimpossibility results\u201d in algorithmic fairness suggest that a predictive model cannot fully meet two common fairness criteria \u2013 sufficiency and separation \u2013 except under extraordinary circumstances. These findings have sparked a discussion on fairness in algorithms, prompting debates over whether predictive models can avoid unfair discrimination based on protected attributes, such as ethnicity or gender. As shown by Otto Sahlgren, however, the discussion of the impossibility results would gain from importing some of the tools developed in the philosophical literature on feasibility. Utilizing these tools, Sahlgren sketches a cautiously optimistic view of how algorithmic fairness can be made feasible in restricted local decision-making. While we think it is a welcome move to inject the literature on feasibility into the debate on algorithmic fairness, Sahlgren says very little about what are the general gains of bringing in feasibility considerations in theorizing algorithmic fairness. How, more precisely, does it help us make assessments about fairness in algorithmic decision-making? This is what is addressed in this Reply. More specifically, our two-fold argument is that feasibility plays an important but limited role for algorithmic fairness. We end by offering a sketch of a framework, which may be useful for theorizing feasibility in algorithmic fairness.",
      "citationCount": 1,
      "doi": "10.1007/s13347-024-00835-8",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/fc7f91d95b88689314617e08b784044061c45e3d",
      "venue": "Philosophy & Technology",
      "journal": {
        "name": "Philosophy & Technology",
        "volume": "38"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0a49c157f2565a95187108b07ad2d8231bd77e86",
      "title": "A Feminist Account of Intersectional Algorithmic Fairness",
      "authors": [
        {
          "name": "Marie Mirsch",
          "authorId": "2377073672"
        },
        {
          "name": "Laila Wegner",
          "authorId": "2331068415"
        },
        {
          "name": "Jonas Strube",
          "authorId": "2376541652"
        },
        {
          "name": "C. Leicht-Scholten",
          "authorId": "2347733474"
        }
      ],
      "year": 2025,
      "abstract": "Intersectionality has profoundly influenced research and political action by revealing how interconnected systems of privilege and oppression influence lived experiences, yet its integration into algorithmic fairness research remains limited. Existing approaches often rely on single-axis or formal subgroup frameworks that risk oversimplifying social realities and neglecting structural inequalities. We propose Substantive Intersectional Algorithmic Fairness, extending Green's (2022) notion of substantive algorithmic fairness with insights from intersectional feminist theory. Building on this foundation, we introduce ten desiderata within the ROOF methodology to guide the design, assessment, and deployment of algorithmic systems in ways that address systemic inequities while mitigating harms to intersectionally marginalized communities. Rather than prescribing fixed operationalizations, these desiderata encourage reflection on assumptions of neutrality, the use of protected attributes, the inclusion of multiply marginalized groups, and enhancing algorithmic systems'potential. Our approach emphasizes that fairness cannot be separated from social context, and that in some cases, principled non-deployment may be necessary. By bridging computational and social science perspectives, we provide actionable guidance for more equitable, inclusive, and context-sensitive intersectional algorithmic practices.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.17944",
      "arxivId": "2508.17944",
      "url": "https://www.semanticscholar.org/paper/0a49c157f2565a95187108b07ad2d8231bd77e86",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.17944"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a6ab28aca6db3088235203e36fefdcc5bcb73db1",
      "title": "On Algorithmic Fairness in Medical Practice",
      "authors": [
        {
          "name": "Thomas Grote",
          "authorId": "1423968387"
        },
        {
          "name": "G. Keeling",
          "authorId": "12604592"
        }
      ],
      "year": 2022,
      "abstract": "Abstract The application of machine-learning technologies to medical practice promises to enhance the capabilities of healthcare professionals in the assessment, diagnosis, and treatment, of medical conditions. However, there is growing concern that algorithmic bias may perpetuate or exacerbate existing health inequalities. Hence, it matters that we make precise the different respects in which algorithmic bias can arise in medicine, and also make clear the normative relevance of these different kinds of algorithmic bias for broader questions about justice and fairness in healthcare. In this paper, we provide the building blocks for an account of algorithmic bias and its normative relevance in medicine.",
      "citationCount": 22,
      "doi": "10.1017/S0963180121000839",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a6ab28aca6db3088235203e36fefdcc5bcb73db1",
      "venue": "Cambridge Quarterly of Healthcare Ethics",
      "journal": {
        "name": "Cambridge Quarterly of Healthcare Ethics",
        "pages": "83 - 94",
        "volume": "31"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a3cd8ffe54f5b27911612ecdfb0b2a7525d084af",
      "title": "Educating Software and AI Stakeholders About Algorithmic Fairness, Accountability, Transparency and Ethics",
      "authors": [
        {
          "name": "Veronika Bogina",
          "authorId": "3368624"
        },
        {
          "name": "Alan Hartman",
          "authorId": "2063971294"
        },
        {
          "name": "T. Kuflik",
          "authorId": "1770193"
        },
        {
          "name": "Avital Shulner-Tal",
          "authorId": "2075154007"
        }
      ],
      "year": 2021,
      "abstract": "This paper discusses educating stakeholders of algorithmic systems (systems that apply Artificial Intelligence/Machine learning algorithms) in the areas of algorithmic fairness, accountability, transparency and ethics (FATE). We begin by establishing the need for such education and identifying the intended consumers of educational materials on the topic. We discuss the topics of greatest concern and in need of educational resources; we also survey the existing materials and past experiences in such education, noting the scarcity of suitable material on aspects of fairness in particular. We use an example of a college admission platform to illustrate our ideas. We conclude with recommendations for further work in the area and report on the first steps taken towards achieving this goal in the framework of an academic graduate seminar course, a graduate summer school, an embedded lecture in a software engineering course, and a workshop for high school teachers.",
      "citationCount": 93,
      "doi": "10.1007/s40593-021-00248-0",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a3cd8ffe54f5b27911612ecdfb0b2a7525d084af",
      "venue": "International Journal of Artificial Intelligence in Education",
      "journal": {
        "name": "International Journal of Artificial Intelligence in Education",
        "pages": "808 - 833",
        "volume": "32"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "d2712df0a1a224ac8955a662746a8b505682f4c2",
      "title": "Robustness Implies Fairness in Causal Algorithmic Recourse",
      "authors": [
        {
          "name": "A. Ehyaei",
          "authorId": "47515476"
        },
        {
          "name": "Amir-Hossein Karimi",
          "authorId": "145926563"
        },
        {
          "name": "B. Scholkopf",
          "authorId": "1707625"
        },
        {
          "name": "Setareh Maghsudi",
          "authorId": "1822482"
        }
      ],
      "year": 2023,
      "abstract": "Algorithmic recourse discloses the internal procedures of a black-box decision process where decisions have significant consequences by providing recommendations to empower beneficiaries to achieve a more favorable outcome. To ensure an effective remedy, suggested interventions must not only be cost-effective but also robust and fair. To that end, it is essential to provide similar explanations to similar individuals. This study explores the concept of individual fairness and adversarial robustness in causal algorithmic recourse and addresses the challenge of achieving both. To resolve the challenges, we propose a new framework for defining adversarially robust recourse. That setting observes the protected feature as a pseudometric and demonstrates that individual fairness is a special case of adversarial robustness. Finally, we introduce the fair robust recourse problem and establish solutions to achieve both desirable properties both theoretically and empirically.",
      "citationCount": 17,
      "doi": "10.1145/3593013.3594057",
      "arxivId": "2302.03465",
      "url": "https://www.semanticscholar.org/paper/d2712df0a1a224ac8955a662746a8b505682f4c2",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "f709068f035f912983c559a198c48ae7c15a1118",
      "title": "Escaping the 'Impossibility of Fairness': From Formal to Substantive Algorithmic Fairness",
      "authors": [
        {
          "name": "Ben Green",
          "authorId": "87122962"
        }
      ],
      "year": 2021,
      "abstract": "In the face of compounding crises of social and economic inequality, many have turned to algorithmic decision-making to achieve greater fairness in society. As these efforts intensify, reasoning within the burgeoning field of \u201calgorithmic fairness\u201d increasingly shapes how fairness manifests in practice. This paper interrogates whether algorithmic fairness provides the appropriate conceptual and practical tools for enhancing social equality. I argue that the dominant, \u201cformal\u201d approach to algorithmic fairness is ill-equipped as a framework for pursuing equality, as its narrow frame of analysis generates restrictive approaches to reform. In light of these shortcomings, I propose an alternative: a \u201csubstantive\u201d approach to algorithmic fairness that centers opposition to social hierarchies and provides a more expansive analysis of how to address inequality. This substantive approach enables more fruitful theorizing about the role of algorithms in combatting oppression. The distinction between formal and substantive algorithmic fairness is exemplified by each approach\u2019s responses to the \u201cimpossibility of fairness\u201d (an incompatibility between mathematical definitions of algorithmic fairness). While the formal approach requires us to accept the \u201cimpossibility of fairness\u201d as a harsh limit on efforts to enhance equality, the substantive approach allows us to escape the \u201cimpossibility of fairness\u201d by suggesting reforms that are not subject to this false dilemma and that are better equipped to ameliorate conditions of social oppression.",
      "citationCount": 65,
      "doi": "10.2139/ssrn.3883649",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f709068f035f912983c559a198c48ae7c15a1118",
      "venue": "Social Science Research Network",
      "journal": {
        "name": "InfoSciRN: Machine Learning (Sub-Topic)"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0f8e215f13b22fbdfd7a2428054fa09a1927e977",
      "title": "Affirmative Algorithms: Relational Equality as Algorithmic Fairness",
      "authors": [
        {
          "name": "Marilyn Zhang",
          "authorId": "2117894061"
        }
      ],
      "year": 2022,
      "abstract": "Many statistical fairness notions have been proposed for algorithmic decision-making systems, and especially public safety pretrial risk assessment (PSPRA) algorithms such as COMPAS. Most fairness notions equalize something between groups, whether it is false positive rates or accuracy. In fact, I demonstrate that most prominent notions have their basis in equalizing some form of accuracy. However, statistical fairness metrics often do not capture the substantive point of equality. I argue that equal accuracy is not only difficult to measure but also unsatisfactory for ensuring equal justice. In response, I introduce philosopher Elizabeth Anderson\u2019s theory of relational equality as a fruitful alternative framework: to relate as equals, people need access to certain basic capabilities. I show that relational equality requires Affirmative PSPRA algorithms that lower risk scores for Black defendants. This is because fairness based on relational equality means considering the impact of PSPRA algorithms\u2019 decisions on access to basic capabilities. This impact is racially asymmetric in an unjust society. I make three main contributions: (1) I illustrate the shortcomings of statistical fairness notions in their reliance on equalizing some form of accuracy; (2) I present the first comprehensive ethical defense of Affirmative PSPRA algorithms, based on fairness in terms of relational equality instead; and (3) I show that equalizing accuracy is neither sufficient nor necessary for fairness based on relational equality. Overall, this work serves narrowly as a reason to re-evaluate algorithmic fairness for PSPRA algorithms, and serves broadly as an example of how discussions of algorithmic fairness can benefit from egalitarian philosophical frameworks.",
      "citationCount": 15,
      "doi": "10.1145/3531146.3533115",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/0f8e215f13b22fbdfd7a2428054fa09a1927e977",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "4aad069a072c5a11ae2c5ba69ab5086f5f206ba6",
      "title": "Disability, fairness, and algorithmic bias in AI recruitment",
      "authors": [
        {
          "name": "Nicholas Tilmes",
          "authorId": "2162982744"
        }
      ],
      "year": 2022,
      "abstract": null,
      "citationCount": 89,
      "doi": "10.1007/s10676-022-09633-2",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/4aad069a072c5a11ae2c5ba69ab5086f5f206ba6",
      "venue": "Ethics and Information Technology",
      "journal": {
        "name": "Ethics and Information Technology",
        "volume": "24"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a9a7058b39768ece13608e31341cfb16c4faf2c3",
      "title": "Algorithmic Fairness from a Non-ideal Perspective",
      "authors": [
        {
          "name": "S. Fazelpour",
          "authorId": "4876895"
        },
        {
          "name": "Zachary Chase Lipton",
          "authorId": "32219137"
        }
      ],
      "year": 2020,
      "abstract": "Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In the hopes of mitigating these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might hope to observe in a fair world, offering a variety of algorithms that attempt to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to fair machine learning to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and ideal worlds. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of their actions, naive applications of ideal thinking can lead to misguided policies. In this paper, we demonstrate a connection between the recent literature on fair machine learning and the ideal approach in political philosophy, and show that some recently uncovered shortcomings in proposed algorithms reflect broader troubles faced by the ideal approach. We work this analysis through for different formulations of fairness and conclude with a critical discussion of real-world impacts and directions for new research.",
      "citationCount": 107,
      "doi": "10.1145/3375627.3375828",
      "arxivId": "2001.09773",
      "url": "https://www.semanticscholar.org/paper/a9a7058b39768ece13608e31341cfb16c4faf2c3",
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "63052b2cee27b5f8488fe0a2f8891d5c76a249f3",
      "title": "Rethinking Algorithmic Fairness in the Context of Information Access",
      "authors": [
        {
          "name": "E. Udoh",
          "authorId": "40354671"
        },
        {
          "name": "Xiaojun (Jenny) Yuan",
          "authorId": "1410318900"
        },
        {
          "name": "Abebe Rorissa",
          "authorId": "2594683"
        }
      ],
      "year": 2022,
      "abstract": "The internet has revolutionized the information seeking landscape, heightening expectations that access to information will hasten the attainment of several of the United Nation (UN)'s sustainable development goals (SDGs), although digital and content divides persist and information access is demonstrably steeped in inequalities, partly due to underlying intricate, complex, and often opaque AI/ML algorithms. While equity in information access remains a long shot goal, this poster asks the fundamental question: can access to information at least be fair? And what exactly does algorithmic fairness mean in the context of information access? Drawing from themes in information search and social informatics, especially the political valence in technologies such as online decision and filtering algorithms, this poster aims to analyze current nuances of algorithmic fairness in order to identify those intricate issues that should inform and constitute any working definition and framework to study algorithmic fairness in the context of information access in an increasingly globalized world.",
      "citationCount": 2,
      "doi": "10.1002/pra2.736",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/63052b2cee27b5f8488fe0a2f8891d5c76a249f3",
      "venue": "ASIS&T Annual Meeting",
      "journal": {
        "name": "Proceedings of the Association for Information Science and Technology",
        "volume": "59"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "9bcda3b85ad842fb980f3d0f33f6f1867328c69a",
      "title": "Ethical Considerations in AI: Bias Mitigation and Fairness in Algorithmic Decision Making",
      "authors": [
        {
          "name": "Grace Sullivan",
          "authorId": "2396830843"
        },
        {
          "name": "Thomas Richardson",
          "authorId": "2396875294"
        }
      ],
      "year": 2025,
      "abstract": "The rapid integration of artificial intelligence (AI) into critical decision-making domains\u2014such as healthcare, finance, law enforcement, and hiring\u2014has raised significant ethical concerns regarding bias and fairness. Algorithmic decision-making systems, if not carefully designed and monitored, risk perpetuating and amplifying societal biases, leading to unfair and discriminatory outcomes. This paper explores the ethical considerations surrounding AI, focusing on bias mitigation and fairness in algorithmic systems. We examine the sources of bias in AI models, including biased training data, algorithmic design choices, and systemic inequities. Furthermore, we review existing approaches to bias mitigation, such as fairness-aware machine learning techniques, adversarial debiasing, and regulatory frameworks that promote transparency and accountability. The paper also discusses the trade-offs between fairness, accuracy, and interpretability, emphasizing the need for interdisciplinary collaboration to develop ethical AI systems. By analyzing current challenges and emerging solutions, this study provides a roadmap for responsible AI development that prioritizes fairness, reduces bias, and fosters trust in automated decision-making.",
      "citationCount": 0,
      "doi": "10.65521/ijacte.v12i2.112",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/9bcda3b85ad842fb980f3d0f33f6f1867328c69a",
      "venue": "International Journal on Advanced Computer Theory and Engineering",
      "journal": {
        "name": "International Journal on Advanced Computer Theory and Engineering"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "88ec635afe17e9d3c5259764a076bacb822b23bf",
      "title": "How Much Effort Is Enough? Fairness in Algorithmic Recourse Through the Lens of Substantive Equality of Opportunity",
      "authors": [
        {
          "name": "Andrew Bell",
          "authorId": "2239195821"
        },
        {
          "name": "Jo\u00e3o Fonseca",
          "authorId": "2239195588"
        },
        {
          "name": "Carlo Abrate",
          "authorId": "89449460"
        },
        {
          "name": "Francesco Bonchi",
          "authorId": "2179558887"
        },
        {
          "name": "Julia Stoyanovich",
          "authorId": "2281825322"
        }
      ],
      "year": 2025,
      "abstract": "Algorithmic recourse, or enabling individuals to reverse a negative outcome, has gained attention as a means of supporting human agency in interactions with artificial intelligence (AI) systems. However, recent work has shown that even if a decision-making classifier is fair according to reasonable criteria, recourse itself may be unfair. Members of disadvantaged groups may have to work harder than their more privileged peers to reverse a negative outcome. In this paper, we introduce effort-aware fairness, which treats algorithmic recourse through the lens of substantive equality of opportunity. It acknowledges that individuals from different groups may not be comparably well-equipped to act on their recourse, resulting in disparities in their chances of reversing unfavorable outcomes. These disparities, shaped by differing effort distributions among demographic groups, can be exacerbated over time. We provide a formal definition of effort-aware fairness, propose fairness metrics, and then develop an intervention that improves recourse fairness by rewarding effort. Through empirical comparison with existing strategies, we demonstrate that this intervention successfully mitigates disparities. Our conceptual framework and experimental evaluation build upon prior work that uses an agent-based model for simulating real-world recourse over time.",
      "citationCount": 0,
      "doi": "10.1145/3757887.3763014",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/88ec635afe17e9d3c5259764a076bacb822b23bf",
      "venue": "Proceedings of the 5th ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",
      "journal": {
        "name": "Proceedings of the 5th ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization"
      },
      "publicationTypes": [
        "Book",
        "Conference"
      ]
    },
    {
      "paperId": "4e73dc9237c71f1d9ecdd807767d8b367939f5f4",
      "title": "\u201cIt takes a village to write a really good paper\u201d: A\u00a0normative framework for peer reviewing in philosophy",
      "authors": [
        {
          "name": "Samantha Copeland",
          "authorId": "2255514997"
        },
        {
          "name": "Lavinia Marin",
          "authorId": "2167976631"
        }
      ],
      "year": 2024,
      "abstract": "That there is a \u201ccrisis of peer review\u201d at the moment is not in dispute, but sufficient attention has not yet been paid to the normative potential that lies in current calls for reform. In contrast to approaches to \u201cfixing\u201d the problems in peer review, which tend to maintain the status quo in terms of professionalising opportunities, this paper addresses the needs of philosophers and how peer\u2010review reform can be an opportunity to improve the academic discipline of philosophy, whereby progress is understood as making the discipline more fair to the global academic community and more conducive to the flourishing of academic philosophers. The paper evaluates recent categories of relevant norms and correlating reforms. In conclusion, it recommends that philosophy pursue the norms of transparency and democracy explicitly when proposing peer\u2010review reform and suggest that proposals for forum\u2010based models of peer review are most likely to support those norms.",
      "citationCount": 2,
      "doi": "10.1111/meta.12670",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/4e73dc9237c71f1d9ecdd807767d8b367939f5f4",
      "venue": "Metaphilosophy",
      "journal": {
        "name": "Metaphilosophy"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "68968dd49dde6e99226529e4caed6f3fb76da899",
      "title": "Fairness-Aware Classification Based on Rawlsian Veil of Ignorance: A Mathematical Framework for Bias Detection and Mitigation in Machine Learning",
      "authors": [
        {
          "name": "Lin Chen",
          "authorId": "2397387049"
        }
      ],
      "year": 2025,
      "abstract": "Machine learning's penetration into high-stakes decision-making\u2014credit approvals, healthcare triage, criminal risk assessment\u2014has amplified pre-existing societal inequities rather than ameliorating them. This study operationalizes John Rawls's \"veil of ignorance\" (1971) as a computational principle for binary classifiers, confronting a gap: most fairness metrics lack philosophical grounding while Rawlsian theories remain mathematically unformalized. Through three empirical phases\u2014(1) baseline logistic regression on full feature sets, (2) bias quantification via disaggregated metrics across protected groups, and (3) mitigation via pre-processing blindess and post-processing threshold optimization\u2014we demonstrate how ignorance of demographic attributes can be algorithmically imposed. Using the German Credit Dataset (n=1,000), we expose a 12.9% accuracy gap between gender groups in standard models. Our framework collapses demographic parity difference from 15.7% to 0.1% while paradoxically boosting accuracy by 2.8% (from 72.0% to 74.0%), challenging the fairness-accuracy sacrifice orthodoxy. Counterintuitively, naive feature removal worsened bias (+8.9%), only proxy-aware pruning achieved DPD reduction of 32.6%. These findings suggest that Rawlsian principles, when translated into constrained optimization, yield Pareto-superior solutions\u2014though we argue such technical fixes must complement, not substitute for, institutional reform.",
      "citationCount": 0,
      "doi": "10.54097/v1xa7p32",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/68968dd49dde6e99226529e4caed6f3fb76da899",
      "venue": "Academic Journal of Science and Technology",
      "journal": {
        "name": "Academic Journal of Science and Technology"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c54f634f38745f9914571bcec15384a761eac200",
      "title": "ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications",
      "authors": [
        {
          "name": "James Afful",
          "authorId": "2366074882"
        }
      ],
      "year": 2025,
      "abstract": "As machine learning systems are increasingly deployed in high-stakes domains such as criminal justice, finance, and healthcare, the demand for interpretable and trustworthy models has intensified. Despite the proliferation of local explanation techniques, including SHAP, LIME, and counterfactual methods, there exists no standardized, reproducible framework for their comparative evaluation, particularly in fairness-sensitive settings. We introduce ExplainBench, an open-source benchmarking suite for systematic evaluation of local model explanations across ethically consequential datasets. ExplainBench provides unified wrappers for popular explanation algorithms, integrates end-to-end pipelines for model training and explanation generation, and supports evaluation via fidelity, sparsity, and robustness metrics. The framework includes a Streamlit-based graphical interface for interactive exploration and is packaged as a Python module for seamless integration into research workflows. We demonstrate ExplainBench on datasets commonly used in fairness research, such as COMPAS, UCI Adult Income, and LendingClub, and showcase how different explanation methods behave under a shared experimental protocol. By enabling reproducible, comparative analysis of local explanations, ExplainBench advances the methodological foundations of interpretable machine learning and facilitates accountability in real-world AI systems.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2506.06330",
      "arxivId": "2506.06330",
      "url": "https://www.semanticscholar.org/paper/c54f634f38745f9914571bcec15384a761eac200",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.06330"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b9a03723dc0ba2621076c66ae64ec157791e1b08",
      "title": "Shaping an adaptive approach to address the ambiguity of fairness in AI: Theory, framework, and illustrations",
      "authors": [
        {
          "name": "Swaptik Chowdhury",
          "authorId": "101125726"
        },
        {
          "name": "L. Klautzer",
          "authorId": "8620214"
        }
      ],
      "year": 2025,
      "abstract": "\n The adoption of AI is pervasive, often operating behind the scenes and influencing decisions without our explicit awareness. It impacts different aspects of our lives, from personalized recommendations to crucial determinations like hiring decisions or credit approvals. Yet, even to their developers, AI algorithms\u2019 opacity raises concerns about fairness. The biases inherent in our data further complicate matters, as current AI systems often lack moral or logical judgment, relying solely on predictive outputs derived from learned data patterns. Efforts to address fairness in AI models face significant challenges, as different definitions of fairness can lead to conflicting outcomes. Despite attempts to mitigate biases and optimize fairness criteria, achieving a universal and satisfactory solution remains elusive. The multidimensional nature of fairness, with its roots in philosophy and evolving concepts in organizational justice, underscores the complexity of the task. Technology is inherently political, shaped by various societal factors and human biases. Recognizing this, stakeholders must engage in nuanced discussions about the types of fairness relevant in specific contexts and the potential trade-offs involved. Just as in other spheres of decision-making, navigating trade-offs is inevitable, requiring a flexible approach informed by diverse perspectives.\n This study acknowledges that achieving fairness in AI is not about prescribing a singular definition or solution but adapting to evolving needs and values. Embracing ambiguity and tension in decision-making can lead to more inclusive outcomes. An interdisciplinary examination of application-specific and consensus-driven frameworks is adopted to consider fairness in AI. By evaluating factors such as application nuances, procedural frameworks, and stakeholder dynamics, this study demonstrates the framework\u2019s expansive potential applicability in understanding and operationalizing fairness by the way of two illustrations.",
      "citationCount": 1,
      "doi": "10.1017/cfl.2025.7",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/b9a03723dc0ba2621076c66ae64ec157791e1b08",
      "venue": "Law and Governance",
      "journal": {
        "name": "Cambridge Forum on AI: Law and Governance"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d4ef9e53839c384ab6382a289e185d563f2813d8",
      "title": "From Utilitarian to Rawlsian Designs for Algorithmic Fairness",
      "authors": [
        {
          "name": "Daniel E. Rigobon",
          "authorId": "88964187"
        }
      ],
      "year": 2023,
      "abstract": "There is a lack of consensus within the literature as to how `fairness' of algorithmic systems can be measured, and different metrics can often be at odds. In this paper, we approach this task by drawing on the ethical frameworks of utilitarianism and John Rawls. Informally, these two theories of distributive justice measure the `good' as either a population's sum of utility, or worst-off outcomes, respectively. We present a parameterized class of objective functions that interpolates between these two (possibly) conflicting notions of the `good'. This class is shown to represent a relaxation of the Rawlsian `veil of ignorance', and its sequence of optimal solutions converges to both a utilitarian and Rawlsian optimum. Several other properties of this class are studied, including: 1) a relationship to regularized optimization, 2) feasibility of consistent estimation, and 3) algorithmic cost. In several real-world datasets, we compute optimal solutions and construct the tradeoff between utilitarian and Rawlsian notions of the `good'. Empirically, we demonstrate that increasing model complexity can manifest strict improvements to both measures of the `good'. This work suggests that the proper degree of `fairness' can be informed by a designer's preferences over the space of induced utilitarian and Rawlsian `good'.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2302.03567",
      "arxivId": "2302.03567",
      "url": "https://www.semanticscholar.org/paper/d4ef9e53839c384ab6382a289e185d563f2813d8",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2302.03567"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 50,
  "errors": []
}
