{
  "status": "success",
  "source": "semantic_scholar",
  "query": "intersectional fairness",
  "results": [
    {
      "paperId": "97e3e0e07a9f363051d17df510637eb2f9e31110",
      "title": "Machine Learning with Multitype Protected Attributes: Intersectional Fairness through Regularisation",
      "authors": [
        {
          "name": "Ho Ming Lee",
          "authorId": "2380235331"
        },
        {
          "name": "Katrien Antonio",
          "authorId": "2379935825"
        },
        {
          "name": "Benjamin Avanzi",
          "authorId": "2379935529"
        },
        {
          "name": "Lorenzo Marchi",
          "authorId": "2379935333"
        },
        {
          "name": "Rui Zhou",
          "authorId": "2390446340"
        }
      ],
      "year": 2025,
      "abstract": "Ensuring equitable treatment (fairness) across protected attributes (such as gender or ethnicity) is a critical issue in machine learning. Most existing literature focuses on binary classification, but achieving fairness in regression tasks-such as insurance pricing or hiring score assessments-is equally important. Moreover, anti-discrimination laws also apply to continuous attributes, such as age, for which many existing methods are not applicable. In practice, multiple protected attributes can exist simultaneously; however, methods targeting fairness across several attributes often overlook so-called\"fairness gerrymandering\", thereby ignoring disparities among intersectional subgroups (e.g., African-American women or Hispanic men). In this paper, we propose a distance covariance regularisation framework that mitigates the association between model predictions and protected attributes, in line with the fairness definition of demographic parity, and that captures both linear and nonlinear dependencies. To enhance applicability in the presence of multiple protected attributes, we extend our framework by incorporating two multivariate dependence measures based on distance covariance: the previously proposed joint distance covariance (JdCov) and our novel concatenated distance covariance (CCdCov), which effectively address fairness gerrymandering in both regression and classification tasks involving protected attributes of various types. We discuss and illustrate how to calibrate regularisation strength, including a method based on Jensen-Shannon divergence, which quantifies dissimilarities in prediction distributions across groups. We apply our framework to the COMPAS recidivism dataset and a large motor insurance claims dataset.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2509.08163",
      "arxivId": "2509.08163",
      "url": "https://www.semanticscholar.org/paper/97e3e0e07a9f363051d17df510637eb2f9e31110",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.08163"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8ee46050163873ef1f60efa1d7e3d1facbf4f808",
      "title": "Intersectional Fairness in Reinforcement Learning with Large State and Constraint Spaces",
      "authors": [
        {
          "name": "Eric Eaton",
          "authorId": "2290845692"
        },
        {
          "name": "Marcel Hussing",
          "authorId": "1387979586"
        },
        {
          "name": "Michael Kearns",
          "authorId": "2272749830"
        },
        {
          "name": "Aaron Roth",
          "authorId": "2272478086"
        },
        {
          "name": "S. B. Sengupta",
          "authorId": "2299804130"
        },
        {
          "name": "Jessica Sorrell",
          "authorId": "39715789"
        }
      ],
      "year": 2025,
      "abstract": "In traditional reinforcement learning (RL), the learner aims to solve a single objective optimization problem: find the policy that maximizes expected reward. However, in many real-world settings, it is important to optimize over multiple objectives simultaneously. For example, when we are interested in fairness, states might have feature annotations corresponding to multiple (intersecting) demographic groups to whom reward accrues, and our goal might be to maximize the reward of the group receiving the minimal reward. In this work, we consider a multi-objective optimization problem in which each objective is defined by a state-based reweighting of a single scalar reward function. This generalizes the problem of maximizing the reward of the minimum reward group. We provide oracle-efficient algorithms to solve these multi-objective RL problems even when the number of objectives is exponentially large-for tabular MDPs, as well as for large MDPs when the group functions have additional structure. Finally, we experimentally validate our theoretical results and demonstrate applications on a preferential attachment graph MDP.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2502.11828",
      "arxivId": "2502.11828",
      "url": "https://www.semanticscholar.org/paper/8ee46050163873ef1f60efa1d7e3d1facbf4f808",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.11828"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "35ba67f49e33b5391f5712e9b0457b5ffa6a7f3f",
      "title": "A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges",
      "authors": [
        {
          "name": "Usman Gohar",
          "authorId": "1386345955"
        },
        {
          "name": "Lu Cheng",
          "authorId": "144842921"
        }
      ],
      "year": 2023,
      "abstract": "The widespread adoption of Machine Learning systems, especially in more decision-critical applications such as criminal sentencing and bank loans, has led to increased concerns about fairness implications. Algorithms and metrics have been developed to mitigate and measure these discriminations. More recently, works have identified a more challenging form of bias called intersectional bias, which encompasses multiple sensitive attributes, such as race and gender, together. In this survey, we review the state-of-the-art in intersectional fairness. We present a taxonomy for intersectional notions of fairness and mitigation. Finally, we identify the key challenges and provide researchers with guidelines for future directions.",
      "citationCount": 57,
      "doi": "10.24963/ijcai.2023/742",
      "arxivId": "2305.06969",
      "url": "https://www.semanticscholar.org/paper/35ba67f49e33b5391f5712e9b0457b5ffa6a7f3f",
      "venue": "International Joint Conference on Artificial Intelligence",
      "journal": {
        "pages": "6619-6627"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "d0c6f465b0ce4c69fe4a0e82c8fdf3d33c99e1a3",
      "title": "APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness",
      "authors": [
        {
          "name": "Priyobrata Mondal",
          "authorId": "2306608534"
        },
        {
          "name": "Faizanuddin Ansari",
          "authorId": "2226529323"
        },
        {
          "name": "Swagatam Das",
          "authorId": "2267691301"
        }
      ],
      "year": 2025,
      "abstract": "Ensuring fairness in machine learning models is critical, especially when biases compound across intersecting protected attributes like race, gender, and age. While existing methods address fairness for single attributes, they fail to capture the nuanced, multiplicative biases faced by intersectional subgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first framework to explicitly model intersectional fairness as a joint optimization problem over the Cartesian product of sensitive attributes. APFEx combines three key innovations- (1) an adaptive multi-objective optimizer that dynamically switches between Pareto cone projection, gradient weighting, and exploration strategies to navigate fairness-accuracy trade-offs, (2) differentiable intersectional fairness metrics enabling gradient-based optimization of non-smooth subgroup disparities, and (3) theoretical guarantees of convergence to Pareto-optimal solutions. Experiments on four real-world datasets demonstrate APFEx's superiority, reducing fairness violations while maintaining competitive accuracy. Our work bridges a critical gap in fair ML, providing a scalable, model-agnostic solution for intersectional fairness.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2509.13908",
      "arxivId": "2509.13908",
      "url": "https://www.semanticscholar.org/paper/d0c6f465b0ce4c69fe4a0e82c8fdf3d33c99e1a3",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.13908"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6717a1867928fc891b519eea3a919b9480170b43",
      "title": "Easy ensemble classifier-group and intersectional fairness and threshold (EEC-GIFT): a fairness-aware machine learning framework for lung cancer screening eligibility using real-world data",
      "authors": [
        {
          "name": "Piyawan Conahan",
          "authorId": "2351241929"
        },
        {
          "name": "Lary A. Robinson",
          "authorId": "2238570625"
        },
        {
          "name": "Trung Le",
          "authorId": "2351410533"
        },
        {
          "name": "Gilmer Valdes",
          "authorId": "2351241326"
        },
        {
          "name": "M. Schabath",
          "authorId": "3639944"
        },
        {
          "name": "Margaret M Byrne",
          "authorId": "2321582319"
        },
        {
          "name": "Lee Green",
          "authorId": "2351242457"
        },
        {
          "name": "I. El Naqa",
          "authorId": "2283191838"
        },
        {
          "name": "Yi Luo",
          "authorId": "2351318206"
        }
      ],
      "year": 2025,
      "abstract": "Abstract Background We use real-world data to develop a lung cancer screening (LCS) eligibility mechanism that is both accurate and free from racial bias. Methods Our data came from the Prostate, Lung, Colorectal, and Ovarian (PLCO) cancer screening trial. We built a systematic fairness-aware machine learning framework by integrating a Group and Intersectional Fairness and Threshold (GIFT) strategy with an easy ensemble classifier\u2014(EEC-) or logistic regression\u2014(LR-) based model. The best LCS eligibility mechanism EEC-GIFT* and LR-GIFT* were applied to the testing dataset and their performances were compared to the 2021 US Preventive Services Task Force (USPSTF) criteria and PLCOM2012 model. The equal opportunity difference (EOD) of developing lung cancer between Black and White smokers was used to evaluate mechanism fairness. Results The fairness of LR-GIFT* or EEC-GIFT* during training was notably greater than that of the LR or EEC models without greatly reducing their accuracy. During testing, the EEC-GIFT* (85.16% vs 78.08%, P\u2009<\u2009.001) and LR-GIFT* (85.98% vs 78.08%, P\u2009<\u2009.001) models significantly improved sensitivity without sacrificing specificity compared to the 2021 USPSTF criteria. The EEC-GIFT* (0.785 vs 0.788, P\u2009=\u2009.28) and LR-GIFT* (0.785 vs 0.788, P\u2009=\u2009.30) showed similar area under receiver operating characteristic curve values compared to the PLCOM2012 model. While the average EODs between Blacks and Whites were significant for the 2021 USPSTF criteria (0.0673, P\u2009<\u2009.001), PLCOM2012 (0.0566, P\u2009<\u2009.001), and LR-GIFT* (0.0081, P\u2009<\u2009.001), the EEC-GIFT* model was unbiased (0.0034, P\u2009=\u2009.07). Conclusion Our EEC-GIFT* LCS eligibility mechanism can significantly mitigate racial biases in eligibility determination without compromising its predictive performance.",
      "citationCount": 0,
      "doi": "10.1093/jncics/pkaf030",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6717a1867928fc891b519eea3a919b9480170b43",
      "venue": "JNCI Cancer Spectrum",
      "journal": {
        "name": "JNCI Cancer Spectrum",
        "volume": "9"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e3d57a5e02b6d38a90090ad9db56bdc5a31eee67",
      "title": "Diversity Drives Fairness: Ensemble of Higher Order Mutants for Intersectional Fairness of Machine Learning Software",
      "authors": [
        {
          "name": "Zhenpeng Chen",
          "authorId": "2267391486"
        },
        {
          "name": "Xinyue Li",
          "authorId": "2326804097"
        },
        {
          "name": "Jie M. Zhang",
          "authorId": "2293042568"
        },
        {
          "name": "Federica Sarro",
          "authorId": "2326298201"
        },
        {
          "name": "Yang Liu",
          "authorId": "2329053854"
        }
      ],
      "year": 2024,
      "abstract": "Intersectional fairness is a critical requirement for Machine Learning (ML) software, demanding fairness across subgroups defined by multiple protected attributes. This paper introduces FairHOME, a novel ensemble approach using higher order mutation of inputs to enhance intersectional fairness of ML software during the inference phase. Inspired by social science theories highlighting the benefits of diversity, FairHOME generates mutants representing diverse subgroups for each input instance, thus broadening the array of perspectives to foster a fairer decision-making process. Unlike conventional ensemble methods that combine predictions made by different models, FairHOME combines predictions for the original input and its mutants, all generated by the same ML model, to reach a final decision. Notably, FairHOME is even applicable to deployed ML software as it bypasses the need for training new models. We extensively evaluate FairHOME against seven state-of-the-art fairness improvement methods across 24 decision-making tasks using widely adopted metrics. FairHOME consistently outperforms existing methods across all metrics considered. On average, it enhances intersectional fairness by 47.5 %, surpassing the currently best-performing method by 9.6 percentage points.",
      "citationCount": 5,
      "doi": "10.1109/ICSE55347.2025.00122",
      "arxivId": "2412.08167",
      "url": "https://www.semanticscholar.org/paper/e3d57a5e02b6d38a90090ad9db56bdc5a31eee67",
      "venue": "International Conference on Software Engineering",
      "journal": {
        "name": "2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)",
        "pages": "743-755"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "c547b6ee8a364da28b1e66cd3dbaa768d2b480ae",
      "title": "Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification",
      "authors": [
        {
          "name": "Yupeng Zhang",
          "authorId": "2399038188"
        },
        {
          "name": "Adam G. Dunn",
          "authorId": "2355651815"
        },
        {
          "name": "Usman Naseem",
          "authorId": "1394609613"
        },
        {
          "name": "Jinman Kim",
          "authorId": "2354056788"
        }
      ],
      "year": 2025,
      "abstract": "Medical artificial intelligence (AI) systems, particularly multimodal vision-language models (VLM), often exhibit intersectional biases where models are systematically less confident in diagnosing marginalised patient subgroups. Such bias can lead to higher rates of inaccurate and missed diagnoses due to demographically skewed data and divergent distributions of diagnostic certainty. Current fairness interventions frequently fail to address these gaps or compromise overall diagnostic performance to achieve statistical parity among the subgroups. In this study, we developed Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardises diagnostic certainty across intersectional patient subgroups. Unlike traditional debiasing methods, this approach equalises the model's decision confidence without requiring sensitive demographic data during clinical inference. We evaluated this approach using 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), and 10,000 fundus images for glaucoma detection (Harvard-FairVLMed), stratifying performance by intersectional age, gender, and race attributes. In the dermatology cohort, the proposed method reduced the overall intersectional missed diagnosis gap (difference in True Positive Rate, $\\Delta$TPR) from 0.50 to 0.26 while improving the overall Area Under the Curve (AUC) from 0.94 to 0.97 compared to standard training. Similarly, for glaucoma screening, the method reduced $\\Delta$TPR from 0.41 to 0.31, achieving a better AUC of 0.72 (vs. 0.71 baseline). This establishes a scalable framework for developing high-stakes clinical decision support systems that are both accurate and can perform equitably across diverse patient subgroups, ensuring reliable performance without increasing privacy risks.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2512.15249",
      "url": "https://www.semanticscholar.org/paper/c547b6ee8a364da28b1e66cd3dbaa768d2b480ae",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "aa6f1eed851fc4bb6d87606cac5fd0ef2334aa99",
      "title": "Synthetic Data Generation for Intersectional Fairness by Leveraging Hierarchical Group Structure",
      "authors": [
        {
          "name": "Gaurav Maheshwari",
          "authorId": "2267392480"
        },
        {
          "name": "A. Bellet",
          "authorId": "1702915"
        },
        {
          "name": "Pascal Denis",
          "authorId": "153833319"
        },
        {
          "name": "Mikaela Keller",
          "authorId": "2218259019"
        }
      ],
      "year": 2024,
      "abstract": "In this paper, we introduce a data augmentation approach specifically tailored to enhance intersectional fairness in classification tasks. Our method capitalizes on the hierarchical structure inherent to intersectionality, by viewing groups as intersections of their parent categories. This perspective allows us to augment data for smaller groups by learning a transformation function that combines data from these parent groups. Our empirical analysis, conducted on four diverse datasets including both text and images, reveals that classifiers trained with this data augmentation approach achieve superior intersectional fairness and are more robust to ``leveling down'' when compared to methods optimizing traditional group fairness metrics.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2405.14521",
      "arxivId": "2405.14521",
      "url": "https://www.semanticscholar.org/paper/aa6f1eed851fc4bb6d87606cac5fd0ef2334aa99",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2405.14521"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a4070607bd1800fb6ab116717b5178acc3d7ab52",
      "title": "Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities",
      "authors": [
        {
          "name": "D. Moukheiber",
          "authorId": "2114905585"
        },
        {
          "name": "S. Mahindre",
          "authorId": "2048030245"
        },
        {
          "name": "Lama Moukheiber",
          "authorId": "2145841875"
        },
        {
          "name": "M. Moukheiber",
          "authorId": "2139921488"
        },
        {
          "name": "Mingchen Gao",
          "authorId": "2293714675"
        }
      ],
      "year": 2024,
      "abstract": "There has been significant progress in implementing deep learning models in disease diagnosis using chest X- rays. Despite these advancements, inherent biases in these models can lead to disparities in prediction accuracy across protected groups. In this study, we propose a framework to achieve accurate diagnostic outcomes and ensure fairness across intersectional groups in high-dimensional chest X- ray multi-label classification. Transcending traditional protected attributes, we consider complex interactions within social determinants, enabling a more granular benchmark and evaluation of fairness. We present a simple and robust method that involves retraining the last classification layer of pre-trained models using a balanced dataset across groups. Additionally, we account for fairness constraints and integrate class-balanced fine-tuning for multi-label settings. The evaluation of our method on the MIMIC-CXR dataset demonstrates that our framework achieves an optimal tradeoff between accuracy and fairness compared to baseline methods.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2403.18196",
      "arxivId": "2403.18196",
      "url": "https://www.semanticscholar.org/paper/a4070607bd1800fb6ab116717b5178acc3d7ab52",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2403.18196"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e5edccd7ffc42cccf67a75dd34d6074ee1da0b4c",
      "title": "Evaluating Intersectional Fairness in Algorithmic Decision Making Using Intersectional Differential Algorithmic Functioning",
      "authors": [
        {
          "name": "Youmi Suk",
          "authorId": "1914207216"
        },
        {
          "name": "Kyung (Chris) T. Han",
          "authorId": "2321665628"
        }
      ],
      "year": 2024,
      "abstract": "Ensuring fairness is crucial in developing modern algorithms and tests. To address potential biases and discrimination in algorithmic decision making, researchers have drawn insights from the test fairness literature, notably the work on differential algorithmic functioning (DAF) by Suk and Han. Nevertheless, the exploration of intersectionality in fairness investigations, within both test fairness and algorithmic fairness fields, is still relatively new. In this paper, we propose an extension of the DAF framework to include the concept of intersectionality. Similar to DAF, the proposed notion for intersectionality, which we term \u201cinteractive DAF,\u201d leverages ideas from test fairness and algorithmic fairness. We also provide methods based on the generalized Mantel\u2013Haenszel test, generalized logistic regression, and regularized group regression to detect DAF, interactive DAF, or other subtypes of DAF. Specifically, we employ regularized group regression with three different penalties and examine their performance via a simulation study. Finally, we demonstrate our intersectional DAF framework in real-world applications on grade retention and conditional cash transfer programs in education.",
      "citationCount": 2,
      "doi": "10.3102/10769986241269820",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e5edccd7ffc42cccf67a75dd34d6074ee1da0b4c",
      "venue": "Journal of educational and behavioral statistics",
      "journal": {
        "name": "Journal of Educational and Behavioral Statistics",
        "pages": "833 - 862",
        "volume": "50"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "fc493a47effb71650a4d4c402fdf2f909f6c0ff2",
      "title": "Invisible Inequalities - Intersectional Fairness in Educational AI",
      "authors": [
        {
          "name": "Marie Mirsch",
          "authorId": "2377073672"
        },
        {
          "name": "Jonas Strube",
          "authorId": "2376541652"
        },
        {
          "name": "C. Leicht-Scholten",
          "authorId": "2347733474"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/fc493a47effb71650a4d4c402fdf2f909f6c0ff2",
      "venue": "EWAF",
      "journal": {
        "pages": "403-409"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ad81697cd665a0f3f56e33a953943d948f054a1d",
      "title": "Fair Without Leveling Down: A New Intersectional Fairness Definition",
      "authors": [
        {
          "name": "Gaurav Maheshwari",
          "authorId": "2267392480"
        },
        {
          "name": "A. Bellet",
          "authorId": "1702915"
        },
        {
          "name": "Pascal Denis",
          "authorId": "153833319"
        },
        {
          "name": "Mikaela Keller",
          "authorId": "2218259019"
        }
      ],
      "year": 2023,
      "abstract": "In this work, we consider the problem of intersectional group fairness in the classification setting, where the objective is to learn discrimination-free models in the presence of several intersecting sensitive groups. First, we illustrate various shortcomings of existing fairness measures commonly used to capture intersectional fairness. Then, we propose a new definition called the $\\alpha$-Intersectional Fairness, which combines the absolute and the relative performance across sensitive groups and can be seen as a generalization of the notion of differential fairness. We highlight several desirable properties of the proposed definition and analyze its relation to other fairness measures. Finally, we benchmark multiple popular in-processing fair machine learning approaches using our new fairness definition and show that they do not achieve any improvement over a simple baseline. Our results reveal that the increase in fairness measured by previous definitions hides a\"leveling down\"effect, i.e., degrading the best performance over groups rather than improving the worst one.",
      "citationCount": 2,
      "doi": "10.18653/v1/2023.emnlp-main.558",
      "arxivId": "2305.12495",
      "url": "https://www.semanticscholar.org/paper/ad81697cd665a0f3f56e33a953943d948f054a1d",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "pages": "9018-9032"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "0f38838ece2fb3ff9218f2fd942703fe13f336fa",
      "title": "Bounding and Approximating Intersectional Fairness through Marginal Fairness",
      "authors": [
        {
          "name": "Mathieu Molina",
          "authorId": "2167153588"
        },
        {
          "name": "P. Loiseau",
          "authorId": "143710789"
        }
      ],
      "year": 2022,
      "abstract": "Discrimination in machine learning often arises along multiple dimensions (a.k.a. protected attributes); it is then desirable to ensure \\emph{intersectional fairness} -- i.e., that no subgroup is discriminated against. It is known that ensuring \\emph{marginal fairness} for every dimension independently is not sufficient in general. Due to the exponential number of subgroups, however, directly measuring intersectional fairness from data is impossible. In this paper, our primary goal is to understand in detail the relationship between marginal and intersectional fairness through statistical analysis. We first identify a set of sufficient conditions under which an exact relationship can be obtained. Then, we prove bounds (easily computable through marginal fairness and other meaningful statistical quantities) in high-probability on intersectional fairness in the general case. Beyond their descriptive value, we show that these theoretical bounds can be leveraged to derive a heuristic improving the approximation and bounds of intersectional fairness by choosing, in a relevant manner, protected attributes for which we describe intersectional subgroups. Finally, we test the performance of our approximations and bounds on real and synthetic data-sets.",
      "citationCount": 12,
      "doi": "10.48550/arXiv.2206.05828",
      "arxivId": "2206.05828",
      "url": "https://www.semanticscholar.org/paper/0f38838ece2fb3ff9218f2fd942703fe13f336fa",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2206.05828"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0542b13e9877b3dd3c34d4700f21d67433fab3fc",
      "title": "InfoFair: Information-Theoretic Intersectional Fairness",
      "authors": [
        {
          "name": "Jian Kang",
          "authorId": "2111625448"
        },
        {
          "name": "Tiankai Xie",
          "authorId": "151239711"
        },
        {
          "name": "Xintao Wu",
          "authorId": "7916525"
        },
        {
          "name": "Ross Maciejewski",
          "authorId": "1786514"
        },
        {
          "name": "Hanghang Tong",
          "authorId": "8163721"
        }
      ],
      "year": 2021,
      "abstract": "Algorithmic fairness is becoming increasingly important in data mining and machine learning. Among others, a foundational notation is group fairness. The vast majority of the existing works on group fairness, with a few exceptions, primarily focus on debiasing with respect to a single sensitive attribute, despite the fact that the co-existence of multiple sensitive attributes (e.g., gender, race, marital status, etc.) in the real-world is commonplace. As such, methods that can ensure a fair learning outcome with respect to all sensitive attributes of concern simultaneously need to be developed. In this paper, we study the problem of information-theoretic intersectional fairness (InfoFair), where statistical parity, a representative group fairness measure, is guaranteed among demographic groups formed by multiple sensitive attributes of interest. We formulate it as a mutual information minimization problem and propose a generic end-to-end algorithmic framework to solve it. The key idea is to leverage a variational representation of mutual information, which considers the variational distribution between learning outcomes and sensitive attributes, as well as the density ratio between the variational and the original distributions. Our proposed framework is generalizable to many different settings, including other statistical notions of fairness, and could handle any type of learning task equipped with a gradientbased optimizer. Empirical evaluations in the fair classification task on three real-world datasets demonstrate that our proposed framework can effectively debias the classification results with minimal impact to the classification accuracy.",
      "citationCount": 22,
      "doi": "10.1109/BigData55660.2022.10020588",
      "arxivId": "2105.11069",
      "url": "https://www.semanticscholar.org/paper/0542b13e9877b3dd3c34d4700f21d67433fab3fc",
      "venue": "2022 IEEE International Conference on Big Data (Big Data)",
      "journal": {
        "name": "2022 IEEE International Conference on Big Data (Big Data)",
        "pages": "1455-1464"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "ff5defa1fc34ea4f2ce1843a070c6a662c1dd023",
      "title": "Intersectional Fairness: A Fractal Approach",
      "authors": [
        {
          "name": "Giulio Filippi",
          "authorId": "2204771370"
        },
        {
          "name": "Sara Zannone",
          "authorId": "19214542"
        },
        {
          "name": "A. Koshiyama",
          "authorId": "2127728158"
        }
      ],
      "year": 2023,
      "abstract": "The issue of fairness in AI has received an increasing amount of attention in recent years. The problem can be approached by looking at different protected attributes (e.g., ethnicity, gender, etc) independently, but fairness for individual protected attributes does not imply intersectional fairness. In this work, we frame the problem of intersectional fairness within a geometrical setting. We project our data onto a hypercube, and split the analysis of fairness by levels, where each level encodes the number of protected attributes we are intersecting over. We prove mathematically that, while fairness does not propagate\"down\"the levels, it does propagate\"up\"the levels. This means that ensuring fairness for all subgroups at the lowest intersectional level (e.g., black women, white women, black men and white men), will necessarily result in fairness for all the above levels, including each of the protected attributes (e.g., ethnicity and gender) taken independently. We also derive a formula describing the variance of the set of estimated success rates on each level, under the assumption of perfect fairness. Using this theoretical finding as a benchmark, we define a family of metrics which capture overall intersectional bias. Finally, we propose that fairness can be metaphorically thought of as a\"fractal\"problem. In fractals, patterns at the smallest scale repeat at a larger scale. We see from this example that tackling the problem at the lowest possible level, in a bottom-up manner, leads to the natural emergence of fair AI. We suggest that trustworthiness is necessarily an emergent, fractal and relational property of the AI system.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2302.12683",
      "arxivId": "2302.12683",
      "url": "https://www.semanticscholar.org/paper/ff5defa1fc34ea4f2ce1843a070c6a662c1dd023",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2302.12683"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d47a311297ff0ebf44d8206a7cfc9b482cdeac97",
      "title": "Auditing and Achieving Intersectional Fairness in Classification Problems",
      "authors": [
        {
          "name": "Giulio Morina",
          "authorId": "1397045220"
        },
        {
          "name": "V. Oliinyk",
          "authorId": "2143777620"
        },
        {
          "name": "J. Waton",
          "authorId": "82890266"
        },
        {
          "name": "Ines Marusic",
          "authorId": "47316675"
        },
        {
          "name": "K. Georgatzis",
          "authorId": "2318881"
        }
      ],
      "year": 2019,
      "abstract": "Machine learning algorithms are extensively used to make increasingly more consequential decisions, so that achieving optimal predictive performance can no longer be the only focus. This paper explores intersectional fairness, that is fairness when intersections of multiple sensitive attributes -- such as race, age, nationality, etc. -- are considered. Previous research has mainly been focusing on fairness with respect to a single sensitive attribute, with intersectional fairness being comparatively less studied despite its critical importance for modern machine learning applications. We introduce intersectional fairness metrics by extending prior work, and provide different methodologies to audit discrimination in a given dataset or model outputs. Secondly, we develop novel post-processing techniques to mitigate any detected bias in a classification model. Our proposed methodology does not rely on any assumptions regarding the underlying model and aims at guaranteeing fairness while preserving good predictive performance. Finally, we give guidance on a practical implementation, showing how the proposed methods perform on a real-world dataset.",
      "citationCount": 44,
      "doi": null,
      "arxivId": "1911.01468",
      "url": "https://www.semanticscholar.org/paper/d47a311297ff0ebf44d8206a7cfc9b482cdeac97",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/1911.01468"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "02e3fb747056e69ff454a47e2d6a53396e73b101",
      "title": "How to Capture Intersectional Fairness",
      "authors": [
        {
          "name": "Gaurav Maheshwari",
          "authorId": "143738727"
        },
        {
          "name": "A. Bellet",
          "authorId": "1702915"
        },
        {
          "name": "Pascal Denis",
          "authorId": "153833319"
        },
        {
          "name": "Mikaela Keller",
          "authorId": "2218259019"
        }
      ],
      "year": 2023,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.48550/arXiv.2305.12495",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/02e3fb747056e69ff454a47e2d6a53396e73b101",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2305.12495"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e697c495d35513c864e0123d5177210d56bf21a5",
      "title": "MithraCoverage: A System for Investigating Population Bias for Intersectional Fairness",
      "authors": [
        {
          "name": "Zhongjun (Mark) Jin",
          "authorId": "2848828"
        },
        {
          "name": "Mengjing Xu",
          "authorId": "2110683475"
        },
        {
          "name": "Chenkai Sun",
          "authorId": "1726046634"
        },
        {
          "name": "Abolfazl Asudeh",
          "authorId": "1717283"
        },
        {
          "name": "H. V. Jagadish",
          "authorId": "145531067"
        }
      ],
      "year": 2020,
      "abstract": "Data-driven technologies are only as good as the data they work with. On the other hand, data scientists have often limited control on how the data is collected. Failing to contain adequate number of instances from minority (sub)groups, known as population bias, is a major reason for model unfairness and disparate performance across different groups. We demonstrate MithraCoverage, a system for investigating population bias over the intersection of multiple attributes. We use the concept of coverage for identifying intersectional subgroups with inadequate representation in the dataset. MithraCoverage is a web application with an interactive visual interface that allows data scientists to explore the dataset and identify subgroups with poor coverage.",
      "citationCount": 45,
      "doi": "10.1145/3318464.3384689",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e697c495d35513c864e0123d5177210d56bf21a5",
      "venue": "SIGMOD Conference",
      "journal": {
        "name": "Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
      ]
    },
    {
      "paperId": "572061394835ab299380c067e19a0fb3711cf29d",
      "title": "Leveling up in Intersectional Fairness",
      "authors": [],
      "year": 2023,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/572061394835ab299380c067e19a0fb3711cf29d",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "09d68d1a24cce6887b140229f1c42ee2eeeecd24",
      "title": "Bayesian Modeling of Intersectional Fairness: The Variance of Bias",
      "authors": [
        {
          "name": "James R. Foulds",
          "authorId": "40289577"
        },
        {
          "name": "Rashidul Islam",
          "authorId": "47755547"
        },
        {
          "name": "Kamrun Keya",
          "authorId": "52028519"
        },
        {
          "name": "Shimei Pan",
          "authorId": "2239443126"
        }
      ],
      "year": 2018,
      "abstract": "Intersectionality is a framework that analyzes how interlocking systems of power and oppression affect individuals along overlapping dimensions including race, gender, sexual orientation, class, and disability. Intersectionality theory therefore implies it is important that fairness in artificial intelligence systems be protected with regard to multi-dimensional protected attributes. However, the measurement of fairness becomes statistically challenging in the multi-dimensional setting due to data sparsity, which increases rapidly in the number of dimensions, and in the values per dimension. We present a Bayesian probabilistic modeling approach for the reliable, data-efficient estimation of fairness with multi-dimensional protected attributes, which we apply to two existing intersectional fairness metrics. Experimental results on census data and the COMPAS criminal justice recidivism dataset demonstrate the utility of our methodology, and show that Bayesian methods are valuable for the modeling and measurement of fairness in an intersectional context.",
      "citationCount": 43,
      "doi": "10.1137/1.9781611976236.48",
      "arxivId": "1811.07255",
      "url": "https://www.semanticscholar.org/paper/09d68d1a24cce6887b140229f1c42ee2eeeecd24",
      "venue": "SDM",
      "journal": {
        "pages": "424-432"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "97822b6664c62ddf297f47e341830e74e474d291",
      "title": "Bayesian Modeling of Intersectional Fairness: The Variance of Bias (Supplementary Material)\u2217",
      "authors": [
        {
          "name": "James R. Foulds",
          "authorId": "40289577"
        },
        {
          "name": "Rashidul Islam",
          "authorId": "47755547"
        },
        {
          "name": "Kamrun Keya",
          "authorId": "52028519"
        },
        {
          "name": "Shimei Pan",
          "authorId": "2239443126"
        }
      ],
      "year": 2020,
      "abstract": null,
      "citationCount": 1,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/97822b6664c62ddf297f47e341830e74e474d291",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "4ce2d061cac3cc627bf4e248facf716974d0678b",
      "title": "Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach",
      "authors": [
        {
          "name": "S. Vethman",
          "authorId": "148158658"
        },
        {
          "name": "Quirine T. S. Smit",
          "authorId": "2367794645"
        },
        {
          "name": "Nina M. van Liebergen",
          "authorId": "2367795987"
        },
        {
          "name": "Cor J. Veenman",
          "authorId": "2320615279"
        }
      ],
      "year": 2025,
      "abstract": "Achieving fair use of AI systems is a multi-faceted challenge. Intersectionality, rooted in Black Feminist movements, is increasingly used to address the interconnected nature of discrimination such as racism, ableism, and sexism. Yet in AI research, intersectionality is often reduced to a narrow technical lens, focused on algorithmic bias between subgroups defined by protected attributes and addressed through fairness metrics. This algorithmic frame sidelines key aspects of intersectionality, such as power relations, social justice, and structural inequality. Still, AI experts play a central role in development and deployment, and therefore should act to limit unjust outcomes. This study offers actionable guidance for AI experts, grounded in a broader intersectional perspective. Through a thematic analysis of AI fairness papers on key aspects of intersectionality, evaluated through community engagement, we identify five themes with concrete recommendations: 1) insisting on collaboration in interdisciplinary teams, 2) embedding reflection and recognizing positionality, 3) approaching communities and facilitating co-ownership, 4) engaging with power dynamics and social context, and 5) assessing the framing and nuance of data and metrics. Participating experts noted barriers such as tech-optimism and fear of insufficient knowledge. Still, they valued the recommendations for communicating the importance of intersectionality and initiating more just AI practices. We call on AI experts to meet this challenge through interdisciplinary collaboration with diverse communities.",
      "citationCount": 4,
      "doi": "10.1145/3715275.3732210",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/4ce2d061cac3cc627bf4e248facf716974d0678b",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "71059e2c3aef42aa05e4c96e8bc1a4f9700607ed",
      "title": "Intersectional Divergence: Measuring Fairness in Regression",
      "authors": [
        {
          "name": "Joe Germino",
          "authorId": "2164042422"
        },
        {
          "name": "Nuno Moniz",
          "authorId": "2241357425"
        },
        {
          "name": "Nitesh V. Chawla",
          "authorId": "2286872295"
        }
      ],
      "year": 2025,
      "abstract": "Fairness in machine learning research is commonly framed in the context of classification tasks, leaving critical gaps in regression. In this paper, we propose a novel approach to measure intersectional fairness in regression tasks, going beyond the focus on single protected attributes from existing work to consider combinations of all protected attributes. Furthermore, we contend that it is insufficient to measure the average error of groups without regard for imbalanced domain preferences. Accordingly, we propose Intersectional Divergence (ID) as the first fairness measure for regression tasks that 1) describes fair model behavior across multiple protected attributes and 2) differentiates the impact of predictions in target ranges most relevant to users. We extend our proposal demonstrating how ID can be adapted into a loss function, IDLoss, that satisfies convergence guarantees and has piecewise smooth properties that enable practical optimization. Through an extensive experimental evaluation, we demonstrate how ID allows unique insights into model behavior and fairness, and how incorporating IDLoss into optimization can considerably improve single-attribute and intersectional model fairness while maintaining a competitive balance in predictive performance.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2505.00830",
      "arxivId": "2505.00830",
      "url": "https://www.semanticscholar.org/paper/71059e2c3aef42aa05e4c96e8bc1a4f9700607ed",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.00830"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0a49c157f2565a95187108b07ad2d8231bd77e86",
      "title": "A Feminist Account of Intersectional Algorithmic Fairness",
      "authors": [
        {
          "name": "Marie Mirsch",
          "authorId": "2377073672"
        },
        {
          "name": "Laila Wegner",
          "authorId": "2331068415"
        },
        {
          "name": "Jonas Strube",
          "authorId": "2376541652"
        },
        {
          "name": "C. Leicht-Scholten",
          "authorId": "2347733474"
        }
      ],
      "year": 2025,
      "abstract": "Intersectionality has profoundly influenced research and political action by revealing how interconnected systems of privilege and oppression influence lived experiences, yet its integration into algorithmic fairness research remains limited. Existing approaches often rely on single-axis or formal subgroup frameworks that risk oversimplifying social realities and neglecting structural inequalities. We propose Substantive Intersectional Algorithmic Fairness, extending Green's (2022) notion of substantive algorithmic fairness with insights from intersectional feminist theory. Building on this foundation, we introduce ten desiderata within the ROOF methodology to guide the design, assessment, and deployment of algorithmic systems in ways that address systemic inequities while mitigating harms to intersectionally marginalized communities. Rather than prescribing fixed operationalizations, these desiderata encourage reflection on assumptions of neutrality, the use of protected attributes, the inclusion of multiply marginalized groups, and enhancing algorithmic systems'potential. Our approach emphasizes that fairness cannot be separated from social context, and that in some cases, principled non-deployment may be necessary. By bridging computational and social science perspectives, we provide actionable guidance for more equitable, inclusive, and context-sensitive intersectional algorithmic practices.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.17944",
      "arxivId": "2508.17944",
      "url": "https://www.semanticscholar.org/paper/0a49c157f2565a95187108b07ad2d8231bd77e86",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.17944"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "08f642ea816aa471fc6de7808a689406b65e0f9c",
      "title": "Metamorphic Testing for Fairness Evaluation in Large Language Models: Identifying Intersectional Bias in LLaMA and GPT",
      "authors": [
        {
          "name": "Harishwar Reddy",
          "authorId": "2355079340"
        },
        {
          "name": "Madhusudan Srinivasan",
          "authorId": "47500437"
        },
        {
          "name": "Upulee Kanewala",
          "authorId": "3039897"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) have made significant strides in Natural Language Processing but remain vulnerable to fairness-related issues, often reflecting biases inherent in their training data. These biases pose risks, particularly when LLMs are deployed in sensitive areas such as healthcare, finance, and law. This paper introduces a metamorphic testing approach to systematically identify fairness bugs in LLMs. We define and apply a set of fairness-oriented metamorphic relations (MRs) to assess the LLaMA and GPT model, a state-of-the-art LLM, across diverse demographic inputs. Our methodology includes generating source and follow-up test cases for each MR and analyzing model responses for fairness violations. The results demonstrate the effectiveness of MT in exposing bias patterns, especially in relation to tone and sentiment, and highlight specific intersections of sensitive attributes that frequently reveal fairness faults. This research improves fairness testing in LLMs, providing a structured approach to detect and mitigate biases and improve model robustness in fairness-sensitive applications.",
      "citationCount": 2,
      "doi": "10.1109/SERA65747.2025.11154488",
      "arxivId": "2504.07982",
      "url": "https://www.semanticscholar.org/paper/08f642ea816aa471fc6de7808a689406b65e0f9c",
      "venue": "International Conference on Software Engineering Research and Applications",
      "journal": {
        "name": "2025 IEEE/ACIS 23rd International Conference on Software Engineering Research, Management and Applications (SERA)",
        "pages": "239-246"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "2b71df864747ff814595c4f156fc91991be08bc7",
      "title": "Intersectional Two-sided Fairness in Recommendation",
      "authors": [
        {
          "name": "Yifan Wang",
          "authorId": "2282675108"
        },
        {
          "name": "Peijie Sun",
          "authorId": "3685571"
        },
        {
          "name": "Weizhi Ma",
          "authorId": "2903964"
        },
        {
          "name": "Min Zhang",
          "authorId": "2279744113"
        },
        {
          "name": "Yuan Zhang",
          "authorId": "2282533511"
        },
        {
          "name": "Peng Jiang",
          "authorId": "2282536044"
        },
        {
          "name": "Shaoping Ma",
          "authorId": "8093158"
        }
      ],
      "year": 2024,
      "abstract": "Fairness of recommender systems (RS) has attracted increasing attention recently. Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously. However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously. To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups. Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups. Extensive experiments and analyses on three public datasets show that our proposed approach effectively alleviates the intersectional two-sided unfairness and consistently outperforms previous state-of-the-art methods.",
      "citationCount": 16,
      "doi": "10.1145/3589334.3645518",
      "arxivId": "2402.02816",
      "url": "https://www.semanticscholar.org/paper/2b71df864747ff814595c4f156fc91991be08bc7",
      "venue": "The Web Conference",
      "journal": {
        "name": "Proceedings of the ACM Web Conference 2024"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
      ]
    },
    {
      "paperId": "6a6fad8b363f6376101739175763acb8229e99a3",
      "title": "Enhancing Tabular GAN Fairness: The Impact of Intersectional Feature Selection",
      "authors": [
        {
          "name": "T. Dehdarirad",
          "authorId": "2522756"
        },
        {
          "name": "Ericka Johnson",
          "authorId": "2265084850"
        },
        {
          "name": "Gabriel Eilertsen",
          "authorId": "2300473139"
        },
        {
          "name": "Saghi Hajisharif",
          "authorId": "2743020"
        }
      ],
      "year": 2024,
      "abstract": "Traditional GAN (Generative Adversarial Network) architectures often reproduce biases present in their training data, leading to synthetic data that may unfairly impact certain subgroups. Past efforts to improve fairness in GANs usually target single demographic categories, like sex or race, but overlook intersectionality. Our approach addresses this gap by integrating an intersectionality framework with explainability techniques to identify and select problematic sensitive features. These insights are then used to develop intersectional fairness constraints integrated into the GAN training process. We aim to enhance fairness and maintain diverse subgroup representation by addressing intersections of multiple demographic attributes. Specifically, we adjusted the loss functions of two state-of-the-art GAN models for tabular data, including an intersectional demographic parity constraint. Our evaluations indicate that this approach significantly improves fairness in synthetically generated datasets. We compared the outcomes using Adult, and Diabetes datasets when considering the intersection of two sensitive features versus focusing on a single sensitive attribute, demonstrating the effectiveness of our method in capturing more complex biases.",
      "citationCount": 2,
      "doi": "10.1109/ICMLA61862.2024.00176",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6a6fad8b363f6376101739175763acb8229e99a3",
      "venue": "International Conference on Machine Learning and Applications",
      "journal": {
        "name": "2024 International Conference on Machine Learning and Applications (ICMLA)",
        "pages": "1146-1151"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "fa1ece069a14b88be133136294d0b98a4d6364bb",
      "title": "Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening",
      "authors": [
        {
          "name": "Kevin T Webster",
          "authorId": "2372772420"
        }
      ],
      "year": 2025,
      "abstract": "The increasing use of generative AI for resume screening is predicated on the assumption that it offers an unbiased alternative to biased human decision-making. However, this belief fails to address a critical question: are these AI systems fundamentally competent at the evaluative tasks they are meant to perform? This study investigates the question of competence through a two-part audit of eight major AI platforms. Experiment 1 confirmed complex, contextual racial and gender biases, with some models penalizing candidates merely for the presence of demographic signals. Experiment 2, which evaluated core competence, provided a critical insight: some models that appeared unbiased were, in fact, incapable of performing a substantive evaluation, relying instead on superficial keyword matching. This paper introduces the\"Illusion of Neutrality\"to describe this phenomenon, where an apparent lack of bias is merely a symptom of a model's inability to make meaningful judgments. This study recommends that organizations and regulators adopt a dual-validation framework, auditing AI hiring tools for both demographic bias and demonstrable competence to ensure they are both equitable and effective.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2507.11548",
      "arxivId": "2507.11548",
      "url": "https://www.semanticscholar.org/paper/fa1ece069a14b88be133136294d0b98a4d6364bb",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.11548"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "85152ab48a237708c63a385645861b095d6420a9",
      "title": "Moving from Fairness to Justice: Intentional Algorithmic Solutions Through an Intersectional Lens",
      "authors": [
        {
          "name": "Kenya S. Andrews",
          "authorId": "2377852790"
        }
      ],
      "year": 2025,
      "abstract": "In this forum we explore different perspectives for how to apply intersectionality as a critical framework for design across multiple contexts. --- Yolanda A. Rankin and Jakita O. Thomas, Editors",
      "citationCount": 0,
      "doi": "10.1145/3760549",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/85152ab48a237708c63a385645861b095d6420a9",
      "venue": "Interactions",
      "journal": {
        "name": "Interactions",
        "pages": "58 - 60",
        "volume": "32"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0416c3db92ac9a4e9f05db71570d8223efd33f55",
      "title": "Challenging \"subgroup Fairness\": Towards Intersectional Algorithmic Fairness Based on Personas",
      "authors": [
        {
          "name": "M. Decker",
          "authorId": "2331070137"
        },
        {
          "name": "Laila Wegner",
          "authorId": "2331068415"
        },
        {
          "name": "C. Leicht-Scholten",
          "authorId": "2347733474"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 2,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/0416c3db92ac9a4e9f05db71570d8223efd33f55",
      "venue": "EWAF",
      "journal": null,
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "615bf4da8b20989fa9555d4193bc79bf063e2328",
      "title": "Integrating Fair Representation Learning with Fairness Regularization for Intersectional Group Fairness",
      "authors": [
        {
          "name": "David Quashigah Dzakpasu",
          "authorId": "2326979224"
        },
        {
          "name": "Jixue Liu",
          "authorId": "2108374005"
        },
        {
          "name": "Jiuyong Li",
          "authorId": "2257372048"
        },
        {
          "name": "Lin Liu",
          "authorId": "2146017365"
        }
      ],
      "year": 2024,
      "abstract": "In the pursuit of intersectional group fairness in machine learning models, significant attention has been directed towards fair representation learning methods. These methods aim to mitigate bias in training data by encoding data effectively while removing sensitive attribute information. However, existing fair representation learning methods often assume that decoupling sensitive attribute information from the latent representation will automatically lead to fairness on any downstream tasks learnt on the non-sensitive subspace of the latent representation. Nonetheless, biases can persist even when using representations devoid of sensitive attribute information. This is due to the learning algorithm's influence during downstream task training. In this paper, we propose a method dubbed FairReg which integrates fairness regularization with fair representation learning. This unified approach creates a more comprehensive and robust framework for ensuring intersectional group fairness in machine learning models. Empirical evaluations conducted on two real-world depression prediction datasets demonstrate the effectiveness of our method in improving intersectional group fairness compared to existing approaches.",
      "citationCount": 1,
      "doi": "10.1145/3627673.3679802",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/615bf4da8b20989fa9555d4193bc79bf063e2328",
      "venue": "International Conference on Information and Knowledge Management",
      "journal": {
        "name": "Proceedings of the 33rd ACM International Conference on Information and Knowledge Management"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "16ce2af6de69db770b1d91cb573eb1ff57193c00",
      "title": "Fairness at Every Intersection: Uncovering and Mitigating Intersectional Biases in Multimodal Clinical Predictions",
      "authors": [
        {
          "name": "R. Ramachandranpillai",
          "authorId": "1392792012"
        },
        {
          "name": "Kishore Sampath",
          "authorId": "2333355655"
        },
        {
          "name": "Ayaazuddin Mohammad",
          "authorId": "2333357499"
        },
        {
          "name": "Malihe Alikhani",
          "authorId": "2715920"
        }
      ],
      "year": 2024,
      "abstract": "Biases in automated clinical decision-making using Electronic Healthcare Records (EHR) impose significant disparities in patient care and treatment outcomes. Conventional approaches have primarily focused on bias mitigation strategies stemming from single attributes, overlooking intersectional subgroups -- groups formed across various demographic intersections (such as race, gender, ethnicity, etc.). Rendering single-attribute mitigation strategies to intersectional subgroups becomes statistically irrelevant due to the varying distribution and bias patterns across these subgroups. The multimodal nature of EHR -- data from various sources such as combinations of text, time series, tabular, events, and images -- adds another layer of complexity as the influence on minority groups may fluctuate across modalities. In this paper, we take the initial steps to uncover potential intersectional biases in predictions by sourcing extensive multimodal datasets, MIMIC-Eye1 and MIMIC-IV ED, and propose mitigation at the intersectional subgroup level. We perform and benchmark downstream tasks and bias evaluation on the datasets by learning a unified text representation from multimodal sources, harnessing the enormous capabilities of the pre-trained clinical Language Models (LM), MedBERT, Clinical BERT, and Clinical BioBERT. Our findings indicate that the proposed sub-group-specific bias mitigation is robust across different datasets, subgroups, and embeddings, demonstrating effectiveness in addressing intersectional biases in multimodal settings.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2412.00606",
      "arxivId": "2412.00606",
      "url": "https://www.semanticscholar.org/paper/16ce2af6de69db770b1d91cb573eb1ff57193c00",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2412.00606"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "fc350cf5d034e9df819cbbb92b58b221dae0b070",
      "title": "Intersectional consequences for marginal fairness in prediction models of emergency admissions",
      "authors": [
        {
          "name": "Elle Lett",
          "authorId": "1879204208"
        },
        {
          "name": "Shakiba Shahbandegan",
          "authorId": "2329140417"
        },
        {
          "name": "Yuval Barak-Corren",
          "authorId": "1401454702"
        },
        {
          "name": "Andy Fine",
          "authorId": "2329140729"
        },
        {
          "name": "William G. La Cava",
          "authorId": "2329140896"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.1101/2024.11.05.24316769",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/fc350cf5d034e9df819cbbb92b58b221dae0b070",
      "venue": "medRxiv",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "21a39ee100b048bfa72164801af472d787aa8739",
      "title": "Differential Fairness: An Intersectional Framework for Fair AI",
      "authors": [
        {
          "name": "Rashidul Islam",
          "authorId": "47755547"
        },
        {
          "name": "Kamrun Keya",
          "authorId": "52028519"
        },
        {
          "name": "Shimei Pan",
          "authorId": "2239443126"
        },
        {
          "name": "A. Sarwate",
          "authorId": "9208982"
        },
        {
          "name": "James R. Foulds",
          "authorId": "40289577"
        }
      ],
      "year": 2023,
      "abstract": "We propose definitions of fairness in machine learning and artificial intelligence systems that are informed by the framework of intersectionality, a critical lens from the legal, social science, and humanities literature which analyzes how interlocking systems of power and oppression affect individuals along overlapping dimensions including gender, race, sexual orientation, class, and disability. We show that our criteria behave sensibly for any subset of the set of protected attributes, and we prove economic, privacy, and generalization guarantees. Our theoretical results show that our criteria meaningfully operationalize AI fairness in terms of real-world harms, making the measurements interpretable in a manner analogous to differential privacy. We provide a simple learning algorithm using deterministic gradient methods, which respects our intersectional fairness criteria. The measurement of fairness becomes statistically challenging in the minibatch setting due to data sparsity, which increases rapidly in the number of protected attributes and in the values per protected attribute. To address this, we further develop a practical learning algorithm using stochastic gradient methods which incorporates stochastic estimation of the intersectional fairness criteria on minibatches to scale up to big data. Case studies on census data, the COMPAS criminal recidivism dataset, the HHP hospitalization data, and a loan application dataset from HMDA demonstrate the utility of our methods.",
      "citationCount": 17,
      "doi": "10.3390/e25040660",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/21a39ee100b048bfa72164801af472d787aa8739",
      "venue": "Entropy",
      "journal": {
        "name": "Entropy",
        "volume": "25"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a38b4829bf074ed01111a106637bd65b7da548cd",
      "title": "Improving Intersectional Group Fairness Using Conditional Generative Adversarial Network and Transfer Learning",
      "authors": [
        {
          "name": "David Quashigah Dzakpasu",
          "authorId": "2326979224"
        },
        {
          "name": "Jixue Liu",
          "authorId": "2108374005"
        },
        {
          "name": "Jiuyong Li",
          "authorId": "2257372048"
        },
        {
          "name": "Lin Liu",
          "authorId": "2261397729"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/978-981-96-0348-0_11",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a38b4829bf074ed01111a106637bd65b7da548cd",
      "venue": "Applied Informatics",
      "journal": {
        "pages": "139-153"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2f93f745625e1e66d1a8d16465c4bf239977f235",
      "title": "Characterizing Intersectional Group Fairness with Worst-Case Comparisons",
      "authors": [
        {
          "name": "A. Ghosh",
          "authorId": "47581570"
        },
        {
          "name": "Lea Genuit",
          "authorId": "2044444153"
        },
        {
          "name": "Mary Reagan",
          "authorId": "2069779566"
        }
      ],
      "year": 2021,
      "abstract": "Machine Learning or Arti\ufb01cial Intelligence algorithms have gained considerable scrutiny in recent times owing to their propensity towards imitating and amplifying existing prejudices in society. This has led to a niche but growing body of work that identi\ufb01es and attempts to \ufb01x these biases. A \ufb01rst step towards making these algorithms more fair is designing metrics that measure unfairness. Most existing work in this \ufb01eld deals with either a binary view of fairness (protected vs. unprotected groups) or politically de\ufb01ned categories (race or gender). Such categorization misses the important nuance of intersectionality - biases can often be ampli\ufb01ed in subgroups that combine membership from di\ufb00erent categories, especially if such a subgroup is particularly underrepresented in historical platforms of opportunity. In this paper, we discuss why fairness metrics need to be looked at under the lens of intersectionality, identify existing work in intersectional fairness, suggest a simple worst case comparison method to expand the de\ufb01nitions of existing group fairness metrics to incorporate intersectionality, and \ufb01nally conclude with the social, legal and political framework to handle intersectional fairness in the modern context.",
      "citationCount": 63,
      "doi": null,
      "arxivId": "2101.01673",
      "url": "https://www.semanticscholar.org/paper/2f93f745625e1e66d1a8d16465c4bf239977f235",
      "venue": "AIDBEI",
      "journal": {
        "pages": "22-34"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "fad383d5d9b91b7ad0083419341bfc780f1c30ed",
      "title": "MANI-Rank: Multiple Attribute and Intersectional Group Fairness for Consensus Ranking",
      "authors": [
        {
          "name": "Kathleen Cachel",
          "authorId": "2176779395"
        },
        {
          "name": "Elke A. Rundensteiner",
          "authorId": "1715020"
        },
        {
          "name": "Lane Harrison",
          "authorId": "2061185877"
        }
      ],
      "year": 2022,
      "abstract": "Combining the preferences of many rankers into one single consensus ranking is critical for consequential applications from hiring and admissions to lending. While group fairness has been extensively studied for classification, group fairness in rankings and in particular rank aggregation remains in its infancy. Recent work introduced the concept of fair rank aggregation for combining rankings but restricted to the case when candidates have a single binary protected attribute, i.e., they fall into two groups only. Yet it remains an open problem how to create a consensus ranking that represents the preferences of all rankers while ensuring fair treatment for candidates with multiple protected attributes such as gender, race, and nationality. In this work, we are the first to define and solve this open Multi-attribute Fair Consensus Ranking (MFCR) problem. As a foundation, we design novel group fairness criteria for rankings, called MANI-Rank, ensuring fair treatment of groups defined by individual protected attributes and their intersection. Leveraging the MANI-Rank criteria, we develop a series of algorithms that for the first time tackle the MFCR problem. Our experimental study with a rich variety of consensus scenarios demonstrates our MFCR methodology is the only approach to achieve both intersectional and protected attribute fairness while also representing the preferences expressed through many base rankings. Our real world case study on merit scholarships illustrates the effectiveness of our MFCR methods to mitigate bias across multiple protected attributes and their intersections.",
      "citationCount": 14,
      "doi": "10.48550/arXiv.2207.10020",
      "arxivId": "2207.10020",
      "url": "https://www.semanticscholar.org/paper/fad383d5d9b91b7ad0083419341bfc780f1c30ed",
      "venue": "IEEE International Conference on Data Engineering",
      "journal": {
        "name": "2022 IEEE 38th International Conference on Data Engineering (ICDE)",
        "pages": "1124-1137"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "e937894159a7932a9a6d1ce57a67f7a8d729ba61",
      "title": "One-vs.-One Mitigation of Intersectional Bias: A General Method for Extending Fairness-Aware Binary Classification",
      "authors": [
        {
          "name": "Kenji Kobayashi",
          "authorId": "2218941588"
        },
        {
          "name": "Yuri Nakao",
          "authorId": "82143319"
        }
      ],
      "year": 2021,
      "abstract": null,
      "citationCount": 6,
      "doi": "10.1007/978-3-030-87687-6_5",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e937894159a7932a9a6d1ce57a67f7a8d729ba61",
      "venue": "Advances in Intelligent Systems and Computing",
      "journal": {
        "name": "Advances in Intelligent Systems and Computing"
      },
      "publicationTypes": null
    },
    {
      "paperId": "39e4dfa4ec7e2bee44a9c5b940e640eb39c3442f",
      "title": "An Intersectional Definition of Fairness",
      "authors": [
        {
          "name": "James R. Foulds",
          "authorId": "40289577"
        },
        {
          "name": "Shimei Pan",
          "authorId": "2239443126"
        }
      ],
      "year": 2018,
      "abstract": "We propose differential fairness, a multi-attribute definition of fairness in machine learning which is informed by intersectionality, a critical lens arising from the humanities literature, leveraging connections between differential privacy and legal notions of fairness. We show that our criterion behaves sensibly for any subset of the set of protected attributes, and we prove economic, privacy, and generalization guarantees. We provide a learning algorithm which respects our differential fairness criterion. Experiments on the COMPAS criminal recidivism dataset and census data demonstrate the utility of our methods.",
      "citationCount": 213,
      "doi": "10.1109/ICDE48307.2020.00203",
      "arxivId": "1807.08362",
      "url": "https://www.semanticscholar.org/paper/39e4dfa4ec7e2bee44a9c5b940e640eb39c3442f",
      "venue": "IEEE International Conference on Data Engineering",
      "journal": {
        "name": "2020 IEEE 36th International Conference on Data Engineering (ICDE)",
        "pages": "1918-1921"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "9c1d5345ccebe2260082e5be6b95d3adeea83ac0",
      "title": "One-vs.-One Mitigation of Intersectional Bias: A General Method to Extend Fairness-Aware Binary Classification",
      "authors": [
        {
          "name": "Kenji Kobayashi",
          "authorId": "153175070"
        },
        {
          "name": "Yuri Nakao",
          "authorId": "82143319"
        }
      ],
      "year": 2020,
      "abstract": "With the widespread adoption of machine learning in the real world, the impact of the discriminatory bias has attracted attention. In recent years, various methods to mitigate the bias have been proposed. However, most of them have not considered intersectional bias, which brings unfair situations where people belonging to specific subgroups of a protected group are treated worse when multiple sensitive attributes are taken into consideration. To mitigate this bias, in this paper, we propose a method called One-vs.-One Mitigation by applying a process of comparison between each pair of subgroups related to sensitive attributes to the fairness-aware machine learning for binary classification. We compare our method and the conventional fairness-aware binary classification methods in comprehensive settings using three approaches (pre-processing, in-processing, and post-processing), six metrics (the ratio and difference of demographic parity, equalized odds, and equal opportunity), and two real-world datasets (Adult and COMPAS). As a result, our method mitigates the intersectional bias much better than conventional methods in all the settings. With the result, we open up the potential of fairness-aware binary classification for solving more realistic problems occurring when there are multiple sensitive attributes.",
      "citationCount": 8,
      "doi": null,
      "arxivId": "2010.13494",
      "url": "https://www.semanticscholar.org/paper/9c1d5345ccebe2260082e5be6b95d3adeea83ac0",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2010.13494"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d9d50e6d98f01f357357eafde24ab66370fc3559",
      "title": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System",
      "authors": [
        {
          "name": "Yashar Deldjoo",
          "authorId": "2614755"
        },
        {
          "name": "T. D. Noia",
          "authorId": "1737962"
        }
      ],
      "year": 2024,
      "abstract": "This work takes a critical stance on previous studies concerning fairness evaluation in Large-Language Model (LLM)-based recommender systems, which have primarily assessed consumer fairness by comparing recommendation lists generated with and without sensitive user attributes. Such approaches implicitly treat discrepancies in recommended items as biases, overlooking whether these changes might stem from genuine personalization aligned with true preferences of users. Moreover, these earlier studies typically address single sensitive attributes in isolation, neglecting the complex interplay of intersectional identities. In response to these shortcomings, we introduce CFaiRLLM, an enhanced evaluation framework that not only incorporates true preference alignment but also rigorously examines intersectional fairness by considering overlapping sensitive attributes. Additionally, CFaiRLLM introduces diverse user profile sampling strategies\u2014random, top-rated, and recency-focused\u2014to better understand the impact of profile generation fed to LLMs in light of inherent token limitations in these systems. Given that fairness depends on accurately understanding users\u2019 tastes and preferences, these strategies provide a more realistic assessment of fairness within RecLLMs. To validate the efficacy of CFaiRLLM, we conducted extensive experiments using MovieLens and LastFM datasets, applying various sampling strategies and sensitive attribute configurations. The evaluation metrics include both item similarity measures and true preference alignment considering both hit and ranking (Jaccard Similarity and PRAG), thereby conducting a multi-faceted analysis of recommendation fairness. The results demonstrated that true preference alignment offers a more personalized and fair assessment compared to similarity-based measures, revealing significant disparities when sensitive and intersectional attributes are incorporated. Notably, our study finds that intersectional attributes amplify fairness gaps more prominently, especially in less structured domains such as music recommendations in LastFM. These findings suggest that future fairness evaluations in RecLLMs should incorporate true preference alignment to ensure equitable and genuinely personalized recommendations.",
      "citationCount": 37,
      "doi": "10.1145/3725853",
      "arxivId": "2403.05668",
      "url": "https://www.semanticscholar.org/paper/d9d50e6d98f01f357357eafde24ab66370fc3559",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "journal": {
        "name": "ACM Transactions on Intelligent Systems and Technology",
        "pages": "1 - 31",
        "volume": "16"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6f30ca7af29b1d0da7ccdbbd9e4588ccd50f0b39",
      "title": "Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation",
      "authors": [
        {
          "name": "Farjana Yesmin",
          "authorId": "2266203896"
        }
      ],
      "year": 2025,
      "abstract": "Machine learning models trained on imbalanced datasets often exhibit intersectional biases-systematic errors arising from the interaction of multiple attributes such as object class and environmental conditions. This paper presents a data-driven framework for analyzing and mitigating such biases in image classification. We introduce the Intersectional Fairness Evaluation Framework (IFEF), which combines quantitative fairness metrics with interpretability tools to systematically identify bias patterns in model predictions. Building on this analysis, we propose Bias-Weighted Augmentation (BWA), a novel data augmentation strategy that adapts transformation intensities based on subgroup distribution statistics. Experiments on the Open Images V7 dataset with five object classes demonstrate that BWA improves accuracy for underrepresented class-environment intersections by up to 24 percentage points while reducing fairness metric disparities by 35%. Statistical analysis across multiple independent runs confirms the significance of improvements (p<0.05). Our methodology provides a replicable approach for analyzing and addressing intersectional biases in image classification systems.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.16072",
      "arxivId": "2510.16072",
      "url": "https://www.semanticscholar.org/paper/6f30ca7af29b1d0da7ccdbbd9e4588ccd50f0b39",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.16072"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4ce577292bf0d8f220d08e96cbccfc8f1b905a24",
      "title": "Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution",
      "authors": [
        {
          "name": "Falaah Arif Khan",
          "authorId": "1410017982"
        },
        {
          "name": "N. Sivakumar",
          "authorId": "50856025"
        },
        {
          "name": "Yinong Oliver Wang",
          "authorId": "2364688711"
        },
        {
          "name": "Katherine Metcalf",
          "authorId": "2311876721"
        },
        {
          "name": "Cezanne Camacho",
          "authorId": "2375388117"
        },
        {
          "name": "B. Theobald",
          "authorId": "2785748"
        },
        {
          "name": "Luca Zappella",
          "authorId": "1753336"
        },
        {
          "name": "N. Apostoloff",
          "authorId": "3301859"
        }
      ],
      "year": 2025,
      "abstract": "Large language models (LLMs) have achieved impressive performance, leading to their widespread adoption as decision-support tools in resource-constrained contexts like hiring and admissions. There is, however, scientific consensus that AI systems can reflect and exacerbate societal biases, raising concerns about identity-based harm when used in critical social contexts. Prior work has laid a solid foundation for assessing bias in LLMs by evaluating demographic disparities in different language reasoning tasks. In this work, we extend single-axis fairness evaluations to examine intersectional bias, recognizing that when multiple axes of discrimination intersect, they create distinct patterns of disadvantage. We create a new benchmark called WinoIdentity by augmenting the WinoBias dataset with 25 demographic markers across 10 attributes, including age, nationality, and race, intersected with binary gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns. Focusing on harms of omission due to underrepresentation, we investigate bias through the lens of uncertainty and propose a group (un)fairness metric called Coreference Confidence Disparity which measures whether models are more or less confident for some intersectional identities than others. We evaluate five recently published LLMs and find confidence disparities as high as 40% along various demographic attributes including body type, sexual orientation and socio-economic status, with models being most uncertain about doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly, coreference confidence decreases even for hegemonic or privileged markers, indicating that the recent impressive performance of LLMs is more likely due to memorization than logical reasoning. Notably, these are two independent failures in value alignment and validity that can compound to cause social harm.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.07111",
      "arxivId": "2508.07111",
      "url": "https://www.semanticscholar.org/paper/4ce577292bf0d8f220d08e96cbccfc8f1b905a24",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.07111"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "22c52467d9b877c5a33b6a9abe7488f8b86303fa",
      "title": "Addressing intersectional bias in AI recruitment using HITHIRE model: a fair, ethical, green AI and transparent hiring solution for Saudi Arabia\u2019s diverse workforce in line with vision 2030",
      "authors": [
        {
          "name": "Elham Albaroudi",
          "authorId": "2283407563"
        },
        {
          "name": "Taha Mansouri",
          "authorId": "3020040"
        },
        {
          "name": "Mohammad Hatamleh",
          "authorId": "2364213654"
        },
        {
          "name": "A. Alameer",
          "authorId": "2297789050"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.1007/s43681-025-00844-z",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/22c52467d9b877c5a33b6a9abe7488f8b86303fa",
      "venue": "AI and Ethics",
      "journal": {
        "name": "AI and Ethics",
        "volume": "6"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8b987b810a5d0de7bb92abd21a039fcfc87787ec",
      "title": "Bias and Fairness in Automated Loan Approvals: A Systematic Review of Machine Learning Approaches",
      "authors": [
        {
          "name": "Suraiyo Raziyeva",
          "authorId": "2363839919"
        },
        {
          "name": "Meraryslan Meraliyev",
          "authorId": "68976113"
        }
      ],
      "year": 2025,
      "abstract": "\n\n\nArtificial intelligence (AI) is increasingly transforming credit approval processes, enabling financial institutions to assess risk more efficiently and at greater scale. As these systems become more embedded in lending decisions, concerns around fairness, bias, and accountability have grown significantly. Many of these concerns stem from the use of historical data, proxy variables, and model optimization choices that can unintentionally reinforce existing social and economic inequalities. This work presents a systematic overview of the types and sources of bias in AI - driven loan approval systems and critically examines how machine learning techniques attempt to address them. It also highlights emerging solutions, including explainable AI, federated learning, human-in-the-loop frameworks, and intersectional fairness approaches. Despite ongoing advancements, unresolved challenges remain - particularly the need for dynamic fairness monitoring and for addressing intersectional biases affecting individuals from multiple marginalized groups. To bridge these gaps, the paper emphasizes the importance of interdisciplinary collaboration among AI developers, regulatory bodies, and social scientists. It advocates embedding fairness as a core design principle in the development and deployment of future AI systems. Overall, this study contributes to the growing effort to develop more transparent, inclusive, and socially responsible financial technologies.\n\n\n",
      "citationCount": 1,
      "doi": "10.47344/jbzmnx25",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/8b987b810a5d0de7bb92abd21a039fcfc87787ec",
      "venue": "Journal of Emerging Technologies and Computing",
      "journal": {
        "name": "Journal of Emerging Technologies and Computing"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "4673d953f59423d068abc0f21961767bb536b508",
      "title": "From Narratives to Code: Using Intersectional Methodology (IM) for Algorithmic Designs",
      "authors": [
        {
          "name": "Princess Chihurumnaya Samuel",
          "authorId": "2384619217"
        }
      ],
      "year": 2025,
      "abstract": "In this paper, we present our research using intersectionality as a standpoint for the theoretical and methodological evaluation and design of a human-centered interactive web-based, and narrative focused prototype. This prototype system functions as a participatory toolkit through which marginalized users can submit, annotate and explore personal stories of discriminatory algorithmic encounters. Existing traditional fairness models tend to treat algorithmic bias as a technical defect - something to be corrected by recalibrating datasets or adjusting model weights. However, such approaches often ignore the deeper structural systemic sociohistorical roots of inequality and the experiences of those most harmed by technological systems. Our research challenges these reductive paradigms by providing and operationalizing intersectionality methodology as a new methodological framework for bias evaluation and equity grounded design. By incorporating the lived experiences of algorithmic violence towards Black women and others existing at the intersection of multiple identities into computing workflows, this study aims to shift the paradigms of traditional-algorithmic system design approach and epistemological computing research frameworks to design and develop a participatory rich narrative-based system that relies on the four strategies of intersectionality methodology. This scholarship employs participatory narrative-data collection, reflexive analytics, hybrid integration of natural language processing technology, researcher positionality and reflexive logging, data annotation, and feminist data visualizations. Intersectional ethics of contextualized research frameworks, integrated research methodological pluralism, and embedded reflexive documentation are the main contributions of this work. This dissertation not only contributes as a functional research tool but bridges the gap between critical theory and computation of bias-aware systems.",
      "citationCount": 0,
      "doi": "10.1145/3736251.3757105",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/4673d953f59423d068abc0f21961767bb536b508",
      "venue": "Proceedings of the ACM Global on Computing Education Conference 2025 Vol 2",
      "journal": {
        "name": "Proceedings of the ACM Global on Computing Education Conference 2025 Vol 2"
      },
      "publicationTypes": [
        "Book",
        "Conference"
      ]
    },
    {
      "paperId": "859cee56d2cfee2653adec10c8f601883e19f380",
      "title": "Intersectional Unfairness Discovery",
      "authors": [
        {
          "name": "Gezheng Xu",
          "authorId": "2049043240"
        },
        {
          "name": "Qi Chen",
          "authorId": "2157956022"
        },
        {
          "name": "C. Ling",
          "authorId": "2059988586"
        },
        {
          "name": "Boyu Wang",
          "authorId": "2258324568"
        },
        {
          "name": "Changjian Shui",
          "authorId": "35701651"
        }
      ],
      "year": 2024,
      "abstract": "AI systems have been shown to produce unfair results for certain subgroups of population, highlighting the need to understand bias on certain sensitive attributes. Current research often falls short, primarily focusing on the subgroups characterized by a single sensitive attribute, while neglecting the nature of intersectional fairness of multiple sensitive attributes. This paper focuses on its one fundamental aspect by discovering diverse high-bias subgroups under intersectional sensitive attributes. Specifically, we propose a Bias-Guided Generative Network (BGGN). By treating each bias value as a reward, BGGN efficiently generates high-bias intersectional sensitive attributes. Experiments on real-world text and image datasets demonstrate a diverse and efficient discovery of BGGN. To further evaluate the generated unseen but possible unfair intersectional sensitive attributes, we formulate them as prompts and use modern generative AI to produce new texts and images. The results of frequently generating biased data provides new insights of discovering potential unfairness in popular modern generative AI systems. Warning: This paper contains generative examples that are offensive in nature.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2405.20790",
      "arxivId": "2405.20790",
      "url": "https://www.semanticscholar.org/paper/859cee56d2cfee2653adec10c8f601883e19f380",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2405.20790"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "e0c175aa94746dcd9b332e467d2db9d03fd47fff",
      "title": "Fairness in Artificial Intelligence: A Comprehensive Review of Bias Detection: A Systematic Literature",
      "authors": [
        {
          "name": "Lokesh Kumar",
          "authorId": "2362773802"
        }
      ],
      "year": 2025,
      "abstract": "ABSTRACT\n\nIn this paper, bias refers to the systematic preference of AIs for some groups and against others, which can cause harm. Despite the progress in AI technologies like recommendation systems, generative models, and predictive analytics, AI systems suck up biases from datasets, algorithms, and operational processes. It is important to tackle bias as AI is already, or plans to be, used in areas like healthcare, education, and governance. The main causes of bias are data bias, where datasets are unbalanced, and algorithm bias, where the design of the algorithm is unfair.\n\nThe focus of this review is to identify the types of biases and the importance of fairness in AI systems. Current research is trying to develop datasets, fairness metrics, and debiasing heuristics, but each has its own drawbacks. The majority of metrics do not capture intersectional biases properly, and the mitigation techniques generally lead to residual or domain-agnostic biases being left unmitigated. Also, most of the frameworks do not consider the contextual biases that are specific to non-Western societies, particularly Indian society.\n\nTo address these gaps, this review assesses the current datasets, bias quantification metrics, and debiasing approaches. It also discusses the weaknesses of current solutions and suggests future research directions. Some of the suggested directions include developing comprehensive, high- quality, and region-specific datasets, developing new fairness metrics that are suitable for various application domains of AI, and developing efficient and scalable debiasing approaches for both generative and multimodal AI systems. This comprehensive review is expected to advance the quest for fair and reliable AI systems with a view on fairness in various settings around the world.\n\nKeywords: Bias Detection in AI, Algorithmic Bias, Dataset Bias, Fairness Metrics, LLMs",
      "citationCount": 1,
      "doi": "10.55041/isjem03725",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e0c175aa94746dcd9b332e467d2db9d03fd47fff",
      "venue": "International Scientific Journal of Engineering and Management",
      "journal": {
        "name": "International Scientific Journal of Engineering and Management"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "f810af61e1e72315c15263aeef874d58d03fe06b",
      "title": "Recalibrating Human\u2013Machine Relations through Bias-Aware Machine Learning: Technical Pathways to Fairness and Trust",
      "authors": [
        {
          "name": "Esam Othman",
          "authorId": "2356961490"
        },
        {
          "name": "R. Mahafdah",
          "authorId": "20441919"
        }
      ],
      "year": 2025,
      "abstract": "Considering the importance of artificial intelligence (AI) in decision-making processes in various fields such as health, law and finance, the concern for bias and fairness of decision making has increased. This paper presents an extensive discussion of bias-aware machine learning(Ml) such as fairness-aware modeling, detection and mitigation. The paper demonstrates aspects of fairness, different forms of algorithmic bias including intersectional bias and how biased systems impact society. The paper turns to appreciation of dentieth, Trust Dynamics, Legal and Regulatory Frameworks And in the Context of Promoting Transparency: Exploring the Role of Explainable AI (XAI). Taking into account the current advances for combatting bias, also pre-processing, in-processing, and post-processing methods, for instance, draw on examples from major domains of interest. Apart from the improvements AIs have achieved, existing challenges involve little attention to relationship among different identities, poor frameworks in place for implementation and operation in other parts of the world, inadequate abuse detection mechanisms among others. Regarding this, we present some of the research questions that focus on the notions of transparency, privacy protected fairness audits, and shared control with the aim of guiding the growth of fair, responsible, and competent AI systems.",
      "citationCount": 1,
      "doi": "10.63332/joph.v5i4.1091",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f810af61e1e72315c15263aeef874d58d03fe06b",
      "venue": "Journal of Posthumanism",
      "journal": {
        "name": "Journal of Posthumanism"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2cf0f5ee410b19a866d0fd885f6a8d2a77ae77d4",
      "title": "MMM-fair: An Interactive Toolkit for Exploring and Operationalizing Multi-Fairness Trade-offs",
      "authors": [
        {
          "name": "Swati Swati",
          "authorId": "2312756830"
        },
        {
          "name": "Arjun Roy",
          "authorId": "47762613"
        },
        {
          "name": "Emmanouil Panagiotou",
          "authorId": "2295579739"
        },
        {
          "name": "E. Ntoutsi",
          "authorId": "2264487539"
        }
      ],
      "year": 2025,
      "abstract": "Fairness-aware classification requires balancing performance and fairness, often intensified by intersectional biases. Conflicting fairness definitions further complicate the task, making it difficult to identify universally fair solutions. Despite growing regulatory and societal demands for equitable AI, popular toolkits offer limited support for exploring multi-dimensional fairness and related trade-offs. To address this, we present mmm-fair, an open-source toolkit leveraging boosting-based ensemble approaches that dynamically optimizes model weights to jointly minimize classification errors and diverse fairness violations, enabling flexible multi-objective optimization. The system empowers users to deploy models that align with their context-specific needs while reliably uncovering intersectional biases often missed by state-of-the-art methods. In a nutshell, mmm-fair uniquely combines in-depth multi-attribute fairness, multi-objective optimization, a no-code, chat-based interface, LLM-powered explanations, interactive Pareto exploration for model selection, custom fairness constraint definition, and deployment-ready models in a single open-source toolkit, a combination rarely found in existing fairness tools. Demo walkthrough available at: https://youtu.be/_rcpjlXFqkw.",
      "citationCount": 1,
      "doi": "10.1145/3746252.3761476",
      "arxivId": "2509.08156",
      "url": "https://www.semanticscholar.org/paper/2cf0f5ee410b19a866d0fd885f6a8d2a77ae77d4",
      "venue": "International Conference on Information and Knowledge Management",
      "journal": {
        "name": "Proceedings of the 34th ACM International Conference on Information and Knowledge Management"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
      ]
    }
  ],
  "count": 50,
  "errors": []
}
