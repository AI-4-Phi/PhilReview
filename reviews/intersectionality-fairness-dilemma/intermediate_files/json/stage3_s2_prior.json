{
  "status": "success",
  "source": "semantic_scholar",
  "query": "prioritarianism algorithmic fairness",
  "results": [
    {
      "paperId": "fc7f91d95b88689314617e08b784044061c45e3d",
      "title": "Algorithmic Fairness and Feasibility",
      "authors": [
        {
          "name": "Eva Erman",
          "authorId": "2338612784"
        },
        {
          "name": "Markus Furendal",
          "authorId": "113631409"
        },
        {
          "name": "Niklas M\u00f6ller",
          "authorId": "145497600"
        }
      ],
      "year": 2025,
      "abstract": "The \u201cimpossibility results\u201d in algorithmic fairness suggest that a predictive model cannot fully meet two common fairness criteria \u2013 sufficiency and separation \u2013 except under extraordinary circumstances. These findings have sparked a discussion on fairness in algorithms, prompting debates over whether predictive models can avoid unfair discrimination based on protected attributes, such as ethnicity or gender. As shown by Otto Sahlgren, however, the discussion of the impossibility results would gain from importing some of the tools developed in the philosophical literature on feasibility. Utilizing these tools, Sahlgren sketches a cautiously optimistic view of how algorithmic fairness can be made feasible in restricted local decision-making. While we think it is a welcome move to inject the literature on feasibility into the debate on algorithmic fairness, Sahlgren says very little about what are the general gains of bringing in feasibility considerations in theorizing algorithmic fairness. How, more precisely, does it help us make assessments about fairness in algorithmic decision-making? This is what is addressed in this Reply. More specifically, our two-fold argument is that feasibility plays an important but limited role for algorithmic fairness. We end by offering a sketch of a framework, which may be useful for theorizing feasibility in algorithmic fairness.",
      "citationCount": 1,
      "doi": "10.1007/s13347-024-00835-8",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/fc7f91d95b88689314617e08b784044061c45e3d",
      "venue": "Philosophy & Technology",
      "journal": {
        "name": "Philosophy & Technology",
        "volume": "38"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "16b873e2f515c97a7ad0e9235ed5f0a176234df6",
      "title": "Algorithmic fairness: challenges to building an effective regulatory regime",
      "authors": [
        {
          "name": "Greg Demirchyan",
          "authorId": "2378452979"
        }
      ],
      "year": 2025,
      "abstract": "Unfair treatment by artificial intelligence toward protected groups has become an important topic of discussion. Its potential for causing harm has spurred many to think that legislation aimed at regulating AI systems is essential. In the US, laws have already been proposed both by Congress as well as by several key states. However, a number of challenges stand in the way of effective legislation. Proposed laws mandating testing for fairness must articulate clear positions on how fairness is defined. But the task of selecting a suitable definition (or definitions) of fairness is not a simple one. Experts in AI continue to disagree as to what constitutes algorithmic fairness, which has led to an ever-expanding list of definitions that are highly technical in nature and require expertise that most legislators simply do not possess. Complicating things further, several of the proposed definitions are incommensurable with one another, making a cross-jurisdictional regulatory regime incorporating different standards of fairness susceptible to inconsistent determinations. On top of all this, legislators must also contend with existing laws prohibiting group-based discrimination that codify conceptions of fairness that may not be suitable for evaluating certain algorithms. In this article, I examine these challenges in detail, and suggest ways to deal with them such that the regulatory regime that emerges is one that is more effective in carrying out its intended purpose.",
      "citationCount": 2,
      "doi": "10.3389/frai.2025.1637134",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/16b873e2f515c97a7ad0e9235ed5f0a176234df6",
      "venue": "Frontiers in Artificial Intelligence",
      "journal": {
        "name": "Frontiers in Artificial Intelligence",
        "volume": "8"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b467047443b0a51b2e2fbe76113e7afd58f5e896",
      "title": "The epistemic dimension of algorithmic fairness: assessing its impact in innovation diffusion and fair policy making",
      "authors": [
        {
          "name": "E. Villa",
          "authorId": "2170938088"
        },
        {
          "name": "Camilla Quaresmini",
          "authorId": "2216795056"
        },
        {
          "name": "Valentina Breschi",
          "authorId": "2289848310"
        },
        {
          "name": "V. Schiaffonati",
          "authorId": "2226297"
        },
        {
          "name": "M. Tanelli",
          "authorId": "2241565846"
        }
      ],
      "year": 2025,
      "abstract": "Algorithmic fairness is an expanding field that addresses a range of discrimination issues associated with algorithmic processes. However, most works in the literature focus on analyzing it only from an ethical perspective, focusing on moral principles and values that should be considered in the design and evaluation of algorithms, while disregarding the epistemic dimension related to knowledge transmission and validation. However, this aspect of algorithmic fairness should also be included in the debate, as it is crucial to introduce a specific type of harm: an individual may be systematically excluded from the dissemination of knowledge due to the attribution of a credibility deficit/excess. In this work, we specifically focus on characterizing and analyzing the impact of this credibility deficit or excess on the diffusion of innovations on a societal scale, a phenomenon driven by individual attitudes and social interactions, and also by the strength of mutual connections. Indeed, discrimination might shape the latter, ultimately modifying how innovations spread within the network. In this light, to incorporate, also from a formal point of view, the epistemic dimension in innovation diffusion models becomes paramount, especially if these models are intended to support fair policy design. For these reasons, we formalize the epistemic properties of a social environment, by extending the well-established Linear Threshold Model (LTM) in an epistemic direction to show the impact of epistemic biases in innovation diffusion. Focusing on the impact of epistemic bias in both open-loop and closed-loop scenarios featuring optimal fostering policies, our results shed light on the pivotal role the epistemic dimension might have in the debate of algorithmic fairness in decision-making.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2504.02856",
      "arxivId": "2504.02856",
      "url": "https://www.semanticscholar.org/paper/b467047443b0a51b2e2fbe76113e7afd58f5e896",
      "venue": "EWAF",
      "journal": {
        "pages": "116-134"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7b14f49c2e1e87b90b485b40cb5b6a0921bd8dcc",
      "title": "Algorithmic Fairness and Educational Justice",
      "authors": [
        {
          "name": "A. Wolf",
          "authorId": "2359165335"
        }
      ],
      "year": 2025,
      "abstract": "Much has been written about how to improve the fairness of AI tools for decision\u2010making but less has been said about how to approach this new field from the perspective of philosophy of education. My goal in this paper is to bring together criteria from the general algorithmic fairness literature with prominent values of justice defended by philosophers of education. Some kinds of fairness criteria appear better suited than others for realizing these values. Considering these criteria for cases of automated decision\u2010making in education reveals that when the aim of justice is equal respect and belonging, this is best served by using statistical definitions of fairness to constrain decision\u2010making. By contrast, distributive aims of justice are best promoted by thinking of fairness in terms of the intellectual virtues of human decision\u2010makers who use algorithmic tools.",
      "citationCount": 0,
      "doi": "10.1111/edth.70029",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7b14f49c2e1e87b90b485b40cb5b6a0921bd8dcc",
      "venue": "Educational Theory",
      "journal": {
        "name": "Educational Theory"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0a49c157f2565a95187108b07ad2d8231bd77e86",
      "title": "A Feminist Account of Intersectional Algorithmic Fairness",
      "authors": [
        {
          "name": "Marie Mirsch",
          "authorId": "2377073672"
        },
        {
          "name": "Laila Wegner",
          "authorId": "2331068415"
        },
        {
          "name": "Jonas Strube",
          "authorId": "2376541652"
        },
        {
          "name": "C. Leicht-Scholten",
          "authorId": "2347733474"
        }
      ],
      "year": 2025,
      "abstract": "Intersectionality has profoundly influenced research and political action by revealing how interconnected systems of privilege and oppression influence lived experiences, yet its integration into algorithmic fairness research remains limited. Existing approaches often rely on single-axis or formal subgroup frameworks that risk oversimplifying social realities and neglecting structural inequalities. We propose Substantive Intersectional Algorithmic Fairness, extending Green's (2022) notion of substantive algorithmic fairness with insights from intersectional feminist theory. Building on this foundation, we introduce ten desiderata within the ROOF methodology to guide the design, assessment, and deployment of algorithmic systems in ways that address systemic inequities while mitigating harms to intersectionally marginalized communities. Rather than prescribing fixed operationalizations, these desiderata encourage reflection on assumptions of neutrality, the use of protected attributes, the inclusion of multiply marginalized groups, and enhancing algorithmic systems'potential. Our approach emphasizes that fairness cannot be separated from social context, and that in some cases, principled non-deployment may be necessary. By bridging computational and social science perspectives, we provide actionable guidance for more equitable, inclusive, and context-sensitive intersectional algorithmic practices.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.17944",
      "arxivId": "2508.17944",
      "url": "https://www.semanticscholar.org/paper/0a49c157f2565a95187108b07ad2d8231bd77e86",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.17944"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "75a30b455370caf8b156df1a8cabca8c8ab5c4e3",
      "title": "Algorithmic Fairness in AI Surrogates for End-of-Life Decision-Making",
      "authors": [
        {
          "name": "Muhammad Aurangzeb Ahmad",
          "authorId": "2387129914"
        }
      ],
      "year": 2025,
      "abstract": "Artificial intelligence surrogates are systems designed to infer preferences when individuals lose decision-making capacity. Fairness in such systems is a domain that has been insufficiently explored. Traditional algorithmic fairness frameworks are insufficient for contexts where decisions are relational, existential, and culturally diverse. This paper explores an ethical framework for algorithmic fairness in AI surrogates by mapping major fairness notions onto potential real-world end-of-life scenarios. It then examines fairness across moral traditions. The authors argue that fairness in this domain extends beyond parity of outcomes to encompass moral representation, fidelity to the patient's values, relationships, and worldview.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.16056",
      "arxivId": "2510.16056",
      "url": "https://www.semanticscholar.org/paper/75a30b455370caf8b156df1a8cabca8c8ab5c4e3",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.16056"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c23799e7ede2133c55f53389a3dcb95a05ad8948",
      "title": "Algorithmic Fairness, Decision Thresholds, and the Separateness of Persons",
      "authors": [
        {
          "name": "S. Holm",
          "authorId": "2194079469"
        }
      ],
      "year": 2025,
      "abstract": "Can an algorithmic procedure for allocating a medical treatment such as chemotherapy be fair, if it entails that the therapy should be administered to candidates, who are better off, in expectation, if they do not to receive it? In this article I argue (i) that the answer is \u201cno\u201d and (ii) that popular statistical fairness criteria must answer \u201cyes.\u201d I then argue that fairness requires that the procedure allocates the therapy for the right reason, and that the right kind of reason must respect the separateness of persons. I then anchor my proposed individual-level approach to algorithmic fairness in John Broome's theory of fairness and conclude that algorithmic fairness requires individual decision thresholds.",
      "citationCount": 0,
      "doi": "10.1145/3715275.3732113",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/c23799e7ede2133c55f53389a3dcb95a05ad8948",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "4a771318b60ca4f860b713d6f306bf919ca0a7a9",
      "title": "Algorithmic fairness as sociotechnical system",
      "authors": [
        {
          "name": "M. Bargh",
          "authorId": "2824885"
        },
        {
          "name": "Sunil Choenni",
          "authorId": "1735595"
        },
        {
          "name": "Floris Ter Braak",
          "authorId": "2362486724"
        }
      ],
      "year": 2025,
      "abstract": "Organizations and enterprises search for ways to exploit the vast amount of data that is produced by citizens, sensors, devices and administrative processes. Capitalizing on the produced data should be done responsibly by preventing, mitigating and managing undesired side effects such as violation of rules and regulations, human rights, ethical principles as well as privacy and security requirements. A key challenge in employing data, algorithms and data-driven systems is to adhere to the principle of fairness and justice. In this contribution we focus on the issue of algorithmic fairness, which itself can be framed as a sociotechnical system with interacting social and technical/formal subsystems. Information is a key construct of any sociotechnical system, where information creation and exchange can ease the opacity of interactions between the social and formal subsystems, and of interactions between the subsystems and the environment in which they operate. Based on literature, we categorize the types and flows of the information construct within the sociotechnical systems of algorithmic fairness in 7 categories. As such, the presented insights about the 7 categories of the information construct can form a common mental model whereby social and technical disciplines can inform each other systematically and align their views on algorithmic fairness.",
      "citationCount": 0,
      "doi": "10.59490/dgo.2025.952",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/4a771318b60ca4f860b713d6f306bf919ca0a7a9",
      "venue": "Conference on Digital Government Research",
      "journal": {
        "name": "Conference on Digital Government Research"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "48745c33405b17eeac4aaa5310f47136c7807724",
      "title": "Algorithmic Fairness as an Inconsistent Concept",
      "authors": [
        {
          "name": "Patrik Hummel",
          "authorId": "2302759524"
        }
      ],
      "year": 2025,
      "abstract": "\n In this article, I investigate whether algorithmic fairness is an inconsistent concept (the inconsistency thesis). Drawing on the work of Kevin Scharp, inconsistent concepts can apply and disapply at the same time (2.). It is shown that paradigmatic issues of algorithmic fairness fit this description (3.). Similarities and differences to received views (4.) and alternatives to the inconsistency thesis are considered (5.). Suggestions are articulated on how the inconsistency thesis might hold ground nevertheless, or at the very least denotes a distinctive option in argumentative space whose status and implications merit further evaluation.",
      "citationCount": 0,
      "doi": "10.5406/21521123.62.1.04",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/48745c33405b17eeac4aaa5310f47136c7807724",
      "venue": "American Philosophical Quarterly",
      "journal": {
        "name": "American Philosophical Quarterly"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "99e78e297653b9c1af7fdac7c7634deabcaf3523",
      "title": "Why causal inference is necessary for algorithmic fairness",
      "authors": [
        {
          "name": "Alexander Williams Tolbert",
          "authorId": "2380249150"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/s11229-025-05044-0",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/99e78e297653b9c1af7fdac7c7634deabcaf3523",
      "venue": "Synthese",
      "journal": {
        "name": "Synthese",
        "volume": "206"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4b90e5d6e6974f0715ce3dd8be75bfacb2ac0ffd",
      "title": "Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics",
      "authors": [
        {
          "name": "Tin Nguyen",
          "authorId": "2258676993"
        },
        {
          "name": "Jiannan Xu",
          "authorId": "2261376321"
        },
        {
          "name": "Zora Che",
          "authorId": "2173680156"
        },
        {
          "name": "Phuong-Anh Nguyen-Le",
          "authorId": "2349539106"
        },
        {
          "name": "Rushil Dandamudi",
          "authorId": "2363493595"
        },
        {
          "name": "Donald Braman",
          "authorId": "2359258957"
        },
        {
          "name": "Furong Huang",
          "authorId": "2303418156"
        },
        {
          "name": "Hal Daum'e",
          "authorId": "2344615259"
        },
        {
          "name": "Zubin Jelveh",
          "authorId": "1866984"
        }
      ],
      "year": 2025,
      "abstract": "Although popularized AI fairness metrics, e.g., demographic parity, have uncovered bias in AI-assisted decision-making outcomes, they do not consider how much effort one has spent to get to where one is today in the input feature space. However, the notion of effort is important in how Philosophy and humans understand fairness. We propose a philosophy-informed approach to conceptualize and evaluate Effort-aware Fairness (EaF), grounded in the concept of Force, which represents the temporal trajectory of predictive features coupled with inertia. Besides theoretical formulation, our empirical contributions include: (1) a pre-registered human subjects experiment, which shows that for both stages of the (individual) fairness evaluation process, people consider the temporal trajectory of a predictive feature more than its aggregate value; (2) pipelines to compute Effort-aware Individual/Group Fairness in the criminal justice and personal finance contexts. Our work may enable AI model auditors to uncover and potentially correct unfair decisions against individuals who have spent significant efforts to improve but are still stuck with systemic disadvantages outside their control.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2505.19317",
      "arxivId": "2505.19317",
      "url": "https://www.semanticscholar.org/paper/4b90e5d6e6974f0715ce3dd8be75bfacb2ac0ffd",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.19317"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "e1d144ccd667679f8b20d7d3cc554a9189aa0665",
      "title": "Fourth European Workshop on Algorithmic Fairness (EWAF'25)",
      "authors": [
        {
          "name": "Hilde J. P. Weerts",
          "authorId": "32345170"
        },
        {
          "name": "Mykola Pechenizkiy",
          "authorId": "1691997"
        },
        {
          "name": "Doris Allhutter",
          "authorId": "2377561957"
        },
        {
          "name": "Ana Maria Corr\u00eaa",
          "authorId": "2377562621"
        },
        {
          "name": "Thomas Grote",
          "authorId": "2377562338"
        },
        {
          "name": "Cynthia C. S. Liem",
          "authorId": "2377562146"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e1d144ccd667679f8b20d7d3cc554a9189aa0665",
      "venue": "EWAF",
      "journal": {
        "pages": "1-9"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "767edcb0adf9c3cf165476d7c0ef936e5d97f6f6",
      "title": "Theoretical Foundations of Algorithmic Fairness: A Unified Framework Through Pinsker's Inequality",
      "authors": [
        {
          "name": "Agus Sudjianto",
          "authorId": "2379811869"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.2139/ssrn.5402916",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/767edcb0adf9c3cf165476d7c0ef936e5d97f6f6",
      "venue": "Social Science Research Network",
      "journal": {
        "name": "SSRN Electronic Journal"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "29ef4cf6c34564b0c5a93d2eca88fda279c88aee",
      "title": "On Feasibility and Algorithmic Fairness: A Reply to Erman, Furendal, and M\u00f6ller",
      "authors": [
        {
          "name": "Otto Sahlgren",
          "authorId": "2320675295"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/s13347-025-00844-1",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/29ef4cf6c34564b0c5a93d2eca88fda279c88aee",
      "venue": "Philosophy & Technology",
      "journal": {
        "name": "Philosophy & Technology",
        "volume": "38"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "614469a7d50b36c6fdf036beac030289bbc54f3d",
      "title": "Data Speak but Sometimes Lie: A Game-Theoretic Approach to Data Bias and Algorithmic Fairness",
      "authors": [
        {
          "name": "Chiara Manganini",
          "authorId": "2305898679"
        },
        {
          "name": "Esther Anna Corsi",
          "authorId": "35127815"
        },
        {
          "name": "Giuseppe Primiero",
          "authorId": "2280919609"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1016/j.ijar.2025.109608",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/614469a7d50b36c6fdf036beac030289bbc54f3d",
      "venue": "International Journal of Approximate Reasoning",
      "journal": {
        "name": "International Journal of Approximate Reasoning"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8a4992cb4924a2a5dd17c637c27c8933d78b6c76",
      "title": "The ideals program in algorithmic fairness",
      "authors": [
        {
          "name": "Rush T. Stewart",
          "authorId": "2324363804"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 2,
      "doi": "10.1007/s00146-024-02106-8",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/8a4992cb4924a2a5dd17c637c27c8933d78b6c76",
      "venue": "Ai & Society",
      "journal": {
        "name": "AI & SOCIETY",
        "pages": "2273 - 2283",
        "volume": "40"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "37a051949dced9529ebeae245bf4122092b3c4ef",
      "title": "What\u2019s Impossible about Algorithmic Fairness?",
      "authors": [
        {
          "name": "Otto Sahlgren",
          "authorId": "2320675295"
        }
      ],
      "year": 2024,
      "abstract": "The now well-known impossibility results of algorithmic fairness demonstrate that an error-prone predictive model cannot simultaneously satisfy two plausible conditions for group fairness apart from exceptional circumstances where groups exhibit equal base rates. The results sparked, and continue to shape, lively debates surrounding algorithmic fairness conditions and the very possibility of building fair predictive models. This article, first, highlights three underlying points of disagreement in these debates, which have led to diverging assessments of the feasibility of fairness in prediction-based decision-making. Second, the article explores whether and in what sense fairness as defined by the conjunction of the implicated fairness conditions is (un)attainable. Drawing on philosophical literature on the concept of feasibility and the role of feasibility in normative theory, I outline a cautiously optimistic argument for the diachronic feasibility of fairness. In line with recent works on the topic, I argue that fairness can be made possible through collective efforts to eliminate inequalities that feed into local decision-making procedures.",
      "citationCount": 5,
      "doi": "10.1007/s13347-024-00814-z",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/37a051949dced9529ebeae245bf4122092b3c4ef",
      "venue": "Philosophy & Technology",
      "journal": {
        "name": "Philosophy & Technology",
        "volume": "37"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "00a334319af95a019250391867d0784a763341c0",
      "title": "Algorithmic Fairness and Color-blind Racism: Navigating the Intersection",
      "authors": [
        {
          "name": "Jamelle Watson-Daniels",
          "authorId": "1388406103"
        }
      ],
      "year": 2024,
      "abstract": "Our focus lies at the intersection between two broader research perspectives: (1) the scientific study of algorithms and (2) the scholarship on race and racism. Many streams of research related to algorithmic fairness have been born out of interest at this intersection. We think about this intersection as the product of work derived from both sides. From (1) algorithms to (2) racism, the starting place might be an algorithmic question or method connected to a conceptualization of racism. On the other hand, from (2) racism to (1) algorithms, the starting place could be recognizing a setting where a legacy of racism is known to persist and drawing connections between that legacy and the introduction of algorithms into this setting. In either direction, meaningful disconnection can occur when conducting research at the intersection of racism and algorithms. The present paper urges collective reflection on research directions at this intersection. Despite being primarily motivated by instances of racial bias, research in algorithmic fairness remains mostly disconnected from scholarship on racism. In particular, there has not been an examination connecting algorithmic fairness discussions directly to the ideology of color-blind racism; we aim to fill this gap. We begin with a review of an essential account of color-blind racism then we review racial discourse within algorithmic fairness research and underline significant patterns, shifts and disconnects. Ultimately, we argue that researchers can improve the navigation of the landscape at the intersection by recognizing ideological shifts as such and iteratively re-orienting towards maintaining meaningful connections across interdisciplinary lines.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2402.07778",
      "arxivId": "2402.07778",
      "url": "https://www.semanticscholar.org/paper/00a334319af95a019250391867d0784a763341c0",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.07778"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "11ac9190b4349db1f18b9a7b5a5b42410668fb77",
      "title": "What's Distributive Justice Got to Do with It? Rethinking Algorithmic Fairness from the Perspective of Approximate Justice",
      "authors": [
        {
          "name": "Corinna Hertweck",
          "authorId": "1785374742"
        },
        {
          "name": "Christoph Heitz",
          "authorId": "151473392"
        },
        {
          "name": "Michele Loi",
          "authorId": "2311696127"
        }
      ],
      "year": 2024,
      "abstract": "In the field of algorithmic fairness, many fairness criteria have been proposed. Oftentimes, their proposal is only accompanied by a loose link to ideas from moral philosophy -- which makes it difficult to understand when the proposed criteria should be used to evaluate the fairness of a decision-making system. More recently, researchers have thus retroactively tried to tie existing fairness criteria to philosophical concepts. Group fairness criteria have typically been linked to egalitarianism, a theory of distributive justice. This makes it tempting to believe that fairness criteria mathematically represent ideals of distributive justice and this is indeed how they are typically portrayed. In this paper, we will discuss why the current approach of linking algorithmic fairness and distributive justice is too simplistic and, hence, insufficient. We argue that in the context of imperfect decision-making systems -- which is what we deal with in algorithmic fairness -- we should not only care about what the ideal distribution of benefits/harms among individuals would look like but also about how deviations from said ideal are distributed. Our claim is that algorithmic fairness is concerned with unfairness in these deviations. This requires us to rethink the way in which we, as algorithmic fairness researchers, view distributive justice and use fairness criteria.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2407.12488",
      "arxivId": "2407.12488",
      "url": "https://www.semanticscholar.org/paper/11ac9190b4349db1f18b9a7b5a5b42410668fb77",
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "pages": "597-608"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0416c3db92ac9a4e9f05db71570d8223efd33f55",
      "title": "Challenging \"subgroup Fairness\": Towards Intersectional Algorithmic Fairness Based on Personas",
      "authors": [
        {
          "name": "M. Decker",
          "authorId": "2331070137"
        },
        {
          "name": "Laila Wegner",
          "authorId": "2331068415"
        },
        {
          "name": "C. Leicht-Scholten",
          "authorId": "2347733474"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 2,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/0416c3db92ac9a4e9f05db71570d8223efd33f55",
      "venue": "EWAF",
      "journal": null,
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f065072c961028004a2029fadb4402085084d535",
      "title": "The Intersectionality Problem for Algorithmic Fairness",
      "authors": [
        {
          "name": "Johannes Himmelreich",
          "authorId": "2329187919"
        },
        {
          "name": "Arbie Hsu",
          "authorId": "2329185104"
        },
        {
          "name": "Kristian Lum",
          "authorId": "2329185564"
        },
        {
          "name": "Ellen Veomett",
          "authorId": "2307085482"
        }
      ],
      "year": 2024,
      "abstract": "A yet unmet challenge in algorithmic fairness is the problem of intersectionality, that is, achieving fairness across the intersection of multiple groups -- and verifying that such fairness has been attained. Because intersectional groups tend to be small, verifying whether a model is fair raises statistical as well as moral-methodological challenges. This paper (1) elucidates the problem of intersectionality in algorithmic fairness, (2) develops desiderata to clarify the challenges underlying the problem and guide the search for potential solutions, (3) illustrates the desiderata and potential solutions by sketching a proposal using simple hypothesis testing, and (4) evaluates, partly empirically, this proposal against the proposed desiderata.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2411.02569",
      "arxivId": "2411.02569",
      "url": "https://www.semanticscholar.org/paper/f065072c961028004a2029fadb4402085084d535",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2411.02569"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "840939ae1b059c436fbee4e6f9100aad84254cf4",
      "title": "Rawlsian Algorithmic Fairness and a Missing Aggregation Property of the Difference Principle",
      "authors": [
        {
          "name": "Ulrik Franke",
          "authorId": "2273573257"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 3,
      "doi": "10.1007/s13347-024-00779-z",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/840939ae1b059c436fbee4e6f9100aad84254cf4",
      "venue": "Philosophy & Technology",
      "journal": {
        "name": "Philosophy & Technology",
        "volume": "37"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a0584af4c029ecd8410a5cafadcea5ca9366386e",
      "title": "A Framework for Defining Algorithmic Fairness in the Context of Information Access",
      "authors": [
        {
          "name": "E. Udoh",
          "authorId": "40354671"
        },
        {
          "name": "Xiaojun (Jenny) Yuan",
          "authorId": "1410318900"
        },
        {
          "name": "Abebe Rorissa",
          "authorId": "2594683"
        }
      ],
      "year": 2024,
      "abstract": "As technologies powered by Artificial Intelligence (AI) and Machine Learning (ML) algorithms increasingly take over personal computing online and public sector domains, they simultaneously raise the promise of an extensively productive and sustainable future, as well as fears of widening inequalities, information and content divide, and a more complex information\u2010seeking landscape. Thus, the hopes of improved accuracy, efficiency, productivity, reduced human bias in decision\u2010making, and access to information are fast giving way to a trove of ethical and human rights issues with far\u2010reaching consequences for accountability, privacy, social justice, equity, inclusion, and informed consent, and public participation in decision\u2010making. Since no technology is entirely free of bias, this paper identifies algorithmic fairness as a more realistic threshold and goal. Building on findings from a previous PRISMA review of relevant literature, the paper proposes a comprehensive framework for defining algorithmic fairness in the context of information access.",
      "citationCount": 1,
      "doi": "10.1002/pra2.1077",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a0584af4c029ecd8410a5cafadcea5ca9366386e",
      "venue": "Proceedings of the Association for Information Science and Technology",
      "journal": {
        "name": "Proceedings of the Association for Information Science and Technology",
        "volume": "61"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "2a1e8bbb7594a2bedf42299d89901e0911fdee9d",
      "title": "Algorithmic Fairness: A Tolerance Perspective",
      "authors": [
        {
          "name": "Renqiang Luo",
          "authorId": "2298756911"
        },
        {
          "name": "Tao Tang",
          "authorId": "2349557940"
        },
        {
          "name": "Feng Xia",
          "authorId": "2296720850"
        },
        {
          "name": "Jiaying Liu",
          "authorId": "2108357538"
        },
        {
          "name": "Chengpei Xu",
          "authorId": "2299404065"
        },
        {
          "name": "Leo Yu Zhang",
          "authorId": "2301411965"
        },
        {
          "name": "Wei Xiang",
          "authorId": "2301313346"
        },
        {
          "name": "Chengqi Zhang",
          "authorId": "2296787002"
        }
      ],
      "year": 2024,
      "abstract": "Recent advancements in machine learning and deep learning have brought algorithmic fairness into sharp focus, illuminating concerns over discriminatory decision making that negatively impacts certain individuals or groups. These concerns have manifested in legal, ethical, and societal challenges, including the erosion of trust in intelligent systems. In response, this survey delves into the existing literature on algorithmic fairness, specifically highlighting its multifaceted social consequences. We introduce a novel taxonomy based on 'tolerance', a term we define as the degree to which variations in fairness outcomes are acceptable, providing a structured approach to understanding the subtleties of fairness within algorithmic decisions. Our systematic review covers diverse industries, revealing critical insights into the balance between algorithmic decision making and social equity. By synthesizing these insights, we outline a series of emerging challenges and propose strategic directions for future research and policy making, with the goal of advancing the field towards more equitable algorithmic systems.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2405.09543",
      "arxivId": "2405.09543",
      "url": "https://www.semanticscholar.org/paper/2a1e8bbb7594a2bedf42299d89901e0911fdee9d",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2405.09543"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "5544f2cafe8cf7641a6ac253e322308717ce4c8b",
      "title": "The problem of fairness in tools for algorithmic fairness",
      "authors": [
        {
          "name": "Xiao-yu Sun",
          "authorId": "2268433017"
        },
        {
          "name": "Bin Ye",
          "authorId": "2194275294"
        },
        {
          "name": "Bao-hua Xia",
          "authorId": "2314035244"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/s43681-024-00533-3",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/5544f2cafe8cf7641a6ac253e322308717ce4c8b",
      "venue": "AI and Ethics",
      "journal": {
        "name": "AI and Ethics",
        "pages": "1847 - 1857",
        "volume": "5"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c9abfd38896423b0b2bf11ba08526048bc97c094",
      "title": "A Systems Thinking Approach to Algorithmic Fairness",
      "authors": [
        {
          "name": "Chris Lam",
          "authorId": "2336875573"
        }
      ],
      "year": 2024,
      "abstract": "Systems thinking provides us with a way to model the algorithmic fairness problem by allowing us to encode prior knowledge and assumptions about where we believe bias might exist in the data generating process. We can then encode these beliefs as a series of causal graphs, enabling us to link AI/ML systems to politics and the law. This allows us to combine techniques from machine learning, causal inference, and system dynamics in order to capture different emergent aspects of the fairness problem. We can use systems thinking to help policymakers on both sides of the political aisle to understand the complex trade-offs that exist from different types of fairness policies, providing a sociotechnical foundation for designing AI policy that is aligned to their political agendas and with society's shared democratic values.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2412.16641",
      "arxivId": "2412.16641",
      "url": "https://www.semanticscholar.org/paper/c9abfd38896423b0b2bf11ba08526048bc97c094",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2412.16641"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "165c904a8b76ced6b8c5fee48fec35cb9f0e6996",
      "title": "Causal Equal Protection as Algorithmic Fairness",
      "authors": [
        {
          "name": "M. Bello",
          "authorId": "2056381967"
        },
        {
          "name": "N. Cangiotti",
          "authorId": "102968595"
        },
        {
          "name": "Michele Loi",
          "authorId": "2311696127"
        }
      ],
      "year": 2024,
      "abstract": "By combining the philosophical literature on statistical evidence and the interdisciplinary literature on algorithmic fairness, we revisit recent objections against classification parity in light of causal analyses of algorithmic fairness and the distinction between predictive and diagnostic evidence. We focus on trial proceedings as a black-box classification algorithm in which defendants are sorted into two groups by convicting or acquitting them. We defend a novel principle, causal equal protection, that combines classification parity with the causal approach. In the do-calculus, causal equal protection requires that individuals should not be subject to uneven risks of classification error because of their protected or socially salient characteristics. The explicit use of protected characteristics, however, may be required if it equalizes these risks.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2402.12062",
      "arxivId": "2402.12062",
      "url": "https://www.semanticscholar.org/paper/165c904a8b76ced6b8c5fee48fec35cb9f0e6996",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.12062"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "fc86c9a402c2b5bf461b7cc2733bd324fcb3b809",
      "title": "Decolonial Artificial Intelligence; Algorithmic Fairness in Alignment with Turkish and Islamic Values",
      "authors": [
        {
          "name": "Yusuf F\u0131r\u0131nc\u0131",
          "authorId": "2337311598"
        }
      ],
      "year": 2024,
      "abstract": "Research works on topics such as; Fairness in Agreement with European Values: An Interdisciplinary Perspective on AI Regulation or Decolonial AI Alignment: Openness, Vi\u015besa -Dharma and Including Excluded Knowledges, generated the motivation to search for Artificial intelligence (AI) alignment with Turkish and Islamic Values. The driving force to this research work is the fact that all of the algorithmic decision-making systems include bias to some extend and the non-western world needs to construct its own value-based technological and sociological development models since there is not much belief left in the so-called international justice or the so-called democratic values. This research includes examination of brief information about the fundamentals of big data, algorithms, and artificial intelligence. Importance of thick data and digital anthropology is emphasized. Misuse and abuse AI has been identified as one of the most important challenges. Vygotsky\u2019s arguments on social learning, social construction of technology theory and worldview theory may provide some of the arguments to construct the idea of an AI approach may be developed in accordance with Turkish and Islamic values. Decolonial AI arguments and fairness in AI approaches have also been utilized to empower our argument. Lastly, brief information on Turkish and Islamic values were presented, limited to the scope of this research.",
      "citationCount": 2,
      "doi": "10.15370/maruifd.1565884",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/fc86c9a402c2b5bf461b7cc2733bd324fcb3b809",
      "venue": "Marmara \u00dcniversitesi \u0130lahiyat Fak\u00fcltesi Dergisi",
      "journal": {
        "name": "Marmara \u00dcniversitesi \u0130lahiyat Fak\u00fcltesi Dergisi"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5cc5be85c81b0379a009cf518f1f0e37800c3fa3",
      "title": "Broomean(ish) Algorithmic Fairness?",
      "authors": [
        {
          "name": "Clinton Castro",
          "authorId": "2335585030"
        }
      ],
      "year": 2024,
      "abstract": "Recently, there has been much discussion of \u2018fair machine learning\u2019: fairness in data\u2010driven decision\u2010making systems (which are often, though not always, made with assistance from machine learning systems). Notorious impossibility results show that we cannot have everything we want here. Such problems call for careful thinking about the foundations of fair machine learning. Sune Holm has identified one promising way forward, which involves applying John Broome's theory of fairness to the puzzles of fair machine learning. Unfortunately, his application of Broome's theory appears to be fatally flawed. This article attempts to rescue Holm's central insight\u2009\u2013\u2009namely, that Broome's theory can be useful to the study of fair machine learning\u2009\u2013\u2009by giving an alternative application of Broome's theory, which involves thinking about fair machine learning in counterfactual (as opposed to merely statistical) terms.",
      "citationCount": 3,
      "doi": "10.1111/japp.12778",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/5cc5be85c81b0379a009cf518f1f0e37800c3fa3",
      "venue": "Journal of Applied Philosophy",
      "journal": {
        "name": "Journal of Applied Philosophy"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4d257a814c1ed31b9a50d6868687486d45f455ec",
      "title": "Beyond Algorithmic Fairness: A Guide to Develop and Deploy Ethical AI-Enabled Decision-Support Tools",
      "authors": [
        {
          "name": "Rosemarie Santa Gonzalez",
          "authorId": "2324797653"
        },
        {
          "name": "Ryan Piansky",
          "authorId": "2159545342"
        },
        {
          "name": "Sue M Bae",
          "authorId": "2321690844"
        },
        {
          "name": "Justin Biddle",
          "authorId": "2321586646"
        },
        {
          "name": "Daniel K. Molzahn",
          "authorId": "2290916189"
        }
      ],
      "year": 2024,
      "abstract": "The integration of artificial intelligence (AI) and optimization hold substantial promise for improving the efficiency, reliability, and resilience of engineered systems. Due to the networked nature of many engineered systems, ethically deploying methodologies at this intersection poses challenges that are distinct from other AI settings, thus motivating the development of ethical guidelines tailored to AI-enabled optimization. This paper highlights the need to go beyond fairness-driven algorithms to systematically address ethical decisions spanning the stages of modeling, data curation, results analysis, and implementation of optimization-based decision support tools. Accordingly, this paper identifies ethical considerations required when deploying algorithms at the intersection of AI and optimization via case studies in power systems as well as supply chain and logistics. Rather than providing a prescriptive set of rules, this paper aims to foster reflection and awareness among researchers and encourage consideration of ethical implications at every step of the decision-making process.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2409.11489",
      "arxivId": "2409.11489",
      "url": "https://www.semanticscholar.org/paper/4d257a814c1ed31b9a50d6868687486d45f455ec",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2409.11489"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8c2fd89a010d72db0cf14599b24e7ef582112c0c",
      "title": "If the Difference Principle Won\u2019t Make a Real Difference in Algorithmic Fairness, What Will?",
      "authors": [
        {
          "name": "Reuben Binns",
          "authorId": "144147883"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 3,
      "doi": "10.1007/s13347-024-00805-0",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/8c2fd89a010d72db0cf14599b24e7ef582112c0c",
      "venue": "Philosophy & Technology",
      "journal": {
        "name": "Philosophy & Technology",
        "volume": "37"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "98f2ee52e1bdb5c33166797c1489a5542de9d1da",
      "title": "Reconciling Algorithmic Fairness Criteria",
      "authors": [],
      "year": 2023,
      "abstract": null,
      "citationCount": 25,
      "doi": "10.1111/papa.12233",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/98f2ee52e1bdb5c33166797c1489a5542de9d1da",
      "venue": "Philosophy &amp; Public Affairs",
      "journal": {
        "name": "Philosophy &amp; Public Affairs"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "be97ded65c252c33471c2ec36f3a2b78dbebdd16",
      "title": "Finding Algorithmic Fairness: An Analysis of how the Literature Constructs Algorithmic Fairness from Different Stakeholder Perspectives",
      "authors": [
        {
          "name": "Stefanie Knippschild",
          "authorId": "2301463851"
        },
        {
          "name": "S. Boell",
          "authorId": "1857412"
        },
        {
          "name": "Kai Riemer",
          "authorId": "2277607901"
        },
        {
          "name": "Sandra Peter",
          "authorId": "2277917124"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/be97ded65c252c33471c2ec36f3a2b78dbebdd16",
      "venue": "ACIS",
      "journal": null,
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7982c69ae79557029ee91ffc959f6b7d60025b4f",
      "title": "Finding Algorithmic Fairness: An Analysis of how Literature Considers Algorithmic Fairness from Different Stakeholder Perspectives",
      "authors": [
        {
          "name": "Stefanie Knippschild",
          "authorId": "2301463851"
        },
        {
          "name": "S. Boell",
          "authorId": "1857412"
        },
        {
          "name": "Kai Riemer",
          "authorId": "2277607901"
        },
        {
          "name": "Sandra Peter",
          "authorId": "2277917124"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7982c69ae79557029ee91ffc959f6b7d60025b4f",
      "venue": "Americas Conference on Information Systems",
      "journal": null,
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2c611169b487bc3e5b887e32dc9d6ebc3a6aa77f",
      "title": "From Counterfactual Fairness to Algorithmic Fairness: Building Principle of Equity in AI Management",
      "authors": [
        {
          "name": "Maggie Minghui Cheng",
          "authorId": "2311310907"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.5465/amproc.2024.17746abstract",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/2c611169b487bc3e5b887e32dc9d6ebc3a6aa77f",
      "venue": "Academy of Management Proceedings",
      "journal": {
        "name": "Academy of Management Proceedings"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "02513bc79ccd26838cea3af4144b102cd4cb9e2f",
      "title": "Engaging an advisory board in discussions about the ethical relevance of algorithmic bias and fairness",
      "authors": [
        {
          "name": "Laura Brandt",
          "authorId": "2238559575"
        },
        {
          "name": "Larry Au",
          "authorId": "2276213840"
        },
        {
          "name": "Clinton Castro",
          "authorId": "2238943557"
        },
        {
          "name": "Gabriel J Odom",
          "authorId": "2193971718"
        }
      ],
      "year": 2025,
      "abstract": "We are an interdisciplinary team of researchers that are working to advance algorithmic fairness in the research of opioid use disorders. We discuss challenges that our research team faced when engaging with our Advisory Board, as well as several strategies that we came up with to help us find a common language to ensure semantic transparency and to ensure the thick alignment with values of affected parties.",
      "citationCount": 1,
      "doi": "10.1038/s41746-025-01711-1",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/02513bc79ccd26838cea3af4144b102cd4cb9e2f",
      "venue": "npj Digital Medicine",
      "journal": {
        "name": "NPJ Digital Medicine",
        "volume": "8"
      },
      "publicationTypes": [
        "Review",
        "JournalArticle"
      ]
    },
    {
      "paperId": "94eb358a2ebc53fecf94d7bf27b6021793587dd8",
      "title": "Beyond \u201cFairness\u201d: Rethinking the Use of Algorithmic Predictions in Criminal Justice",
      "authors": [
        {
          "name": "Tashmia Sabera",
          "authorId": "2129654317"
        }
      ],
      "year": 2025,
      "abstract": "This paper critiques the widespread use of predictive\nalgorithmic tools in criminal justice, such as COMPAS,\narguing that concerns about fairness and accuracy, while\nimportant, fail to address a deeper ethical issue: the\ninfringement of the right to be treated as an individual.\nDrawing on Renee Jorgensen\u2019s work, I argue that\nfairness-based reforms are insufficient because predictive\npunishment is incompatible with the demands of negative\nretributivism, the most compatible theory of punishment\nwith the demands of the right to be treated as an\nindividual. Given the high stakes of criminal law and the\ninherent trade-off in algorithmic fairness metrics, I\ncontend that algorithmic predictions should not be used to\njustify punishment or policing decisions. However, I\npropose that algorithmic tools can be ethically employed in\ndeveloping policies aimed at crime reduction, provided they\nare used to identify causal factors rather than to predict\nindividual behavior. To this end, I advocate a pluralist\nframework: negative retributivism should govern punishment\nand policing, while rights-based consequentialism should\ninform long-term policy goals. This approach aims to\nclarify when the use of algorithms in criminal justice is\nunjustified, and when it may be justified with critical\nrevision.",
      "citationCount": 0,
      "doi": "10.1609/aies.v8i3.36709",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/94eb358a2ebc53fecf94d7bf27b6021793587dd8",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "39dfd0187fd0eeb50487ff32b964783e7f34463d",
      "title": "Equalized odds is a requirement of algorithmic fairness",
      "authors": [
        {
          "name": "David Gray Grant",
          "authorId": "2210869150"
        }
      ],
      "year": 2023,
      "abstract": null,
      "citationCount": 15,
      "doi": "10.1007/s11229-023-04054-0",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/39dfd0187fd0eeb50487ff32b964783e7f34463d",
      "venue": "Synthese",
      "journal": {
        "name": "Synthese",
        "pages": "1-25",
        "volume": "201"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "88ec635afe17e9d3c5259764a076bacb822b23bf",
      "title": "How Much Effort Is Enough? Fairness in Algorithmic Recourse Through the Lens of Substantive Equality of Opportunity",
      "authors": [
        {
          "name": "Andrew Bell",
          "authorId": "2239195821"
        },
        {
          "name": "Jo\u00e3o Fonseca",
          "authorId": "2239195588"
        },
        {
          "name": "Carlo Abrate",
          "authorId": "89449460"
        },
        {
          "name": "Francesco Bonchi",
          "authorId": "2179558887"
        },
        {
          "name": "Julia Stoyanovich",
          "authorId": "2281825322"
        }
      ],
      "year": 2025,
      "abstract": "Algorithmic recourse, or enabling individuals to reverse a negative outcome, has gained attention as a means of supporting human agency in interactions with artificial intelligence (AI) systems. However, recent work has shown that even if a decision-making classifier is fair according to reasonable criteria, recourse itself may be unfair. Members of disadvantaged groups may have to work harder than their more privileged peers to reverse a negative outcome. In this paper, we introduce effort-aware fairness, which treats algorithmic recourse through the lens of substantive equality of opportunity. It acknowledges that individuals from different groups may not be comparably well-equipped to act on their recourse, resulting in disparities in their chances of reversing unfavorable outcomes. These disparities, shaped by differing effort distributions among demographic groups, can be exacerbated over time. We provide a formal definition of effort-aware fairness, propose fairness metrics, and then develop an intervention that improves recourse fairness by rewarding effort. Through empirical comparison with existing strategies, we demonstrate that this intervention successfully mitigates disparities. Our conceptual framework and experimental evaluation build upon prior work that uses an agent-based model for simulating real-world recourse over time.",
      "citationCount": 0,
      "doi": "10.1145/3757887.3763014",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/88ec635afe17e9d3c5259764a076bacb822b23bf",
      "venue": "Proceedings of the 5th ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",
      "journal": {
        "name": "Proceedings of the 5th ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization"
      },
      "publicationTypes": [
        "Book",
        "Conference"
      ]
    },
    {
      "paperId": "9bcda3b85ad842fb980f3d0f33f6f1867328c69a",
      "title": "Ethical Considerations in AI: Bias Mitigation and Fairness in Algorithmic Decision Making",
      "authors": [
        {
          "name": "Grace Sullivan",
          "authorId": "2396830843"
        },
        {
          "name": "Thomas Richardson",
          "authorId": "2396875294"
        }
      ],
      "year": 2025,
      "abstract": "The rapid integration of artificial intelligence (AI) into critical decision-making domains\u2014such as healthcare, finance, law enforcement, and hiring\u2014has raised significant ethical concerns regarding bias and fairness. Algorithmic decision-making systems, if not carefully designed and monitored, risk perpetuating and amplifying societal biases, leading to unfair and discriminatory outcomes. This paper explores the ethical considerations surrounding AI, focusing on bias mitigation and fairness in algorithmic systems. We examine the sources of bias in AI models, including biased training data, algorithmic design choices, and systemic inequities. Furthermore, we review existing approaches to bias mitigation, such as fairness-aware machine learning techniques, adversarial debiasing, and regulatory frameworks that promote transparency and accountability. The paper also discusses the trade-offs between fairness, accuracy, and interpretability, emphasizing the need for interdisciplinary collaboration to develop ethical AI systems. By analyzing current challenges and emerging solutions, this study provides a roadmap for responsible AI development that prioritizes fairness, reduces bias, and fosters trust in automated decision-making.",
      "citationCount": 0,
      "doi": "10.65521/ijacte.v12i2.112",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/9bcda3b85ad842fb980f3d0f33f6f1867328c69a",
      "venue": "International Journal on Advanced Computer Theory and Engineering",
      "journal": {
        "name": "International Journal on Advanced Computer Theory and Engineering"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    }
  ],
  "count": 40,
  "errors": []
}
