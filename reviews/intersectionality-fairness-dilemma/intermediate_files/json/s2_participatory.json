{
  "status": "success",
  "source": "semantic_scholar",
  "query": "participatory fairness assessment",
  "results": [
    {
      "paperId": "da9532f6c20078427e11223d5a4c311d5cc1194d",
      "title": "How fair are we? From conceptualization to automated assessment of fairness definitions",
      "authors": [
        {
          "name": "Giordano d\u2019Aloisio",
          "authorId": "2170270823"
        },
        {
          "name": "Claudio Di Sipio",
          "authorId": "1644891552"
        },
        {
          "name": "A. Marco",
          "authorId": "8496299"
        },
        {
          "name": "D. D. Ruscio",
          "authorId": "2133181"
        }
      ],
      "year": 2024,
      "abstract": "\n Fairness is a critical concept in ethics and social domains, but it is also a challenging property to engineer in software systems. With the increasing use of machine learning in software systems, researchers have been developing techniques to assess the fairness of software systems automatically. Nonetheless, many of these techniques rely upon pre-established fairness definitions, metrics, and criteria, which may fail to encompass the wide-ranging needs and preferences of users and stakeholders. To overcome this limitation, we propose a novel approach, called MODNESS, that enables users to customize and define their fairness concepts using a dedicated modeling environment. Our approach guides the user through the definition of new fairness concepts also in emerging domains, and the specification and composition of metrics for its evaluation through a dedicated domain-specific language. Ultimately, MODNESS generates the source code to implement fair assessment based on these custom definitions. In addition, we elucidate the process we followed to collect and analyze relevant literature on fairness assessment in software engineering (SE). We compare MODNESS with the selected approaches and evaluate how they support the distinguishing features identified by our study. Our findings reveal that i) most of the current approaches do not support user-defined fairness concepts; ii) our approach can cover additional application domains not addressed by currently available tools, e.g., mitigating bias in recommender systems for software engineering and Arduino software component recommendations; iii) MODNESS demonstrates the capability to overcome the limitations of the only two other model-driven engineering-based approaches for fairness assessment.\n\n",
      "citationCount": 8,
      "doi": "10.48550/arXiv.2404.09919",
      "arxivId": "2404.09919",
      "url": "https://www.semanticscholar.org/paper/da9532f6c20078427e11223d5a4c311d5cc1194d",
      "venue": "Journal of Software and Systems Modeling",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2404.09919"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "73e91176453ce5b1eb32a5d42a943b1f25e76d73",
      "title": "Group Fairness in Prediction-Based Decision Making: From Moral Assessment to Implementation",
      "authors": [
        {
          "name": "J. Baumann",
          "authorId": "2058322501"
        },
        {
          "name": "Christoph Heitz",
          "authorId": "151473392"
        }
      ],
      "year": 2022,
      "abstract": "Ensuring fairness of prediction-based decision making is based on statistical group fairness criteria. Which one of these criteria is the morally most appropriate one depends on the context, and its choice requires an ethical analysis. In this paper, we present a step-by-step procedure integrating three elements: (a) a framework for the moral assessment of what fairness means in a given context, based on the recently proposed general principle of \u201cFair equality of chances\u201d (FEC) (b) a mapping of the assessment's results to established statistical group fairness criteria, and (c) a method for integrating the thus-defined fairness into optimal decision making. As a second contribution, we show new applications of the FEC principle and show that, with this extension, the FEC framework covers all types of group fairness criteria: independence, separation, and sufficiency. Third, we introduce an extended version of the FEC principle, which additionally allows accounting for morally irrelevant elements of the fairness assessment and links to well-known relaxations of the fairness criteria. This paper presents a framework to develop fair decision systems in a conceptually sound way, combining the moral and the computational elements of fair prediction-based decision-making in an integrated approach.11Data and code to reproduce our results are available at https://github.com/joebaumann/fair-prediction-based-decision-making.",
      "citationCount": 8,
      "doi": "10.1109/SDS54800.2022.00011",
      "arxivId": "2210.10456",
      "url": "https://www.semanticscholar.org/paper/73e91176453ce5b1eb32a5d42a943b1f25e76d73",
      "venue": "Swiss Conference on Data Science",
      "journal": {
        "name": "2022 9th Swiss Conference on Data Science (SDS)",
        "pages": "19-25"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "50df4685c66abbe80716e7b31d83131c88a30a92",
      "title": "Through the lens of justice. A systematic review on equity and fairness in learning assessment",
      "authors": [
        {
          "name": "Debora Aquario",
          "authorId": "2081586508"
        }
      ],
      "year": 2021,
      "abstract": "Empirical and theoretical studies have highlighted the need to investigate the implications of the introduction of the issue of justice in education. However, little is known about the specific field of learning assessment and about the possible enhancement for assessment processes when inspired by discourses about justice, equity and fairness. What does it mean to rethink assessment through the lens of justice? The present paper aims to uncover key information related to this issue with the aim to provide greater understanding about how to build more equitable assessment practices. The PRISMA guidelines were adopted. Internet-based bibliographic searches were conducted via 2 major electronic databases (ERIC and Education Source) to access studies examining the association between the issues of justice, fairness and equity in assessment. A total of 26 empirical studies meeting the inclusion criteria were identified. The studies reported the attention to both well as a focus on addressing diversity in the classroom moving away from a model of adjustments and reasonable accommodations towards an equitable and universal assessment. Additional research is important to clarify these issues and an important effort should be made to construct better assessment practices based on students and teachers' perceptions of justice.",
      "citationCount": 3,
      "doi": "10.3280/ess2-2021oa12405",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/50df4685c66abbe80716e7b31d83131c88a30a92",
      "venue": "EDUCATION SCIENCES AND SOCIETY",
      "journal": {
        "name": "EDUCATION SCIENCES AND SOCIETY"
      },
      "publicationTypes": [
        "Review"
      ]
    },
    {
      "paperId": "510b8e309741994482850109ca8edff60875e463",
      "title": "Humans' Assessment of Robots as Moral Regulators: Importance of Perceived Fairness and Legitimacy",
      "authors": [
        {
          "name": "Boyoung Kim",
          "authorId": "2124858814"
        },
        {
          "name": "Elizabeth Phillips",
          "authorId": "145336542"
        }
      ],
      "year": 2021,
      "abstract": "Previous research has shown that the fairness and the le- gitimacy of a moral decision-maker are important for people\u2019s acceptance of and compliance with the decision-maker. As technology rapidly advances, there have been increasing hopes and concerns about building arti\ufb01cially intelligent en- tities that are designed to intervene against norm violations. However, it is unclear how people would perceive arti\ufb01cial moral regulators that impose punishment on human wrongdo-ers. Grounded in theories of psychology and law, we predict that the perceived fairness of punishment imposed by a robot would increase the legitimacy of the robot functioning as a moral regulator, which would in turn, increase people\u2019s willingness to accept and comply with the robot\u2019s decisions. We close with a conceptual framework for building a robot moral regulator that successfully can regulate norm violations.",
      "citationCount": 3,
      "doi": null,
      "arxivId": "2110.04729",
      "url": "https://www.semanticscholar.org/paper/510b8e309741994482850109ca8edff60875e463",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2110.04729"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "03cbf45a384627d1d7066ef1af2e50eef7e2cda7",
      "title": "Fairness in human judgement in assessment: a hermeneutic literature review and conceptual framework",
      "authors": [
        {
          "name": "Nyoli Valentine",
          "authorId": "22125985"
        },
        {
          "name": "S. Durning",
          "authorId": "3572700"
        },
        {
          "name": "E. M. Shanahan",
          "authorId": "1854569"
        },
        {
          "name": "L. Schuwirth",
          "authorId": "4145954"
        }
      ],
      "year": 2020,
      "abstract": null,
      "citationCount": 38,
      "doi": "10.1007/s10459-020-10002-1",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/03cbf45a384627d1d7066ef1af2e50eef7e2cda7",
      "venue": "Advances in health sciences education : theory and practice",
      "journal": {
        "name": "Advances in Health Sciences Education",
        "pages": "713-738",
        "volume": "26"
      },
      "publicationTypes": [
        "Review",
        "JournalArticle"
      ]
    },
    {
      "paperId": "66f9d714b1be1f66ba7b74360ae121a205715d39",
      "title": "Applying Foucault to Participatory Assessment in Higher Education: A Case Study in South Africa",
      "authors": [
        {
          "name": "C. Ramhurry",
          "authorId": "79426098"
        }
      ],
      "year": 2022,
      "abstract": "Assessment policy reform has led to the adoption of a \u201cparticipatory\u201d framework of assessment in South African higher education. Using a Foucauldian theoretical lens, this article explores the relation between participatory assessment practices in higher education and social control. Empirical evidence is drawn from assessment practices observed in certain lectures in a South African university and interviews with lecturers. Data is analysed through a Foucauldian lens that forges a connection between disciplinary power, control and regulation. The article then describes the technologies of disciplinary power that play out within the participatory assessment practices and demonstrates what these technologies of power do to assessors and students when they become involved in them. The article argues that participatory assessment in some respects epitomises progressive educational themes, yet, when studied with an eye towards power, it reveals deep contradictions and paradoxes.",
      "citationCount": 0,
      "doi": "10.25159/1947-9417/9453",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/66f9d714b1be1f66ba7b74360ae121a205715d39",
      "venue": "Education as Change",
      "journal": {
        "name": "Education as Change"
      },
      "publicationTypes": null
    },
    {
      "paperId": "aadcbbd31a1006d89e7c1bc4ac1c219b1e41547b",
      "title": "Fairness in human judgement in assessment: a hermeneutic literature review and conceptual framework",
      "authors": [
        {
          "name": "Nyoli Valentine",
          "authorId": "22125985"
        },
        {
          "name": "S. Durning",
          "authorId": "3572700"
        },
        {
          "name": "Ernst Michael Shanahan",
          "authorId": "2193493711"
        },
        {
          "name": "L. Schuwirth",
          "authorId": "4145954"
        }
      ],
      "year": 2020,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/s10459-020-10002-1",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/aadcbbd31a1006d89e7c1bc4ac1c219b1e41547b",
      "venue": "Advances in Health Sciences Education",
      "journal": {
        "name": "Advances in Health Sciences Education",
        "pages": "713 - 738",
        "volume": "26"
      },
      "publicationTypes": [
        "Review"
      ]
    },
    {
      "paperId": "65fcb35ba41a913df88cde99a866e37f7da290e3",
      "title": "Co-Producing AI: Toward an Augmented, Participatory Lifecycle",
      "authors": [
        {
          "name": "Rashid A. Mushkani",
          "authorId": "2089728714"
        },
        {
          "name": "Hugo Berard",
          "authorId": "2327334802"
        },
        {
          "name": "Toumadher Ammar",
          "authorId": "2329097638"
        },
        {
          "name": "Cassandre Chatonnier",
          "authorId": "2374403924"
        },
        {
          "name": "Shin Koseki",
          "authorId": "2327334397"
        }
      ],
      "year": 2025,
      "abstract": "Despite efforts to mitigate the inherent risks and biases of artificial intelligence (AI) algorithms, these algorithms can disproportionately impact culturally marginalized groups. A range of approaches has been proposed to address or reduce these risks, including the development of ethical guidelines and principles for responsible AI, as well as technical solutions that promote algorithmic fairness. Drawing on design justice, expansive learning theory, and recent empirical work on participatory AI, we argue that mitigating these harms requires a fundamental re\u2011architecture of the AI production pipeline. This re\u2011design should center co\u2011production, diversity, equity, inclusion (DEI), and multidisciplinary collaboration. We introduce an augmented AI lifecycle consisting of five interconnected phases: co\u2011framing, co\u2011design, co\u2011implementation, co\u2011deployment, and co\u2011maintenance. The lifecycle is informed by four multidisciplinary workshops and grounded in themes of distributed authority and iterative knowledge exchange. Finally, we relate the proposed lifecycle to several leading ethical frameworks and outline key research questions that remain for scaling participatory governance.",
      "citationCount": 1,
      "doi": "10.1609/aies.v8i2.36674",
      "arxivId": "2508.00138",
      "url": "https://www.semanticscholar.org/paper/65fcb35ba41a913df88cde99a866e37f7da290e3",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.00138"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "67bfc8d80a60b23bb1a8734df1b56c357a43ace0",
      "title": "Ethical Challenges in Data Science: Navigating the Complex Landscape of Responsibility and Fairness",
      "authors": [
        {
          "name": "Chiranjeevi Bura",
          "authorId": "2332100344"
        },
        {
          "name": "Srikanth Kamatala",
          "authorId": "2345457632"
        },
        {
          "name": "Praveen Kumar Myakala",
          "authorId": "2332100398"
        }
      ],
      "year": 2025,
      "abstract": "The rapid advancement of data science and artificial intelligence (AI) has revolutionized decision-making across multiple domains, including healthcare, finance, and law enforcement. However, these advancements come with pressing ethical challenges, such as algorithmic bias, data privacy risks, and lack of transparency. This paper systematically analyzes these ethical concerns, focusing on state-of-the-art methodologies for bias detection, explainable AI (XAI), and privacy-preserving techniques. We provide a comparative evaluation of ethical frameworks, including the ACM Code of Ethics, IEEE Ethically Aligned Design (EAD), and regulatory policies such as GDPR and CCPA. Through in-depth case studies examining biased hiring algorithms, risk assessment models in criminal justice, and data privacy concerns in smart technologies\u2014we highlight real-world implications of unethical AI. Furthermore, we propose a structured approach to bias mitigation, integrating fairness-aware machine learning, adversarial debiasing, and regulatory compliance measures. Our findings contribute to responsible AI governance by identifying best practices and technical solutions that promote fairness, accountability, and transparency in AI-driven systems.",
      "citationCount": 1,
      "doi": "10.47191/ijcsrr/v8-i3-09",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/67bfc8d80a60b23bb1a8734df1b56c357a43ace0",
      "venue": "International Journal of Current Science Research and Review",
      "journal": {
        "name": "International Journal of Current Science Research and Review"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0a49c157f2565a95187108b07ad2d8231bd77e86",
      "title": "A Feminist Account of Intersectional Algorithmic Fairness",
      "authors": [
        {
          "name": "Marie Mirsch",
          "authorId": "2377073672"
        },
        {
          "name": "Laila Wegner",
          "authorId": "2331068415"
        },
        {
          "name": "Jonas Strube",
          "authorId": "2376541652"
        },
        {
          "name": "C. Leicht-Scholten",
          "authorId": "2347733474"
        }
      ],
      "year": 2025,
      "abstract": "Intersectionality has profoundly influenced research and political action by revealing how interconnected systems of privilege and oppression influence lived experiences, yet its integration into algorithmic fairness research remains limited. Existing approaches often rely on single-axis or formal subgroup frameworks that risk oversimplifying social realities and neglecting structural inequalities. We propose Substantive Intersectional Algorithmic Fairness, extending Green's (2022) notion of substantive algorithmic fairness with insights from intersectional feminist theory. Building on this foundation, we introduce ten desiderata within the ROOF methodology to guide the design, assessment, and deployment of algorithmic systems in ways that address systemic inequities while mitigating harms to intersectionally marginalized communities. Rather than prescribing fixed operationalizations, these desiderata encourage reflection on assumptions of neutrality, the use of protected attributes, the inclusion of multiply marginalized groups, and enhancing algorithmic systems'potential. Our approach emphasizes that fairness cannot be separated from social context, and that in some cases, principled non-deployment may be necessary. By bridging computational and social science perspectives, we provide actionable guidance for more equitable, inclusive, and context-sensitive intersectional algorithmic practices.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.17944",
      "arxivId": "2508.17944",
      "url": "https://www.semanticscholar.org/paper/0a49c157f2565a95187108b07ad2d8231bd77e86",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.17944"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "39bf05cf1e3047d57b19d016dfd4a53e39e2b71f",
      "title": "Rawls\u2019s Justice as Fairness and Indonesian Health Policy: A Doctrinal Framework for Equity-Oriented Reform",
      "authors": [
        {
          "name": "Tony Richard Alexander Samosir",
          "authorId": "2393313075"
        },
        {
          "name": "Mei Susanto",
          "authorId": "2393310593"
        }
      ],
      "year": 2025,
      "abstract": "The study aims to analyze the application of John Rawls' Theory of Justice in Indonesia's health system policy, emphasizing how principles of justice can promote fairness and equality in the provision of health services, especially for disadvantaged populations.The method used was a judicial normative (doctrinal) approach. This study critically reviewed legal and philosophical texts relevant to the Indonesian health policy framework, integrating Rawls' theory as an analytical lens.The novelty of this research lies in the operationalization of Rawls' abstract philosophical principles into a structured evaluation framework (Rawlsian grid) and its application to the Indonesian socio-political and cultural context. This allows for a systematic assessment of inequalities in the health system and proposals for equitable policy solutions.The findings show that programs such as the National Health Insurance Scheme (JKN) have contributed to broader access to health care, but disparities remain in remote and underdeveloped areas. Using the Rawlsian grid, the study highlights challenges in budget allocation, health worker distribution, and infrastructure access. Recommended strategies include reallocating health budgets, fair distribution of health workers, expansion of telemedicine, and participatory planning.The concludes that integrating Rawlsian principles of justice through a structured evaluation framework can support a more inclusive and sustainable health system in Indonesia, ensuring fair and equal access to quality health services for all citizens.",
      "citationCount": 0,
      "doi": "10.33506/js.v12i1.4556",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/39bf05cf1e3047d57b19d016dfd4a53e39e2b71f",
      "venue": "JUSTISI",
      "journal": {
        "name": "JUSTISI"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "68968dd49dde6e99226529e4caed6f3fb76da899",
      "title": "Fairness-Aware Classification Based on Rawlsian Veil of Ignorance: A Mathematical Framework for Bias Detection and Mitigation in Machine Learning",
      "authors": [
        {
          "name": "Lin Chen",
          "authorId": "2397387049"
        }
      ],
      "year": 2025,
      "abstract": "Machine learning's penetration into high-stakes decision-making\u2014credit approvals, healthcare triage, criminal risk assessment\u2014has amplified pre-existing societal inequities rather than ameliorating them. This study operationalizes John Rawls's \"veil of ignorance\" (1971) as a computational principle for binary classifiers, confronting a gap: most fairness metrics lack philosophical grounding while Rawlsian theories remain mathematically unformalized. Through three empirical phases\u2014(1) baseline logistic regression on full feature sets, (2) bias quantification via disaggregated metrics across protected groups, and (3) mitigation via pre-processing blindess and post-processing threshold optimization\u2014we demonstrate how ignorance of demographic attributes can be algorithmically imposed. Using the German Credit Dataset (n=1,000), we expose a 12.9% accuracy gap between gender groups in standard models. Our framework collapses demographic parity difference from 15.7% to 0.1% while paradoxically boosting accuracy by 2.8% (from 72.0% to 74.0%), challenging the fairness-accuracy sacrifice orthodoxy. Counterintuitively, naive feature removal worsened bias (+8.9%), only proxy-aware pruning achieved DPD reduction of 32.6%. These findings suggest that Rawlsian principles, when translated into constrained optimization, yield Pareto-superior solutions\u2014though we argue such technical fixes must complement, not substitute for, institutional reform.",
      "citationCount": 0,
      "doi": "10.54097/v1xa7p32",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/68968dd49dde6e99226529e4caed6f3fb76da899",
      "venue": "Academic Journal of Science and Technology",
      "journal": {
        "name": "Academic Journal of Science and Technology"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "690b3d44dac29436e5e647712c01ec0faeb345b8",
      "title": "Epistemic Standards for Participatory Technology Assessment: Suggestions Based Upon Well-Ordered Science",
      "authors": [
        {
          "name": "J. M. Dur\u00e1n",
          "authorId": "144268177"
        },
        {
          "name": "Z. Pirtle",
          "authorId": "16708216"
        }
      ],
      "year": 2020,
      "abstract": "When one wants to use citizen input to inform policy, what should the standards of informedness on the part of the citizens be? While there are moral reasons to allow every citizen to participate and have a voice on every issue, regardless of education and involvement, designers of participatory assessments have to make decisions about how to structure deliberations as well as how much background information and deliberation time to provide to participants. After assessing different frameworks for the relationship between science and society, we use Philip Kitcher's framework of Well-Ordered Science to propose an epistemic standard on how citizen deliberations should be structured. We explore what potential standards follow from this epistemic framework focusing on significance versus scientific and engineering expertise. We argue that citizens should be tutored on the historical context of why scientific questions became significant and deemed scientifically and socially valuable, and if citizens report that they are capable of weighing in on an issue then they should be able to do so. We explore what this standard can mean by looking at actual citizen deliberations tied to the 2014 NASA ECAST Asteroid Initiative Citizen forums. We code different vignettes of citizens debating alternative approaches for Mars exploration based upon what level of information seemed to be sufficient for them to feel comfortable in making a policy position. The analysis provides recommendations on how to design and assess future citizen assessments grounded in properly conveying the historical value context surrounding a scientific issue and trusting citizens to seek out sufficient information to deliberate.",
      "citationCount": 11,
      "doi": "10.1007/s11948-020-00211-7",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/690b3d44dac29436e5e647712c01ec0faeb345b8",
      "venue": "Science and Engineering Ethics",
      "journal": {
        "name": "Science and Engineering Ethics",
        "pages": "1709 - 1741",
        "volume": "26"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "64cd2db9dcecd5023a81083d4e5bbbd43b3223f5",
      "title": "Ethical Aspects of Measuring Intelligence: Towards Competence and Fairness",
      "authors": [
        {
          "name": "T. Logvinenko",
          "authorId": "2265012918"
        },
        {
          "name": "Tatiana N. Kanonir",
          "authorId": "2388319559"
        },
        {
          "name": "E. Orel",
          "authorId": "2141162067"
        },
        {
          "name": "Alena A. Kulikova",
          "authorId": "2254586671"
        }
      ],
      "year": 2024,
      "abstract": "The article is focused on the problem of intelligence measurement, with an emphasis on the ethical aspects of developing and using tests. The history of intelligence measurement provides a variety of examples, problematic from an ethical point of view, which have repeatedly led to negative consequences for both individuals and entire communities. The purpose of this article is to describe current ethical issues in the field of intelligence measurement, their background and historical examples. We discuss the ethical issues in terms of: (1) global approaches to operationalizing intelligence; 2) possible human rights violations resulting from the use of intelligence tests; 3) the fairness of intelligence tests for different groups of respondents; and 4) assessment of test quality in test selection. These issues are examined through the prism of the ethical principles of psychologists, such as respect, honesty, competence, and responsibility. Despite the extensive history of measuring intelligence and research in this area, ethical issues raised decades ago have not lost their relevance. Since ethical questions often do not have clear-cut answers, we believe that engaging in discussions about ethical issues in intelligence testing and exploring potential solutions is itself important and warranted. The content and conclusions of this article may be useful for both researchers and practitioners to make informed decisions in the context of intelligence measurement.",
      "citationCount": 0,
      "doi": "10.22363/2313-1683-2024-21-2-657-682",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/64cd2db9dcecd5023a81083d4e5bbbd43b3223f5",
      "venue": "RUDN Journal of Psychology and Pedagogics",
      "journal": {
        "name": "RUDN Journal of Psychology and Pedagogics"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "899aa4a3f21f4b300f3fb890fbb04efa56f1520b",
      "title": "Understanding Fairness Requirements for ML-based Software",
      "authors": [
        {
          "name": "Luciano Baresi",
          "authorId": "2242736209"
        },
        {
          "name": "Chiara Criscuolo",
          "authorId": "2239390808"
        },
        {
          "name": "C. Ghezzi",
          "authorId": "2248812920"
        }
      ],
      "year": 2023,
      "abstract": "Today's technologies are becoming more and more pervasive and advanced software systems can replace human beings in many different tasks. This is especially true in the case of automated decision-making systems based on machine learning (ML). Important ethical implications arise when such decision systems are used in sensitive contexts (e.g., justice or loans). The elicitation of these implications, that is, of the ethical requirements behind ML-based systems is a new challenge we must address to avoid societal risks. This is particularly urgent for fairness since this notion lacks a precise and commonly accepted definition, thus hampering its assessment. This paper aims to give a comprehensive definition of fairness, present a unified taxonomy of alternative interpretations, define a new decision tree that can guide the choice of the correct interpretation, and carry out a preliminary assessment with experiments in a real-world context.",
      "citationCount": 15,
      "doi": "10.1109/RE57278.2023.00046",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/899aa4a3f21f4b300f3fb890fbb04efa56f1520b",
      "venue": "IEEE International Requirements Engineering Conference",
      "journal": {
        "name": "2023 IEEE 31st International Requirements Engineering Conference (RE)",
        "pages": "341-346"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "a8b8c2e8d270270e3ec0ed20b731ce6cc5298608",
      "title": "Assessment in Islamic Perspective: Balancing Knowledge, Intention Character, and Accountability",
      "authors": [
        {
          "name": "Dr. Sheeba Zafar",
          "authorId": "2372513212"
        }
      ],
      "year": 2025,
      "abstract": "The said paper regarding the assessment in Islamic perspective:\u00a0 Balancing Knowledge, Character and Accountability.\u00a0 Further the assessment in Islamic education extends beyond measuring cognitive achievement; it incorporates a holistic framework that emphasizes intellectual development (\u02bfilm), moral character (akhl\u0101q), intention (niyyah), and accountability (mu\u1e25\u0101sabah).\u00a0 That is rooted on the principals of fairness, ethics and justice. The Islamic assessment cultivates the academic competency and spiritual competency in the learner and teacher.\u00a0 The present study is qualitative and thematic analysis approaches were used, while taking the faculty from Islamic studies department of Higher educational instructions. The senior academia with more than ten years of teaching experience with the doctorate of education was taken for the opinion. The findings, underscore the reforms of the capacity building, development of the framework on the research issue were proposed and further it was recommended that, the Islamic vision need to align with the holistic human development.",
      "citationCount": 1,
      "doi": "10.59075/h8npr269",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a8b8c2e8d270270e3ec0ed20b731ce6cc5298608",
      "venue": "The Critical Review of Social Sciences Studies",
      "journal": {
        "name": "The Critical Review of Social Sciences Studies"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0a8ef5ecb3d4bd0a14b1cc46fda1cb86e5f75a92",
      "title": "Ethics in educational assessment: emerging issues, controversies, and best practices",
      "authors": [
        {
          "name": "Eme Orok Iban Amanso",
          "authorId": "2402990669"
        },
        {
          "name": "Valentine Joseph Owan,",
          "authorId": "2183430171"
        }
      ],
      "year": 2025,
      "abstract": "\u00a0 \nEthical issues in educational assessment have become increasingly significant as schools and educational authorities seek fairness, accountability, and credibility in evaluation processes. This paper discusses the moral dimensions that guide assessment practices and how unethical conduct can weaken the quality and trustworthiness of educational results. Key issues such as bias, validity, reliability, authenticity, confidentiality, privacy, and the rights of test-takers were discussed. Each issue is explored in relation to its practical implications for teachers, learners, and policymakers. The discussion extends to emerging concerns, such as test preparation and coaching, technology-based assessment, inclusion and diversity, social and emotional learning, as well as high-stakes and standardised testing. The paper draws on recent literature and professional codes to explain how these matters influence the fairness and credibility of assessment practices. It argues that ethical awareness and integrity are essential for educators and evaluators to uphold justice and objectivity in measuring learners\u2019 performance. It was suggested that ethical guidelines and best practices that can support fairness, transparency, and accountability in educational assessment be provided. When applied with commitment and honesty, these principles can enhance trust in educational outcomes and contribute to a more just and responsible assessment culture.",
      "citationCount": 0,
      "doi": "10.4314/gjedr.v24i4.4",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/0a8ef5ecb3d4bd0a14b1cc46fda1cb86e5f75a92",
      "venue": "Global Journal of Educational Research",
      "journal": {
        "name": "Global Journal of Educational Research"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d77d439ba2642c462ca4c0bed5071adee674beda",
      "title": "The values of \u2018knowing\u2019 (the future) : Insights from a participatory ethical assessment of emergent biomarkers for AD diagnostics",
      "authors": [
        {
          "name": "K. Nielsen",
          "authorId": "32590647"
        },
        {
          "name": "M. Boenink",
          "authorId": "5913971"
        }
      ],
      "year": 2018,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/d77d439ba2642c462ca4c0bed5071adee674beda",
      "venue": "",
      "journal": {
        "name": "",
        "volume": ""
      },
      "publicationTypes": null
    },
    {
      "paperId": "f69fcf3f3e6d826de921df51b4982e57effde86f",
      "title": "Where Fact Ends and Fairness Begins: Redefining AI Bias Evaluation through Cognitive Biases",
      "authors": [
        {
          "name": "Jen-Tse Huang",
          "authorId": "2161306685"
        },
        {
          "name": "Yuhang Yan",
          "authorId": "2344870184"
        },
        {
          "name": "Linqi Liu",
          "authorId": "2344922432"
        },
        {
          "name": "Yixin Wan",
          "authorId": "2165227666"
        },
        {
          "name": "Wenxuan Wang",
          "authorId": "2336729638"
        },
        {
          "name": "Kai-Wei Chang",
          "authorId": "2284691768"
        },
        {
          "name": "Michael R. Lyu",
          "authorId": "2146840128"
        }
      ],
      "year": 2025,
      "abstract": "Recent failures such as Google Gemini generating people of color in Nazi-era uniforms illustrate how AI outputs can be factually plausible yet socially harmful. AI models are increasingly evaluated for\"fairness,\"yet existing benchmarks often conflate two fundamentally different dimensions: factual correctness and normative fairness. A model may generate responses that are factually accurate but socially unfair, or conversely, appear fair while distorting factual reality. We argue that identifying the boundary between fact and fair is essential for meaningful fairness evaluation. We introduce Fact-or-Fair, a benchmark with (i) objective queries aligned with descriptive, fact-based judgments, and (ii) subjective queries aligned with normative, fairness-based judgments. Our queries are constructed from 19 statistics and are grounded in cognitive psychology, drawing on representativeness bias, attribution bias, and ingroup-outgroup bias to explain why models often misalign fact and fairness. Experiments across ten frontier models reveal different levels of fact-fair trade-offs. By reframing fairness evaluation, we provide both a new theoretical lens and a practical benchmark to advance the responsible model assessments. Our test suite is publicly available at https://github.com/uclanlp/Fact-or-Fair.",
      "citationCount": 2,
      "doi": "10.18653/v1/2025.findings-emnlp.583",
      "arxivId": "2502.05849",
      "url": "https://www.semanticscholar.org/paper/f69fcf3f3e6d826de921df51b4982e57effde86f",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "Findings of the Association for Computational Linguistics: EMNLP 2025"
      },
      "publicationTypes": null
    },
    {
      "paperId": "fc7f91d95b88689314617e08b784044061c45e3d",
      "title": "Algorithmic Fairness and Feasibility",
      "authors": [
        {
          "name": "Eva Erman",
          "authorId": "2338612784"
        },
        {
          "name": "Markus Furendal",
          "authorId": "113631409"
        },
        {
          "name": "Niklas M\u00f6ller",
          "authorId": "145497600"
        }
      ],
      "year": 2025,
      "abstract": "The \u201cimpossibility results\u201d in algorithmic fairness suggest that a predictive model cannot fully meet two common fairness criteria \u2013 sufficiency and separation \u2013 except under extraordinary circumstances. These findings have sparked a discussion on fairness in algorithms, prompting debates over whether predictive models can avoid unfair discrimination based on protected attributes, such as ethnicity or gender. As shown by Otto Sahlgren, however, the discussion of the impossibility results would gain from importing some of the tools developed in the philosophical literature on feasibility. Utilizing these tools, Sahlgren sketches a cautiously optimistic view of how algorithmic fairness can be made feasible in restricted local decision-making. While we think it is a welcome move to inject the literature on feasibility into the debate on algorithmic fairness, Sahlgren says very little about what are the general gains of bringing in feasibility considerations in theorizing algorithmic fairness. How, more precisely, does it help us make assessments about fairness in algorithmic decision-making? This is what is addressed in this Reply. More specifically, our two-fold argument is that feasibility plays an important but limited role for algorithmic fairness. We end by offering a sketch of a framework, which may be useful for theorizing feasibility in algorithmic fairness.",
      "citationCount": 1,
      "doi": "10.1007/s13347-024-00835-8",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/fc7f91d95b88689314617e08b784044061c45e3d",
      "venue": "Philosophy & Technology",
      "journal": {
        "name": "Philosophy & Technology",
        "volume": "38"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1acb23d5c5a6e81250bc7ffedcd096139ba38d40",
      "title": "Evidential reasoning, testimonial injustice and the fairness of the criminal trial",
      "authors": [
        {
          "name": "Federico Picinali",
          "authorId": "102263326"
        }
      ],
      "year": 2023,
      "abstract": "The article argues that the assessment of the relevance and of the probative value of an item of evidence is susceptible to an evaluation on moral grounds (such as fairness), rather than just to an evaluation on epistemic grounds (such as accuracy). In particular, the article shows that an assessment of relevance and of probative value is unfair, and renders the trial unfair, when this assessment instantiates epistemic injustice of the testimonial kind; and that it instantiates such an injustice when, due to identity prejudice against a social group to which one of the parties in the proceedings belongs, the evidence is assessed without considering the experience and stock of knowledge of thisparty. The article offers several examples of this phenomenon. The upshot is that higher courts, whose role includes checking that proceedings have been fair, should dirty their hands more readily than they are currently doing with the evidential reasoning of the first-instance adjudicator. However, the focus should be on preventing unfairness, rather than treating it.",
      "citationCount": 7,
      "doi": "10.33115/udg_bib/qf.i6.22888",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/1acb23d5c5a6e81250bc7ffedcd096139ba38d40",
      "venue": "Quaestio facti Revista internacional sobre razonamiento probatorio",
      "journal": {
        "name": "Quaestio facti. Revista internacional sobre razonamiento probatorio"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b1ac78322b163c0e2373bfe6180aacd77c0da975",
      "title": "Fairness as adequacy: a sociotechnical view on model evaluation in machine learning",
      "authors": [
        {
          "name": "Thomas Grote",
          "authorId": "1423968387"
        }
      ],
      "year": 2023,
      "abstract": "This paper develops an account of model evaluation\u2014with an emphasis on fairness concerns\u2014that takes the social situatedness of ML models as its starting point. Such a view entails that ML models are not deemed isolated entities, but rather tools, used for specific purposes and potentially impacting their social environment in manifold ways. This shift of perspective opens up a new problem space and facilitates rethinking criteria for model evaluation. By drawing on the adequacy-for-purpose view in philosophy of science, epistemic norms and desiderata for an adequate deployment of ML models along the dimensions of Social Objectives, Measurement, Social Dynamics, and interaction are then identified. The account thus developed also highlights why any auditing of ML models that ought to assist in consequential decision-making cannot be limited to an assessment of statistical properties, but needs to incorporate a variety of methods from the social sciences instead. Moreover, while the process of model evaluation might be deemed as a mere technical exercise, it is in fact riddled by epistemic and morally normative considerations.",
      "citationCount": 3,
      "doi": "10.1007/s43681-023-00280-x",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/b1ac78322b163c0e2373bfe6180aacd77c0da975",
      "venue": "AI and Ethics",
      "journal": {
        "name": "AI and Ethics",
        "pages": "427-440",
        "volume": "4"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a8cfe8f54eb3f55d2d3e045a7d14231e64cc5b16",
      "title": "Ethics of Artificial Intelligence: Balancing Innovation with Privacy, Fairness, and Accountability",
      "authors": [
        {
          "name": "Manasseh F. Oguru",
          "authorId": "2382304580"
        }
      ],
      "year": 2025,
      "abstract": "Artificial Intelligence (AI) is transforming societies through its capacity to drive innovation, optimise decision-making, and enhance productivity across diverse sectors. However, the rapid deployment of AI systems raises complex ethical questions that extend beyond technical performance. This review critically examines the ethics of artificial intelligence with emphasis on three central pillars: privacy, fairness, and accountability. AI technologies often rely on vast datasets that risk infringing on individual privacy when mismanaged, necessitating robust frameworks for data governance and consent. Equally pressing is the issue of fairness, as algorithmic bias can perpetuate systemic inequalities and undermine social justice. This concern is particularly acute in sensitive domains such as healthcare, finance, and criminal justice, where biased outputs can have life-altering consequences. Accountability also emerges as a central challenge, as the diffusion of responsibility between developers, organisations, and users creates ambiguity regarding who should be held responsible for harms caused by AI systems. Addressing these ethical dimensions requires an integrated approach that blends technological safeguards with regulatory oversight and societal engagement. The paper explores strategies such as explainable AI, impact assessments, and participatory design as pathways to align innovation with ethical responsibility. Ultimately, the balance between harnessing AI\u2019s transformative potential and safeguarding fundamental rights hinges on continuous dialogue among stakeholders\u2014governments, industry, academia, and civil society. By fostering ethical resilience in AI governance, societies can ensure that innovation does not compromise human dignity, equity, or trust. This work underscores the importance of proactive, interdisciplinary measures to guide the ethical trajectory of AI as it becomes an indispensable part of everyday life.",
      "citationCount": 0,
      "doi": "10.9734/ajarr/2025/v19i101169",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a8cfe8f54eb3f55d2d3e045a7d14231e64cc5b16",
      "venue": "Asian Journal of Advanced Research and Reports",
      "journal": {
        "name": "Asian Journal of Advanced Research and Reports"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "32a39002fe10136a0a42b28ea7de6e8dfdfcb88c",
      "title": "A Unifying Human-Centered AI Fairness Framework",
      "authors": [
        {
          "name": "Munshi Mahbubur Rahman",
          "authorId": "2315371026"
        },
        {
          "name": "Shimei Pan",
          "authorId": "2257320672"
        },
        {
          "name": "James R. Foulds",
          "authorId": "40289577"
        }
      ],
      "year": 2025,
      "abstract": "The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2512.06944",
      "url": "https://www.semanticscholar.org/paper/32a39002fe10136a0a42b28ea7de6e8dfdfcb88c",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "5f82249aa5d28fd790afbce44e90c62eaa399577",
      "title": "Ethical Architecture in AI-Driven Credit: Balancing Inclusion, Fairness, and Transparency",
      "authors": [
        {
          "name": "Dennis Sebastian",
          "authorId": "2058385541"
        }
      ],
      "year": 2025,
      "abstract": "Artificial Intelligence is now radically transforming credit decisioning systems, enabling unparalleled opportunities for financial inclusion, yet also raising tough implications for bias, discrimination, and transparency.\u00a0 As machine learning algorithms take on more work previously performed by underwriting, financial institutions are now having to confront key trade-offs in predictive accuracy, fairness, and accountability. This article analyzes the social effects of AI-based credit systems through a variety of perspectives, including economic implications, social equity aspects, regulatory evolution, and environmental sustainability. A broad ethical architecture framework is proposed, founded upon four foundational pillars: Inclusive Data practices actively sourcing diverse datasets, Explainable Models that utilize methodologies like SHAP to offer understandable decision rationales, Fair Governance implementing systematic bias detection and audit, and Human Oversight that guarantees expert review of consequential decisions. Real-life case illustrations show both the transformative power of alternative data in widening access to credit for low-income and minority groups and the risks of dark algorithms that embed old discrimination in proxy variables. Regulatory regimes in leading jurisdictions increasingly treat credit scoring as high-risk applications, subjecting them to conformity testing, ongoing monitoring, and thorough impact assessments. The struggle between technological creativity and moral accountability characterizes the present, with organizations facing challenging trade-offs between model performance and interpretability, efficiency and fairness, automation and human judgment. Emerging trends indicate obligatory fairness audits, unified transparency reporting requirements, hybrid human-AI governance mechanisms, and algorithmic impact assessments akin to environmental reviews, reshaping competitive forces in financial services fundamentally towards trustworthiness and social accountability.",
      "citationCount": 0,
      "doi": "10.52783/jisem.v10i60s.13275",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/5f82249aa5d28fd790afbce44e90c62eaa399577",
      "venue": "Journal of Information Systems Engineering & Management",
      "journal": {
        "name": "Journal of Information Systems Engineering and Management"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "c054a7d2ce873bdc5c1b0ea53e7f329a9de581ec",
      "title": "The Ethical Backbone of AI-Powered Business Intelligence: Bias, Fairness, and Transparency",
      "authors": [
        {
          "name": "Ravi Teja Medempudi",
          "authorId": "2365767327"
        }
      ],
      "year": 2025,
      "abstract": "The ethical implementation of artificial intelligence in business intelligence systems represents a critical intersection of technological advancement and moral responsibility. As organizations increasingly integrate AI-driven decision-making processes, the imperative for robust ethical frameworks becomes paramount. The focus on data quality, fairness mechanisms, and transparency protocols emerges as essential components for building trustworthy AI systems. Organizations face complex challenges in maintaining data integrity while addressing inherent biases that can perpetuate societal inequities. The implementation of comprehensive monitoring systems, coupled with structured governance frameworks, enables businesses to detect and mitigate potential ethical concerns proactively. Through the establishment of clear communication channels and accountability measures, organizations can foster public trust while ensuring compliance with evolving regulatory standards. The integration of explainable AI techniques and documented impact assessments further strengthens the ethical backbone of AI implementations, leading to improved stakeholder engagement and sustainable technological advancement in the business intelligence landscape.",
      "citationCount": 0,
      "doi": "10.32996/jcsts.2025.7.5.85",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/c054a7d2ce873bdc5c1b0ea53e7f329a9de581ec",
      "venue": "Journal of Computer Science and Technology Studies",
      "journal": {
        "name": "Journal of Computer Science and Technology Studies"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "deefaf7d75e3a2577f78b0de1c0181e66b575288",
      "title": "Exploring Fairness in Scholarly Development: Are We Creating Knowledge Storing Zombies or Curious, Creative and Critical Healthcare Professionals?",
      "authors": [
        {
          "name": "C. R. den Bakker",
          "authorId": "2082336961"
        },
        {
          "name": "A. J. De Beaufort",
          "authorId": "113794811"
        },
        {
          "name": "F. Dekker",
          "authorId": "2068186955"
        },
        {
          "name": "B. Ommering",
          "authorId": "31594971"
        }
      ],
      "year": 2023,
      "abstract": "Abstract Scholarly doctors require research knowledge and skills (Ausbildung), as well as an academic mindset, which includes curiosity, creativity, and critical thinking (Bildung). However, in contrast to knowledge and skills, summative assessment of the development of an academic mindset is not so easy in an objective and so-called \u2018fair\u2019 way. As a result, in practice, assessing knowledge and skills tends to dominate in scholarly development. In this perspective, we explore the issues that arise when we give priority to objective assessment of knowledge and skills in scholarly development to safeguard fairness and, consequently, standardize educational procedures and learning pathways. We argue that eventually this approach may even result in hampered development of a true academic mindset and can be considered unfair rather than fair. To solve this, perhaps we should go back to the core business of the university and in the tradition of founder of the modern university Von Humboldt focus on shaping an academic mindset (Bildung). To rebalance Ausbildung and Bildung in academic education, we should go beyond the assumption that objectivity is a prerequisite for achieving fairness in assessment. Shifting the focus from pure objectivity to both objectivity and subjectivity in assessment as well as learning pathways can assist in protecting fairness and, as a result, bring back Bildung to medical education to ensure future doctors to be true scholars.",
      "citationCount": 0,
      "doi": "10.2147/AMEP.S414578",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/deefaf7d75e3a2577f78b0de1c0181e66b575288",
      "venue": "Advances in Medical Education and Practice",
      "journal": {
        "name": "Advances in Medical Education and Practice",
        "pages": "913 - 917",
        "volume": "14"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0fe5e73d220e829254b6728dfa17e7904c0bc9ec",
      "title": "Cultivating Ecological Understandings in Participatory Action Research - Insights from Dharma Yoga Metaphor",
      "authors": [
        {
          "name": "Shree Krishna Wagle",
          "authorId": "2209550271"
        }
      ],
      "year": 2023,
      "abstract": "As a PhD researcher, I worked with Participatory Action Research (PAR). To this reference, this paper reflects upon my personal experiences and intellectual development in maintaining inner calm and balance when faced with the outcomes and reactions of practitioner research that follow. I achieved this by utilizing the metaphor of Dharma Yoga, derived from the Vedic tradition. In this reflective inquiry, my focus lies in the field of education, where the application of Dharma Yoga metaphor emphasizes ecological principles like authenticity, relationality, and ethical responsibility. Through this emphasis, I hypothesized that ecological PAR encourages individual growth and societal advancement through inner transformation and outer manifestation of those involved in PAR. My reflection, first, observed the prevalence of the procedural aspects of the PAR model prevalent in the Northern Hemisphere, which centers on organizational learning. Thereafter, my reflective observations shifted to the values of fairness, self-governance, and empowerment embodied in the Southern Hemisphere\u2019s PAR paradigm. Mindful of the limitations in both forms of PAR, I began integrating the metaphor of Vedic Dharma Yoga, which eventually created an additional dimension of ecological epistemology. This framework involved reflective introspection, practical implementation, and collaborative involvement as means, through which practitioner researchers in the field of PAR can engage in selfless practical activities and maintaining inner calm and balance throughout the process. Also, it acknowledged the valuable attributes of both Northern and Southern PAR paradigms while enriching them with the intricate ethical connections between practitioner-researchers and their practical applications.",
      "citationCount": 0,
      "doi": "10.54536/ajmri.v2i5.1986",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/0fe5e73d220e829254b6728dfa17e7904c0bc9ec",
      "venue": "American Journal of Multidisciplinary Research and Innovation",
      "journal": {
        "name": "American Journal of Multidisciplinary Research and Innovation"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e75ce147f23e92d60055c92f4fc6dee239384b59",
      "title": "Nullius in Explanans: an ethical risk assessment for explainable AI",
      "authors": [
        {
          "name": "Luca Nannini",
          "authorId": "116344774"
        },
        {
          "name": "Diletta Huyskes",
          "authorId": "2305561576"
        },
        {
          "name": "Enrico Panai",
          "authorId": "2277641467"
        },
        {
          "name": "Giada Pistilli",
          "authorId": "2158858559"
        },
        {
          "name": "Alessio Tartaro",
          "authorId": "2129673252"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 4,
      "doi": "10.1007/s10676-024-09800-7",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e75ce147f23e92d60055c92f4fc6dee239384b59",
      "venue": "Ethics and Information Technology",
      "journal": {
        "name": "Ethics and Information Technology",
        "volume": "27"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "90b9f803410e172e48cbd1021c14a1225ccd2442",
      "title": "AI Fairness in Practice",
      "authors": [
        {
          "name": "David Leslie",
          "authorId": "2264553195"
        },
        {
          "name": "Cami Rinc\u00f3n",
          "authorId": "2057739548"
        },
        {
          "name": "Morgan Briggs",
          "authorId": "2067898199"
        },
        {
          "name": "A. Perini",
          "authorId": "2062963768"
        },
        {
          "name": "Smera Jayadeva",
          "authorId": "2161660608"
        },
        {
          "name": "Ann Borda",
          "authorId": "2292168546"
        },
        {
          "name": "SJ Bennett",
          "authorId": "2292134617"
        },
        {
          "name": "Christopher Burr",
          "authorId": "2264629623"
        },
        {
          "name": "Mhairi Aitken",
          "authorId": "2292169810"
        },
        {
          "name": "Michael Katell",
          "authorId": "2495939"
        },
        {
          "name": "Claudia Fischer",
          "authorId": "2283137439"
        },
        {
          "name": "Janis Wong",
          "authorId": "2292214810"
        },
        {
          "name": "Ismael Kherroubi Garcia",
          "authorId": "2293269425"
        }
      ],
      "year": 2024,
      "abstract": "Reaching consensus on a commonly accepted definition of AI Fairness has long been a central challenge in AI ethics and governance. There is a broad spectrum of views across society on what the concept of fairness means and how it should best be put to practice. In this workbook, we tackle this challenge by exploring how a context-based and society-centred approach to understanding AI Fairness can help project teams better identify, mitigate, and manage the many ways that unfair bias and discrimination can crop up across the AI project workflow. We begin by exploring how, despite the plurality of understandings about the meaning of fairness, priorities of equality and non-discrimination have come to constitute the broadly accepted core of its application as a practical principle. We focus on how these priorities manifest in the form of equal protection from direct and indirect discrimination and from discriminatory harassment. These elements form ethical and legal criteria based upon which instances of unfair bias and discrimination can be identified and mitigated across the AI project workflow. We then take a deeper dive into how the different contexts of the AI project lifecycle give rise to different fairness concerns. This allows us to identify several types of AI Fairness (Data Fairness, Application Fairness, Model Design and Development Fairness, Metric-Based Fairness, System Implementation Fairness, and Ecosystem Fairness) that form the basis of a multi-lens approach to bias identification, mitigation, and management. Building on this, we discuss how to put the principle of AI Fairness into practice across the AI project workflow through Bias Self-Assessment and Bias Risk Management as well as through the documentation of metric-based fairness criteria in a Fairness Position Statement.",
      "citationCount": 11,
      "doi": "10.2139/ssrn.4731838",
      "arxivId": "2403.14636",
      "url": "https://www.semanticscholar.org/paper/90b9f803410e172e48cbd1021c14a1225ccd2442",
      "venue": "Social Science Research Network",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2403.14636"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 30,
  "errors": []
}
