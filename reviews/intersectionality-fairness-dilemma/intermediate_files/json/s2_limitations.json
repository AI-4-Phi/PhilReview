{
  "status": "success",
  "source": "semantic_scholar",
  "query": "limitations fairness formalization",
  "results": [
    {
      "paperId": "5b6e380b30c540d1d7bfefe43b27eca3be0a7c70",
      "title": "Re-formalization of Individual Fairness",
      "authors": [
        {
          "name": "Toshihiro Kamishima",
          "authorId": "34865486"
        }
      ],
      "year": 2023,
      "abstract": "The notion of individual fairness is a formalization of an ethical principle,\"Treating like cases alike,\"which has been argued such as by Aristotle. In a fairness-aware machine learning context, Dwork et al. firstly formalized the notion. In their formalization, a similar pair of data in an unfair space should be mapped to similar positions in a fair space. We propose to re-formalize individual fairness by the statistical independence conditioned by individuals. This re-formalization has the following merits. First, our formalization is compatible with that of Dwork et al. Second, our formalization enables to combine individual fairness with the fairness notion, equalized odds or sufficiency, as well as statistical parity. Third, though their formalization implicitly assumes a pre-process approach for making fair prediction, our formalization is applicable to an in-process or post-process approach.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2309.05521",
      "arxivId": "2309.05521",
      "url": "https://www.semanticscholar.org/paper/5b6e380b30c540d1d7bfefe43b27eca3be0a7c70",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2309.05521"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5e11d769cb7c6442293a14d3f5017d73e72c756b",
      "title": "Inherent Limitations of AI Fairness",
      "authors": [
        {
          "name": "Maarten Buyl",
          "authorId": "1507650929"
        },
        {
          "name": "T. D. Bie",
          "authorId": "51204489"
        }
      ],
      "year": 2022,
      "abstract": "AI fairness should not be considered a panacea: It may have the potential to make society more fair than ever, but it needs critical thought and outside help to make it happen.",
      "citationCount": 30,
      "doi": "10.1145/3624700",
      "arxivId": "2212.06495",
      "url": "https://www.semanticscholar.org/paper/5e11d769cb7c6442293a14d3f5017d73e72c756b",
      "venue": "Communications of the ACM",
      "journal": {
        "name": "Communications of the ACM",
        "pages": "48 - 55",
        "volume": "67"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5f4073100bb27a2106860dbd9c784e703ef771aa",
      "title": "Inherent Limitations of AI Fairness",
      "authors": [
        {
          "name": "Maarten Buyl",
          "authorId": "1507650929"
        },
        {
          "name": "T. D. Bie",
          "authorId": "51204489"
        }
      ],
      "year": 2022,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.48550/arXiv.2212.06495",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/5f4073100bb27a2106860dbd9c784e703ef771aa",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2212.06495"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5d128b869d0b8b9d2b5fcbdd827fada57256eee9",
      "title": "Limitations of Fairness in Machine Learning",
      "authors": [
        {
          "name": "Michael Lohaus",
          "authorId": "2291016571"
        }
      ],
      "year": 2022,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/5d128b869d0b8b9d2b5fcbdd827fada57256eee9",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "7a8f4e26f1a7ac51f4c7400be2cb6f7ae1b731ec",
      "title": "Limitations of Distributive Justice : A Study On Fairness in the Perspective of Relational Justice",
      "authors": [
        {
          "name": "Ae-ryung Kim",
          "authorId": "40648254"
        }
      ],
      "year": 2022,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.17316/kfp.2022.11.38.39",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7a8f4e26f1a7ac51f4c7400be2cb6f7ae1b731ec",
      "venue": "Korean Feminist Philosophy",
      "journal": {
        "name": "Korean Feminist Philosophy"
      },
      "publicationTypes": null
    },
    {
      "paperId": "bae7f0b3448a3eac77886f2a683c0cf9256bb8bf",
      "title": "Fairness and Machine Learning Limitations and Opportunities",
      "authors": [
        {
          "name": "Solon Barocas",
          "authorId": "2881033"
        },
        {
          "name": "Moritz Hardt",
          "authorId": "1775622"
        },
        {
          "name": "Arvind Narayanan",
          "authorId": "47735253"
        }
      ],
      "year": 2018,
      "abstract": null,
      "citationCount": 511,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/bae7f0b3448a3eac77886f2a683c0cf9256bb8bf",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "1bf03cf7bb60fa84cb4e238478d0606e70c6d9d1",
      "title": "When Your Only Tool Is A Hammer: Ethical Limitations of Algorithmic Fairness Solutions in Healthcare Machine Learning",
      "authors": [
        {
          "name": "M. McCradden",
          "authorId": "2383816329"
        },
        {
          "name": "Mjaye L. Mazwi",
          "authorId": "13131345"
        },
        {
          "name": "Shalmali Joshi",
          "authorId": "34287745"
        },
        {
          "name": "James A. Anderson",
          "authorId": "143929237"
        }
      ],
      "year": 2020,
      "abstract": "It is no longer a hypothetical worry that artificial intelligence - more specifically, machine learning (ML) - can propagate the effects of pernicious bias in healthcare. To address these problems, some have proposed the development of 'algorithmic fairness' solutions. The primary goal of these solutions is to constrain the effect of pernicious bias with respect to a given outcome of interest as a function of one's protected identity (i.e., characteristics generally protected by civil or human rights legislation. The technical limitations of these solutions have been well-characterized. Ethically, the problematic implication - of developers, potentially, and end users - is that by virtue of algorithmic fairness solutions a model can be rendered 'objective' (i.e., free from the influence of pernicious bias). The ostensible neutrality of these solutions may unintentionally prompt new consequences for vulnerable groups by obscuring downstream problems due to the persistence of real-world bias. The main epistemic limitation of algorithmic fairness is that it assumes the relationship between the extent of bias's impact on a given health outcome and one's protected identity is mathematically quantifiable. The reality is that social and structural factors confluence in complex and unknown ways to produce health inequalities. Some of these are biologic in nature, and differences like these are directly relevant to predicting a health event and should be incorporated into the model's design. Others are reflective of prejudice, lack of access to healthcare, or implicit bias. Sometimes, there may be a combination. With respect to any specific task, it is difficult to untangle the complex relationships between potentially influential factors and which ones are 'fair' and which are not to inform their inclusion or mitigation in the model's design.",
      "citationCount": 12,
      "doi": "10.1145/3375627.3375824",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/1bf03cf7bb60fa84cb4e238478d0606e70c6d9d1",
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "0c9abc2899ebbd8b2ef94872d4c5afc6fbae2496",
      "title": "Towards fair AI: a review of bias and fairness in machine intelligence",
      "authors": [
        {
          "name": "Venkatesha Kurumayya",
          "authorId": "2248338022"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/s42001-025-00386-8",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/0c9abc2899ebbd8b2ef94872d4c5afc6fbae2496",
      "venue": "Journal of Computational Social Science",
      "journal": {
        "name": "Journal of Computational Social Science",
        "volume": "8"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "f83e4d38d7a689a583d266b589616d69a1b350eb",
      "title": "A Critical Survey on Fairness Benefits of Explainable AI",
      "authors": [
        {
          "name": "Luca Deck",
          "authorId": "2261085460"
        },
        {
          "name": "Jakob Schoeffer",
          "authorId": "2261085112"
        },
        {
          "name": "Maria De-Arteaga",
          "authorId": "2349724822"
        },
        {
          "name": "Niklas K\u00fchl",
          "authorId": "2261943499"
        }
      ],
      "year": 2023,
      "abstract": "In this critical survey, we analyze typical claims on the relationship between explainable AI (XAI) and fairness to disentangle the multidimensional relationship between these two concepts. Based on a systematic literature review and a subsequent qualitative content analysis, we identify seven archetypal claims from 175 scientific articles on the alleged fairness benefits of XAI. We present crucial caveats with respect to these claims and provide an entry point for future discussions around the potentials and limitations of XAI for specific fairness desiderata. Importantly, we notice that claims are often (i) vague and simplistic, (ii) lacking normative grounding, or (iii) poorly aligned with the actual capabilities of XAI. We suggest to conceive XAI not as an ethical panacea but as one of many tools to approach the multidimensional, sociotechnical challenge of algorithmic fairness. Moreover, when making a claim about XAI and fairness, we emphasize the need to be more specific about what kind of XAI method is used, which fairness desideratum it refers to, how exactly it enables fairness, and who is the stakeholder that benefits from XAI.",
      "citationCount": 30,
      "doi": "10.1145/3630106.3658990",
      "arxivId": "2310.13007",
      "url": "https://www.semanticscholar.org/paper/f83e4d38d7a689a583d266b589616d69a1b350eb",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book",
        "Review"
      ]
    },
    {
      "paperId": "97ac11e5a6440eccb70ae7146392ac138c36fa6c",
      "title": "Fairness in Machine Learning",
      "authors": [
        {
          "name": "L. Oneto",
          "authorId": "1682762"
        },
        {
          "name": "S. Chiappa",
          "authorId": "48880818"
        }
      ],
      "year": 2020,
      "abstract": "Machine learning based systems are reaching society at large and in many aspects of everyday life. This phenomenon has been accompanied by concerns about the ethical issues that may arise from the adoption of these technologies. ML fairness is a recently established area of machine learning that studies how to ensure that biases in the data and model inaccuracies do not lead to models that treat individuals unfavorably on the basis of characteristics such as e.g. race, gender, disabilities, and sexual or political orientation. In this manuscript, we discuss some of the limitations present in the current reasoning about fairness and in methods that deal with it, and describe some work done by the authors to address them. More specifically, we show how causal Bayesian networks can play an important role to reason about and deal with fairness, especially in complex unfairness scenarios. We describe how optimal transport theory can be leveraged to develop methods that impose constraints on the full shapes of distributions corresponding to different sensitive attributes, overcoming the limitation of most approaches that approximate fairness desiderata by imposing constraints on the lower order moments or other functions of those distributions. We present a unified framework that encompasses methods that can deal with different settings and fairness criteria, and that enjoys strong theoretical guarantees. We introduce an approach to learn fair representations that can generalize to unseen tasks. Finally, we describe a technique that accounts for legal restrictions about the use of sensitive attributes.",
      "citationCount": 533,
      "doi": "10.1007/978-3-030-43883-8_7",
      "arxivId": "2012.15816",
      "url": "https://www.semanticscholar.org/paper/97ac11e5a6440eccb70ae7146392ac138c36fa6c",
      "venue": "INNSBDDL",
      "journal": {
        "pages": "155-196"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "aa97b92294db7455b2d302a66ad4cde664a4696b",
      "title": "Clinical algorithms, racism, and \u201cfairness\u201d in healthcare: A case of bounded justice",
      "authors": [
        {
          "name": "Sarah A. El-Azab",
          "authorId": "2267118623"
        },
        {
          "name": "Paige Nong",
          "authorId": "3465657"
        }
      ],
      "year": 2023,
      "abstract": "To date, attempts to address racially discriminatory clinical algorithms have largely focused on fairness and the development of models that \u201cdo no harm.\u201d While the push for fairness is rooted in a desire to avoid or ameliorate health disparities, it generally neglects the role of racism in shaping health outcomes and does little to repair harm to patients. These limitations necessitate reconceptualizing how clinical algorithms should be designed and employed in pursuit of racial justice and health equity. A useful lens for this work is bounded justice, a concept and research analytic proposed by Melissa Creary to guide multidisciplinary health equity interventions. We describe how bounded justice offers a lens for (1) articulating the deep injustices embedded in the datasets, methodologies, and sociotechnical infrastructure underlying design and implementation of clinical algorithms and (2) envisioning how these algorithms can be redesigned to contribute to larger efforts that not only address current inequities, but to redress the historical mistreatment of communities of color by biomedical institutions. Thus, the aim of this article is two-fold. First, we apply the bounded justice analytic to fairness and clinical algorithms by describing structural constraints on health equity efforts such as medical device regulatory frameworks, race-based medicine, and racism in data. We then reimagine how clinical algorithms could function as a reparative technology to support justice and empower patients in the healthcare system.",
      "citationCount": 8,
      "doi": "10.1177/20539517231213820",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/aa97b92294db7455b2d302a66ad4cde664a4696b",
      "venue": "Big Data & Society",
      "journal": {
        "name": "Big Data & Society",
        "volume": "10"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b37f9d86a78d1998240897c4fe9c75ed62d1074b",
      "title": "A Critical Survey on Fairness Benefits of XAI",
      "authors": [
        {
          "name": "Luca Deck",
          "authorId": "2261085460"
        },
        {
          "name": "Jakob Schoeffer",
          "authorId": "2261085112"
        },
        {
          "name": "Maria De-Arteaga",
          "authorId": "2349724822"
        },
        {
          "name": "Niklas K\u00fchl",
          "authorId": "2261943499"
        }
      ],
      "year": 2023,
      "abstract": null,
      "citationCount": 7,
      "doi": "10.48550/arXiv.2310.13007",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/b37f9d86a78d1998240897c4fe9c75ed62d1074b",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2310.13007"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "acd6de3ac2a3d9449aae51b87fbb03f6f0020954",
      "title": "The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning",
      "authors": [
        {
          "name": "S. Corbett-Davies",
          "authorId": "1403746185"
        },
        {
          "name": "Sharad Goel",
          "authorId": "143802734"
        }
      ],
      "year": 2018,
      "abstract": null,
      "citationCount": 889,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/acd6de3ac2a3d9449aae51b87fbb03f6f0020954",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/1808.00023"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "b467047443b0a51b2e2fbe76113e7afd58f5e896",
      "title": "The epistemic dimension of algorithmic fairness: assessing its impact in innovation diffusion and fair policy making",
      "authors": [
        {
          "name": "E. Villa",
          "authorId": "2170938088"
        },
        {
          "name": "Camilla Quaresmini",
          "authorId": "2216795056"
        },
        {
          "name": "Valentina Breschi",
          "authorId": "2289848310"
        },
        {
          "name": "V. Schiaffonati",
          "authorId": "2226297"
        },
        {
          "name": "M. Tanelli",
          "authorId": "2241565846"
        }
      ],
      "year": 2025,
      "abstract": "Algorithmic fairness is an expanding field that addresses a range of discrimination issues associated with algorithmic processes. However, most works in the literature focus on analyzing it only from an ethical perspective, focusing on moral principles and values that should be considered in the design and evaluation of algorithms, while disregarding the epistemic dimension related to knowledge transmission and validation. However, this aspect of algorithmic fairness should also be included in the debate, as it is crucial to introduce a specific type of harm: an individual may be systematically excluded from the dissemination of knowledge due to the attribution of a credibility deficit/excess. In this work, we specifically focus on characterizing and analyzing the impact of this credibility deficit or excess on the diffusion of innovations on a societal scale, a phenomenon driven by individual attitudes and social interactions, and also by the strength of mutual connections. Indeed, discrimination might shape the latter, ultimately modifying how innovations spread within the network. In this light, to incorporate, also from a formal point of view, the epistemic dimension in innovation diffusion models becomes paramount, especially if these models are intended to support fair policy design. For these reasons, we formalize the epistemic properties of a social environment, by extending the well-established Linear Threshold Model (LTM) in an epistemic direction to show the impact of epistemic biases in innovation diffusion. Focusing on the impact of epistemic bias in both open-loop and closed-loop scenarios featuring optimal fostering policies, our results shed light on the pivotal role the epistemic dimension might have in the debate of algorithmic fairness in decision-making.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2504.02856",
      "arxivId": "2504.02856",
      "url": "https://www.semanticscholar.org/paper/b467047443b0a51b2e2fbe76113e7afd58f5e896",
      "venue": "EWAF",
      "journal": {
        "pages": "116-134"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "87102f3ffb2bb2b6cb4989336b6e0605b126388d",
      "title": "Interpretability and Fairness in Machine Learning: A Formal Methods Approach",
      "authors": [
        {
          "name": "Bishwamittra Ghosh",
          "authorId": "49522533"
        }
      ],
      "year": 2023,
      "abstract": "The last decades have witnessed significant progress in machine learning with a host of applications of algorithmic decision-making in different safety-critical domains, such as medical, law, education, and transportation. In high-stake domains, machine learning predictions have far-reaching consequences on the end-users. With the aim of applying machine learning for societal goods, there have been increasing efforts to regulate machine learning by imposing interpretability, fairness, robustness, etc. in predictions. Towards responsible and trustworthy machine learning, we propose two research themes in our dissertation research: interpretability and fairness of machine learning classifiers. In particular, we design algorithms to learn interpretable rule-based classifiers, formally verify fairness, and explain the sources of unfairness. Prior approaches to these problems are often limited by scalability, accuracy, or both. To overcome these limitations, we closely integrate automated reasoning, formal methods, and statistics with fairness and interpretability to develop scalable and accurate solutions.",
      "citationCount": 3,
      "doi": "10.24963/ijcai.2023/816",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/87102f3ffb2bb2b6cb4989336b6e0605b126388d",
      "venue": "International Joint Conference on Artificial Intelligence",
      "journal": {
        "pages": "7083-7084"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "4470bea7ded83970fb8237519832b5fb353e20df",
      "title": "What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity",
      "authors": [
        {
          "name": "Youjin Kong",
          "authorId": "2367755918"
        }
      ],
      "year": 2025,
      "abstract": "Fairness in machine learning (ML) has become a rapidly growing area of research. But why, in the first place, is unfairness in ML wrong? And why should we care about improving fairness? Most fair-ML research implicitly appeals to distributive equality: the idea that desirable benefits and goods, such as opportunities (e.g., Barocas et al., 2023), should be equally distributed across society. Unfair ML models, then, are seen as wrong because they unequally distribute such benefits. This paper argues that this exclusive focus on distributive equality offers an incomplete and potentially misleading ethical foundation, especially in the context of text- and image-generation models. Grounding ML fairness in egalitarianism\u2014the view that equality is a fundamental moral and social ideal\u2014requires challenging structural inequality: systematic, institutional, and durable arrangements that privilege some groups while disadvantaging others. Structural inequality manifests through ML systems in two primary forms: allocative harms (e.g., economic loss) and representational harms (e.g., stereotypes, erasure). While distributive equality helps address allocative harms, it fails to explain why representational harms are wrong\u2014that is, why it is wrong for ML systems to reinforce social hierarchies that stratify people into superior and inferior groups\u2014and why ML systems should aim to foster a society where people relate as equals (i.e., relational equality). To address these limitations, the paper proposes a novel multifaceted egalitarian framework for ML fairness that integrates both distributive and relational notions of equality. Drawing on critical social and political philosophy, including the work of Anderson, Young, and Fraser, this framework offers a more comprehensive ethical foundation for tackling the full spectrum of harms perpetuated by ML systems. The paper also outlines practical pathways for implementing the framework across the entire ML pipeline.",
      "citationCount": 1,
      "doi": "10.1145/3766539",
      "arxivId": "2506.16782",
      "url": "https://www.semanticscholar.org/paper/4470bea7ded83970fb8237519832b5fb353e20df",
      "venue": "ACM Journal on Responsible Computing",
      "journal": {
        "name": "ACM Journal on Responsible Computing"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "31e9e58de760a7bdbeb99a8fef60f17e935ac3cf",
      "title": "Equal Treatment: Measuring Fairness using Explanation Distributions",
      "authors": [
        {
          "name": "Carlos Mougan",
          "authorId": "115400334"
        },
        {
          "name": "Laura State",
          "authorId": "2135778709"
        },
        {
          "name": "Antonio Ferrara",
          "authorId": "2056226695"
        },
        {
          "name": "Salvatore Ruggieri",
          "authorId": "2194703719"
        },
        {
          "name": "Steffen Staab",
          "authorId": "2067038375"
        }
      ],
      "year": 2023,
      "abstract": null,
      "citationCount": 2,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/31e9e58de760a7bdbeb99a8fef60f17e935ac3cf",
      "venue": "",
      "journal": null,
      "publicationTypes": [
        "Review"
      ]
    },
    {
      "paperId": "84cf8f970538861e1efbe3e17cb54e427c186c27",
      "title": "AI fairness in practice: Paradigm, challenges, and prospects",
      "authors": [
        {
          "name": "Wenbin Zhang",
          "authorId": "2243249316"
        }
      ],
      "year": 2024,
      "abstract": "Understanding and correcting algorithmic bias in artificial intelligence (AI) has become increasingly important, leading to a surge in research on AI fairness within both the AI community and broader society. Traditionally, this research operates within the constrained supervised learning paradigm, assuming the presence of class labels, independent and identically distributed (IID) data, and batch\u2010based learning necessitating the simultaneous availability of all training data. However, in practice, class labels may be absent due to censoring, data is often represented using non\u2010IID graph structures that capture connections among individual units, and data can arrive and evolve over time. These prevalent real\u2010world data representations limit the applicability of existing fairness literature, which typically addresses fairness in static and tabular supervised learning settings. This paper reviews recent advances in AI fairness aimed at bridging these gaps for practical deployment in real\u2010world scenarios. Additionally, opportunities are envisioned by highlighting the limitations and significant potential for real applications.",
      "citationCount": 21,
      "doi": "10.1002/aaai.12189",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/84cf8f970538861e1efbe3e17cb54e427c186c27",
      "venue": "The AI Magazine",
      "journal": {
        "name": "AI Mag.",
        "pages": "386-395",
        "volume": "45"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "d4cbfb1e0821b30f7d8152cd505ef65f68be0d66",
      "title": "Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens",
      "authors": [
        {
          "name": "Anna Seo Gyeong Choi",
          "authorId": "2375388168"
        },
        {
          "name": "Hoon Choi",
          "authorId": "2375403844"
        }
      ],
      "year": 2025,
      "abstract": "Automatic Speech Recognition (ASR) systems now mediate countless human-technology interactions, yet research on their fairness implications remains surprisingly limited. This paper examines ASR bias through a philosophical lens, arguing that systematic misrecognition of certain speech varieties constitutes more than a technical limitation -- it represents a form of disrespect that compounds historical injustices against marginalized linguistic communities. We distinguish between morally neutral classification (discriminate 1) and harmful discrimination (discriminate 2), demonstrating how ASR systems can inadvertently transform the former into the latter when they consistently misrecognize non-standard dialects. We identify three unique ethical dimensions of speech technologies that differentiate ASR bias from other algorithmic fairness concerns: the temporal burden placed on speakers of non-standard varieties (\"temporal taxation\"), the disruption of conversational flow when systems misrecognize speech, and the fundamental connection between speech patterns and personal/cultural identity. These factors create asymmetric power relationships that existing technical fairness metrics fail to capture. The paper analyzes the tension between linguistic standardization and pluralism in ASR development, arguing that current approaches often embed and reinforce problematic language ideologies. We conclude that addressing ASR bias requires more than technical interventions; it demands recognition of diverse speech varieties as legitimate forms of expression worthy of technological accommodation. This philosophical reframing offers new pathways for developing ASR systems that respect linguistic diversity and speaker autonomy.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.07143",
      "arxivId": "2508.07143",
      "url": "https://www.semanticscholar.org/paper/d4cbfb1e0821b30f7d8152cd505ef65f68be0d66",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.07143"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "78f8d8a3ce86e90a1d4a916ac9667d9679b15e8d",
      "title": "What\u2019s Individual About Individual Fairness?",
      "authors": [
        {
          "name": "S. Ben-David",
          "authorId": "1409749316"
        },
        {
          "name": "Pascale Gourdeau",
          "authorId": "1812785"
        },
        {
          "name": "Tosca Lechner",
          "authorId": "1753285648"
        },
        {
          "name": "Ruth Urner",
          "authorId": "2224151"
        }
      ],
      "year": 2025,
      "abstract": "Individual and group fairness notions abound in the machine learning literature. Each attempts to formalize harm against individuals or groups of people. In this work, we take a step back and aim to characterize, from a learning theory perspective, what is at the heart of individual fairness (IF) notions. We argue that fairness notions should be comparison-based and, in the case of IF notions, that any failure to be fair should give rise to finite evidence of unfairness. We also posit that IF notions should have an unfairness ``direction'', for example via an order on the set of potential decisions. Equipped with this framework, we present various ways unfair classifiers can be compared to each other. \nComparing classifiers is essential in any situation where there is a need to choose between not-perfectly-fair classifiers, e.g., in cases where there exist unavoidable trade-offs between learning objectives. We then adapt score-based measures of individual unfairness to allow us to measure how harm is distributed between population subgroups, which is more in line with group fairness. Crucially, our set-up retains evidence of harm at the individual level, allowing for algorithmic recourse, or potential integrations within legal frameworks.",
      "citationCount": 0,
      "doi": "10.1609/aies.v8i1.36556",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/78f8d8a3ce86e90a1d4a916ac9667d9679b15e8d",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "97c13c17f0651ccf43f81ea1feb92fd14f2a7e47",
      "title": "Fairness, Equality, and Power in Algorithmic Decision-Making",
      "authors": [
        {
          "name": "Maximilian Kasy",
          "authorId": "66099712"
        },
        {
          "name": "Rediet Abebe",
          "authorId": "5651696"
        }
      ],
      "year": 2021,
      "abstract": "Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same \"merit.\" Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by \"merit;\" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.",
      "citationCount": 161,
      "doi": "10.1145/3442188.3445919",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/97c13c17f0651ccf43f81ea1feb92fd14f2a7e47",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle"
      ]
    },
    {
      "paperId": "24fc9b6e958bbf1860d92b4a3b53f4a85359970d",
      "title": "Algorithmic Fairness and Structural Injustice: Insights from Feminist Political Philosophy",
      "authors": [
        {
          "name": "Atoosa Kasirzadeh",
          "authorId": "51880633"
        }
      ],
      "year": 2022,
      "abstract": "Data-driven predictive algorithms are widely used to automate and guide high-stake decision making such as bail and parole recommendation, medical resource distribution, and mortgage allocation. Nevertheless, harmful outcomes biased against vulnerable groups have been reported. The growing research field known as 'algorithmic fairness' aims to mitigate these harmful biases. Its primary methodology consists in proposing mathematical metrics to address the social harms resulting from an algorithm's biased outputs. The metrics are typically motivated by -- or substantively rooted in -- ideals of distributive justice, as formulated by political and legal philosophers. The perspectives of feminist political philosophers on social justice, by contrast, have been largely neglected. Some feminist philosophers have criticized the local scope of the paradigm of distributive justice and have proposed corrective amendments to surmount its limitations. The present paper brings some key insights of feminist political philosophy to algorithmic fairness. The paper has three goals. First, I show that algorithmic fairness does not accommodate structural injustices in its current scope. Second, I defend the relevance of structural injustices -- as pioneered in the contemporary philosophical literature by Iris Marion Young -- to algorithmic fairness. Third, I take some steps in developing the paradigm of 'responsible algorithmic fairness' to correct for errors in the current scope and implementation of algorithmic fairness. I close by some reflections of directions for future research.",
      "citationCount": 47,
      "doi": "10.1145/3514094.3534188",
      "arxivId": "2206.00945",
      "url": "https://www.semanticscholar.org/paper/24fc9b6e958bbf1860d92b4a3b53f4a85359970d",
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "dbc78ba874f3503c0e83b0f1d037e022e2891def",
      "title": "Fair salary according to aristotle: limitations and opportunities",
      "authors": [
        {
          "name": "Jean Jacko",
          "authorId": "152779078"
        }
      ],
      "year": 2025,
      "abstract": "Purpose: This text explores philosophical and economic debates on fair wages, drawing on the \nAristotelian principle of distributive justice. The text examines the challenges inherent in \napplying this principle in practice and explores potential pathways to address and overcome \nthese challenges effectively. The reason for choosing this topic is the practical significance of \nfair pay and the gap in the current scientific literature on distributive justice. \nDesign/methodology/approach: This investigation is a conceptual analysis of economic \ntheories about fair salaries. \nFindings: The study demonstrates that the Aristotelian principle of distributive justice is either \nexplicitly or implicitly assumed in economics, including theories that expressly deny the \ndoctrine of distributive justice. \nResearch limitations/implications: This research does not embrace empirical investigations \nand mathematical methods. Yet, it provides directions and methodologies for further theoretical \nand empirical research on quantitative perspectives on the impact of normative beliefs on \neconomic and managerial decisions regarding salaries. Further studies can go into a detailed \ndiscussion of case studies to incorporate the study's findings into various situational factors. \nPractical implications: The article demonstrates the potential application of the Aristotelian \nconcept of distributive justice in setting salaries. \nSocial implications: The presented analyses may encourage both management theorists and \npractitioners to adopt a balanced perspective on remuneration issues\u2014one that avoids extremes, \nfrom accommodating unfounded entitlement attitudes to ignoring fundamental employee rights. \nSuch an approach should also contribute to a balanced shift in social perception. \nOriginality/value: The study presents a novel perspective on the Aristotelian principle of \ndistributive justice, demonstrating its compatibility with diverse economic theories, including \nthose that explicitly reject the principle. The text is addressed to theorists and decision-makers \ninvolved in matters of remuneration. \nKeywords: distributive justice, paid work, salary, labour meta-ethics, meta-economics. \nCategory of the paper: General review or research paper.",
      "citationCount": 0,
      "doi": "10.29119/1641-3466.2025.226.30",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/dbc78ba874f3503c0e83b0f1d037e022e2891def",
      "venue": "Scientific Papers of Silesian University of Technology. Organization and Management Series",
      "journal": {
        "name": "Scientific Papers of Silesian University of Technology. Organization and Management Series"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "da9532f6c20078427e11223d5a4c311d5cc1194d",
      "title": "How fair are we? From conceptualization to automated assessment of fairness definitions",
      "authors": [
        {
          "name": "Giordano d\u2019Aloisio",
          "authorId": "2170270823"
        },
        {
          "name": "Claudio Di Sipio",
          "authorId": "1644891552"
        },
        {
          "name": "A. Marco",
          "authorId": "8496299"
        },
        {
          "name": "D. D. Ruscio",
          "authorId": "2133181"
        }
      ],
      "year": 2024,
      "abstract": "\n Fairness is a critical concept in ethics and social domains, but it is also a challenging property to engineer in software systems. With the increasing use of machine learning in software systems, researchers have been developing techniques to assess the fairness of software systems automatically. Nonetheless, many of these techniques rely upon pre-established fairness definitions, metrics, and criteria, which may fail to encompass the wide-ranging needs and preferences of users and stakeholders. To overcome this limitation, we propose a novel approach, called MODNESS, that enables users to customize and define their fairness concepts using a dedicated modeling environment. Our approach guides the user through the definition of new fairness concepts also in emerging domains, and the specification and composition of metrics for its evaluation through a dedicated domain-specific language. Ultimately, MODNESS generates the source code to implement fair assessment based on these custom definitions. In addition, we elucidate the process we followed to collect and analyze relevant literature on fairness assessment in software engineering (SE). We compare MODNESS with the selected approaches and evaluate how they support the distinguishing features identified by our study. Our findings reveal that i) most of the current approaches do not support user-defined fairness concepts; ii) our approach can cover additional application domains not addressed by currently available tools, e.g., mitigating bias in recommender systems for software engineering and Arduino software component recommendations; iii) MODNESS demonstrates the capability to overcome the limitations of the only two other model-driven engineering-based approaches for fairness assessment.\n\n",
      "citationCount": 8,
      "doi": "10.48550/arXiv.2404.09919",
      "arxivId": "2404.09919",
      "url": "https://www.semanticscholar.org/paper/da9532f6c20078427e11223d5a4c311d5cc1194d",
      "venue": "Journal of Software and Systems Modeling",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2404.09919"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "bb7143e21b98128927f16c53a17115ab35d1ac5c",
      "title": "(Unfair) Norms in Fairness Research: A Meta-Analysis",
      "authors": [
        {
          "name": "Jennifer Chien",
          "authorId": "2279752768"
        },
        {
          "name": "A. S. Bergman",
          "authorId": "2279758018"
        },
        {
          "name": "Kevin McKee",
          "authorId": "2279736429"
        },
        {
          "name": "Nenad Tomasev",
          "authorId": "2359197877"
        },
        {
          "name": "Vinodkumar Prabhakaran",
          "authorId": "2265650491"
        },
        {
          "name": "Rida Qadri",
          "authorId": "101513478"
        },
        {
          "name": "Nahema Marchal",
          "authorId": "1491046516"
        },
        {
          "name": "William Isaac",
          "authorId": "2292408910"
        },
        {
          "name": "San Diego",
          "authorId": "2270547336"
        },
        {
          "name": "Google Deepmind",
          "authorId": "2135683701"
        },
        {
          "name": "Google Research",
          "authorId": "102566624"
        }
      ],
      "year": 2024,
      "abstract": "Algorithmic fairness has emerged as a critical concern in artificial intelligence (AI) research. However, the development of fair AI systems is not an objective process. Fairness is an inherently subjective concept, shaped by the values, experiences, and identities of those involved in research and development. To better understand the norms and values embedded in current fairness research, we conduct a meta-analysis of algorithmic fairness papers from two leading conferences on AI fairness and ethics, AIES and FAccT, covering a final sample of 139 papers over the period from 2018 to 2022. Our investigation reveals two concerning trends: first, a US-centric perspective dominates throughout fairness research; and second, fairness studies exhibit a widespread reliance on binary codifications of human identity (e.g.,\"Black/White\",\"male/female\"). These findings highlight how current research often overlooks the complexities of identity and lived experiences, ultimately failing to represent diverse global contexts when defining algorithmic bias and fairness. We discuss the limitations of these research design choices and offer recommendations for fostering more inclusive and representative approaches to fairness in AI systems, urging a paradigm shift that embraces nuanced, global understandings of human identity and values.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2407.16895",
      "arxivId": "2407.16895",
      "url": "https://www.semanticscholar.org/paper/bb7143e21b98128927f16c53a17115ab35d1ac5c",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2407.16895"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b78ebd46531d7a309854c9a569e2062bf4e5e426",
      "title": "On formal limitations of causal ecological networks",
      "authors": [
        {
          "name": "P. Damos",
          "authorId": "47138791"
        }
      ],
      "year": 2024,
      "abstract": "Causal multivariate time-series analysis, combined with network theory, provide a powerful tool for studying complex ecological interactions. However, these methods have limitations often underestimated when used in graphical modelling of ecological systems. In this opinion article, I examine the relationship between formal logic methods used to describe causal networks and their inherent statistical and epistemological limitations. I argue that while these methods offer valuable insights, they are restricted by axiomatic assumptions, statistical constraints and the incompleteness of our knowledge. To prove that, I first consider causal networks as formal systems, define causality and formalize their axioms in terms of modal logic and use ecological counterexamples to question the axioms. I also highlight the statistical limitations when using multivariate time-series analysis and Granger causality to develop ecological networks, including the potential for spurious correlations among other data characteristics. Finally, I draw upon G\u00f6del\u2019s incompleteness theorems to highlight the inherent limits of fully understanding complex networks as formal systems and conclude that causal ecological networks are subject to initial rules and data characteristics and, as any formal system, will never fully capture the intricate complexities of the systems they represent. This article is part of the theme issue \u2018Connected interactions: enriching food web research by spatial and social interactions\u2019.",
      "citationCount": 2,
      "doi": "10.1098/rstb.2023.0170",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/b78ebd46531d7a309854c9a569e2062bf4e5e426",
      "venue": "Philosophical Transactions B",
      "journal": {
        "name": "Philosophical Transactions B",
        "volume": "379"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "29e733b3768769cda75b941356619d10f6c4bc4d",
      "title": "\u201cLanguage barrier\u201d in theories of mind and limitations of the computational approach",
      "authors": [
        {
          "name": "Pavel Baryshnikov",
          "authorId": "2311152864"
        }
      ],
      "year": 2024,
      "abstract": "This paper examines from a special perspective the problem of methodological limita\u00adtions of the computational approach in the philosophy of mind and empirical sciences. The main goal is to consistently substantiate the dependence of philosophical \u201cmetaphori\u00adcal dictionaries\u201d on advances in the field of computer science, historical contexts of epis\u00adtemology, formal and methodological limitations of algorithmic and computational proce\u00addures. The key idea is that despite the success of computational models in empirical re\u00adsearch, their conceptual level does not allow us to correctly formulate the question of the ontology of consciousness. Computationalism in philosophical theories of con\u00adsciousness is presented as a practice of word usage, which posits systems of consistent descriptions of the information properties of consciousness and cognitive processes within the framework of certain methodological rules. The limitations of the computa\u00adtional approach are associated with the lack of a scientific theory of subjectivity, the fun\u00addamental irreducibility of external properties of consciousness to internal states, and fun\u00addamental restrictions on the completeness and consistency of computer mathematics. A classification of several anti-computational programs has been carried out. The article discusses various limitations of computational approaches, such as: the complexity of for\u00admalizing nonlinear and hidden processes, the dependence of emotions and intuition on the situational context and individual differences, the difficulty of predicting the emerg\u00ading properties of autonomous and autopoietic systems, and the limitations associated with non-standard computational processes of quantum and dynamic phenomena. A special role is given to the problem of set-theoretic reductive realism within the framework of mathe\u00admatical structuralism and some aspects of p-adic number systems as a possible alternative to a \u201cuniversal language\u201d for the computational philosophy of consciousness.",
      "citationCount": 1,
      "doi": "10.21146/2072-0726-2024-17-2-122-136",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/29e733b3768769cda75b941356619d10f6c4bc4d",
      "venue": "Philosophy Journal",
      "journal": {
        "name": "Philosophy Journal"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "52fcda1bfd02b92d946ef22f4dd001c61f7da039",
      "title": "A New Paradigm for Counterfactual Reasoning in Fairness and Recourse",
      "authors": [
        {
          "name": "Lucius E.J. Bynum",
          "authorId": "2138875066"
        },
        {
          "name": "Joshua R. Loftus",
          "authorId": "48678411"
        },
        {
          "name": "J. Stoyanovich",
          "authorId": "1682824"
        }
      ],
      "year": 2024,
      "abstract": "Counterfactuals underpin numerous techniques for auditing and understanding artificial intelligence (AI) systems. The traditional paradigm for counterfactual reasoning in this literature is the interventional counterfactual, where hypothetical interventions are imagined and simulated. For this reason, the starting point for causal reasoning about legal protections and demographic data in AI is an imagined intervention on a legally-protected characteristic, such as ethnicity, race, gender, disability, age, etc. We ask, for example, what would have happened had your race been different? An inherent limitation of this paradigm is that some demographic interventions \u2014 like interventions on race \u2014 may not be well-defined or translate into the formalisms of interventional counterfactuals. In this work, we explore a new paradigm based instead on the backtracking counterfactual, where rather than imagine hypothetical interventions on legally-protected characteristics, we imagine alternate initial conditions while holding these characteristics fixed. We ask instead, what would explain a counterfactual outcome for you as you actually are or could be? This alternate framework allows us to address many of the same social concerns, but to do so while asking fundamentally different questions that do not rely on demographic interventions.",
      "citationCount": 6,
      "doi": "10.48550/arXiv.2401.13935",
      "arxivId": "2401.13935",
      "url": "https://www.semanticscholar.org/paper/52fcda1bfd02b92d946ef22f4dd001c61f7da039",
      "venue": "International Joint Conference on Artificial Intelligence",
      "journal": {
        "pages": "7092-7100"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "27756e444665dceac31bccca1feed1a5a2d888c2",
      "title": "Democratizing Algorithmic Fairness",
      "authors": [
        {
          "name": "Pak-Hang Wong",
          "authorId": "2144800"
        }
      ],
      "year": 2019,
      "abstract": null,
      "citationCount": 136,
      "doi": "10.1007/s13347-019-00355-w",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/27756e444665dceac31bccca1feed1a5a2d888c2",
      "venue": "Philosophy & Technology",
      "journal": {
        "name": "Philosophy & Technology",
        "pages": "225 - 244",
        "volume": "33"
      },
      "publicationTypes": null
    },
    {
      "paperId": "f5246ffe86813302ea5ebabcdba01ef8118a2b02",
      "title": "Research on the Characteristics, Limitations, and Future Directions of Artificial Intelligence",
      "authors": [
        {
          "name": "Shuhao Liu",
          "authorId": "2332668839"
        }
      ],
      "year": 2024,
      "abstract": "Abstract. The evolution of artificial intelligence (AI) has sparked widespread discussion regarding its potential to replicate or even surpass human intelligence. However, understanding the fundamental differences between AI and human intelligence is crucial for leveraging AI effectively and responsibly. This paper explores the fundamental differences between AI and human intelligence, focusing on structural, learning, and cognitive disparities. By reviewing the history and current state of AI, the paper highlights that despite significant advancements in specific domains, AI still faces limitations in causal reasoning, generalization, common-sense reasoning, and transparency. Based on these limitations, the paper defines AI's application scenarios, including handling unstructured data and automating repetitive tasks. Furthermore, the discussion extends to the prospects of AI in specific fields such as decision support, healthcare, autonomous driving, and finance, emphasizing collaborative work between humans and AI. Finally, the paper underscores that AI should serve as a tool to assist humans in solving complex problems, with a focus on fairness and safety in its societal applications.",
      "citationCount": 0,
      "doi": "10.54254/2755-2721/97/20241398",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f5246ffe86813302ea5ebabcdba01ef8118a2b02",
      "venue": "Applied and Computational Engineering",
      "journal": {
        "name": "Applied and Computational Engineering"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "7c962112626841a0eda8e55d32cdb0f430bca734",
      "title": "Distributive Justice and Fairness Metrics in Automated Decision-making: How Much Overlap Is There?",
      "authors": [
        {
          "name": "M. Kuppler",
          "authorId": "84597545"
        },
        {
          "name": "C. Kern",
          "authorId": "145387065"
        },
        {
          "name": "Ruben L. Bach",
          "authorId": "113302314"
        },
        {
          "name": "F. Kreuter",
          "authorId": "4505092"
        }
      ],
      "year": 2021,
      "abstract": "The advent of powerful prediction algorithms led to increased automation of high-stake decisions regarding the allocation of scarce resources such as government spending and welfare support. This automation bears the risk of perpetuating unwanted discrimination against vulnerable and historically disadvantaged groups. Research on algorithmic discrimination in computer science and other disciplines developed a plethora of fairness metrics to detect and correct discriminatory algorithms. Drawing on robust sociological and philosophical discourse on distributive justice, we identify the limitations and problematic implications of prominent fairness metrics. We show that metrics implementing equality of opportunity only apply when resource allocations are based on deservingness, but fail when allocations should reflect concerns about egalitarianism, sufficiency, and priority. We argue that by cleanly distinguishing between prediction tasks and decision tasks, research on fair machine learning could take better advantage of the rich literature on distributive justice.",
      "citationCount": 21,
      "doi": null,
      "arxivId": "2105.01441",
      "url": "https://www.semanticscholar.org/paper/7c962112626841a0eda8e55d32cdb0f430bca734",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2105.01441"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b586053a1fffa668ad8881d51f719a21eb054f3b",
      "title": "Algorithmic Fairness through the Lens of Causality and Privacy (AFCP) 2022",
      "authors": [
        {
          "name": "Awa Dieng",
          "authorId": "2064110017"
        },
        {
          "name": "Miriam Rateike",
          "authorId": "2267486972"
        },
        {
          "name": "G. Farnadi",
          "authorId": "2086602"
        },
        {
          "name": "Ferdinando Fioretto",
          "authorId": "2355493002"
        },
        {
          "name": "Matt J. Kusner",
          "authorId": "1940272"
        },
        {
          "name": "Jessica Schrouff",
          "authorId": "3212089"
        }
      ],
      "year": 2022,
      "abstract": null,
      "citationCount": 1,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/b586053a1fffa668ad8881d51f719a21eb054f3b",
      "venue": "AFCP",
      "journal": {
        "pages": "1-6"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "dbc0a20e44d8b35221c2006b25885b0076556c6b",
      "title": "Bridging the AI Chasm: Can EBM Address Representation and Fairness in Clinical Machine Learning?",
      "authors": [
        {
          "name": "Nicole Martinez-Martin",
          "authorId": "1404008157"
        },
        {
          "name": "Mildred K. Cho",
          "authorId": "2098811244"
        }
      ],
      "year": 2022,
      "abstract": "example, validation mechanisms used in the silent period reveal that the current accepted clinical practice or treatment disadvantages some groups. In this hypothetical scenario, the ML device might fail to create clinical equipoise required of the silent period not because of its own limitations, but due to the limitations and inequities in current clinical practice. Also, the reframing of the AI chasm also points to another set of differences in goals and priorities between ML, clinical science, and research ethics. As the authors note, \u201cthe ultimate goal of many ML tools in healthcare is the improvement of the health outcomes of patients,\u201d whereas clinical science aims to \u201cimprove the care of patients,\u201d and research ethics prioritizes \u201cethical rigor and protection of individuals\u201d (9). Improving health outcomes, improving care, and ethical rigor and protection are related, but not the same. Because goal of research ethics is not to improve the health outcomes or care of patients who participate in biomedical research, does the governance of ML tools within a research ethics pathway prompt new considerations of therapeutic misconception or increasingly blur the line between care and research? Finally, McCradden et al. provide an example of how the examination of the ethics of ML in health can also contribute to scholarship on the ethics of translational research. For example, Hostiuc et al. argue for an expanded view of the ethics of translational research that builds upon, but is distinct from bioethics. They argue: \u201c[t]he ethics of translational research should go beyond the classical topic of research ethics, or medical ethics. It should not only analyze the ethical issues that can be directly derived from the translational phases but also those derived from the gaps between translational phases (Hostiuc et al 2016, 8).\u201d This piece was written before ML has the prominence that it does now in medicine, and the target article prompts us to consider whether the increasing entry of ML into health care raises ethical issues not only within the phases, but also what, if any, new kinds of ethical issues can emerge in the \u201cgaps\u201d between research ethics phases.",
      "citationCount": 4,
      "doi": "10.1080/15265161.2022.2055212",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/dbc0a20e44d8b35221c2006b25885b0076556c6b",
      "venue": "American Journal of Bioethics",
      "journal": {
        "name": "The American Journal of Bioethics",
        "pages": "30 - 32",
        "volume": "22"
      },
      "publicationTypes": [
        "LettersAndComments",
        "JournalArticle"
      ]
    },
    {
      "paperId": "0eca227a94a9ddf0611264a87587365e538ebb1a",
      "title": "Toward A Logical Theory Of Fairness and Bias",
      "authors": [
        {
          "name": "Vaishak Belle",
          "authorId": "144893617"
        }
      ],
      "year": 2023,
      "abstract": "\n Fairness in machine learning is of considerable interest in recent years owing to the propensity of algorithms trained on historical data to amplify and perpetuate historical biases. In this paper, we argue for a formal reconstruction of fairness definitions, not so much to replace existing definitions but to ground their application in an epistemic setting and allow for rich environmental modeling. Consequently we look into three notions: fairness through unawareness, demographic parity and counterfactual fairness, and formalize these in the epistemic situation calculus.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2306.13659",
      "arxivId": "2306.13659",
      "url": "https://www.semanticscholar.org/paper/0eca227a94a9ddf0611264a87587365e538ebb1a",
      "venue": "Theory and Practice of Logic Programming",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2306.13659"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "106e1064097a51e42a46f3f262d73a6720fadaa8",
      "title": "Causal Conceptions of Fairness and their Consequences",
      "authors": [
        {
          "name": "H. Nilforoshan",
          "authorId": "8791376"
        },
        {
          "name": "Johann D. Gaebler",
          "authorId": "103301121"
        },
        {
          "name": "Ravi Shroff",
          "authorId": "33215252"
        },
        {
          "name": "Sharad Goel",
          "authorId": "143802734"
        }
      ],
      "year": 2022,
      "abstract": "Recent work highlights the role of causality in designing equitable decision-making algorithms. It is not immediately clear, however, how existing causal conceptions of fairness relate to one another, or what the consequences are of using these definitions as design principles. Here, we first assemble and categorize popular causal definitions of algorithmic fairness into two broad families: (1) those that constrain the effects of decisions on counterfactual disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions \\emph{almost always} -- in a measure theoretic sense -- result in strongly Pareto dominated decision policies, meaning there is an alternative, unconstrained policy favored by every stakeholder with preferences drawn from a large, natural class. For example, in the case of college admissions decisions, policies constrained to satisfy causal fairness definitions would be disfavored by every stakeholder with neutral or positive preferences for both academic preparedness and diversity. Indeed, under a prominent definition of causal fairness, we prove the resulting policies require admitting all students with the same probability, regardless of academic qualifications or group membership. Our results highlight formal limitations and potential adverse consequences of common mathematical notions of causal fairness.",
      "citationCount": 51,
      "doi": "10.48550/arXiv.2207.05302",
      "arxivId": "2207.05302",
      "url": "https://www.semanticscholar.org/paper/106e1064097a51e42a46f3f262d73a6720fadaa8",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2207.05302"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "a5d57b5e123899f4d4adda214c08ba05218c898b",
      "title": "Towards formalizing and assessing AI fairness",
      "authors": [
        {
          "name": "A. Schmitz",
          "authorId": "35825920"
        }
      ],
      "year": 2023,
      "abstract": "1 BACKGROUND AND MOTIVATION AI is increasingly penetrating numerous areas of our lives. As AI is taking over sensitive tasks such as credit scoring, claims processing, and support of medical diagnoses, there is a rising demand for AI applications to be \u00bbtrustworthy\u00ab. Given a set of key trustworthiness requirements that recur among numerous AI ethics principles and guidelines [7], [5], [6], organizations and researchers are showing a huge interest in implementing and assessing them for various reasons [10]. In addition to preventing societal harm and protecting the health, safety, and fundamental rights of individuals [2], [6], the assessment of AI trustworthiness characteristics can help improve AI systems and inform business decisions. Moreover, companies need assessment procedures to demonstrate the trustworthiness of their AI products or services to their customers, as well as to prove conformity of their systems with (upcoming) regulatory requirements [2]. Overall, there is a demand for market-ready AI assessments. Although there is a need for implementing and assessing trustworthiness characteristics in AI applications, the operationalization of \u00bbtrustworthy AI\u00ab is still largely open [5], [8]. Notably, the requirements associated with relevant quality dimensions are not technically concise. Often, their subject is unclear (i.e., procedures for specifying the test object in an AI application are missing), and their scope is not well-defined (i.e., under which circumstances and for which application areas requirements should apply). One additional challenge is that the evaluation of trustworthiness characteristics and risks typically depends on the specific use case. Regarding the implementation of trustworthy AI, it is also not clearly defined which entity should address the requirements (e.g., on a technical or",
      "citationCount": 3,
      "doi": "10.1145/3600211.3604762",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a5d57b5e123899f4d4adda214c08ba05218c898b",
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "cf50d4affdfd3a2eb3f64dede1b8e1f942a81231",
      "title": "What's Fair about Individual Fairness?",
      "authors": [
        {
          "name": "W. Fleisher",
          "authorId": "23641786"
        }
      ],
      "year": 2021,
      "abstract": "One of the main lines of research in algorithmic fairness involves individual fairness (IF) methods. Individual fairness is motivated by an intuitive principle, similar treatment, which requires that similar individuals be treated similarly. IF offers a precise account of this principle using distance metrics to evaluate the similarity of individuals. Proponents of individual fairness have argued that it gives the correct definition of algorithmic fairness, and that it should therefore be preferred to other methods for determining fairness. I argue that individual fairness cannot serve as a definition of fairness. Moreover, IF methods should not be given priority over other fairness methods, nor used in isolation from them. To support these conclusions, I describe four in-principle problems for individual fairness as a definition and as a method for ensuring fairness: (1) counterexamples show that similar treatment (and therefore IF) are insufficient to guarantee fairness; (2) IF methods for learning similarity metrics are at risk of encoding human implicit bias; (3) IF requires prior moral judgments, limiting its usefulness as a guide for fairness and undermining its claim to define fairness; and (4) the incommensurability of relevant moral values makes similarity metrics impossible for many tasks. In light of these limitations, I suggest that individual fairness cannot be a definition of fairness, and instead should be seen as one tool among several for ameliorating algorithmic bias.",
      "citationCount": 88,
      "doi": "10.1145/3461702.3462621",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/cf50d4affdfd3a2eb3f64dede1b8e1f942a81231",
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle"
      ]
    },
    {
      "paperId": "564c0f5dda77fd8a6a9cd41a0ec88989d92181ad",
      "title": "Counterfactual Fairness Is Basically Demographic Parity",
      "authors": [
        {
          "name": "Lucas Rosenblatt",
          "authorId": "2066297941"
        },
        {
          "name": "R. T. Witter",
          "authorId": "2062796571"
        }
      ],
      "year": 2022,
      "abstract": "Making fair decisions is crucial to ethically implementing machine learning algorithms in social settings. In this work, we consider the celebrated definition of counterfactual fairness. We begin by showing that an algorithm which satisfies counterfactual fairness also satisfies demographic parity, a far simpler fairness constraint. Similarly, we show that all algorithms satisfying demographic parity can be trivially modified to satisfy counterfactual fairness. Together, our results indicate that counterfactual fairness is basically equivalent to demographic parity, which has important implications for the growing body of work on counterfactual fairness. We then validate our theoretical findings empirically, analyzing three existing algorithms for counterfactual fairness against three simple benchmarks. We find that two simple benchmark algorithms outperform all three existing algorithms---in terms of fairness, accuracy, and efficiency---on several data sets. Our analysis leads us to formalize a concrete fairness goal: to preserve the order of individuals within protected groups. We believe transparency around the ordering of individuals within protected groups makes fair algorithms more trustworthy. By design, the two simple benchmark algorithms satisfy this goal while the existing algorithms do not.",
      "citationCount": 21,
      "doi": "10.48550/arXiv.2208.03843",
      "arxivId": "2208.03843",
      "url": "https://www.semanticscholar.org/paper/564c0f5dda77fd8a6a9cd41a0ec88989d92181ad",
      "venue": "AAAI Conference on Artificial Intelligence",
      "journal": {
        "pages": "14461-14469"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "e22ddbf41ef11b94e7343fda2c870bda9b661d72",
      "title": "A Formalization of Kant's Second Formulation of the Categorical Imperative",
      "authors": [
        {
          "name": "F. Lindner",
          "authorId": "145316253"
        },
        {
          "name": "Marte Bentzen",
          "authorId": "100537868"
        }
      ],
      "year": 2018,
      "abstract": "We present a formalization and computational implementation of the second formulation of Kant's categorical imperative. This ethical principle requires an agent to never treat someone merely as a means but always also as an end. Here we interpret this principle in terms of how persons are causally affected by actions. We introduce Kantian causal agency models in which moral patients, actions, goals, and causal influence are represented, and we show how to formalize several readings of Kant's categorical imperative that correspond to Kant's concept of strict and wide duties towards oneself and others. Stricter versions handle cases where an action directly causally affects oneself or others, whereas the wide version maximizes the number of persons being treated as an end. We discuss limitations of our formalization by pointing to one of Kant's cases that the machinery cannot handle in a satisfying way.",
      "citationCount": 18,
      "doi": null,
      "arxivId": "1801.03160",
      "url": "https://www.semanticscholar.org/paper/e22ddbf41ef11b94e7343fda2c870bda9b661d72",
      "venue": "International Workshop on Deontic Logic in Computer Science",
      "journal": {
        "pages": "211-225"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e56f16d2c9b490d9afb95b43395f2ab834be8afe",
      "title": "Rethinking Personas for Fairness: Algorithmic Transparency and Accountability in Data-Driven Personas",
      "authors": [
        {
          "name": "Joni O. Salminen",
          "authorId": "2734912"
        },
        {
          "name": "Soon-gyo Jung",
          "authorId": "1861541"
        },
        {
          "name": "Shammur A. Chowdhury",
          "authorId": "1725417821"
        },
        {
          "name": "B. Jansen",
          "authorId": "144715575"
        }
      ],
      "year": 2020,
      "abstract": null,
      "citationCount": 7,
      "doi": "10.1007/978-3-030-50334-5_6",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e56f16d2c9b490d9afb95b43395f2ab834be8afe",
      "venue": "Interacci\u00f3n",
      "journal": {
        "pages": "82-100"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 40,
  "errors": []
}
