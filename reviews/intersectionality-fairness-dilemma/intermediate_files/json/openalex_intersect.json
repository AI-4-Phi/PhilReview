{
  "status": "success",
  "source": "openalex",
  "query": "algorithmic fairness intersectionality",
  "results": [
    {
      "openalex_id": "W4283170908",
      "doi": "10.1145/3531146.3533114",
      "title": "Are \u201cIntersectionally Fair\u201d AI Algorithms Really Fair to Women of Color? A Philosophical Analysis",
      "authors": [
        {
          "name": "Youjin Kong",
          "openalex_id": "A5033685060",
          "orcid": "https://orcid.org/0000-0001-5049-8503",
          "institutions": [
            "Oregon State University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-06-20",
      "abstract": "A growing number of studies on fairness in artificial intelligence (AI) use the notion of intersectionality to measure AI fairness. Most of these studies take intersectional fairness to be a matter of statistical parity among intersectional subgroups: an AI algorithm is \"intersectionally fair\" if the probability of the outcome is roughly the same across all subgroups defined by different combinations of the protected attributes. This paper identifies and examines three fundamental problems with this dominant interpretation of intersectional fairness in AI. First, the dominant approach is so preoccupied with the intersection of attributes/categories (e.g., race, gender) that it fails to address the intersection of oppression (e.g., racism, sexism), which is more central to intersectionality as a critical framework. Second, the dominant approach faces a dilemma between infinite regress and fairness gerrymandering: it either keeps splitting groups into smaller subgroups or arbitrarily selects protected groups. Lastly, the dominant view fails to capture what it really means for AI algorithms to be fair, in terms of both distributive and non-distributive fairness. I distinguish a strong sense of AI fairness from a weak sense that is prevalent in the literature, and conclude by envisioning paths towards strong intersectional fairness in AI.",
      "cited_by_count": 48,
      "type": "article",
      "source": {
        "name": "2022 ACM Conference on Fairness, Accountability, and Transparency",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Psychology of Moral and Emotional Judgment",
        "Feminist Epistemology and Gender Studies"
      ],
      "referenced_works_count": 32,
      "url": "https://openalex.org/W4283170908"
    },
    {
      "openalex_id": "W4391801118",
      "doi": "10.48550/arxiv.2402.07778",
      "title": "Algorithmic Fairness and Color-blind Racism: Navigating the Intersection",
      "authors": [
        {
          "name": "Jamelle Watson-Daniels",
          "openalex_id": "A5030753656",
          "orcid": "https://orcid.org/0000-0003-4711-8789"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-02-12",
      "abstract": "Our focus lies at the intersection between two broader research perspectives: (1) the scientific study of algorithms and (2) the scholarship on race and racism. Many streams of research related to algorithmic fairness have been born out of interest at this intersection. We think about this intersection as the product of work derived from both sides. From (1) algorithms to (2) racism, the starting place might be an algorithmic question or method connected to a conceptualization of racism. On the other hand, from (2) racism to (1) algorithms, the starting place could be recognizing a setting where a legacy of racism is known to persist and drawing connections between that legacy and the introduction of algorithms into this setting. In either direction, meaningful disconnection can occur when conducting research at the intersection of racism and algorithms. The present paper urges collective reflection on research directions at this intersection. Despite being primarily motivated by instances of racial bias, research in algorithmic fairness remains mostly disconnected from scholarship on racism. In particular, there has not been an examination connecting algorithmic fairness discussions directly to the ideology of color-blind racism; we aim to fill this gap. We begin with a review of an essential account of color-blind racism then we review racial discourse within algorithmic fairness research and underline significant patterns, shifts and disconnects. Ultimately, we argue that researchers can improve the navigation of the landscape at the intersection by recognizing ideological shifts as such and iteratively re-orienting towards maintaining meaningful connections across interdisciplinary lines.",
      "cited_by_count": 2,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2402.07778"
      },
      "topics": [
        "Digital Economy and Work Transformation",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4391801118"
    },
    {
      "openalex_id": "W4404356287",
      "doi": "10.48550/arxiv.2411.02569",
      "title": "The Intersectionality Problem for Algorithmic Fairness",
      "authors": [
        {
          "name": "Johannes Himmelreich",
          "openalex_id": "A5010707458",
          "orcid": "https://orcid.org/0000-0002-2163-0082"
        },
        {
          "name": "Arbie Hsu",
          "openalex_id": "A5114638681"
        },
        {
          "name": "Kristian Lum",
          "openalex_id": "A5114638682"
        },
        {
          "name": "Ellen Veomett",
          "openalex_id": "A5056992410",
          "orcid": "https://orcid.org/0000-0001-9195-6830"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-11-04",
      "abstract": "A yet unmet challenge in algorithmic fairness is the problem of intersectionality, that is, achieving fairness across the intersection of multiple groups -- and verifying that such fairness has been attained. Because intersectional groups tend to be small, verifying whether a model is fair raises statistical as well as moral-methodological challenges. This paper (1) elucidates the problem of intersectionality in algorithmic fairness, (2) develops desiderata to clarify the challenges underlying the problem and guide the search for potential solutions, (3) illustrates the desiderata and potential solutions by sketching a proposal using simple hypothesis testing, and (4) evaluates, partly empirically, this proposal against the proposed desiderata.",
      "cited_by_count": 1,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2411.02569"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4404356287"
    },
    {
      "openalex_id": "W3032340379",
      "doi": "10.1109/icde48307.2020.00203",
      "title": "An Intersectional Definition of Fairness",
      "authors": [
        {
          "name": "James R. Foulds",
          "openalex_id": "A5010326834",
          "orcid": "https://orcid.org/0000-0003-0935-4182",
          "institutions": [
            "University of Maryland, Baltimore County"
          ]
        },
        {
          "name": "Rashidul Islam",
          "openalex_id": "A5101830965",
          "orcid": "https://orcid.org/0000-0001-5276-5708",
          "institutions": [
            "University of Maryland, Baltimore County"
          ]
        },
        {
          "name": "Kamrun Naher Keya",
          "openalex_id": "A5026287600",
          "orcid": "https://orcid.org/0009-0009-5069-8383",
          "institutions": [
            "University of Maryland, Baltimore County"
          ]
        },
        {
          "name": "Shimei Pan",
          "openalex_id": "A5048111120",
          "orcid": "https://orcid.org/0000-0002-5989-8543",
          "institutions": [
            "University of Maryland, Baltimore County"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-04-01",
      "abstract": "We propose differential fairness, a multi-attribute definition of fairness in machine learning which is informed by intersectionality, a critical lens arising from the humanities literature, leveraging connections between differential privacy and legal notions of fairness. We show that our criterion behaves sensibly for any subset of the set of protected attributes, and we prove economic, privacy, and generalization guarantees. We provide a learning algorithm which respects our differential fairness criterion. Experiments on the COMPAS criminal recidivism dataset and census data demonstrate the utility of our methods.",
      "cited_by_count": 182,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Privacy-Preserving Technologies in Data",
        "Criminal Justice and Corrections Analysis"
      ],
      "referenced_works_count": 32,
      "url": "https://openalex.org/W3032340379"
    },
    {
      "openalex_id": "W4411542038",
      "doi": "10.1145/3715275.3732210",
      "title": "Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach",
      "authors": [
        {
          "name": "Steven Vethman",
          "openalex_id": "A5077364645",
          "orcid": "https://orcid.org/0000-0001-5435-8671",
          "institutions": [
            "Institut d'Etudes Politiques de Paris"
          ]
        },
        {
          "name": "Quirine T. S. Smit",
          "openalex_id": "A5099034832",
          "orcid": "https://orcid.org/0009-0008-8890-4261",
          "institutions": [
            "Netherlands Organisation for Applied Scientific Research"
          ]
        },
        {
          "name": "Nina M. van Liebergen",
          "openalex_id": "A5118593223",
          "institutions": [
            "Netherlands Organisation for Applied Scientific Research"
          ]
        },
        {
          "name": "Cor J. Veenman",
          "openalex_id": "A5013179642",
          "orcid": "https://orcid.org/0000-0002-2645-1198",
          "institutions": [
            "Leiden University"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-06-23",
      "abstract": null,
      "cited_by_count": 3,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Digital Economy and Work Transformation",
        "Law, AI, and Intellectual Property"
      ],
      "referenced_works_count": 92,
      "url": "https://openalex.org/W4411542038"
    },
    {
      "openalex_id": "W4250532311",
      "doi": "10.1177/20539517211044808",
      "title": "Algorithmic reparation",
      "authors": [
        {
          "name": "Jenny L. Davis",
          "openalex_id": "A5101606112",
          "orcid": "https://orcid.org/0000-0003-0952-5842",
          "institutions": [
            "Australian National University"
          ]
        },
        {
          "name": "Apryl Williams",
          "openalex_id": "A5011222893",
          "orcid": "https://orcid.org/0000-0003-4896-9366",
          "institutions": [
            "University of Michigan\u2013Ann Arbor",
            "University of Notre Dame",
            "Institute for Advanced Study"
          ]
        },
        {
          "name": "Michael W. Yang",
          "openalex_id": "A5023750925",
          "institutions": [
            "Australian National University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-07-01",
      "abstract": "Machine learning algorithms pervade contemporary society. They are integral to social institutions, inform processes of governance, and animate the mundane technologies of daily life. Consistently, the outcomes of machine learning reflect, reproduce, and amplify structural inequalities. The field of fair machine learning has emerged in response, developing mathematical techniques that increase fairness based on anti-classification, classification parity, and calibration standards. In practice, these computational correctives invariably fall short, operating from an algorithmic idealism that does not, and cannot, address systemic, Intersectional stratifications. Taking present fair machine learning methods as our point of departure, we suggest instead the notion and practice of algorithmic reparation. Rooted in theories of Intersectionality, reparative algorithms name, unmask, and undo allocative and representational harms as they materialize in sociotechnical form. We propose algorithmic reparation as a foundation for building, evaluating, adjusting, and when necessary, omitting and eradicating machine learning systems.",
      "cited_by_count": 121,
      "type": "article",
      "source": {
        "name": "Big Data & Society",
        "type": "journal",
        "issn": [
          "2053-9517"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://journals.sagepub.com/doi/pdf/10.1177/20539517211044808"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 80,
      "url": "https://openalex.org/W4250532311"
    },
    {
      "openalex_id": "W4285059900",
      "doi": "10.1561/1500000079",
      "title": "Fairness in Information Access Systems",
      "authors": [
        {
          "name": "Michael D. Ekstrand",
          "openalex_id": "A5015092805",
          "orcid": "https://orcid.org/0000-0003-2467-0108",
          "institutions": [
            "Boise State University"
          ]
        },
        {
          "name": "Anubrata Das",
          "openalex_id": "A5110921585",
          "institutions": [
            "The University of Texas at Austin"
          ]
        },
        {
          "name": "Robin Burke",
          "openalex_id": "A5043134791",
          "orcid": "https://orcid.org/0000-0001-5766-6434",
          "institutions": [
            "University of Colorado Boulder",
            "The University of Texas at Austin"
          ]
        },
        {
          "name": "Fernando D\u00edaz",
          "openalex_id": "A5101492251",
          "orcid": "https://orcid.org/0000-0003-2345-1288",
          "institutions": [
            "Mila - Quebec Artificial Intelligence Institute"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-07-11",
      "abstract": "Recommendation, information retrieval, and other information access systems pose unique challenges for investigating and applying the fairness and non-discrimination concepts that have been developed for studying other machine learning systems. While fair information access shares many commonalities with fair classification, there are important differences: the multistakeholder nature of information access applications, the rank-based problem setting, the centrality of personalization in many cases, and the role of user response all complicate the problem of identifying precisely what types and operationalizations of fairness may be relevant. In this monograph, we present a taxonomy of the various dimensions of fair information access and survey the literature to date on this new and rapidly-growing topic. We preface this with brief introductions to information access and algorithmic fairness to facilitate the use of this work by scholars with experience in one (or neither) of these fields who wish to study their intersection. We conclude with several open problems in fair information access, along with some suggestions for how to approach research in this space. Recommendation, information retrieval, and other information access systems pose unique challenges for investigating and applying the fairness and non-discrimination concepts that have been developed for studying other machine learning systems. While fair information access shares many commonalities with fair classification, there are important differences such as the multistakeholder nature of information access applications, the rank-based problem setting, the centrality of personalization in many cases, and the role of user response. These all complicate the problem of identifying precisely what types and operationalizations of fairness may be relevant. In this monograph, the authors present a taxonomy of the various dimensions of fair information access and survey the literature to date on this new and rapidly-growing topic. They preface this with brief introductions to information access and algorithmic fairness to facilitate the use of this work by scholars who wish to study their intersection. The authors conclude with several open problems in fair information access and present suggestions for how to approach research in this space.",
      "cited_by_count": 108,
      "type": "article",
      "source": {
        "name": "Foundations and Trends\u00ae in Information Retrieval",
        "type": "journal",
        "issn": [
          "1554-0669",
          "1554-0677"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2105.05779"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4285059900"
    },
    {
      "openalex_id": "W4405068114",
      "doi": "10.2196/59045",
      "title": "Intersection of Performance, Interpretability, and Fairness in Neural Prototype Tree for Chest X-Ray Pathology Detection: Algorithm Development and Validation Study",
      "authors": [
        {
          "name": "Hongbo Chen",
          "openalex_id": "A5093099754",
          "orcid": "https://orcid.org/0009-0005-5823-9406",
          "institutions": [
            "University of Toronto"
          ]
        },
        {
          "name": "Myrtede Alfred",
          "openalex_id": "A5009601221",
          "orcid": "https://orcid.org/0000-0003-0045-0426",
          "institutions": [
            "University of Toronto"
          ]
        },
        {
          "name": "Andrew D. Brown",
          "openalex_id": "A5002300337",
          "orcid": "https://orcid.org/0000-0002-5389-308X",
          "institutions": [
            "St. Michael's Hospital"
          ]
        },
        {
          "name": "Angela Atinga",
          "openalex_id": "A5021502221",
          "orcid": "https://orcid.org/0000-0002-6681-6008",
          "institutions": [
            "Health Sciences Centre",
            "Sunnybrook Health Science Centre"
          ]
        },
        {
          "name": "Eldan Cohen",
          "openalex_id": "A5087004754",
          "orcid": "https://orcid.org/0000-0001-5767-6683",
          "institutions": [
            "University of Toronto"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-12-05",
      "abstract": "Background While deep learning classifiers have shown remarkable results in detecting chest X-ray (CXR) pathologies, their adoption in clinical settings is often hampered by the lack of transparency. To bridge this gap, this study introduces the neural prototype tree (NPT), an interpretable image classifier that combines the diagnostic capability of deep learning models and the interpretability of the decision tree for CXR pathology detection. Objective This study aimed to investigate the utility of the NPT classifier in 3 dimensions, including performance, interpretability, and fairness, and subsequently examined the complex interaction between these dimensions. We highlight both local and global explanations of the NPT classifier and discuss its potential utility in clinical settings. Methods This study used CXRs from the publicly available Chest X-ray 14, CheXpert, and MIMIC-CXR datasets. We trained 6 separate classifiers for each CXR pathology in all datasets, 1 baseline residual neural network (ResNet)\u2013152, and 5 NPT classifiers with varying levels of interpretability. Performance, interpretability, and fairness were measured using the area under the receiver operating characteristic curve (ROC AUC), interpretation complexity (IC), and mean true positive rate (TPR) disparity, respectively. Linear regression analyses were performed to investigate the relationship between IC and ROC AUC, as well as between IC and mean TPR disparity. Results The performance of the NPT classifier improved as the IC level increased, surpassing that of ResNet-152 at IC level 15 for the Chest X-ray 14 dataset and IC level 31 for the CheXpert and MIMIC-CXR datasets. The NPT classifier at IC level 1 exhibited the highest degree of unfairness, as indicated by the mean TPR disparity. The magnitude of unfairness, as measured by the mean TPR disparity, was more pronounced in groups differentiated by age (chest X-ray 14 0.112, SD 0.015; CheXpert 0.097, SD 0.010; MIMIC 0.093, SD 0.017) compared to sex (chest X-ray 14 0.054 SD 0.012; CheXpert 0.062, SD 0.008; MIMIC 0.066, SD 0.013). A significant positive relationship between interpretability (ie, IC level) and performance (ie, ROC AUC) was observed across all CXR pathologies (P&lt;.001). Furthermore, linear regression analysis revealed a significant negative relationship between interpretability and fairness (ie, mean TPR disparity) across age and sex subgroups (P&lt;.001). Conclusions By illuminating the intricate relationship between performance, interpretability, and fairness of the NPT classifier, this research offers insightful perspectives that could guide future developments in effective, interpretable, and equitable deep learning classifiers for CXR pathology detection.",
      "cited_by_count": 2,
      "type": "article",
      "source": {
        "name": "JMIR Formative Research",
        "type": "journal",
        "issn": [
          "2561-326X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://doi.org/10.2196/59045"
      },
      "topics": [
        "COVID-19 diagnosis using AI",
        "Radiomics and Machine Learning in Medical Imaging",
        "AI in cancer detection"
      ],
      "referenced_works_count": 70,
      "url": "https://openalex.org/W4405068114"
    },
    {
      "openalex_id": "W4404465235",
      "doi": "10.2196/preprints.59045",
      "title": "Intersection of Performance, Interpretability, and Fairness in Neural Prototype Tree for Chest X-Ray Pathology Detection: Algorithm Development and Validation Study (Preprint)",
      "authors": [
        {
          "name": "Hongbo Chen",
          "openalex_id": "A5093099754",
          "orcid": "https://orcid.org/0009-0005-5823-9406"
        },
        {
          "name": "Myrtede Alfred",
          "openalex_id": "A5009601221",
          "orcid": "https://orcid.org/0000-0003-0045-0426"
        },
        {
          "name": "Andrew D. Brown",
          "openalex_id": "A5002300337",
          "orcid": "https://orcid.org/0000-0002-5389-308X"
        },
        {
          "name": "Angela Atinga",
          "openalex_id": "A5021502221",
          "orcid": "https://orcid.org/0000-0002-6681-6008"
        },
        {
          "name": "Eldan Cohen",
          "openalex_id": "A5087004754",
          "orcid": "https://orcid.org/0000-0001-5767-6683"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-03-31",
      "abstract": "<sec> <title>BACKGROUND</title> While deep learning classifiers have shown remarkable results in detecting chest X-ray (CXR) pathologies, their adoption in clinical settings is often hampered by the lack of transparency. To bridge this gap, this study introduces the neural prototype tree (NPT), an interpretable image classifier that combines the diagnostic capability of deep learning models and the interpretability of the decision tree for CXR pathology detection. </sec> <sec> <title>OBJECTIVE</title> This study aimed to investigate the utility of the NPT classifier in 3 dimensions, including performance, interpretability, and fairness, and subsequently examined the complex interaction between these dimensions. We highlight both local and global explanations of the NPT classifier and discuss its potential utility in clinical settings. </sec> <sec> <title>METHODS</title> This study used CXRs from the publicly available Chest X-ray 14, CheXpert, and MIMIC-CXR datasets. We trained 6 separate classifiers for each CXR pathology in all datasets, 1 baseline residual neural network (ResNet)\u2013152, and 5 NPT classifiers with varying levels of interpretability. Performance, interpretability, and fairness were measured using the area under the receiver operating characteristic curve (ROC AUC), interpretation complexity (IC), and mean true positive rate (TPR) disparity, respectively. Linear regression analyses were performed to investigate the relationship between IC and ROC AUC, as well as between IC and mean TPR disparity. </sec> <sec> <title>RESULTS</title> The performance of the NPT classifier improved as the IC level increased, surpassing that of ResNet-152 at IC level 15 for the Chest X-ray 14 dataset and IC level 31 for the CheXpert and MIMIC-CXR datasets. The NPT classifier at IC level 1 exhibited the highest degree of unfairness, as indicated by the mean TPR disparity. The magnitude of unfairness, as measured by the mean TPR disparity, was more pronounced in groups differentiated by age (chest X-ray 14 0.112, SD 0.015; CheXpert 0.097, SD 0.010; MIMIC 0.093, SD 0.017) compared to sex (chest X-ray 14 0.054 SD 0.012; CheXpert 0.062, SD 0.008; MIMIC 0.066, SD 0.013). A significant positive relationship between interpretability (ie, IC level) and performance (ie, ROC AUC) was observed across all CXR pathologies (&lt;i&gt;P&lt;/i&gt;&lt;.001). Furthermore, linear regression analysis revealed a significant negative relationship between interpretability and fairness (ie, mean TPR disparity) across age and sex subgroups (&lt;i&gt;P&lt;/i&gt;&lt;.001). </sec> <sec> <title>CONCLUSIONS</title> By illuminating the intricate relationship between performance, interpretability, and fairness of the NPT classifier, this research offers insightful perspectives that could guide future developments in effective, interpretable, and equitable deep learning classifiers for CXR pathology detection. </sec>",
      "cited_by_count": 2,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://doi.org/10.2196/preprints.59045"
      },
      "topics": [
        "Radiomics and Machine Learning in Medical Imaging",
        "COVID-19 diagnosis using AI",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 63,
      "url": "https://openalex.org/W4404465235"
    },
    {
      "openalex_id": "W3164841514",
      "doi": "10.1155/2021/4037533",
      "title": "Intelligent Traffic Management System Based on the Internet of Vehicles (IoV)",
      "authors": [
        {
          "name": "Samir A. Elsagheer Mohamed",
          "openalex_id": "A5066251602",
          "orcid": "https://orcid.org/0000-0003-4388-1998",
          "institutions": [
            "Aswan University",
            "Egypt-Japan University of Science and Technology",
            "Qassim University"
          ]
        },
        {
          "name": "Khaled Al-Shalfan",
          "openalex_id": "A5020948205",
          "institutions": [
            "Imam Mohammad ibn Saud Islamic University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-05-26",
      "abstract": "The present era is marked by rapid improvement and advances in technology. One of the most essential areas that demand improvement is the traffic signal, as it constitutes the core of the traffic system. This demand becomes stringent with the development of Smart Cities. Unfortunately, road traffic is currently controlled by very old traffic signals (tri-color signals) regardless of the relentless effort devoted to developing and improving the traffic flow. These traditional traffic signals have many problems including inefficient time management in road intersections; they are not immune to some environmental conditions, like rain; and they have no means of giving priority to emergency vehicles. New technologies like Vehicular Ad-hoc Networks (VANET) and Internet of Vehicles (IoV) enable vehicles to communicate with those nearby and with a dedicated infrastructure wirelessly. In this paper, we propose a new traffic management system based on the existing VANET and IoV that is suitable for future traffic systems and Smart Cities. In this paper, we present the architecture of our proposed Intelligent Traffic Management System (ITMS) and Smart Traffic Signal (STS) controller. We present local traffic management of an intersection based on the demands of future Smart Cities for fairness, reducing commute time, providing reasonable traffic flow, reducing traffic congestion, and giving priority to emergency vehicles. Simulation results showed that the proposed system outperforms the traditional management system and could be a candidate for the traffic management system in future Smart Cities. Our proposed adaptive algorithm not only significantly reduces the average waiting time (delay) but also increases the number of serviced vehicles. Besides, we present the implemented hardware prototype for STS.",
      "cited_by_count": 119,
      "type": "article",
      "source": {
        "name": "Journal of Advanced Transportation",
        "type": "journal",
        "issn": [
          "0197-6729",
          "2042-3195"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://downloads.hindawi.com/journals/jat/2021/4037533.pdf"
      },
      "topics": [
        "Vehicular Ad Hoc Networks (VANETs)",
        "Traffic Prediction and Management Techniques",
        "Traffic control and management"
      ],
      "referenced_works_count": 88,
      "url": "https://openalex.org/W3164841514"
    },
    {
      "openalex_id": "W4312663109",
      "doi": "10.56315/pscf3-21kearns",
      "title": "The Ethical Algorithm: The Science of Socially Aware Algorithm Design",
      "authors": [
        {
          "name": "Michael Kearns",
          "openalex_id": "A5029730907",
          "orcid": "https://orcid.org/0000-0001-7569-0147"
        },
        {
          "name": "Aaron Roth",
          "openalex_id": "A5057693522",
          "orcid": "https://orcid.org/0000-0002-0586-0515"
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-03-01",
      "abstract": "THE ETHICAL ALGORITHM: The Science of Socially Aware Algorithm Design by Michael Kearns and Aaron Roth. New York: Oxford University Press, 2019. 232 pages. Hardcover; $24.95. ISBN: 9780190948207. *Can an algorithm be ethical? That question appears to be similar to asking if a hammer can be ethical. Isn't the ethics solely related to how the hammer is used? Using it to build a house seems ethical; using it to harm another person would be immoral. *That line of thinking would be appropriate if the algorithm were something as simple as a sorting routine. If we sort the list of names in a wedding guest book so that the thank-you cards can be sent more systematically, its use would be acceptable; sorting a list of email addresses by education level in order to target people with a scam would be immoral. *The algorithms under consideration in The Ethical Algorithm are of a different nature, and the ethical issues are more complex. These algorithms are of fairly recent origin. They arise as we try to make use of vast collections of data to make more-accurate decisions: for example, using income, credit history, current debt level, and education level to approve or disapprove a loan application. A second example would be the use of high school GPA, ACT or SAT scores, and extra-curricular activities to determine college admissions. *The algorithms under consideration use machine-learning techniques (a branch of artificial intelligence) to look at the success rates of past student admissions and instruct the machine-learning algorithm to determine a set of criteria that successfully distinguish (with minimal errors) between those past students who graduated and those who didn't. That set of criteria (called a \"model\") can then be used to predict the success of future applicants. *The ethical component is important because such machine-learning algorithms optimize with particular goals as targets. And there tend to be unintended consequences--such as higher rates of rejection of applicants of color who would actually have succeeded. The solution to this problem requires more than just adding social equity goals as part of what is to be optimized--although that is an important step. *The authors advocate the development of precise definitions of the social goals we seek, and then the development of algorithmic techniques that help produce those goals. One important example is the social goal of privacy. What follows leaves out many important ideas found in the book, but illustrates the key points. Kearns and Roth cite the release in the mid-1990s of a dataset containing medical records for all state employees of Massachusetts. The dataset was intended for the use of medical researchers. The governor assured the employees that identifying information had been removed--names, social security numbers, and addresses. Two weeks later, Latanya Sweeney, a PhD student at MIT, sent the governor his medical records from that dataset. It cost her $20 to legally purchase the voter rolls for the city of Cambridge, MA. She then correlated that with other publicly available information to eliminate every other person from the medical dataset other than the governor himself. *Achieving data privacy is not as simple as was originally thought. To make progress, a good definition of privacy is needed. One useful definition is the notion of differential privacy: \"nothing about an individual should be learnable from a dataset that cannot be learned from the same dataset but with the individual's data removed\" (p. 36). This needs to also prevent identification by merging multiple datasets (for example, the medical records from several hospitals from which we might be able to identify an individual by looking for intersections on a few key attributes such as age, gender, and illness). One way to achieve this goal is to add randomness to the data. This can be done in a manner in which the probability of determining an individual changes very little by adding or removing that person's data to/from the dataset. *A very clever technique for adding this random noise can be found in a randomized response, an idea introduced in the 1960s to get accurate information in polls about sensitive topics (such as, \"have you cheated on your taxes?\"). The respondent is told to flip a coin. If it is a head, answer truthfully. If it is a tail, flip a second time and answer \"yes\" if it is a head and \"no\" if it is a tail. Suppose the true proportion of people who cheat on their taxes is p. Some pretty simple math shows that with a sufficiently large sample size (larger than needed for surveys that are less sensitive), the measured proportion, m, of \"yes\" responses will be close to m = \u00bc + \u00bd p. We can then approximate p as 2m - \u00bd, and still give individuals reasonable deniability. If I answer \"yes\" and a hacker finds my record, there is still a 25% chance that my true answer is \"no.\" My privacy has been effectively protected. So we can achieve reasonable privacy at the cost of needing a larger dataset. *This short book discusses privacy, fairness, multiplayer games (such as using apps to direct your morning commute), pitfalls in scientific research, accountability, the singularity (a future time where machines might become \"smarter\" than humans), and more. Sufficient detail is given so that the reader can understand the ideas and the fundamental aspects of the algorithms without requiring a degree in mathematics or computer science. *One of the fundamental issues driving the need for ethical algorithms is the unintended consequences that result from well-intended choices. This is not a new phenomenon--Lot made a choice based on the data he had available: \"Lot looked about him, and saw that the plain of the Jordan was well watered everywhere like the garden of the Lord, like the land of Egypt ...\" Genesis 13:10 (NRSV). But by choosing that apparently desirable location, Lot brought harm to his family. *I have often pondered the command of Jesus in Matthew 10:16 where he instructs us to \"be wise as serpents and innocent as doves.\" Perhaps one way to apply this command is to be wise as we are devising algorithms to make sure that they do no harm. We should be willing to give up some efficiency in order to achieve more equitable results. *Reviewed by Eric Gossett, Department of Mathematics and Computer Science, Bethel University, St. Paul, MN 55112.",
      "cited_by_count": 266,
      "type": "article",
      "source": {
        "name": "Perspectives on Science and Christian Faith",
        "type": "journal",
        "issn": [
          "0892-2675"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4312663109"
    },
    {
      "openalex_id": "W3107969206",
      "doi": "10.1093/jamia/ocaa283",
      "title": "Addressing bias in prediction models by improving subpopulation calibration",
      "authors": [
        {
          "name": "Noam Barda",
          "openalex_id": "A5091779604",
          "orcid": "https://orcid.org/0000-0002-3400-235X",
          "institutions": [
            "Ben-Gurion University of the Negev",
            "Clalit Health Services",
            "Harvard University"
          ]
        },
        {
          "name": "Gal Yona",
          "openalex_id": "A5037714222",
          "institutions": [
            "Weizmann Institute of Science"
          ]
        },
        {
          "name": "Guy N. Rothblum",
          "openalex_id": "A5057788395",
          "orcid": "https://orcid.org/0000-0001-5273-6472",
          "institutions": [
            "Weizmann Institute of Science"
          ]
        },
        {
          "name": "Philip Greenland",
          "openalex_id": "A5005650203",
          "orcid": "https://orcid.org/0000-0002-6327-2439",
          "institutions": [
            "Northwestern University"
          ]
        },
        {
          "name": "Morton Leibowitz",
          "openalex_id": "A5057087955",
          "institutions": [
            "Clalit Health Services"
          ]
        },
        {
          "name": "Ran D. Balicer",
          "openalex_id": "A5079572059",
          "orcid": "https://orcid.org/0000-0002-7783-6362",
          "institutions": [
            "Clalit Health Services",
            "Ben-Gurion University of the Negev"
          ]
        },
        {
          "name": "Eitan Bachmat",
          "openalex_id": "A5083255118",
          "orcid": "https://orcid.org/0000-0001-7153-100X",
          "institutions": [
            "Ben-Gurion University of the Negev"
          ]
        },
        {
          "name": "Noa Dagan",
          "openalex_id": "A5057364990",
          "orcid": "https://orcid.org/0000-0001-8811-7825",
          "institutions": [
            "Clalit Health Services",
            "Ben-Gurion University of the Negev",
            "Harvard University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-10-26",
      "abstract": "Abstract Objective To illustrate the problem of subpopulation miscalibration, to adapt an algorithm for recalibration of the predictions, and to validate its performance. Materials and Methods In this retrospective cohort study, we evaluated the calibration of predictions based on the Pooled Cohort Equations (PCE) and the fracture risk assessment tool (FRAX) in the overall population and in subpopulations defined by the intersection of age, sex, ethnicity, socioeconomic status, and immigration history. We next applied the recalibration algorithm and assessed the change in calibration metrics, including calibration-in-the-large. Results 1 021 041 patients were included in the PCE population, and 1 116 324 patients were included in the FRAX population. Baseline overall model calibration of the 2 tested models was good, but calibration in a substantial portion of the subpopulations was poor. After applying the algorithm, subpopulation calibration statistics were greatly improved, with the variance of the calibration-in-the-large values across all subpopulations reduced by 98.8% and 94.3% in the PCE and FRAX models, respectively. Discussion Prediction models in medicine are increasingly common. Calibration, the agreement between predicted and observed risks, is commonly poor for subpopulations that were underrepresented in the development set of the models, resulting in bias and reduced performance for these subpopulations. In this work, we empirically evaluated an adapted version of the fairness algorithm designed by Hebert-Johnson et al. (2017) and demonstrated its use in improving subpopulation miscalibration. Conclusion A postprocessing and model-independent fairness algorithm for recalibration of predictive models greatly decreases the bias of subpopulation miscalibration and thus increases fairness and equality.",
      "cited_by_count": 58,
      "type": "article",
      "source": {
        "name": "Journal of the American Medical Informatics Association",
        "type": "journal",
        "issn": [
          "1067-5027",
          "1527-974X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/7936516"
      },
      "topics": [
        "Machine Learning in Healthcare",
        "Artificial Intelligence in Healthcare",
        "Statistical Methods and Inference"
      ],
      "referenced_works_count": 43,
      "url": "https://openalex.org/W3107969206"
    },
    {
      "openalex_id": "W3164035696",
      "doi": "10.4230/lipics.forc.2021.7",
      "title": "Causal Intersectionality and Fair Ranking",
      "authors": [
        {
          "name": "Ke Yang",
          "openalex_id": "A5061285256",
          "orcid": "https://orcid.org/0000-0002-1617-5986",
          "institutions": [
            "University of Applied Sciences and Arts of Southern Switzerland",
            "Shandong University of Political Science and Law",
            "Sup\u00e9lec"
          ]
        },
        {
          "name": "Joshua R. Loftus",
          "openalex_id": "A5006180749",
          "orcid": "https://orcid.org/0000-0002-2905-1632",
          "institutions": [
            "London School of Economics and Political Science"
          ]
        },
        {
          "name": "Julia Stoyanovich",
          "openalex_id": "A5082830839",
          "orcid": "https://orcid.org/0000-0002-1587-0450"
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-01-01",
      "abstract": "In this paper we propose a causal modeling approach to intersectional fairness, and a flexible, task-specific method for computing intersectionally fair rankings. Rankings are used in many contexts, ranging from Web search to college admissions, but causal inference for fair rankings has received limited attention. Additionally, the growing literature on causal fairness has directed little attention to intersectionality. By bringing these issues together in a formal causal framework we make the application of intersectionality in algorithmic fairness explicit, connected to important real world effects and domain knowledge, and transparent about technical limitations. We experimentally evaluate our approach on real and synthetic datasets, exploring its behavior under different structural assumptions.",
      "cited_by_count": 19,
      "type": "book-chapter",
      "source": {
        "name": "Leibniz-Zentrum f\u00fcr Informatik (Schloss Dagstuhl)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.FORC.2021.7"
      },
      "topics": [
        "Game Theory and Voting Systems",
        "Gender Politics and Representation",
        "Electoral Systems and Political Participation"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W3164035696"
    },
    {
      "openalex_id": "W4385768144",
      "doi": "10.24963/ijcai.2023/742",
      "title": "A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges",
      "authors": [
        {
          "name": "Usman Gohar",
          "openalex_id": "A5088196515",
          "orcid": "https://orcid.org/0009-0001-8859-4928",
          "institutions": [
            "Iowa State University"
          ]
        },
        {
          "name": "Lu Cheng",
          "openalex_id": "A5100384333",
          "orcid": "https://orcid.org/0000-0003-1460-8108",
          "institutions": [
            "University of Illinois Chicago"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-08-01",
      "abstract": "The widespread adoption of Machine Learning systems, especially in more decision-critical applications such as criminal sentencing and bank loans, has led to increased concerns about fairness implications. Algorithms and metrics have been developed to mitigate and measure these discriminations. More recently, works have identified a more challenging form of bias called intersectional bias, which encompasses multiple sensitive attributes, such as race and gender, together. In this survey, we review the state-of-the-art in intersectional fairness. We present a taxonomy for intersectional notions of fairness and mitigation. Finally, we identify the key challenges and provide researchers with guidelines for future directions.",
      "cited_by_count": 25,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.ijcai.org/proceedings/2023/0742.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 58,
      "url": "https://openalex.org/W4385768144"
    },
    {
      "openalex_id": "W4310358880",
      "doi": "10.1016/j.trc.2022.103933",
      "title": "COOR-PLT: A hierarchical control model for coordinating adaptive platoons of connected and autonomous vehicles at signal-free intersections based on deep reinforcement learning",
      "authors": [
        {
          "name": "Duowei Li",
          "openalex_id": "A5078101985",
          "orcid": "https://orcid.org/0000-0002-1940-2435",
          "institutions": [
            "Tsinghua University",
            "Nanyang Technological University"
          ]
        },
        {
          "name": "Feng Zhu",
          "openalex_id": "A5031222391",
          "orcid": "https://orcid.org/0000-0002-9814-6053",
          "institutions": [
            "Nanyang Technological University"
          ]
        },
        {
          "name": "Tianyi Chen",
          "openalex_id": "A5100783473",
          "orcid": "https://orcid.org/0000-0003-1744-9579",
          "institutions": [
            "Nanyang Technological University"
          ]
        },
        {
          "name": "Yiik Diew Wong",
          "openalex_id": "A5010228559",
          "orcid": "https://orcid.org/0000-0001-7419-5777",
          "institutions": [
            "Nanyang Technological University"
          ]
        },
        {
          "name": "Chunli Zhu",
          "openalex_id": "A5042218238",
          "orcid": "https://orcid.org/0000-0002-6109-9484",
          "institutions": [
            "Beijing Institute of Technology"
          ]
        },
        {
          "name": "Jianping Wu",
          "openalex_id": "A5055302018",
          "orcid": "https://orcid.org/0000-0002-6698-3607",
          "institutions": [
            "Tsinghua University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-11-29",
      "abstract": "Platooning and coordination are two implementation strategies that are frequently proposed for traffic control of connected and autonomous vehicles (CAVs) at signal-free intersections instead of using conventional traffic signals. However, few studies have attempted to integrate both strategies to better facilitate the CAV control at signal-free intersections. To this end, this study proposes a hierarchical control model, named COOR-PLT, to coordinate adaptive CAV platoons at a signal-free intersection based on deep reinforcement learning (DRL). COOR-PLT has a two-layer framework. The first layer uses a centralized control strategy to form adaptive platoons. The optimal size of each platoon is determined by considering multiple objectives (i.e., efficiency, fairness and energy saving). The second layer employs a decentralized control strategy to coordinate multiple platoons passing through the intersection. Each platoon is labeled with coordinated status or independent status, upon which its passing priority is determined. As an efficient DRL algorithm, Deep Q-network (DQN) is adopted to determine platoon sizes and passing priorities respectively in the two layers. The model is validated and examined on the simulator Simulation of Urban Mobility (SUMO). The simulation results demonstrate that the model is able to: (1) achieve satisfactory convergence performances; (2) adaptively determine platoon size in response to varying traffic conditions; and (3) completely avoid deadlocks at the intersection. By comparison with other control methods, the model manifests its superiority of adopting adaptive platooning and DRL-based coordination strategies. Also, the model outperforms several state-of-the-art methods on reducing travel time and fuel consumption in different traffic conditions.",
      "cited_by_count": 51,
      "type": "article",
      "source": {
        "name": "Transportation Research Part C Emerging Technologies",
        "type": "journal",
        "issn": [
          "0968-090X",
          "1879-2359"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://doi.org/10.1016/j.trc.2022.103933"
      },
      "topics": [
        "Traffic control and management",
        "Traffic and Road Safety",
        "Autonomous Vehicle Technology and Safety"
      ],
      "referenced_works_count": 62,
      "url": "https://openalex.org/W4310358880"
    },
    {
      "openalex_id": "W4317385006",
      "doi": "10.1109/tits.2023.3235774",
      "title": "Cooperative Platoon Formation of Connected and Autonomous Vehicles: Toward Efficient Merging Coordination at Unsignalized Intersections",
      "authors": [
        {
          "name": "Zhiyun Deng",
          "openalex_id": "A5064896724",
          "orcid": "https://orcid.org/0000-0002-6176-3242",
          "institutions": [
            "Huazhong University of Science and Technology"
          ]
        },
        {
          "name": "Kaidi Yang",
          "openalex_id": "A5075233338",
          "orcid": "https://orcid.org/0000-0001-5120-2866",
          "institutions": [
            "National University of Singapore"
          ]
        },
        {
          "name": "Weiming Shen",
          "openalex_id": "A5062049138",
          "orcid": "https://orcid.org/0000-0001-5204-7992",
          "institutions": [
            "Huazhong University of Science and Technology"
          ]
        },
        {
          "name": "Yanjun Shi",
          "openalex_id": "A5101558759",
          "orcid": "https://orcid.org/0000-0003-2078-2137",
          "institutions": [
            "Dalian University of Technology"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-18",
      "abstract": "This paper presents a Vehicle-Platoon-Aware Bi-Level Optimization Algorithm for Autonomous Intersection Management (VPA-AIM) to coordinate the merging of Connected and Automated Vehicles at unsignalized intersections. The constraint-coupled bi-level optimization is operated within a rolling horizon to balance traffic performance and computational efficiency. In each decision step, the platoon formation scheme is incorporated into an upper-level traffic scheduling model as decision variables to pursue an optimal schedule from a systemic view. Meanwhile, the passing sequence and timeslots of vehicles are jointly optimized with the platoon configuration scheme by virtue of real-time traffic states to improve operational efficiency and fairness. After that, a lower-level trajectory planning model will generate dynamically-feasible and energy-efficient trajectories according to the given schedule and coupling constraints with the objective of improving space utilization to prevent spillbacks. Moreover, the quantifiable connection between the makespan of traffic scheduling schemes and the occurrence of spillbacks is established, demonstrating that the cooperative platoon formation strategy is effective in avoiding and mitigating spillbacks in normal and saturated traffic states. Additionally, the proposed algorithm can be extended to mixed traffic scenarios. Numerical experiments are conducted on extensive scenarios with different arrival flows, where the Constraint Programming technique is employed to produce the optimal schedule. Experimental results indicate the superiority of the proposed approach in optimality and stability with reasonable sub-second computation time for real-life applications.",
      "cited_by_count": 51,
      "type": "article",
      "source": {
        "name": "IEEE Transactions on Intelligent Transportation Systems",
        "type": "journal",
        "issn": [
          "1524-9050",
          "1558-0016"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Traffic control and management",
        "Transportation Planning and Optimization",
        "Traffic Prediction and Management Techniques"
      ],
      "referenced_works_count": 95,
      "url": "https://openalex.org/W4317385006"
    },
    {
      "openalex_id": "W2997747012",
      "doi": "10.1609/aaai.v34i07.6999",
      "title": "Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression",
      "authors": [
        {
          "name": "Zhaohui Zheng",
          "openalex_id": "A5101740169",
          "orcid": "https://orcid.org/0000-0002-7664-2363",
          "institutions": [
            "Tianjin University"
          ]
        },
        {
          "name": "Ping Wang",
          "openalex_id": "A5100413044",
          "orcid": "https://orcid.org/0000-0002-7773-469X",
          "institutions": [
            "Tianjin University"
          ]
        },
        {
          "name": "Wei Liu",
          "openalex_id": "A5087448195",
          "orcid": "https://orcid.org/0009-0008-7101-9530",
          "institutions": [
            "Tianjin University"
          ]
        },
        {
          "name": "Jinze Li",
          "openalex_id": "A5100728945",
          "orcid": "https://orcid.org/0000-0003-0697-057X",
          "institutions": [
            "China People's Public Security University"
          ]
        },
        {
          "name": "Rongguang Ye",
          "openalex_id": "A5067337261",
          "institutions": [
            "Tianjin University"
          ]
        },
        {
          "name": "Dongwei Ren",
          "openalex_id": "A5051402211",
          "orcid": "https://orcid.org/0000-0002-0965-6810",
          "institutions": [
            "Tianjin University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-04-03",
      "abstract": "Bounding box regression is the crucial step in object detection. In existing methods, while \u2113n-norm loss is widely adopted for bounding box regression, it is not tailored to the evaluation metric, i.e., Intersection over Union (IoU). Recently, IoU loss and generalized IoU (GIoU) loss have been proposed to benefit the IoU metric, but still suffer from the problems of slow convergence and inaccurate regression. In this paper, we propose a Distance-IoU (DIoU) loss by incorporating the normalized distance between the predicted box and the target box, which converges much faster in training than IoU and GIoU losses. Furthermore, this paper summarizes three geometric factors in bounding box regression, i.e., overlap area, central point distance and aspect ratio, based on which a Complete IoU (CIoU) loss is proposed, thereby leading to faster convergence and better performance. By incorporating DIoU and CIoU losses into state-of-the-art object detection algorithms, e.g., YOLO v3, SSD and Faster R-CNN, we achieve notable performance gains in terms of not only IoU metric but also GIoU metric. Moreover, DIoU can be easily adopted into non-maximum suppression (NMS) to act as the criterion, further boosting performance improvement. The source code and trained models are available at https://github.com/Zzh-tju/DIoU.",
      "cited_by_count": 3749,
      "type": "article",
      "source": {
        "name": "Proceedings of the AAAI Conference on Artificial Intelligence",
        "type": "conference",
        "issn": [
          "2159-5399",
          "2374-3468"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "https://ojs.aaai.org/index.php/AAAI/article/download/6999/6853"
      },
      "topics": [
        "Advanced Neural Network Applications",
        "Domain Adaptation and Few-Shot Learning",
        "COVID-19 diagnosis using AI"
      ],
      "referenced_works_count": 28,
      "url": "https://openalex.org/W2997747012"
    },
    {
      "openalex_id": "W4285613665",
      "doi": "10.1561/9781638280415",
      "title": "Fairness in Information Access Systems",
      "authors": [
        {
          "name": "Michael D. Ekstrand",
          "openalex_id": "A5015092805",
          "orcid": "https://orcid.org/0000-0003-2467-0108"
        },
        {
          "name": "Anubrata Das",
          "openalex_id": "A5110921585"
        },
        {
          "name": "Robin Burke",
          "openalex_id": "A5043134791",
          "orcid": "https://orcid.org/0000-0001-5766-6434"
        },
        {
          "name": "Fernando D\u00edaz",
          "openalex_id": "A5101492251",
          "orcid": "https://orcid.org/0000-0003-2345-1288"
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-01-01",
      "abstract": "Recommendation, information retrieval, and other information access systems pose unique challenges for investigating and applying the fairness and non-discrimination concepts that have been developed for studying other machine learning systems. While fair information access shares many commonalities with fair classification, there are important differences such as the multistakeholder nature of information access applications, the rank-based problem setting, the centrality of personalization in many cases, and the role of user response. These all complicate the problem of identifying precisely what types and operationalizations of fairness may be relevant. In this monograph, the authors present a taxonomy of the various dimensions of fair information access and survey the literature to date on this new and rapidly-growing topic. They preface this with brief introductions to information access and algorithmic fairness to facilitate the use of this work by scholars who wish to study their intersection. The authors conclude with several open problems in fair information access and present suggestions for how to approach research in this space.",
      "cited_by_count": 31,
      "type": "book",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4285613665"
    },
    {
      "openalex_id": "W4387397186",
      "doi": "10.1038/s41467-023-41974-4",
      "title": "Improving model fairness in image-based computer-aided diagnosis",
      "authors": [
        {
          "name": "Mingquan Lin",
          "openalex_id": "A5000776140",
          "orcid": "https://orcid.org/0000-0003-0862-6588",
          "institutions": [
            "Cornell University",
            "Weill Cornell Medicine"
          ]
        },
        {
          "name": "Tianhao Li",
          "openalex_id": "A5070111623",
          "institutions": [
            "The University of Texas at Austin"
          ]
        },
        {
          "name": "Yifan Yang",
          "openalex_id": "A5101623103",
          "orcid": "https://orcid.org/0000-0003-4414-9176",
          "institutions": [
            "National Institutes of Health",
            "National Center for Biotechnology Information"
          ]
        },
        {
          "name": "Gregory Holste",
          "openalex_id": "A5071860390",
          "orcid": "https://orcid.org/0000-0002-5657-3081",
          "institutions": [
            "The University of Texas at Austin"
          ]
        },
        {
          "name": "Ying Ding",
          "openalex_id": "A5047170063",
          "orcid": "https://orcid.org/0000-0003-2567-2009",
          "institutions": [
            "The University of Texas at Austin"
          ]
        },
        {
          "name": "Sarah H. Van Tassel",
          "openalex_id": "A5041069035",
          "institutions": [
            "Cornell University",
            "Weill Cornell Medicine"
          ]
        },
        {
          "name": "Kyle D. Kovacs",
          "openalex_id": "A5029327968",
          "orcid": "https://orcid.org/0000-0001-7568-6703",
          "institutions": [
            "Cornell University",
            "Weill Cornell Medicine"
          ]
        },
        {
          "name": "George Shih",
          "openalex_id": "A5018859884",
          "orcid": "https://orcid.org/0000-0002-8356-2011",
          "institutions": [
            "Weill Cornell Medicine",
            "Cornell University"
          ]
        },
        {
          "name": "Zhangyang Wang",
          "openalex_id": "A5048522863",
          "orcid": "https://orcid.org/0000-0002-2050-5693",
          "institutions": [
            "The University of Texas at Austin"
          ]
        },
        {
          "name": "Zhiyong Lu",
          "openalex_id": "A5083081872",
          "orcid": "https://orcid.org/0000-0001-9998-916X",
          "institutions": [
            "National Institutes of Health",
            "National Center for Biotechnology Information"
          ]
        },
        {
          "name": "Fei Wang",
          "openalex_id": "A5100455768",
          "orcid": "https://orcid.org/0000-0001-9459-9461",
          "institutions": [
            "Cornell University",
            "Weill Cornell Medicine"
          ]
        },
        {
          "name": "Yifan Peng",
          "openalex_id": "A5085113833",
          "orcid": "https://orcid.org/0000-0001-9309-8331",
          "institutions": [
            "Weill Cornell Medicine",
            "Cornell University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-10-06",
      "abstract": "Abstract Deep learning has become a popular tool for computer-aided diagnosis using medical images, sometimes matching or exceeding the performance of clinicians. However, these models can also reflect and amplify human bias, potentially resulting inaccurate missed diagnoses. Despite this concern, the problem of improving model fairness in medical image classification by deep learning has yet to be fully studied. To address this issue, we propose an algorithm that leverages the marginal pairwise equal opportunity to reduce bias in medical image classification. Our evaluations across four tasks using four independent large-scale cohorts demonstrate that our proposed algorithm not only improves fairness in individual and intersectional subgroups but also maintains overall performance. Specifically, the relative change in pairwise fairness difference between our proposed model and the baseline model was reduced by over 35%, while the relative change in AUC value was typically within 1%. By reducing the bias generated by deep learning models, our proposed approach can potentially alleviate concerns about the fairness and reliability of image-based computer-aided diagnosis.",
      "cited_by_count": 25,
      "type": "article",
      "source": {
        "name": "Nature Communications",
        "type": "journal",
        "issn": [
          "2041-1723"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.nature.com/articles/s41467-023-41974-4.pdf"
      },
      "topics": [
        "AI in cancer detection",
        "Artificial Intelligence in Healthcare and Education",
        "COVID-19 diagnosis using AI"
      ],
      "referenced_works_count": 31,
      "url": "https://openalex.org/W4387397186"
    },
    {
      "openalex_id": "W4283328594",
      "doi": "10.1002/aaai.12054",
      "title": "The multisided complexity of fairness in recommender systems",
      "authors": [
        {
          "name": "Nasim Sonboli",
          "openalex_id": "A5004521816",
          "orcid": "https://orcid.org/0000-0002-6988-7397",
          "institutions": [
            "University of Colorado Boulder"
          ]
        },
        {
          "name": "Robin Burke",
          "openalex_id": "A5043134791",
          "orcid": "https://orcid.org/0000-0001-5766-6434",
          "institutions": [
            "University of Colorado Boulder"
          ]
        },
        {
          "name": "Michael D. Ekstrand",
          "openalex_id": "A5015092805",
          "orcid": "https://orcid.org/0000-0003-2467-0108",
          "institutions": [
            "Boise State University"
          ]
        },
        {
          "name": "Rishabh Mehrotra",
          "openalex_id": "A5018503243",
          "orcid": "https://orcid.org/0000-0002-0836-4605"
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-06-01",
      "abstract": "Abstract Recommender systems are poised at the interface between stakeholders: for example, job applicants and employers in the case of recommendations of employment listings, or artists and listeners in the case of music recommendation. In such multisided platforms, recommender systems play a key role in enabling discovery of products and information at large scales. However, as they have become more and more pervasive in society, the equitable distribution of their benefits and harms have been increasingly under scrutiny, as is the case with machine learning generally. While recommender systems can exhibit many of the biases encountered in other machine learning settings, the intersection of personalization and multisidedness makes the question of fairness in recommender systems manifest itself quite differently. In this article, we discuss recent work in the area of multisided fairness in recommendation, starting with a brief introduction to core ideas in algorithmic fairness and multistakeholder recommendation. We describe techniques for measuring fairness and algorithmic approaches for enhancing fairness in recommendation outputs. We also discuss feedback and popularity effects that can lead to unfair recommendation outcomes. Finally, we introduce several promising directions for future research in this area.",
      "cited_by_count": 23,
      "type": "article",
      "source": {
        "name": "AI Magazine",
        "type": "journal",
        "issn": [
          "0738-4602",
          "2371-9621"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/aaai.12054"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Blockchain Technology Applications and Security",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 72,
      "url": "https://openalex.org/W4283328594"
    },
    {
      "openalex_id": "W4396733184",
      "doi": "10.1007/s11831-024-10134-2",
      "title": "A Comprehensive Review of Bias in Deep Learning Models: Methods, Impacts, and Future Directions",
      "authors": [
        {
          "name": "Milind Shah",
          "openalex_id": "A5101641091",
          "orcid": "https://orcid.org/0009-0001-6077-3924"
        },
        {
          "name": "Nitesh Sureja",
          "openalex_id": "A5086433535",
          "orcid": "https://orcid.org/0000-0002-4429-1597"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-05-08",
      "abstract": null,
      "cited_by_count": 55,
      "type": "review",
      "source": {
        "name": "Archives of Computational Methods in Engineering",
        "type": "journal",
        "issn": [
          "1134-3060",
          "1886-1784"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Adversarial Robustness in Machine Learning",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 42,
      "url": "https://openalex.org/W4396733184"
    },
    {
      "openalex_id": "W4391706801",
      "doi": "10.30574/ijsra.2024.11.1.0235",
      "title": "AI and ethics in business: A comprehensive review of responsible AI practices and corporate responsibility",
      "authors": [
        {
          "name": "Funmilola Olatundun Olatoye",
          "openalex_id": "A5093362229"
        },
        {
          "name": "Kehinde Feranmi Awonuga",
          "openalex_id": "A5093804235"
        },
        {
          "name": "Noluthando Zamanjomane Mhlongo",
          "openalex_id": "A5109687100"
        },
        {
          "name": "Chidera Victoria Ibeh",
          "openalex_id": "A5093799248"
        },
        {
          "name": "Oluwafunmi Adijat Elufioye",
          "openalex_id": "A5093441276"
        },
        {
          "name": "Ndubuisi Leonard Ndubuisi",
          "openalex_id": "A5093720301"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-02-09",
      "abstract": "As artificial intelligence (AI) continues to revolutionize business landscapes, the ethical implications of its deployment have garnered significant attention. This paper presents a comprehensive review of the intersection between AI and ethics in the context of corporate responsibility. The integration of AI into business processes necessitates a thorough understanding of responsible AI practices to ensure that technological advancements align with ethical standards and societal values. The first dimension explored in this review is the critical importance of transparency in AI algorithms and decision-making processes. Businesses adopting AI technologies must prioritize transparency to build trust among stakeholders, ensuring that the decision-making processes are understandable and accountable. Ethical considerations also extend to issues of bias and fairness, prompting the need for diverse and inclusive datasets to prevent discriminatory outcomes. Corporate responsibility in the realm of AI extends beyond technical aspects, encompassing the broader socio-economic impact of AI implementation. The review highlights the significance of considering the effects of AI on employment, inequality, and accessibility. Businesses are urged to adopt ethical guidelines that prioritize the well-being of employees and society at large, mitigating the potential negative consequences of AI on employment dynamics and social structures. Furthermore, the paper delves into the ethical considerations surrounding data privacy and security, emphasizing the importance of responsible data handling practices. As businesses accumulate vast amounts of data, it becomes imperative to prioritize the protection of individuals' privacy rights, reinforcing the ethical foundation of AI applications. This comprehensive review underscores the need for businesses to integrate responsible AI practices within the framework of corporate responsibility. By prioritizing transparency, fairness, and ethical data practices, organizations can navigate the complex terrain of AI implementation while ensuring alignment with societal values and ethical standards. This synthesis of AI and ethics in business is essential for fostering a sustainable and responsible technological future.",
      "cited_by_count": 50,
      "type": "review",
      "source": {
        "name": "International Journal of Science and Research Archive",
        "type": "journal",
        "issn": [
          "2582-8185"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://ijsra.net/sites/default/files/IJSRA-2024-0235.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 58,
      "url": "https://openalex.org/W4391706801"
    },
    {
      "openalex_id": "W2998245413",
      "doi": "10.1007/s42001-020-00082-9",
      "title": "Where are we? Using Scopus to map the literature at the intersection between artificial intelligence and research on crime",
      "authors": [
        {
          "name": "Gian Maria Campedelli",
          "openalex_id": "A5023509726",
          "orcid": "https://orcid.org/0000-0002-7734-7956",
          "institutions": [
            "University of Trento"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-09-27",
      "abstract": "Abstract Research on artificial intelligence (AI) applications has spread over many scientific disciplines. Scientists have tested the power of intelligent algorithms developed to predict (or learn from) natural, physical and social phenomena. This also applies to crime-related research problems. Nonetheless, studies that map the current state of the art at the intersection between AI and crime are lacking. What are the current research trends in terms of topics in this area? What is the structure of scientific collaboration when considering works investigating criminal issues using machine learning, deep learning, and AI in general? What are the most active countries in this specific scientific sphere? Using data retrieved from the Scopus database, this work quantitatively analyzes 692 published works at the intersection between AI and crime employing network science to respond to these questions. Results show that researchers are mainly focusing on cyber-related criminal topics and that relevant themes such as algorithmic discrimination, fairness, and ethics are considerably overlooked. Furthermore, data highlight the extremely disconnected structure of co-authorship networks. Such disconnectedness may represent a substantial obstacle to a more solid community of scientists interested in these topics. Additionally, the graph of scientific collaboration indicates that countries that are more prone to engage in international partnerships are generally less central in the network. This means that scholars working in highly productive countries (e.g. the United States, China) tend to mostly collaborate domestically. Finally, current issues and future developments within this scientific area are also discussed.",
      "cited_by_count": 32,
      "type": "article",
      "source": {
        "name": "Journal of Computational Social Science",
        "type": "journal",
        "issn": [
          "2432-2717",
          "2432-2725"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s42001-020-00082-9.pdf"
      },
      "topics": [
        "Crime Patterns and Interventions",
        "Ethics and Social Impacts of AI",
        "Cybercrime and Law Enforcement Studies"
      ],
      "referenced_works_count": 91,
      "url": "https://openalex.org/W2998245413"
    },
    {
      "openalex_id": "W4387580323",
      "doi": "10.1007/s11042-023-16950-1",
      "title": "Smart road management system for prioritized autonomous vehicles under vehicle-to-everything (V2X) communication",
      "authors": [
        {
          "name": "Awad Bin Naeem",
          "openalex_id": "A5049989175",
          "orcid": "https://orcid.org/0000-0002-1634-7653",
          "institutions": [
            "National College of Business Administration and Economics"
          ]
        },
        {
          "name": "Abdul Majid Soomro",
          "openalex_id": "A5059471388",
          "orcid": "https://orcid.org/0009-0003-5159-3731",
          "institutions": [
            "National College of Business Administration and Economics"
          ]
        },
        {
          "name": "Hafiz Muhammad Saim",
          "openalex_id": "A5009803031",
          "institutions": [
            "Riphah International University"
          ]
        },
        {
          "name": "Hassaan Malik",
          "openalex_id": "A5046866614",
          "orcid": "https://orcid.org/0000-0002-4402-5088",
          "institutions": [
            "National College of Business Administration and Economics"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-10-12",
      "abstract": null,
      "cited_by_count": 28,
      "type": "article",
      "source": {
        "name": "Multimedia Tools and Applications",
        "type": "journal",
        "issn": [
          "1380-7501",
          "1573-7721"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Traffic Prediction and Management Techniques",
        "Traffic control and management",
        "Autonomous Vehicle Technology and Safety"
      ],
      "referenced_works_count": 19,
      "url": "https://openalex.org/W4387580323"
    },
    {
      "openalex_id": "W3100645984",
      "doi": "10.18653/v1/2020.clinicalnlp-1.33",
      "title": "Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal Clinical NLP",
      "authors": [
        {
          "name": "John Chen",
          "openalex_id": "A5101650717",
          "orcid": "https://orcid.org/0000-0002-9443-4795",
          "institutions": [
            "University of Toronto",
            "Vector Institute"
          ]
        },
        {
          "name": "Ian Berlot-Attwell",
          "openalex_id": "A5052822472",
          "institutions": [
            "University of Toronto",
            "Vector Institute"
          ]
        },
        {
          "name": "Xindi Wang",
          "openalex_id": "A5100781427",
          "orcid": "https://orcid.org/0000-0002-6548-3547",
          "institutions": [
            "Western University",
            "Vector Institute"
          ]
        },
        {
          "name": "Safwan Hossain",
          "openalex_id": "A5101918330",
          "orcid": "https://orcid.org/0000-0002-4909-6651",
          "institutions": [
            "University of Toronto",
            "Vector Institute"
          ]
        },
        {
          "name": "Frank Rudzicz",
          "openalex_id": "A5056256317",
          "orcid": "https://orcid.org/0000-0002-1139-3423",
          "institutions": [
            "University of Toronto",
            "Vector Institute"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-01",
      "abstract": "Clinical machine learning is increasingly multimodal, collected in both structured tabular formats and unstructured forms such as free text. We propose a novel task of exploring fairness on a multimodal clinical dataset, adopting equalized odds for the downstream medical prediction tasks. To this end, we investigate a modality-agnostic fairness algorithm - equalized odds post processing - and compare it to a text-specific fairness algorithm: debiased clinical word embeddings. Despite the fact that debiased word embeddings do not explicitly address equalized odds of protected groups, we show that a text-specific approach to fairness may simultaneously achieve a good balance of performance classical notions of fairness. Our work opens the door for future work at the critical intersection of clinical NLP and fairness.",
      "cited_by_count": 14,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://aclanthology.org/2020.clinicalnlp-1.33.pdf"
      },
      "topics": [
        "Topic Modeling",
        "Machine Learning in Healthcare",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 36,
      "url": "https://openalex.org/W3100645984"
    },
    {
      "openalex_id": "W4366091953",
      "doi": "10.3390/e25040660",
      "title": "Differential Fairness: An Intersectional Framework for Fair AI",
      "authors": [
        {
          "name": "Rashidul Islam",
          "openalex_id": "A5101830965",
          "orcid": "https://orcid.org/0000-0001-5276-5708",
          "institutions": [
            "University of Maryland, Baltimore County"
          ]
        },
        {
          "name": "Kamrun Naher Keya",
          "openalex_id": "A5026287600",
          "orcid": "https://orcid.org/0009-0009-5069-8383",
          "institutions": [
            "University of Maryland, Baltimore County"
          ]
        },
        {
          "name": "Shimei Pan",
          "openalex_id": "A5048111120",
          "orcid": "https://orcid.org/0000-0002-5989-8543",
          "institutions": [
            "University of Maryland, Baltimore County"
          ]
        },
        {
          "name": "Anand D. Sarwate",
          "openalex_id": "A5087162046",
          "orcid": "https://orcid.org/0000-0001-6123-5282",
          "institutions": [
            "Rutgers, The State University of New Jersey"
          ]
        },
        {
          "name": "James R. Foulds",
          "openalex_id": "A5010326834",
          "orcid": "https://orcid.org/0000-0003-0935-4182",
          "institutions": [
            "University of Maryland, Baltimore County"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-04-14",
      "abstract": "We propose definitions of fairness in machine learning and artificial intelligence systems that are informed by the framework of intersectionality, a critical lens from the legal, social science, and humanities literature which analyzes how interlocking systems of power and oppression affect individuals along overlapping dimensions including gender, race, sexual orientation, class, and disability. We show that our criteria behave sensibly for any subset of the set of protected attributes, and we prove economic, privacy, and generalization guarantees. Our theoretical results show that our criteria meaningfully operationalize AI fairness in terms of real-world harms, making the measurements interpretable in a manner analogous to differential privacy. We provide a simple learning algorithm using deterministic gradient methods, which respects our intersectional fairness criteria. The measurement of fairness becomes statistically challenging in the minibatch setting due to data sparsity, which increases rapidly in the number of protected attributes and in the values per protected attribute. To address this, we further develop a practical learning algorithm using stochastic gradient methods which incorporates stochastic estimation of the intersectional fairness criteria on minibatches to scale up to big data. Case studies on census data, the COMPAS criminal recidivism dataset, the HHP hospitalization data, and a loan application dataset from HMDA demonstrate the utility of our methods.",
      "cited_by_count": 16,
      "type": "article",
      "source": {
        "name": "Entropy",
        "type": "journal",
        "issn": [
          "1099-4300"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/1099-4300/25/4/660/pdf?version=1681483006"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Privacy-Preserving Technologies in Data",
        "Criminal Justice and Corrections Analysis"
      ],
      "referenced_works_count": 74,
      "url": "https://openalex.org/W4366091953"
    },
    {
      "openalex_id": "W4312085420",
      "doi": "10.1016/j.adhoc.2022.103061",
      "title": "SmartLight: A smart efficient traffic light scheduling algorithm for green road intersections",
      "authors": [
        {
          "name": "Maram Bani Younes",
          "openalex_id": "A5005168882",
          "orcid": "https://orcid.org/0000-0002-3844-6409",
          "institutions": [
            "Philadelphia University",
            "University of Ottawa"
          ]
        },
        {
          "name": "Azzedine Boukerche",
          "openalex_id": "A5012195474",
          "orcid": "https://orcid.org/0000-0002-3851-9938",
          "institutions": [
            "University of Ottawa"
          ]
        },
        {
          "name": "Floriano De Rango",
          "openalex_id": "A5075581736",
          "orcid": "https://orcid.org/0000-0003-4901-6233",
          "institutions": [
            "University of Calabria"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-12-05",
      "abstract": null,
      "cited_by_count": 21,
      "type": "article",
      "source": {
        "name": "Ad Hoc Networks",
        "type": "journal",
        "issn": [
          "1570-8705",
          "1570-8713"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Vehicle emissions and performance",
        "Traffic control and management",
        "Transportation Planning and Optimization"
      ],
      "referenced_works_count": 42,
      "url": "https://openalex.org/W4312085420"
    },
    {
      "openalex_id": "W3113471301",
      "doi": "10.48550/arxiv.2012.14285",
      "title": "Affirmative Algorithms: The Legal Grounds for Fairness as Awareness",
      "authors": [
        {
          "name": "Daniel E. Ho",
          "openalex_id": "A5058408154",
          "orcid": "https://orcid.org/0000-0002-2195-5469"
        },
        {
          "name": "Alice Xiang",
          "openalex_id": "A5035786206",
          "orcid": "https://orcid.org/0000-0001-7907-9353"
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-12-18",
      "abstract": "While there has been a flurry of research in algorithmic fairness, what is less recognized is that modern antidiscrimination law may prohibit the adoption of such techniques. We make three contributions. First, we discuss how such approaches will likely be deemed \"algorithmic affirmative action,\" posing serious legal risks of violating equal protection, particularly under the higher education jurisprudence. Such cases have increasingly turned toward anticlassification, demanding \"individualized consideration\" and barring formal, quantitative weights for race regardless of purpose. This case law is hence fundamentally incompatible with fairness in machine learning. Second, we argue that the government-contracting cases offer an alternative grounding for algorithmic fairness, as these cases permit explicit and quantitative race-based remedies based on historical discrimination by the actor. Third, while limited, this doctrinal approach also guides the future of algorithmic fairness, mandating that adjustments be calibrated to the entity's responsibility for historical discrimination causing present-day disparities. The contractor cases provide a legally viable path for algorithmic fairness under current constitutional doctrine but call for more research at the intersection of algorithmic fairness and causal inference to ensure that bias mitigation is tailored to specific causes and mechanisms of bias.",
      "cited_by_count": 16,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2012.14285"
      },
      "topics": [
        "Digital Economy and Work Transformation",
        "Ethics and Social Impacts of AI",
        "Legal and Policy Issues"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W3113471301"
    },
    {
      "openalex_id": "W4312131163",
      "doi": "10.1111/mice.12956",
      "title": "Modeling adaptive platoon and reservation\u2010based intersection control for connected and autonomous vehicles employing deep reinforcement learning",
      "authors": [
        {
          "name": "Duowei Li",
          "openalex_id": "A5078101985",
          "orcid": "https://orcid.org/0000-0002-1940-2435",
          "institutions": [
            "Tsinghua University",
            "Nanyang Technological University"
          ]
        },
        {
          "name": "Jianping Wu",
          "openalex_id": "A5055302018",
          "orcid": "https://orcid.org/0000-0002-6698-3607",
          "institutions": [
            "Tsinghua University"
          ]
        },
        {
          "name": "Feng Zhu",
          "openalex_id": "A5031222391",
          "orcid": "https://orcid.org/0000-0002-9814-6053",
          "institutions": [
            "Nanyang Technological University"
          ]
        },
        {
          "name": "Tianyi Chen",
          "openalex_id": "A5100783473",
          "orcid": "https://orcid.org/0000-0003-1744-9579",
          "institutions": [
            "Nanyang Technological University"
          ]
        },
        {
          "name": "Yiik Diew Wong",
          "openalex_id": "A5010228559",
          "orcid": "https://orcid.org/0000-0001-7419-5777",
          "institutions": [
            "Nanyang Technological University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-12-12",
      "abstract": null,
      "cited_by_count": 29,
      "type": "article",
      "source": {
        "name": "Computer-Aided Civil and Infrastructure Engineering",
        "type": "journal",
        "issn": [
          "1093-9687",
          "1467-8667"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Traffic control and management",
        "Transportation Planning and Optimization",
        "Autonomous Vehicle Technology and Safety"
      ],
      "referenced_works_count": 57,
      "url": "https://openalex.org/W4312131163"
    },
    {
      "openalex_id": "W4381144979",
      "doi": "10.18608/hla22.020",
      "title": "An Introduction to Fairness, Absence of Bias, and Equity in Learning Analytics",
      "authors": [
        {
          "name": "Suraj Uttamchandani",
          "openalex_id": "A5003257891",
          "orcid": "https://orcid.org/0000-0002-9521-9384",
          "institutions": [
            "Indiana University Bloomington"
          ]
        },
        {
          "name": "Joshua Quick",
          "openalex_id": "A5080376098",
          "orcid": "https://orcid.org/0000-0001-5921-4984",
          "institutions": [
            "Indiana University Bloomington"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-01-01",
      "abstract": "In this chapter, we examine the ways educational justice has been and may be taken up in learning analytics research. To do so, we first outline how we see equity as playing a necessary role in the future development of the learning analytics community. Next, we review how equity has been explored in this area heretofore, focusing on notions of algorithmic fairness and absence of bias. Then, we turn to newer political approaches to the study of learning that are emerging in the learning sciences. We summarize trends in this research\u2019s conceptualizations of equity and the political dimensions of learning. Finally, we connect these related ways of thinking about social justice with respect to learning analytics, and examine the tensions and possibilities at their intersection. We close with some recommendations for the learning analytics field to ensure that it contributes to positive educational change moving into the future.",
      "cited_by_count": 22,
      "type": "book-chapter",
      "source": {
        "name": "Solar eBooks",
        "type": "ebook platform",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://solaresearch.org/wp-content/uploads/hla22/HLA22_Chapter_20_Uttamchandani.pdf"
      },
      "topics": [
        "Online Learning and Analytics"
      ],
      "referenced_works_count": 61,
      "url": "https://openalex.org/W4381144979"
    },
    {
      "openalex_id": "W4407857156",
      "doi": "10.30574/wjarr.2025.25.2.0571",
      "title": "Algorithmic bias, data ethics, and governance: Ensuring fairness, transparency and compliance in AI-powered business analytics applications",
      "authors": [
        {
          "name": "Julien Kiesse Bahangulu",
          "openalex_id": "A5116388335"
        },
        {
          "name": "Louis Owusu-Berko",
          "openalex_id": "A5116384490"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-02-22",
      "abstract": "The widespread adoption of AI-powered business analytics applications has revolutionized decision-making, yet it has also introduced significant challenges related to algorithmic bias, data ethics, and governance. As organizations increasingly rely on machine learning and big data analytics for customer profiling, credit scoring, hiring decisions, and predictive analytics, concerns about fairness, transparency, and compliance have intensified. Algorithmic biases\u2014often stemming from biased training data, flawed model assumptions, and insufficient diversity in datasets\u2014can result in discriminatory outcomes, reinforcing societal inequalities and reputational risks for businesses. To address these concerns, robust data ethics frameworks must be integrated into AI governance strategies. Ethical AI principles emphasize accountability, explainability, and bias mitigation techniques, ensuring that decision-making algorithms are transparent and justifiable. Organizations must implement bias detection methods, fairness-aware machine learning models, and continuous audits to minimize unintended consequences. Additionally, regulatory frameworks such as GDPR, CCPA, and AI-specific compliance laws necessitate stringent governance practices to protect consumer rights and data privacy. Beyond compliance, fostering public trust in AI-powered analytics requires organizations to adopt ethical data stewardship, ensuring that AI models align with corporate social responsibility (CSR) initiatives and stakeholder expectations. The intersection of data ethics, algorithmic accountability, and regulatory compliance presents both challenges and opportunities for businesses seeking to leverage AI responsibly. This paper examines key strategies for mitigating algorithmic bias, establishing ethical AI governance models, and ensuring fairness in data-driven business applications, providing a roadmap for organizations to enhance transparency, compliance, and equitable AI adoption.",
      "cited_by_count": 25,
      "type": "article",
      "source": {
        "name": "World Journal of Advanced Research and Reviews",
        "type": "journal",
        "issn": [
          "2581-9615"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://doi.org/10.30574/wjarr.2025.25.2.0571"
      },
      "topics": [
        "Big Data and Business Intelligence",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4407857156"
    },
    {
      "openalex_id": "W3036364450",
      "doi": "10.1109/access.2020.3002825",
      "title": "Robust Autonomous Intersection Control Approach for Connected Autonomous Vehicles",
      "authors": [
        {
          "name": "Yuheng Zhang",
          "openalex_id": "A5101786268",
          "orcid": "https://orcid.org/0000-0002-3907-6761",
          "institutions": [
            "Beijing University of Posts and Telecommunications"
          ]
        },
        {
          "name": "Luning Liu",
          "openalex_id": "A5101677958",
          "orcid": "https://orcid.org/0000-0001-6376-652X",
          "institutions": [
            "Beijing University of Posts and Telecommunications"
          ]
        },
        {
          "name": "Zhaoming Lu",
          "openalex_id": "A5019111566",
          "orcid": "https://orcid.org/0000-0002-0182-1770",
          "institutions": [
            "Beijing University of Posts and Telecommunications"
          ]
        },
        {
          "name": "Luhan Wang",
          "openalex_id": "A5057155774",
          "orcid": "https://orcid.org/0000-0002-7056-5416",
          "institutions": [
            "Beijing University of Posts and Telecommunications"
          ]
        },
        {
          "name": "Xiangming Wen",
          "openalex_id": "A5000851572",
          "orcid": "https://orcid.org/0000-0003-2793-6696",
          "institutions": [
            "Beijing University of Posts and Telecommunications"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-01",
      "abstract": "Traffic light-free intersection control is envisioned to alleviate congestion and manage vehicles intelligently. With the help of vehicle-to-infrastructure (V2I) communication and edge computing (EC), vehicles are instructed to cross the intersection with high vehicle safety and traffic efficiency without traffic lights. However, unstable channel conditions can lead to the reduction of traveling safety. In this paper, we propose a robust autonomous intersection control (AIC) approach with global optimization scheduling, which protects connected autonomous vehicles from collision under any channel conditions while achieving decent traffic efficiency. In particular, we propose an AIC model that gives vehicles certain autonomy under centralized control to ensure the traveling safety in case of some emergencies. By conducting an interference graph, we simplify the AIC problem as a weighted maximal clique problem with restriction. To improve the fairness and efficiency in terms of vehicle passage, multiple factors such as travel delay, traffic of the current lane and passengers' desired speed are considered. Furthermore, we propose a heuristic algorithm to search the solution space. For further optimization, a particle swarm optimization algorithm is proposed, achieving a near-optimal result with adjustable overhead. Finally, we build the simulation model and conduct a comparative performance evaluation. Simulation results demonstrate the superiority of our proposed scheme.",
      "cited_by_count": 17,
      "type": "article",
      "source": {
        "name": "IEEE Access",
        "type": "journal",
        "issn": [
          "2169-3536"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09122409.pdf"
      },
      "topics": [
        "Traffic control and management",
        "Vehicular Ad Hoc Networks (VANETs)",
        "Autonomous Vehicle Technology and Safety"
      ],
      "referenced_works_count": 44,
      "url": "https://openalex.org/W3036364450"
    },
    {
      "openalex_id": "W3127067752",
      "doi": "10.1007/s11042-020-10222-y",
      "title": "About auction strategies for intersection management when human-driven and autonomous vehicles coexist",
      "authors": [
        {
          "name": "Giacomo Cabri",
          "openalex_id": "A5074734549",
          "orcid": "https://orcid.org/0000-0002-4942-2453",
          "institutions": [
            "University of Modena and Reggio Emilia"
          ]
        },
        {
          "name": "Luca Gherardini",
          "openalex_id": "A5034557044",
          "orcid": "https://orcid.org/0000-0002-7147-0734",
          "institutions": [
            "University of Modena and Reggio Emilia"
          ]
        },
        {
          "name": "Manuela Montangero",
          "openalex_id": "A5002315056",
          "orcid": "https://orcid.org/0000-0001-8827-5495",
          "institutions": [
            "University of Modena and Reggio Emilia"
          ]
        },
        {
          "name": "Filippo Muzzini",
          "openalex_id": "A5041657169",
          "orcid": "https://orcid.org/0000-0001-6523-0366",
          "institutions": [
            "University of Modena and Reggio Emilia"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-02-06",
      "abstract": null,
      "cited_by_count": 25,
      "type": "article",
      "source": {
        "name": "Multimedia Tools and Applications",
        "type": "journal",
        "issn": [
          "1380-7501",
          "1573-7721"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://hdl.handle.net/11380/1232497"
      },
      "topics": [
        "Traffic control and management",
        "Transportation and Mobility Innovations",
        "Transportation Planning and Optimization"
      ],
      "referenced_works_count": 27,
      "url": "https://openalex.org/W3127067752"
    },
    {
      "openalex_id": "W4289533941",
      "doi": "10.1109/icde53745.2022.00089",
      "title": "MANI-Rank: Multiple Attribute and Intersectional Group Fairness for Consensus Ranking",
      "authors": [
        {
          "name": "Kathleen Cachel",
          "openalex_id": "A5041530401",
          "orcid": "https://orcid.org/0000-0002-1260-0914",
          "institutions": [
            "Worcester Polytechnic Institute"
          ]
        },
        {
          "name": "Elke A. Rundensteiner",
          "openalex_id": "A5008269094",
          "orcid": "https://orcid.org/0000-0001-5375-9254",
          "institutions": [
            "Worcester Polytechnic Institute"
          ]
        },
        {
          "name": "Lane Harrison",
          "openalex_id": "A5087146581",
          "orcid": "https://orcid.org/0000-0003-3029-2799",
          "institutions": [
            "Worcester Polytechnic Institute"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-05-01",
      "abstract": "Combining the preferences of many rankers into one single consensus ranking is critical for consequential applications from hiring and admissions to lending. While group fairness has been extensively studied for classification, group fairness in rankings and in particular rank aggregation remains in its infancy. Recent work introduced the concept of fair rank aggregation for combining rankings but restricted to the case when candidates have a single binary protected attribute, i.e., they fall into two groups only. Yet it remains an open problem how to create a consensus ranking that represents the preferences of all rankers while ensuring fair treatment for candidates with multiple protected attributes such as gender, race, and nationality. In this work, we are the first to define and solve this open Multi-attribute Fair Consensus Ranking (MFCR) problem. As a foundation, we design novel group fairness criteria for rankings, called MANI-Rank, ensuring fair treatment of groups defined by individual protected attributes and their intersection. Leveraging the MANI-Rank criteria, we develop a series of algorithms that for the first time tackle the MFCR problem. Our experimental study with a rich variety of consensus scenarios demonstrates our MFCR methodology is the only approach to achieve both intersectional and protected attribute fairness while also representing the preferences expressed through many base rankings. Our real world case study on merit scholarships illustrates the effectiveness of our MFCR methods to mitigate bias across multiple protected attributes and their intersections.",
      "cited_by_count": 14,
      "type": "article",
      "source": {
        "name": "2022 IEEE 38th International Conference on Data Engineering (ICDE)",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Game Theory and Voting Systems"
      ],
      "referenced_works_count": 56,
      "url": "https://openalex.org/W4289533941"
    },
    {
      "openalex_id": "W3121014527",
      "doi": "10.48550/arxiv.2101.01673",
      "title": "Characterizing Intersectional Group Fairness with Worst-Case Comparisons",
      "authors": [
        {
          "name": "Avijit Ghosh",
          "openalex_id": "A5101463080",
          "orcid": "https://orcid.org/0000-0002-8540-3698",
          "institutions": [
            "Universidad del Noreste"
          ]
        },
        {
          "name": "Lea Genuit",
          "openalex_id": "A5038513427"
        },
        {
          "name": "Mary Reagan",
          "openalex_id": "A5022214218"
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-01-05",
      "abstract": "Machine Learning or Artificial Intelligence algorithms have gained considerable scrutiny in recent times owing to their propensity towards imitating and amplifying existing prejudices in society. This has led to a niche but growing body of work that identifies and attempts to fix these biases. A first step towards making these algorithms more fair is designing metrics that measure unfairness. Most existing work in this field deals with either a binary view of fairness (protected vs. unprotected groups) or politically defined categories (race or gender). Such categorization misses the important nuance of intersectionality - biases can often be amplified in subgroups that combine membership from different categories, especially if such a subgroup is particularly underrepresented in historical platforms of opportunity. In this paper, we discuss why fairness metrics need to be looked at under the lens of intersectionality, identify existing work in intersectional fairness, suggest a simple worst case comparison method to expand the definitions of existing group fairness metrics to incorporate intersectionality, and finally conclude with the social, legal and political framework to handle intersectional fairness in the modern context.",
      "cited_by_count": 9,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2101.01673"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Political Philosophy and Ethics",
        "Feminist Epistemology and Gender Studies"
      ],
      "referenced_works_count": 30,
      "url": "https://openalex.org/W3121014527"
    },
    {
      "openalex_id": "W4320024232",
      "doi": "10.1109/bigdata55660.2022.10020588",
      "title": "InfoFair: Information-Theoretic Intersectional Fairness",
      "authors": [
        {
          "name": "Jian Kang",
          "openalex_id": "A5100700905",
          "orcid": "https://orcid.org/0000-0003-3902-7131",
          "institutions": [
            "University of Illinois Urbana-Champaign"
          ]
        },
        {
          "name": "Tiankai Xie",
          "openalex_id": "A5004845067",
          "orcid": "https://orcid.org/0009-0008-8832-0063",
          "institutions": [
            "Arizona State University"
          ]
        },
        {
          "name": "Xintao Wu",
          "openalex_id": "A5008463509",
          "orcid": "https://orcid.org/0000-0002-2823-3063",
          "institutions": [
            "University of Arkansas at Fayetteville"
          ]
        },
        {
          "name": "Ross Maciejewski",
          "openalex_id": "A5026799813",
          "orcid": "https://orcid.org/0000-0001-8803-6355",
          "institutions": [
            "Arizona State University"
          ]
        },
        {
          "name": "Hanghang Tong",
          "openalex_id": "A5068043486",
          "orcid": "https://orcid.org/0000-0003-4405-3887",
          "institutions": [
            "University of Illinois Urbana-Champaign"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-12-17",
      "abstract": "Algorithmic fairness is becoming increasingly important in data mining and machine learning. Among others, a foundational notation is group fairness. The vast majority of the existing works on group fairness, with a few exceptions, primarily focus on debiasing with respect to a single sensitive attribute, despite the fact that the co-existence of multiple sensitive attributes (e.g., gender, race, marital status, etc.) in the real-world is commonplace. As such, methods that can ensure a fair learning outcome with respect to all sensitive attributes of concern simultaneously need to be developed. In this paper, we study the problem of information-theoretic intersectional fairness (InfoFair), where statistical parity, a representative group fairness measure, is guaranteed among demographic groups formed by multiple sensitive attributes of interest. We formulate it as a mutual information minimization problem and propose a generic end-to-end algorithmic framework to solve it. The key idea is to leverage a variational representation of mutual information, which considers the variational distribution between learning outcomes and sensitive attributes, as well as the density ratio between the variational and the original distributions. Our proposed framework is generalizable to many different settings, including other statistical notions of fairness, and could handle any type of learning task equipped with a gradientbased optimizer. Empirical evaluations in the fair classification task on three real-world datasets demonstrate that our proposed framework can effectively debias the classification results with minimal impact to the classification accuracy.",
      "cited_by_count": 16,
      "type": "article",
      "source": {
        "name": "2022 IEEE International Conference on Big Data (Big Data)",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 63,
      "url": "https://openalex.org/W4320024232"
    },
    {
      "openalex_id": "W4312349151",
      "doi": "10.1007/978-3-031-23223-7_2",
      "title": "Disproportionate Subgroup Impacts and\u00a0Other Challenges of\u00a0Fairness in\u00a0Artificial Intelligence for\u00a0Medical Image Analysis",
      "authors": [
        {
          "name": "Emma A. M. Stanley",
          "openalex_id": "A5038385431",
          "orcid": "https://orcid.org/0000-0002-7802-6820",
          "institutions": [
            "University of Calgary"
          ]
        },
        {
          "name": "Matthias Wilms",
          "openalex_id": "A5051753610",
          "orcid": "https://orcid.org/0000-0001-8845-360X",
          "institutions": [
            "University of Calgary",
            "Alberta Children's Hospital"
          ]
        },
        {
          "name": "Nils D. Forkert",
          "openalex_id": "A5040728822",
          "orcid": "https://orcid.org/0000-0003-2556-3224",
          "institutions": [
            "University of Calgary",
            "Alberta Children's Hospital"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-01-01",
      "abstract": null,
      "cited_by_count": 17,
      "type": "book-chapter",
      "source": {
        "name": "Lecture notes in computer science",
        "type": "book series",
        "issn": [
          "0302-9743",
          "1611-3349"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Healthcare cost, quality, practices",
        "Ethics in Clinical Research"
      ],
      "referenced_works_count": 23,
      "url": "https://openalex.org/W4312349151"
    },
    {
      "openalex_id": "W3165260750",
      "doi": "10.1145/3461702.3462568",
      "title": "Who Gets What, According to Whom? An Analysis of Fairness Perceptions in Service Allocation",
      "authors": [
        {
          "name": "Jacqueline Hannan",
          "openalex_id": "",
          "institutions": [
            "University at Buffalo, State University of New York"
          ]
        },
        {
          "name": "Huei-Yen Winnie Chen",
          "openalex_id": "",
          "institutions": [
            "University at Buffalo, State University of New York"
          ]
        },
        {
          "name": "Kenneth Joseph",
          "openalex_id": "",
          "institutions": [
            "University at Buffalo, State University of New York"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-07-21",
      "abstract": "Algorithmic fairness research has traditionally been linked to the\\ndisciplines of philosophy, ethics, and economics, where notions of fairness are\\nprescriptive and seek objectivity. Increasingly, however, scholars are turning\\nto the study of what different people perceive to be fair, and how these\\nperceptions can or should help to shape the design of machine learning,\\nparticularly in the policy realm. The present work experimentally explores five\\nnovel research questions at the intersection of the \"Who,\" \"What,\" and \"How\" of\\nfairness perceptions. Specifically, we present the results of a multi-factor\\nconjoint analysis study that quantifies the effects of the specific context in\\nwhich a question is asked, the framing of the given question, and who is\\nanswering it. Our results broadly suggest that the \"Who\" and \"What,\" at least,\\nmatter in ways that are 1) not easily explained by any one theoretical\\nperspective, 2) have critical implications for how perceptions of fairness\\nshould be measured and/or integrated into algorithmic decision-making systems.\\n",
      "cited_by_count": 14,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3461702.3462568"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Psychology of Moral and Emotional Judgment",
        "Social and Intergroup Psychology"
      ],
      "referenced_works_count": 25,
      "url": "https://openalex.org/W3165260750"
    },
    {
      "openalex_id": "W4392861789",
      "doi": "10.1371/journal.pone.0298417",
      "title": "A dynamic traffic signal scheduling system based on improved greedy algorithm",
      "authors": [
        {
          "name": "Guangling Sun",
          "openalex_id": "A5104179979",
          "institutions": [
            "Hefei University of Technology",
            "Anhui Jianzhu University"
          ]
        },
        {
          "name": "Rui Qi",
          "openalex_id": "A5084762257",
          "orcid": "https://orcid.org/0009-0009-4162-7452",
          "institutions": [
            "Anhui Jianzhu University"
          ]
        },
        {
          "name": "Yulong Liu",
          "openalex_id": "A5031261618",
          "orcid": "https://orcid.org/0000-0002-0439-6925",
          "institutions": [
            "Anhui Jianzhu University"
          ]
        },
        {
          "name": "Feng Xu",
          "openalex_id": "A5108831337",
          "orcid": "https://orcid.org/0009-0005-9691-0328",
          "institutions": [
            "Anhui Jianzhu University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-03-15",
      "abstract": "Urbanization has led to accelerated traffic congestion, posing a significant obstacle to urban development. Traditional traffic signal scheduling methods are often inefficient and cumbersome, resulting in unnecessary waiting times for vehicles and pedestrians, exacerbating the traffic situation. To address this issue, this article proposes a dynamic traffic signal scheduling system based on an improved greedy algorithm. Unlike conventional approaches, we introduce a reward function and a cost model to ensure fair scheduling plans. A constraint function is also established, and the traffic signal scheduling is iterated through the feasible matrix using the greedy algorithm to simplify the decision-making process and enhance solution efficiency. Moreover, an emergency module is integrated to prioritize special emergency vehicles, reducing their response time during emergencies. To validate the effectiveness of our dynamic traffic signal scheduling system, we conducted simulation experiments using the Simulation of Urban Mobility (SUMO) traffic simulation suite and the SUMO traffic control interface Traci. The results indicate that our system significantly improves intersection throughput and adapts well to various traffic conditions, effectively resolving urban traffic congestion while ensuring fair scheduling plans.",
      "cited_by_count": 16,
      "type": "article",
      "source": {
        "name": "PLoS ONE",
        "type": "journal",
        "issn": [
          "1932-6203"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0298417&type=printable"
      },
      "topics": [
        "Transportation Planning and Optimization",
        "Traffic control and management",
        "Traffic Prediction and Management Techniques"
      ],
      "referenced_works_count": 39,
      "url": "https://openalex.org/W4392861789"
    },
    {
      "openalex_id": "W4380366644",
      "doi": "10.1145/3593013.3594007",
      "title": "The Possibility of Fairness: Revisiting the Impossibility Theorem in Practice",
      "authors": [
        {
          "name": "Andrew Bell",
          "openalex_id": "A5101752875",
          "orcid": "https://orcid.org/0000-0001-6010-9030",
          "institutions": [
            "New York University"
          ]
        },
        {
          "name": "Lucius Bynum",
          "openalex_id": "A5038439834",
          "orcid": "https://orcid.org/0000-0002-9247-2595",
          "institutions": [
            "New York University"
          ]
        },
        {
          "name": "Nazarii Drushchak",
          "openalex_id": "A5014987268",
          "orcid": "https://orcid.org/0000-0002-5056-3026",
          "institutions": [
            "Ukrainian Catholic University"
          ]
        },
        {
          "name": "Tetiana Zakharchenko",
          "openalex_id": "A5078740312",
          "orcid": "https://orcid.org/0009-0007-3702-2704",
          "institutions": [
            "Ukrainian Catholic University"
          ]
        },
        {
          "name": "Lucas Rosenblatt",
          "openalex_id": "A5068868453",
          "orcid": "https://orcid.org/0000-0001-6952-4361",
          "institutions": [
            "New York University"
          ]
        },
        {
          "name": "Julia Stoyanovich",
          "openalex_id": "A5082830839",
          "orcid": "https://orcid.org/0000-0002-1587-0450",
          "institutions": [
            "New York University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-06-12",
      "abstract": "The \"impossibility theorem\" \u2014 which is considered foundational in algorithmic fairness literature \u2014 asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a practitioner's perspective of fairness), it becomes possible to identify abundant sets of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when \u2014 and to what degree \u2014 fairness along multiple criteria can be achieved. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed.",
      "cited_by_count": 16,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://doi.org/10.1145/3593013.3594007"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Qualitative Comparative Analysis Research"
      ],
      "referenced_works_count": 23,
      "url": "https://openalex.org/W4380366644"
    },
    {
      "openalex_id": "W4380357747",
      "doi": "10.1145/3593013.3593979",
      "title": "Multi-dimensional Discrimination in Law and Machine Learning - A Comparative Overview",
      "authors": [
        {
          "name": "Arjun Roy",
          "openalex_id": "A5078516991",
          "orcid": "https://orcid.org/0000-0002-4279-9442",
          "institutions": [
            "Freie Universit\u00e4t Berlin",
            "Universit\u00e4t der Bundeswehr M\u00fcnchen"
          ]
        },
        {
          "name": "Jan Horstmann",
          "openalex_id": "A5101745396",
          "orcid": "https://orcid.org/0009-0002-1038-694X"
        },
        {
          "name": "Eirini Ntoutsi",
          "openalex_id": "A5089247128",
          "orcid": "https://orcid.org/0000-0001-5729-1003",
          "institutions": [
            "Universit\u00e4t der Bundeswehr M\u00fcnchen"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-06-12",
      "abstract": "AI-driven decision-making can lead to discrimination against certain individuals or social groups based on protected characteristics/attributes such as race, gender, or age. The domain of fairness-aware machine learning focuses on methods and algorithms for understanding, mitigating, and accounting for bias in AI/ML models. Still, thus far, the vast majority of the proposed methods assess fairness based on a single protected attribute, e.g. only gender or race. In reality, though, human identities are multi-dimensional, and discrimination can occur based on more than one protected characteristic, leading to the so-called \"multi-dimensional discrimination\" or \"multi-dimensional fairness\" problem. While well-elaborated in legal literature, the multi-dimensionality of discrimination is less explored in the machine learning community. Recent approaches in this direction mainly follow the so-called intersectional fairness definition from the legal domain, whereas other notions like additive and sequential discrimination are less studied or not considered thus far. In this work, we overview the different definitions of multi-dimensional discrimination/fairness in the legal domain as well as how they have been transferred/ operationalized (if) in the fairness-aware machine learning domain. By juxtaposing these two domains, we draw the connections, identify the limitations, and point out open research directions.",
      "cited_by_count": 18,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Law, AI, and Intellectual Property"
      ],
      "referenced_works_count": 57,
      "url": "https://openalex.org/W4380357747"
    },
    {
      "openalex_id": "W4230971985",
      "doi": "10.1007/978-3-030-74697-1_15",
      "title": "Towards Causal Benchmarking of Biasin Face Analysis Algorithms",
      "authors": [
        {
          "name": "Guha Balakrishnan",
          "openalex_id": "A5081710525",
          "orcid": "https://orcid.org/0000-0001-8703-1368",
          "institutions": [
            "Amazon (United States)"
          ]
        },
        {
          "name": "Yuanjun Xiong",
          "openalex_id": "A5005925571",
          "orcid": "https://orcid.org/0000-0002-6391-4921"
        },
        {
          "name": "Wei Xia",
          "openalex_id": "A5067662547",
          "orcid": "https://orcid.org/0000-0001-8988-8381"
        },
        {
          "name": "Pietro Perona",
          "openalex_id": "A5026825380",
          "orcid": "https://orcid.org/0000-0002-7583-5809",
          "institutions": [
            "Amazon (United States)"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-01-01",
      "abstract": null,
      "cited_by_count": 21,
      "type": "book-chapter",
      "source": {
        "name": "Advances in computer vision and pattern recognition",
        "type": "book series",
        "issn": [
          "2191-6586",
          "2191-6594"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Face recognition and analysis",
        "Face and Expression Recognition",
        "Face Recognition and Perception"
      ],
      "referenced_works_count": 47,
      "url": "https://openalex.org/W4230971985"
    },
    {
      "openalex_id": "W4313644063",
      "doi": "10.1109/tiv.2023.3234888",
      "title": "FAIR: Towards Impartial Resource Allocation for Intelligent Vehicles With Automotive Edge Computing",
      "authors": [
        {
          "name": "Haoxin Wang",
          "openalex_id": "A5077861883",
          "orcid": "https://orcid.org/0000-0002-8732-6200",
          "institutions": [
            "Georgia State University"
          ]
        },
        {
          "name": "Jiang Xie",
          "openalex_id": "A5019147177",
          "orcid": "https://orcid.org/0000-0003-0683-4308",
          "institutions": [
            "University of North Carolina at Charlotte"
          ]
        },
        {
          "name": "Muhana Magboul Ali Muslam",
          "openalex_id": "A5081976403",
          "orcid": "https://orcid.org/0000-0002-0238-4162",
          "institutions": [
            "Imam Mohammad ibn Saud Islamic University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-06",
      "abstract": "The emerging vehicular connected applications, such as cooperative automated driving and intersection collision warning, show great potentials to improve the driving safety, where vehicles can share the data collected by a variety of on-board sensors with surrounding vehicles and roadside infrastructures. Transmitting and processing this huge amount of sensory data introduces new challenges for automotive edge computing with traditional wireless communication networks. In this work, we address the problem of traditional asymmetrical network resource allocation for uplink and downlink connections that can significantly degrade the performance of vehicular connected applications. An end-to-end automotive edge networking system, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">FAIR</i> , is proposed to provide <underline xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">f</u> ast, sc <underline xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">a</u> lable, and <underline xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i</u> mpa <underline xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">r</u> tial connected services for intelligent vehicles with edge computing, which can be applied to any traffic scenes and road topology. The core of <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">FAIR</i> is our proposed symmetrical network resource allocation algorithm deployed at edge servers and service adaptation algorithm equipped on intelligent vehicles. Extensive simulations are conducted to validate our proposed <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">FAIR</i> by leveraging real-world traffic dataset. Simulation results demonstrate that <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">FAIR</i> outperforms existing solutions in a variety of traffic scenes and road topology.",
      "cited_by_count": 27,
      "type": "article",
      "source": {
        "name": "IEEE Transactions on Intelligent Vehicles",
        "type": "journal",
        "issn": [
          "2379-8858",
          "2379-8904"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Vehicular Ad Hoc Networks (VANETs)",
        "Age of Information Optimization",
        "Green IT and Sustainability"
      ],
      "referenced_works_count": 34,
      "url": "https://openalex.org/W4313644063"
    },
    {
      "openalex_id": "W4391002626",
      "doi": "10.1016/j.trb.2023.102876",
      "title": "Delay-throughput tradeoffs for signalized networks with finite queue capacity",
      "authors": [
        {
          "name": "Shaohua Cui",
          "openalex_id": "A5072470185",
          "orcid": "https://orcid.org/0000-0002-5885-3124",
          "institutions": [
            "Chalmers University of Technology",
            "Ministry of Transport",
            "Beihang University"
          ]
        },
        {
          "name": "Yongjie Xue",
          "openalex_id": "A5056143888",
          "orcid": "https://orcid.org/0000-0001-8937-1622",
          "institutions": [
            "Beihang University"
          ]
        },
        {
          "name": "Kun Gao",
          "openalex_id": "A5041748565",
          "orcid": "https://orcid.org/0000-0002-4175-850X",
          "institutions": [
            "Chalmers University of Technology"
          ]
        },
        {
          "name": "Kai Wang",
          "openalex_id": "A5006216343",
          "orcid": "https://orcid.org/0000-0002-9687-783X",
          "institutions": [
            "Tsinghua University"
          ]
        },
        {
          "name": "Bin Yu",
          "openalex_id": "A5101502351",
          "orcid": "https://orcid.org/0000-0002-9166-013X",
          "institutions": [
            "Beihang University",
            "Ministry of Transport"
          ]
        },
        {
          "name": "Xiaobo Qu",
          "openalex_id": "A5040302830",
          "orcid": "https://orcid.org/0000-0003-0973-3756",
          "institutions": [
            "Tsinghua University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-01-18",
      "abstract": null,
      "cited_by_count": 22,
      "type": "article",
      "source": {
        "name": "Transportation Research Part B Methodological",
        "type": "journal",
        "issn": [
          "0191-2615",
          "1879-2367"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Traffic control and management",
        "Transportation Planning and Optimization",
        "Traffic Prediction and Management Techniques"
      ],
      "referenced_works_count": 86,
      "url": "https://openalex.org/W4391002626"
    },
    {
      "openalex_id": "W4390236716",
      "doi": "10.51594/estj.v4i6.668",
      "title": "ARTIFICIAL INTELLIGENCE IN THE ERA OF 4IR: DRIVERS, CHALLENGES AND OPPORTUNITIES",
      "authors": [
        {
          "name": "Ibegbulam C.M",
          "openalex_id": "A5104220173"
        },
        {
          "name": "J. A. Olowonubi",
          "openalex_id": "A5025906210",
          "institutions": [
            "Development Research and Projects Centre"
          ]
        },
        {
          "name": "S. A. Fatounde",
          "openalex_id": "A5000707050",
          "institutions": [
            "Development Research and Projects Centre"
          ]
        },
        {
          "name": "O. A. Oyegunwa",
          "openalex_id": "A5035564002",
          "institutions": [
            "Development Research and Projects Centre"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-12-26",
      "abstract": "This abstract explores the intersection of Artificial Intelligence (AI) and the Fourth Industrial Revolution (4IR), focusing on the drivers, challenges, and opportunities of this transformative landscape. AI is driving the adoption of advanced technologies like data analytics, machine learning, and deep learning, unlocking unprecedented capabilities. However, AI faces ethical challenges such as fairness, transparency, and accountability, technical limitations, regulatory complexities, and societal impacts like job displacement. Despite these challenges, AI offers vast opportunities across industries, such as personalized healthcare diagnostics and autonomous systems. The future of AI in the 4IR requires a nuanced understanding of these challenges and opportunities, refining AI algorithms, addressing biases, and ensuring ethical deployment. Robust regulatory frameworks, reskilling initiatives, and the synthesis of AI with emerging technologies like quantum computing and edge computing are also crucial.&#x0D; Keywords: Artificial Intelligence; Fourth Industrial Revolution(4ir); Opportunities, Developing Countries, Ethical Challenges.",
      "cited_by_count": 19,
      "type": "article",
      "source": {
        "name": "Engineering Science & Technology Journal",
        "type": "journal",
        "issn": [
          "2708-8944",
          "2708-8952"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "https://fepbl.com/index.php/estj/article/download/668/847"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 56,
      "url": "https://openalex.org/W4390236716"
    },
    {
      "openalex_id": "W3022766285",
      "doi": "10.1145/3377930.3390157",
      "title": "Genetic programming approaches to learning fair classifiers",
      "authors": [
        {
          "name": "William La Cava",
          "openalex_id": "",
          "institutions": [
            "California University of Pennsylvania"
          ]
        },
        {
          "name": "Jason H. Moore",
          "openalex_id": "",
          "institutions": [
            "California University of Pennsylvania"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-06-25",
      "abstract": "Society has come to rely on algorithms like classifiers for important\\ndecision making, giving rise to the need for ethical guarantees such as\\nfairness. Fairness is typically defined by asking that some statistic of a\\nclassifier be approximately equal over protected groups within a population. In\\nthis paper, current approaches to fairness are discussed and used to motivate\\nalgorithmic proposals that incorporate fairness into genetic programming for\\nclassification. We propose two ideas. The first is to incorporate a fairness\\nobjective into multi-objective optimization. The second is to adapt lexicase\\nselection to define cases dynamically over intersections of protected groups.\\nWe describe why lexicase selection is well suited to pressure models to perform\\nwell across the potentially infinitely many subgroups over which fairness is\\ndesired. We use a recent genetic programming approach to construct models on\\nfour datasets for which fairness constraints are necessary, and empirically\\ncompare performance to prior methods utilizing game-theoretic solutions.\\nMethods are assessed based on their ability to generate trade-offs of subgroup\\nfairness and accuracy that are Pareto optimal. The result show that genetic\\nprogramming methods in general, and random search in particular, are well\\nsuited to this task.\\n",
      "cited_by_count": 15,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2004.13282"
      },
      "topics": [
        "Evolutionary Algorithms and Applications",
        "Reinforcement Learning in Robotics",
        "Evolutionary Game Theory and Cooperation"
      ],
      "referenced_works_count": 16,
      "url": "https://openalex.org/W3022766285"
    },
    {
      "openalex_id": "W3160893503",
      "doi": null,
      "title": "Fairness and Discrimination in Information Access Systems",
      "authors": [
        {
          "name": "Michael D. Ekstrand",
          "openalex_id": "A5015092805",
          "orcid": "https://orcid.org/0000-0003-2467-0108",
          "institutions": [
            "Boise State University"
          ]
        },
        {
          "name": "Anubrata Das",
          "openalex_id": "A5075823509",
          "orcid": "https://orcid.org/0000-0002-5412-6149",
          "institutions": [
            "The University of Texas at Austin"
          ]
        },
        {
          "name": "Robin Burke",
          "openalex_id": "A5043134791",
          "orcid": "https://orcid.org/0000-0001-5766-6434",
          "institutions": [
            "University of Colorado System",
            "University of Colorado Boulder"
          ]
        },
        {
          "name": "Fernando D\u00edaz",
          "openalex_id": "A5101492251",
          "orcid": "https://orcid.org/0000-0003-2345-1288",
          "institutions": [
            "Association for Computing Machinery"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-05-12",
      "abstract": "Recommendation, information retrieval, and other information access systems pose unique challenges for investigating and applying the fairness and non-discrimination concepts that have been developed for studying other machine learning systems. While fair information access shares many commonalities with fair classification, the multistakeholder nature of information access applications, the rank-based problem setting, the centrality of personalization in many cases, and the role of user response complicate the problem of identifying precisely what types and operationalizations of fairness may be relevant, let alone measuring or promoting them. \r\nIn this monograph, we present a taxonomy of the various dimensions of fair information access and survey the literature to date on this new and rapidly-growing topic. We preface this with brief introductions to information access and algorithmic fairness, to facilitate use of this work by scholars with experience in one (or neither) of these fields who wish to learn about their intersection. We conclude with several open problems in fair information access, along with some suggestions for how to approach research in this space.",
      "cited_by_count": 13,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2105.05779.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Mobile Crowdsensing and Crowdsourcing",
        "Privacy, Security, and Data Protection"
      ],
      "referenced_works_count": 153,
      "url": "https://openalex.org/W3160893503"
    },
    {
      "openalex_id": "W4212765518",
      "doi": "10.1145/3488560.3498432",
      "title": "Enumerating Fair Packages for Group Recommendations",
      "authors": [
        {
          "name": "Ryoma Sato",
          "openalex_id": "A5040188906",
          "orcid": "https://orcid.org/0000-0001-6912-4464",
          "institutions": [
            "Kyoto University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-02-11",
      "abstract": "Package-to-group recommender systems recommend a set of unified items to a group of people. Different from conventional settings, it is not easy to measure the utility of group recommendations because it involves more than one user. In particular, fairness is crucial in group recommendations. Even if some members in a group are substantially satisfied with a recommendation, it is undesirable if other members are ignored to increase the total utility. Many methods for evaluating and applying the fairness of group recommendations have been proposed in the literature. However, all these methods maximize the score and output only one package. This is in contrast to conventional recommender systems, which output several (e.g., top-K) candidates. This can be problematic because a group can be dissatisfied with the recommended package owing to some unobserved reasons, even if the score is high. To address this issue, we propose a method to enumerate fair packages efficiently. Our method furthermore supports filtering queries, such as top-K and intersection, to select favorite packages when the list is long. We confirm that our algorithm scales to large datasets and can balance several aspects of the utility of the packages.",
      "cited_by_count": 17,
      "type": "article",
      "source": {
        "name": "Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Recommender Systems and Techniques",
        "Advanced Bandit Algorithms Research",
        "Data Management and Algorithms"
      ],
      "referenced_works_count": 33,
      "url": "https://openalex.org/W4212765518"
    },
    {
      "openalex_id": "W3133121169",
      "doi": "10.1109/tits.2021.3056658",
      "title": "Security Vulnerabilities and Protection Algorithms for Backpressure-Based Traffic Signal Control at an Isolated Intersection",
      "authors": [
        {
          "name": "Chia-Cheng Yen",
          "openalex_id": "A5101526670",
          "orcid": "https://orcid.org/0000-0002-8420-9762",
          "institutions": [
            "University of California, Davis"
          ]
        },
        {
          "name": "Dipak Ghosal",
          "openalex_id": "A5003841285",
          "orcid": "https://orcid.org/0000-0002-3827-263X",
          "institutions": [
            "University of California, Davis"
          ]
        },
        {
          "name": "Michael Zhang",
          "openalex_id": "A5100612833",
          "orcid": "https://orcid.org/0000-0002-4647-3888",
          "institutions": [
            "University of California, Davis"
          ]
        },
        {
          "name": "Chen\u2010Nee Chuah",
          "openalex_id": "A5017383502",
          "orcid": "https://orcid.org/0000-0002-2772-387X",
          "institutions": [
            "University of California, Davis"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-02-19",
      "abstract": "There is an increasing trend in the use of wireless communication along with new traffic signal control (TSC) algorithms to leverage and accommodate connected and autonomous vehicles. However, this development has increased the potential for cyber-attacks on TSC that can undermine the benefits of these new algorithms. An advanced persistent adversary can learn the behavior of TSC algorithms and launch attacks to preferentially get green time and/or to create traffic congestion in one intersection which can spread to the entire network. In this paper, we consider backpressure-based (BP-based) TSC algorithms and compare their performance under two misinformation attacks - 1) time spoofing attack in which vehicles alter their arrival times at the intersection and 2) ghost vehicle attack in which vehicles disconnect the wireless communication and thereby hide from the TSC. We show that these misinformation can influence the signal phases determined by BP-based TSC algorithms. We consider an adversary that determines a set of arriving vehicles to be attack vehicles from many candidate sets (attack strategies) in order to maximize the number of disrupted signal phases. We show that by formulating the problem as a 0/1 Knapsack problem, the adversary can explore the space of attack strategies and determine the optimal strategy that maximally compromises the performance in terms of average delay and fairness. We propose two protection algorithms, namely, auction-based (APA) and hybrid-based (HPA) algorithms and show that they are able to mitigate the impacts of the misinformation attacks.",
      "cited_by_count": 16,
      "type": "article",
      "source": {
        "name": "IEEE Transactions on Intelligent Transportation Systems",
        "type": "journal",
        "issn": [
          "1524-9050",
          "1558-0016"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://escholarship.org/uc/item/0wk000zg"
      },
      "topics": [
        "Vehicular Ad Hoc Networks (VANETs)",
        "Traffic control and management",
        "Blockchain Technology Applications and Security"
      ],
      "referenced_works_count": 27,
      "url": "https://openalex.org/W3133121169"
    },
    {
      "openalex_id": "W4221048244",
      "doi": "10.3390/make4010011",
      "title": "Developing a Novel Fair-Loan Classifier through a Multi-Sensitive Debiasing Pipeline: DualFair",
      "authors": [
        {
          "name": "Arashdeep Singh",
          "openalex_id": "A5080623433",
          "orcid": "https://orcid.org/0000-0002-6109-451X"
        },
        {
          "name": "Jashandeep Singh",
          "openalex_id": "A5103240846",
          "orcid": "https://orcid.org/0000-0001-8186-7383"
        },
        {
          "name": "Ariba Khan",
          "openalex_id": "A5053578798",
          "orcid": "https://orcid.org/0000-0002-7613-5636",
          "institutions": [
            "Massachusetts Institute of Technology",
            "Vassar College"
          ]
        },
        {
          "name": "Amar Gupta",
          "openalex_id": "A5089511560",
          "orcid": "https://orcid.org/0000-0001-9306-1256",
          "institutions": [
            "Massachusetts Institute of Technology",
            "Vassar College"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-03-12",
      "abstract": "Machine learning (ML) models are increasingly being used for high-stake applications that can greatly impact people\u2019s lives. Sometimes, these models can be biased toward certain social groups on the basis of race, gender, or ethnicity. Many prior works have attempted to mitigate this \u201cmodel discrimination\u201d by updating the training data (pre-processing), altering the model learning process (in-processing), or manipulating the model output (post-processing). However, more work can be done in extending this situation to intersectional fairness, where we consider multiple sensitive parameters (e.g., race) and sensitive options (e.g., black or white), thus allowing for greater real-world usability. Prior work in fairness has also suffered from an accuracy\u2013fairness trade-off that prevents both accuracy and fairness from being high. Moreover, the previous literature has not clearly presented holistic fairness metrics that work with intersectional fairness. In this paper, we address all three of these problems by (a) creating a bias mitigation technique called DualFair and (b) developing a new fairness metric (i.e., AWI, a measure of bias of an algorithm based upon inconsistent counterfactual predictions) that can handle intersectional fairness. Lastly, we test our novel mitigation method using a comprehensive U.S. mortgage lending dataset and show that our classifier, or fair loan predictor, obtains relatively high fairness and accuracy metrics.",
      "cited_by_count": 9,
      "type": "article",
      "source": {
        "name": "Machine Learning and Knowledge Extraction",
        "type": "journal",
        "issn": [
          "2504-4990"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2504-4990/4/1/11/pdf?version=1647239220"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 20,
      "url": "https://openalex.org/W4221048244"
    }
  ],
  "count": 50,
  "errors": []
}
