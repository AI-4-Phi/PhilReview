{
  "status": "success",
  "source": "semantic_scholar",
  "query": "subgroup fairness",
  "results": [
    {
      "paperId": "747f34e8063a8ccdca309c6657de75f1dfb69ed2",
      "title": "Doubly-Regressing Approach for Subgroup Fairness",
      "authors": [
        {
          "name": "Kyungseon Lee",
          "authorId": "2388021563"
        },
        {
          "name": "Kunwoong Kim",
          "authorId": "2115763935"
        },
        {
          "name": "Jihu Lee",
          "authorId": "2360353793"
        },
        {
          "name": "Dongyoon Yang",
          "authorId": "2167315030"
        },
        {
          "name": "Yongdai Kim",
          "authorId": "2292403403"
        }
      ],
      "year": 2025,
      "abstract": "Algorithmic fairness is a socially crucial topic in real-world applications of AI. Among many notions of fairness, subgroup fairness is widely studied when multiple sensitive attributes (e.g., gender, race, age) are present. However, as the number of sensitive attributes grows, the number of subgroups increases accordingly, creating heavy computational burdens and data sparsity problem (subgroups with too small sizes). In this paper, we develop a novel learning algorithm for subgroup fairness which resolves these issues by focusing on subgroups with sufficient sample sizes as well as marginal fairness (fairness for each sensitive attribute). To this end, we formalize a notion of subgroup-subset fairness and introduce a corresponding distributional fairness measure called the supremum Integral Probability Metric (supIPM). Building on this formulation, we propose the Doubly Regressing Adversarial learning for subgroup Fairness (DRAF) algorithm, which reduces a surrogate fairness gap for supIPM with much less computation than directly reducing supIPM. Theoretically, we prove that the proposed surrogate fairness gap is an upper bound of supIPM. Empirically, we show that the DRAF algorithm outperforms baseline methods in benchmark datasets, specifically when the number of sensitive attributes is large so that many subgroups are very small.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.21091",
      "arxivId": "2510.21091",
      "url": "https://www.semanticscholar.org/paper/747f34e8063a8ccdca309c6657de75f1dfb69ed2",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.21091"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "58782cef10ad88f43864ed5c98cd549c430cb735",
      "title": "M\u00b2FGB: A Min-Max Gradient Boosting Framework for Subgroup Fairness",
      "authors": [
        {
          "name": "J. S. B. Pereira",
          "authorId": "2323528189"
        },
        {
          "name": "Giovani Valdrighi",
          "authorId": "2373214840"
        },
        {
          "name": "M. M. Raimundo",
          "authorId": "35473883"
        }
      ],
      "year": 2025,
      "abstract": "In recent years, fairness in machine learning has emerged as a critical concern to ensure that developed and deployed predictive models do not have disadvantageous predictions for marginalized groups. It is essential to mitigate discrimination against individuals based on protected attributes such as gender and race. In this work, we consider applying subgroup justice concepts to gradient-boosting machines designed for supervised learning problems. Our approach expanded gradient-boosting methodologies to explore a broader range of objective functions, which combines conventional losses such as the ones from classification and regression and a min-max fairness term. We study relevant theoretical properties of the solution of the min-max optimization problem. The optimization process explored the primal-dual problems at each boosting round. This generic framework can be adapted to diverse fairness concepts. The proposed min-max primal-dual gradient boosting algorithm was theoretically shown to converge under mild conditions and empirically shown to be a powerful and flexible approach to address binary and subgroup fairness.",
      "citationCount": 0,
      "doi": "10.1145/3715275.3732197",
      "arxivId": "2504.12458",
      "url": "https://www.semanticscholar.org/paper/58782cef10ad88f43864ed5c98cd549c430cb735",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "aedaec05717a0a040980339ebe40d8671ee43a68",
      "title": "Subgroup fairness based on shared counterfactuals",
      "authors": [
        {
          "name": "Alejandro Kuratomi",
          "authorId": "2051746915"
        },
        {
          "name": "Zed Lee",
          "authorId": "1894773947"
        },
        {
          "name": "P. Tsaparas",
          "authorId": "1701195"
        },
        {
          "name": "E. Pitoura",
          "authorId": "1781993"
        },
        {
          "name": "Tony Lindgren",
          "authorId": "2265672346"
        },
        {
          "name": "Guilherme Dinis Junior",
          "authorId": "2346511670"
        },
        {
          "name": "Panagiotis Papapetrou",
          "authorId": "2265587978"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/s10115-025-02555-7",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/aedaec05717a0a040980339ebe40d8671ee43a68",
      "venue": "Knowledge and Information Systems",
      "journal": {
        "name": "Knowledge and Information Systems",
        "pages": "10863 - 10901",
        "volume": "67"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0a89047464df7e1d03033522a3cdf64831906965",
      "title": "Distribution-Specific Auditing for Subgroup Fairness",
      "authors": [
        {
          "name": "Daniel Hsu",
          "authorId": "2281824349"
        },
        {
          "name": "Jizhou Huang",
          "authorId": "2281902958"
        },
        {
          "name": "Brendan Juba",
          "authorId": "2281826656"
        }
      ],
      "year": 2024,
      "abstract": "We study the problem of auditing classifiers with the notion of statistical subgroup fairness. Kearns et al. (2018) has shown that the problem of auditing combinatorial subgroups fairness is as hard as agnostic learning. Essentially all work on remedying statistical measures of discrimination against subgroups assumes access to an oracle for this problem, despite the fact that no efficient algorithms are known for it. If we assume the data distribution is Gaussian, or even merely log-concave, then a recent line of work has discovered efficient agnostic learning algorithms for halfspaces. Unfortunately, the reduction of Kearns et al. was formulated in terms of weak,\"distribution-free\"learning, and thus did not establish a connection for families such as log-concave distributions. In this work, we give positive and negative results on auditing for Gaussian distributions: On the positive side, we present an alternative approach to leverage these advances in agnostic learning and thereby obtain the first polynomial-time approximation scheme (PTAS) for auditing nontrivial combinatorial subgroup fairness: we show how to audit statistical notions of fairness over homogeneous halfspace subgroups when the features are Gaussian. On the negative side, we find that under cryptographic assumptions, no polynomial-time algorithm can guarantee any nontrivial auditing, even under Gaussian feature distributions, for general halfspace subgroups.",
      "citationCount": 1,
      "doi": "10.4230/LIPIcs.FORC.2024.5",
      "arxivId": "2401.16439",
      "url": "https://www.semanticscholar.org/paper/0a89047464df7e1d03033522a3cdf64831906965",
      "venue": "Symposium on Foundations of Responsible Computing",
      "journal": {
        "pages": "5:1-5:20"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "48422ab36d7e0f284af1b0d5afb6257c43ed13b2",
      "title": "Polynomial time auditing of statistical subgroup fairness for Gaussian data",
      "authors": [
        {
          "name": "Daniel Hsu",
          "authorId": "2281824349"
        },
        {
          "name": "Jizhou Huang",
          "authorId": "2281902958"
        },
        {
          "name": "Brendan Juba",
          "authorId": "2281826656"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.48550/arXiv.2401.16439",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/48422ab36d7e0f284af1b0d5afb6257c43ed13b2",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2401.16439"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0416c3db92ac9a4e9f05db71570d8223efd33f55",
      "title": "Challenging \"subgroup Fairness\": Towards Intersectional Algorithmic Fairness Based on Personas",
      "authors": [
        {
          "name": "M. Decker",
          "authorId": "2331070137"
        },
        {
          "name": "Laila Wegner",
          "authorId": "2331068415"
        },
        {
          "name": "C. Leicht-Scholten",
          "authorId": "2347733474"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 2,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/0416c3db92ac9a4e9f05db71570d8223efd33f55",
      "venue": "EWAF",
      "journal": null,
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2e2e9b6b88fcef57b8093becb28aa1af8f094c5e",
      "title": "An Empirical Study of Rich Subgroup Fairness for Machine Learning",
      "authors": [
        {
          "name": "Michael Kearns",
          "authorId": "81338045"
        },
        {
          "name": "Seth Neel",
          "authorId": "5880154"
        },
        {
          "name": "Aaron Roth",
          "authorId": "1682008"
        },
        {
          "name": "Zhiwei Steven Wu",
          "authorId": "1768074"
        }
      ],
      "year": 2018,
      "abstract": "Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.",
      "citationCount": 214,
      "doi": "10.1145/3287560.3287592",
      "arxivId": "1808.08166",
      "url": "https://www.semanticscholar.org/paper/2e2e9b6b88fcef57b8093becb28aa1af8f094c5e",
      "venue": "FAT",
      "journal": {
        "name": "Proceedings of the Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "c75465ac0f1ad86f4a7820c30649c9a0f7b39755",
      "title": "Insufficiently Justified Disparate Impact: A New Criterion for Subgroup Fairness",
      "authors": [
        {
          "name": "Neil Menghani",
          "authorId": "2356813367"
        },
        {
          "name": "E. McFowland",
          "authorId": "1993988"
        },
        {
          "name": "Daniel B. Neill",
          "authorId": "2181848"
        }
      ],
      "year": 2023,
      "abstract": "In this paper, we develop a new criterion,\"insufficiently justified disparate impact\"(IJDI), for assessing whether recommendations (binarized predictions) made by an algorithmic decision support tool are fair. Our novel, utility-based IJDI criterion evaluates false positive and false negative error rate imbalances, identifying statistically significant disparities between groups which are present even when adjusting for group-level differences in base rates. We describe a novel IJDI-Scan approach which can efficiently identify the intersectional subpopulations, defined across multiple observed attributes of the data, with the most significant IJDI. To evaluate IJDI-Scan's performance, we conduct experiments on both simulated and real-world data, including recidivism risk assessment and credit scoring. Further, we implement and evaluate approaches to mitigating IJDI for the detected subpopulations in these domains.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2306.11181",
      "arxivId": "2306.11181",
      "url": "https://www.semanticscholar.org/paper/c75465ac0f1ad86f4a7820c30649c9a0f7b39755",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2306.11181"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ed66d3bd097494f05dc98c1e15c3d8dcaa74d77c",
      "title": "Using Under-Represented Subgroup Fine Tuning to Improve Fairness for Disease Prediction",
      "authors": [
        {
          "name": "Yanchen Wang",
          "authorId": "153709709"
        },
        {
          "name": "Rex Bone",
          "authorId": "2347433299"
        },
        {
          "name": "Will Fleisher",
          "authorId": "2347425137"
        },
        {
          "name": "C. Gresenz",
          "authorId": "6192497"
        },
        {
          "name": "Jean Mitchell",
          "authorId": "2304897054"
        },
        {
          "name": "W. Klaauw",
          "authorId": "8236979"
        },
        {
          "name": "Crystal Wang",
          "authorId": "2327823335"
        },
        {
          "name": "Lisa Singh",
          "authorId": "2311157557"
        }
      ],
      "year": 2025,
      "abstract": "The role of artificial intelligence is growing in healthcare and disease prediction. Because of its potential impact and demographic disparities that have been identified in machine learning models for disease prediction, there are growing concerns about transparency, accountability and fairness of these predictive models. However, very little research has investigated methods for improving model fairness in disease prediction, particularly when the sensitive attribute is multivariate and when the distribution of sensitive attribute groups is highly skewed. In this work, we explore algorithmic fairness when predicting heart disease and Alzheimer's Disease and Related Dementias (ADRD). We propose a fine tuning approach to improve model fairness that takes advantage of observations from the majority groups to build a pre-trained model and uses observations from each underrepresented subgroup to fine tune the pre-trained model, thereby incorporating additional specific knowledge about each subgroup. We find that our fine tuning approach performs better than other algorithmic fairness fixing methods across all subgroups even if the subgroup distribution is very imbalanced and some subgroups are very small. This is an important step toward understanding approaches for improving fairness for healthcare and disease prediction.",
      "citationCount": 1,
      "doi": "10.5220/0013318600003911",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/ed66d3bd097494f05dc98c1e15c3d8dcaa74d77c",
      "venue": "BIOSTEC : HEALTHINF",
      "journal": {
        "name": "Biomedical engineering systems and technologies, international joint conference, BIOSTEC ... revised selected papers. BIOSTEC",
        "pages": "\n          240-253\n        ",
        "volume": "2"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a4bee5d1ec866169b840ff5e957b47da4849e2d6",
      "title": "Subgroup Fairness in Graph-based Spam Detection",
      "authors": [
        {
          "name": "Jiaxin Liu",
          "authorId": "2108354616"
        },
        {
          "name": "Yuefei Lyu",
          "authorId": "1807465313"
        },
        {
          "name": "Xi Zhang",
          "authorId": "2108286275"
        },
        {
          "name": "Sihong Xie",
          "authorId": "3131378"
        }
      ],
      "year": 2022,
      "abstract": null,
      "citationCount": 2,
      "doi": "10.48550/arXiv.2204.11164",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a4bee5d1ec866169b840ff5e957b47da4849e2d6",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2204.11164"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "e8b451ec23253c44e396c1433285adf7c93f1fac",
      "title": "Subgroup fairness in two-sided markets",
      "authors": [
        {
          "name": "Quan Zhou",
          "authorId": "2116289774"
        },
        {
          "name": "Jakub Marecek",
          "authorId": "2128242"
        },
        {
          "name": "R. Shorten",
          "authorId": "1799122"
        }
      ],
      "year": 2021,
      "abstract": "It is well known that two-sided markets are unfair in a number of ways. For example, female drivers on ride-hailing platforms earn less than their male colleagues per mile driven. Similar observations have been made for other minority subgroups in other two-sided markets. Here, we suggest a novel market-clearing mechanism for two-sided markets, which promotes equalization of the pay per hour worked across multiple subgroups, as well as within each subgroup. In the process, we introduce a novel notion of subgroup fairness (which we call Inter-fairness), which can be combined with other notions of fairness within each subgroup (called Intra-fairness), and the utility for the customers (Customer-Care) in the objective of the market-clearing problem. Although the novel non-linear terms in the objective complicate market clearing by making the problem non-convex, we show that a certain non-convex augmented Lagrangian relaxation can be approximated to any precision in time polynomial in the number of market participants using semidefinite programming, thanks to its \u201chidden convexity\u201d. This makes it possible to implement the market-clearing mechanism efficiently. On the example of driver-ride assignment in an Uber-like system, we demonstrate the efficacy and scalability of the approach and trade-offs between Inter- and Intra-fairness.",
      "citationCount": 3,
      "doi": "10.1371/journal.pone.0281443",
      "arxivId": "2106.02702",
      "url": "https://www.semanticscholar.org/paper/e8b451ec23253c44e396c1433285adf7c93f1fac",
      "venue": "PLoS ONE",
      "journal": {
        "name": "PLOS ONE",
        "volume": "18"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b4a6924a79cc944e3d81fff504d068389865119a",
      "title": "Advancing subgroup fairness via sleeping experts",
      "authors": [
        {
          "name": "Avrim Blum",
          "authorId": "1690967"
        },
        {
          "name": "Thodoris Lykouris",
          "authorId": "2558458"
        }
      ],
      "year": 2019,
      "abstract": "We study methods for improving fairness to subgroups in settings with overlapping populations and sequential predictions. Classical notions of fairness focus on the balance of some property across different populations. However, in many applications the goal of the different groups is not to be predicted equally but rather to be predicted well. We demonstrate that the task of satisfying this guarantee for multiple overlapping groups is not straightforward and show that for the simple objective of unweighted average of false negative and false positive rate, satisfying this for overlapping populations can be statistically impossible even when we are provided predictors that perform well separately on each subgroup. On the positive side, we show that when individuals are equally important to the different groups they belong to, this goal is achievable; to do so, we draw a connection to the sleeping experts literature in online learning. Motivated by the one-sided feedback in natural settings of interest, we extend our results to such a feedback model. We also provide a game-theoretic interpretation of our results, examining the incentives of participants to join the system and to provide the system full information about predictors they may possess. We end with several interesting open problems concerning the strength of guarantees that can be achieved in a computationally efficient manner.",
      "citationCount": 38,
      "doi": "10.4230/LIPIcs.ITCS.2020.55",
      "arxivId": "1909.08375",
      "url": "https://www.semanticscholar.org/paper/b4a6924a79cc944e3d81fff504d068389865119a",
      "venue": "Information Technology Convergence and Services",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/1909.08375"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "548aee0adaac7674572f41f128bd0d2680b80d25",
      "title": "Improving Fairness in Credit Lending Models using Subgroup Threshold Optimization",
      "authors": [
        {
          "name": "Cecilia Ying",
          "authorId": "2165227837"
        },
        {
          "name": "Stephen Thomas",
          "authorId": "2165301306"
        }
      ],
      "year": 2024,
      "abstract": "In an effort to improve the accuracy of credit lending decisions, many financial intuitions are now using predictions from machine learning models. While such predictions enjoy many advantages, recent research has shown that the predictions have the potential to be biased and unfair towards certain subgroups of the population. To combat this, several techniques have been introduced to help remove the bias and improve the overall fairness of the predictions. We introduce a new fairness technique, called \\textit{Subgroup Threshold Optimizer} (\\textit{STO}), that does not require any alternations to the input training data nor does it require any changes to the underlying machine learning algorithm, and thus can be used with any existing machine learning pipeline. STO works by optimizing the classification thresholds for individual subgroups in order to minimize the overall discrimination score between them. Our experiments on a real-world credit lending dataset show that STO can reduce gender discrimination by over 90\\%.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2403.10652",
      "arxivId": "2403.10652",
      "url": "https://www.semanticscholar.org/paper/548aee0adaac7674572f41f128bd0d2680b80d25",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2403.10652"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5f4c0d77724379e86b768b06a4c44ae5d211d89a",
      "title": "Fairpriori: Improving Biased Subgroup Discovery for Deep Neural Network Fairness",
      "authors": [
        {
          "name": "Kacy Zhou",
          "authorId": "2310191592"
        },
        {
          "name": "Jiawen Wen",
          "authorId": "2275350863"
        },
        {
          "name": "Nan Yang",
          "authorId": "2310202686"
        },
        {
          "name": "Dong Yuan",
          "authorId": "2275160928"
        },
        {
          "name": "Qinghua Lu",
          "authorId": "2309410721"
        },
        {
          "name": "Huaming Chen",
          "authorId": "2266558187"
        }
      ],
      "year": 2024,
      "abstract": "While deep learning has become a core functional module of most software systems, concerns regarding the fairness of ML predictions have emerged as a significant issue that affects prediction results due to discrimination. Intersectional bias, which disproportionately affects members of subgroups, is a prime example of this. For instance, a machine learning model might exhibit bias against darker-skinned women, while not showing bias against individuals with darker skin or women. This problem calls for effective fairness testing before the deployment of such deep learning models in real-world scenarios. However, research into detecting such bias is currently limited compared to research on individual and group fairness. Existing tools to investigate intersectional bias lack important features such as support for multiple fairness metrics, fast and efficient computation, and user-friendly interpretation. This paper introduces Fairpriori, a novel biased subgroup discovery method, which aims to address these limitations. Fairpriori incorporates the frequent itemset generation algorithm to facilitate effective and efficient investigation of intersectional bias by producing fast fairness metric calculations on subgroups of a dataset. Through comparison with the state-of-the-art methods (e.g., Themis, FairFictPlay, and TestSGD) under similar conditions, Fairpriori demonstrates superior effectiveness and efficiency when identifying intersectional bias. Specifically, Fairpriori is easier to use and interpret, supports a wider range of use cases by accommodating multiple fairness metrics, and exhibits higher efficiency in computing fairness metrics. These findings showcase Fairpriori's potential for effectively uncovering subgroups affected by intersectional bias, supported by its open-source tooling at https://anonymous.4open.science/r/Fairpriori-0320.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2407.01595",
      "arxivId": "2407.01595",
      "url": "https://www.semanticscholar.org/paper/5f4c0d77724379e86b768b06a4c44ae5d211d89a",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2407.01595"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a4070607bd1800fb6ab116717b5178acc3d7ab52",
      "title": "Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities",
      "authors": [
        {
          "name": "D. Moukheiber",
          "authorId": "2114905585"
        },
        {
          "name": "S. Mahindre",
          "authorId": "2048030245"
        },
        {
          "name": "Lama Moukheiber",
          "authorId": "2145841875"
        },
        {
          "name": "M. Moukheiber",
          "authorId": "2139921488"
        },
        {
          "name": "Mingchen Gao",
          "authorId": "2293714675"
        }
      ],
      "year": 2024,
      "abstract": "There has been significant progress in implementing deep learning models in disease diagnosis using chest X- rays. Despite these advancements, inherent biases in these models can lead to disparities in prediction accuracy across protected groups. In this study, we propose a framework to achieve accurate diagnostic outcomes and ensure fairness across intersectional groups in high-dimensional chest X- ray multi-label classification. Transcending traditional protected attributes, we consider complex interactions within social determinants, enabling a more granular benchmark and evaluation of fairness. We present a simple and robust method that involves retraining the last classification layer of pre-trained models using a balanced dataset across groups. Additionally, we account for fairness constraints and integrate class-balanced fine-tuning for multi-label settings. The evaluation of our method on the MIMIC-CXR dataset demonstrates that our framework achieves an optimal tradeoff between accuracy and fairness compared to baseline methods.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2403.18196",
      "arxivId": "2403.18196",
      "url": "https://www.semanticscholar.org/paper/a4070607bd1800fb6ab116717b5178acc3d7ab52",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2403.18196"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c3ed22f7a27188beda65de760daeda10376e4df4",
      "title": "Understanding algorithmic fairness for clinical prediction in terms of subgroup net benefit and health equity",
      "authors": [
        {
          "name": "J. Benitez-Aurioles",
          "authorId": "2215333021"
        },
        {
          "name": "A. Joules",
          "authorId": "2334741574"
        },
        {
          "name": "I. Brusini",
          "authorId": "2334740546"
        },
        {
          "name": "N. Peek",
          "authorId": "2268673407"
        },
        {
          "name": "M. Sperrin",
          "authorId": "2263793668"
        }
      ],
      "year": 2024,
      "abstract": "There are concerns about the fairness of clinical prediction models. 'Fair' models are defined as those for which their performance or predictions are not inappropriately influenced by protected attributes such as ethnicity, gender, or socio-economic status. Researchers have raised concerns that current algorithmic fairness paradigms enforce strict egalitarianism in healthcare, levelling down the performance of models in higher-performing subgroups instead of improving it in lower-performing ones. We propose assessing the fairness of a prediction model by expanding the concept of net benefit, using it to quantify and compare the clinical impact of a model in different subgroups. We use this to explore how a model distributes benefit across a population, its impact on health inequalities, and its role in the achievement of health equity. We show how resource constraints might introduce necessary trade-offs between health equity and other objectives of healthcare systems. We showcase our proposed approach with the development of two clinical prediction models: 1) a prognostic type 2 diabetes model used by clinicians to enrol patients into a preventive care lifestyle intervention programme, and 2) a lung cancer screening algorithm used to allocate diagnostic scans across the population. This approach helps modellers better understand if a model upholds health equity by considering its performance in a clinical and social context.",
      "citationCount": 1,
      "doi": null,
      "arxivId": "2412.07879",
      "url": "https://www.semanticscholar.org/paper/c3ed22f7a27188beda65de760daeda10376e4df4",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "854cee242b69a792f00f91c0d94c1f022e07396f",
      "title": "COSCFair: Ensuring Subgroup Fairness Through Fair Classification Framework",
      "authors": [
        {
          "name": "Beg\u00fcm Hattato\u011flu",
          "authorId": "1394668352"
        },
        {
          "name": "Enas Khwaileh",
          "authorId": "1585154639"
        },
        {
          "name": "A. Qahtan",
          "authorId": "2523205"
        },
        {
          "name": "Heysem Kaya",
          "authorId": "38007788"
        },
        {
          "name": "Yannis Velegrakis",
          "authorId": "2163752"
        }
      ],
      "year": 2021,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/854cee242b69a792f00f91c0d94c1f022e07396f",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "c53a5c8a815712791b349ad8692d7fa93b52c745",
      "title": "Subgroup Harm Assessor: Identifying Potential Fairness-Related Harms and Predictive Bias",
      "authors": [
        {
          "name": "A. Dubowski",
          "authorId": "15160656"
        },
        {
          "name": "Hilde J. P. Weerts",
          "authorId": "32345170"
        },
        {
          "name": "A. Wolters",
          "authorId": "5952678"
        },
        {
          "name": "Mykola Pechenizkiy",
          "authorId": "1691997"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/978-3-031-70371-3_31",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/c53a5c8a815712791b349ad8692d7fa93b52c745",
      "venue": "ECML/PKDD",
      "journal": {
        "pages": "413-417"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d95bd7917eba287cbf10f202856e4862b990c360",
      "title": "Data Augmentation via Subgroup Mixup for Improving Fairness",
      "authors": [
        {
          "name": "Madeline Navarro",
          "authorId": "1936823928"
        },
        {
          "name": "C. Little",
          "authorId": "2167317465"
        },
        {
          "name": "Genevera I. Allen",
          "authorId": "2302572"
        },
        {
          "name": "Santiago Segarra",
          "authorId": "2239197971"
        }
      ],
      "year": 2023,
      "abstract": "In this work, we propose data augmentation via pairwise mixup across subgroups to improve group fairness. Many real-world applications of machine learning systems exhibit biases across certain groups due to underrepresentation or training data that reflects societal biases. Inspired by the successes of mixup for improving classification performance, we develop a pairwise mixup scheme to augment training data and encourage fair and accurate decision boundaries for all subgroups. Data augmentation for group fairness allows us to add new samples of underrepresented groups to balance subpopulations. Furthermore, our method allows us to use the generalization ability of mixup to improve both fairness and accuracy. We compare our proposed mixup to existing data augmentation and bias mitigation approaches on both synthetic simulations and real-world benchmark fair classification data, demonstrating that we are able to achieve fair outcomes with robust if not improved accuracy.",
      "citationCount": 9,
      "doi": "10.1109/ICASSP48485.2024.10446564",
      "arxivId": "2309.07110",
      "url": "https://www.semanticscholar.org/paper/d95bd7917eba287cbf10f202856e4862b990c360",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "journal": {
        "name": "ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
        "pages": "7350-7354"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "7fb8f78023d747f5eb4bd627dd22cf17cb924fb3",
      "title": "Efficiently Satisfying Subgroup Fairness in Generalized Classification Settings",
      "authors": [
        {
          "name": "Matt A. King",
          "authorId": "34683533"
        },
        {
          "name": "Fahim Tajwar",
          "authorId": "83137609"
        }
      ],
      "year": 2019,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7fb8f78023d747f5eb4bd627dd22cf17cb924fb3",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "86c4bb73bcc10a17faee13034a1742008cb001df",
      "title": "Subgroup Generalization and Fairness of Graph Neural Networks",
      "authors": [
        {
          "name": "Jiaqi W. Ma",
          "authorId": "47793019"
        },
        {
          "name": "Junwei Deng",
          "authorId": "2153368600"
        },
        {
          "name": "Qiaozhu Mei",
          "authorId": "1743469"
        }
      ],
      "year": 2021,
      "abstract": "Despite enormous successful applications of graph neural networks (GNNs), theoretical understanding of their generalization ability, especially for node-level tasks where data are not independent and identically-distributed (IID), has been sparse. The theoretical investigation of the generalization performance is beneficial for understanding fundamental issues (such as fairness) of GNN models and designing better learning methods. In this paper, we present a novel PAC-Bayesian analysis for GNNs under a non-IID semi-supervised learning setup. Moreover, we analyze the generalization performances on different subgroups of unlabeled nodes, which allows us to further study an accuracy-(dis)parity-style (un)fairness of GNNs from a theoretical perspective. Under reasonable assumptions, we demonstrate that the distance between a test subgroup and the training set can be a key factor affecting the GNN performance on that subgroup, which calls special attention to the training node selection for fair learning. Experiments across multiple GNN models and datasets support our theoretical results.",
      "citationCount": 91,
      "doi": null,
      "arxivId": "2106.15535",
      "url": "https://www.semanticscholar.org/paper/86c4bb73bcc10a17faee13034a1742008cb001df",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "pages": "1048-1061"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "528fb2ab93bd2a63354513a191b83907ccf4ccb5",
      "title": "Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning",
      "authors": [
        {
          "name": "Natalie Dullerud",
          "authorId": "2367794429"
        },
        {
          "name": "Karsten Roth",
          "authorId": "77340962"
        },
        {
          "name": "Kimia Hamidieh",
          "authorId": "2141059217"
        },
        {
          "name": "Nicolas Papernot",
          "authorId": "2367796360"
        },
        {
          "name": "M. Ghassemi",
          "authorId": "2804918"
        }
      ],
      "year": 2022,
      "abstract": "Deep metric learning (DML) enables learning with less supervision through its emphasis on the similarity structure of representations. There has been much work on improving generalization of DML in settings like zero-shot retrieval, but little is known about its implications for fairness. In this paper, we are the first to evaluate state-of-the-art DML methods trained on imbalanced data, and to show the negative impact these representations have on minority subgroup performance when used for downstream tasks. In this work, we first define fairness in DML through an analysis of three properties of the representation space -- inter-class alignment, intra-class alignment, and uniformity -- and propose finDML, the fairness in non-balanced DML benchmark to characterize representation fairness. Utilizing finDML, we find bias in DML representations to propagate to common downstream classification tasks. Surprisingly, this bias is propagated even when training data in the downstream task is re-balanced. To address this problem, we present Partial Attribute De-correlation (PARADE) to de-correlate feature representations from sensitive attributes and reduce performance gaps between subgroups in both embedding space and downstream metrics.",
      "citationCount": 16,
      "doi": "10.48550/arXiv.2203.12748",
      "arxivId": "2203.12748",
      "url": "https://www.semanticscholar.org/paper/528fb2ab93bd2a63354513a191b83907ccf4ccb5",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2203.12748"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "14d8d5e612028842d35cb5865f8fecf9ecc63c00",
      "title": "Blind Pareto Fairness and Subgroup Robustness",
      "authors": [
        {
          "name": "Natalia Mart\u00ednez",
          "authorId": "144323649"
        },
        {
          "name": "Mart\u00edn Bertr\u00e1n",
          "authorId": "37335063"
        },
        {
          "name": "Afroditi Papadaki",
          "authorId": "83557736"
        },
        {
          "name": "M. Rodrigues",
          "authorId": "4838771"
        },
        {
          "name": "G. Sapiro",
          "authorId": "1699339"
        }
      ],
      "year": 2021,
      "abstract": null,
      "citationCount": 34,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/14d8d5e612028842d35cb5865f8fecf9ecc63c00",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "pages": "7492-7501"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "ea065bce0f3c60537dd3731cac87c1e419b7f243",
      "title": "Disproportionate Subgroup Impacts and Other Challenges of Fairness in Artificial Intelligence for Medical Image Analysis",
      "authors": [
        {
          "name": "E. Stanley",
          "authorId": "144186182"
        },
        {
          "name": "M. Wilms",
          "authorId": "1752177"
        },
        {
          "name": "N. Forkert",
          "authorId": "144578371"
        }
      ],
      "year": 2022,
      "abstract": null,
      "citationCount": 13,
      "doi": "10.1007/978-3-031-23223-7_2",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/ea065bce0f3c60537dd3731cac87c1e419b7f243",
      "venue": "EPIMI/ML-CDS@MICCAI",
      "journal": {
        "pages": "14-25"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "9af7ee16755e343a57d338a27253a57ec381bf7c",
      "title": "Clustering-Based Subgroup Detection for Automated Fairness Analysis",
      "authors": [
        {
          "name": "J. Sch\u00e4fer",
          "authorId": "1902564053"
        },
        {
          "name": "L. Wiese",
          "authorId": "1970342"
        }
      ],
      "year": 2022,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.1007/978-3-031-15743-1_5",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/9af7ee16755e343a57d338a27253a57ec381bf7c",
      "venue": "Symposium on Advances in Databases and Information Systems",
      "journal": {
        "pages": "45-55"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "92b90f4bd30b7ea51b6f8d3cfc0c0c80a27c218c",
      "title": "ASDF-Dashboard: Automated Subgroup Detection and Fairness Analysis",
      "authors": [
        {
          "name": "J. Sch\u00e4fer",
          "authorId": "1902564053"
        },
        {
          "name": "L. Wiese",
          "authorId": "1970342"
        }
      ],
      "year": 2022,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/92b90f4bd30b7ea51b6f8d3cfc0c0c80a27c218c",
      "venue": "Lernen, Wissen, Daten, Analysen",
      "journal": {
        "pages": "45-56"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "72986df9d1a8c7332a9d5ead4c63013d13e2184d",
      "title": "Are Your Reviewers Being Treated Equally? Discovering Subgroup Structures to Improve Fairness in Spam Detection",
      "authors": [
        {
          "name": "Jiaxin Liu",
          "authorId": "2108354616"
        },
        {
          "name": "Yuefei Lyu",
          "authorId": "1807465313"
        },
        {
          "name": "Xi Zhang",
          "authorId": "47957054"
        },
        {
          "name": "Sihong Xie",
          "authorId": "3131378"
        }
      ],
      "year": 2022,
      "abstract": "User-generated reviews of products are vital assets of online commerce, such as Amazon and Yelp, while fake reviews are prevalent to mislead customers. GNN is the state-of-the-art method that detects suspicious reviewers by exploiting the topologies of the graph connecting reviewers, reviews, and target products. However, the discrepancy in the detection accuracy over different groups of reviewers can degrade reviewer engagement and customer trust in the review websites. Unlike the previous belief that the difference between the groups causes unfairness, we study the subgroup structures within the groups that can also cause discrepancies in treating different groups. This paper addresses the challenges of defining, approximating, and utilizing a new subgroup structure for fair spam detection. We first identify subgroup structures in the review graph that lead to discrepant accuracy in the groups. The complex dependencies over the review graph create difficulties in teasing out subgroups hidden within larger groups. We design a model that can be trained to jointly infer the hidden subgroup memberships and exploits the membership for calibrating the detection accuracy across groups. Comprehensive comparisons against baselines on three large Yelp review datasets demonstrate that the subgroup membership can be identified and exploited for group fairness.",
      "citationCount": 2,
      "doi": null,
      "arxivId": "2204.11164",
      "url": "https://www.semanticscholar.org/paper/72986df9d1a8c7332a9d5ead4c63013d13e2184d",
      "venue": "",
      "journal": null,
      "publicationTypes": [
        "Review"
      ]
    },
    {
      "paperId": "f4209c041c15eea45015ba27576dc702209a3c4a",
      "title": "AI Fairness at Subgroup Level - A Structured Literature Review",
      "authors": [
        {
          "name": "Luis L\u00e4mmermann",
          "authorId": "2392357972"
        },
        {
          "name": "Patrick Richter",
          "authorId": "2071102394"
        },
        {
          "name": "Amelie Zwickel",
          "authorId": "2179560024"
        },
        {
          "name": "Moritz Markgraf",
          "authorId": "35445565"
        }
      ],
      "year": 2022,
      "abstract": null,
      "citationCount": 2,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f4209c041c15eea45015ba27576dc702209a3c4a",
      "venue": "European Conference on Information Systems",
      "journal": null,
      "publicationTypes": [
        "JournalArticle",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "6a4cde3fde260b816a9a783eaf81eb5792f518dc",
      "title": "Black Loans Matter: Distributionally Robust Fairness for Fighting Subgroup Discrimination",
      "authors": [
        {
          "name": "Mark Weber",
          "authorId": null
        },
        {
          "name": "M. Yurochkin",
          "authorId": "8202372"
        },
        {
          "name": "S. Botros",
          "authorId": "33005557"
        },
        {
          "name": "Vanio Markov",
          "authorId": "2331363"
        }
      ],
      "year": 2020,
      "abstract": "Algorithmic fairness in lending today relies on group fairness metrics for monitoring statistical parity across protected groups. This approach is vulnerable to subgroup discrimination by proxy, carrying significant risks of legal and reputational damage for lenders and blatantly unfair outcomes for borrowers. Practical challenges arise from the many possible combinations and subsets of protected groups. We motivate this problem against the backdrop of historical and residual racism in the United States polluting all available training data and raising public sensitivity to algorithimic bias. We review the current regulatory compliance protocols for fairness in lending and discuss their limitations relative to the contributions state-of-the-art fairness methods may afford. We propose a solution for addressing subgroup discrimination, while adhering to existing group fairness requirements, from recent developments in individual fairness methods and corresponding fair metric learning algorithms.",
      "citationCount": 15,
      "doi": null,
      "arxivId": "2012.01193",
      "url": "https://www.semanticscholar.org/paper/6a4cde3fde260b816a9a783eaf81eb5792f518dc",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2012.01193"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "62ebbad3257322a43c2d91484e12c59ab969910b",
      "title": "Pareto-Efficient Fairness for Skewed Subgroup Data",
      "authors": [
        {
          "name": "Alyssa Lees",
          "authorId": "49982610"
        },
        {
          "name": "Ananth Balashankar",
          "authorId": "2593082"
        },
        {
          "name": "Chris Welty",
          "authorId": "143778120"
        },
        {
          "name": "L. Subramanian",
          "authorId": "1710917"
        }
      ],
      "year": 2019,
      "abstract": null,
      "citationCount": 7,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/62ebbad3257322a43c2d91484e12c59ab969910b",
      "venue": "",
      "journal": {
        "name": "",
        "volume": ""
      },
      "publicationTypes": null
    },
    {
      "paperId": "f543ce81141972a1e0182afe4f8590e1dac7902f",
      "title": "One Size Fits None: Rethinking Fairness in Medical AI",
      "authors": [
        {
          "name": "Roland Roller",
          "authorId": "2315304116"
        },
        {
          "name": "Michael Hahn",
          "authorId": "2367277094"
        },
        {
          "name": "A. Ravichandran",
          "authorId": "33548150"
        },
        {
          "name": "B. Osmanodja",
          "authorId": "2082396273"
        },
        {
          "name": "Florian Oetke",
          "authorId": "2367277906"
        },
        {
          "name": "Zeineb Sassi",
          "authorId": "2277746975"
        },
        {
          "name": "A. Burchardt",
          "authorId": "2495532"
        },
        {
          "name": "Klaus Netter",
          "authorId": "2315307027"
        },
        {
          "name": "Klemens Budde",
          "authorId": "2237201973"
        },
        {
          "name": "Anne Herrmann",
          "authorId": "2294552934"
        },
        {
          "name": "Tobias Strapatsas",
          "authorId": "2315305492"
        },
        {
          "name": "Peter Dabrock",
          "authorId": "2251918435"
        },
        {
          "name": "Sebastian M\u00f6ller",
          "authorId": "2315304777"
        }
      ],
      "year": 2025,
      "abstract": "Machine learning (ML) models are increasingly used to support clinical decision-making. However, real-world medical datasets are often noisy, incomplete, and imbalanced, leading to performance disparities across patient subgroups. These differences raise fairness concerns, particularly when they reinforce existing disadvantages for marginalized groups. In this work, we analyze several medical prediction tasks and demonstrate how model performance varies with patient characteristics. While ML models may demonstrate good overall performance, we argue that subgroup-level evaluation is essential before integrating them into clinical workflows. By conducting a performance analysis at the subgroup level, differences can be clearly identified-allowing, on the one hand, for performance disparities to be considered in clinical practice, and on the other hand, for these insights to inform the responsible development of more effective models. Thereby, our work contributes to a practical discussion around the subgroup-sensitive development and deployment of medical ML models and the interconnectedness of fairness and transparency.",
      "citationCount": 3,
      "doi": "10.18653/v1/2025.gebnlp-1.25",
      "arxivId": "2506.14400",
      "url": "https://www.semanticscholar.org/paper/f543ce81141972a1e0182afe4f8590e1dac7902f",
      "venue": "Proceedings of the 6th Workshop on Gender Bias in Natural Language Processing (GeBNLP)",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.14400"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "27110f595819d4a00083738ecba6663c09704de3",
      "title": "Developing Fairness-Aware Task Decomposition to Improve Equity in Post-Spinal Fusion Complication Prediction",
      "authors": [
        {
          "name": "Yining Yuan",
          "authorId": "2382506161"
        },
        {
          "name": "J. B. Tamo",
          "authorId": "2335664359"
        },
        {
          "name": "Wenqi Shi",
          "authorId": "2153422648"
        },
        {
          "name": "Yishan Zhong",
          "authorId": "2195947297"
        },
        {
          "name": "Micky C. Nnamdi",
          "authorId": "2253668558"
        },
        {
          "name": "B. R. Brenn",
          "authorId": "2394417399"
        },
        {
          "name": "Steven W. Hwang",
          "authorId": "2381913388"
        },
        {
          "name": "M. D. Wang",
          "authorId": "2386162920"
        }
      ],
      "year": 2025,
      "abstract": "Fairness in clinical prediction models remains an open challenge, as many methods oversimplify outcomes and inadvertently propagate demographic biases. This issue is especially consequential in spinal fusion surgery for scoliosis, a high-risk procedure with heterogeneous patient outcomes. We present FAIR-MTL, a fairness-aware multitask learning framework for equitable and fine-grained prediction of postoperative complication severity. FAIR-MTL integrates Sensitive Set Invariance (SSI) to identify latent subgroups that are invariant to sensitive attributes, such as age and gender. These subgroups form task-specific branches within a shared multitask neural network, enabling personalized modeling and subgroup adaptivity while jointly optimizing predictive accuracy and fairness constraints. FAIR-MTL achieves an AUC of 0.86 and accuracy of 75% across four complication severity classes while reducing the average demographic parity difference to 0.055 and equalized odds difference to 0.094 for gender, and 0.056 and 0.148, respectively for age, substantially improving fairness over standard baselines. SHAP and Gini importance analyses highlight clinically relevant predictors, including hematocrit, hemoglobin, and patient weight, providing transparency at both global and individual levels. Our results demonstrate that integrating fairness-aware task decomposition into model design enables equitable, interpretable, and clinically actionable predictions for surgical risk stratification.",
      "citationCount": 1,
      "doi": "10.1145/3765612.3767246",
      "arxivId": "2512.00598",
      "url": "https://www.semanticscholar.org/paper/27110f595819d4a00083738ecba6663c09704de3",
      "venue": "Proceedings of the 16th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics",
      "journal": {
        "name": "Proceedings of the 16th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics"
      },
      "publicationTypes": [
        "Book",
        "Conference"
      ]
    },
    {
      "paperId": "3a800a8a75a996bece545e7507dea863cf43fe24",
      "title": "Fairness for the People, by the People: Minority Collective Action",
      "authors": [
        {
          "name": "Omri Ben-Dov",
          "authorId": "2047690874"
        },
        {
          "name": "Samira Samadi",
          "authorId": "2301015441"
        },
        {
          "name": "Amartya Sanyal",
          "authorId": "2301015600"
        },
        {
          "name": "Alexandru Tifrea",
          "authorId": "2380529321"
        }
      ],
      "year": 2025,
      "abstract": "Machine learning models often preserve biases present in training data, leading to unfair treatment of certain minority groups. Despite an array of existing firm-side bias mitigation techniques, they typically incur utility costs and require organizational buy-in. Recognizing that many models rely on user-contributed data, end-users can induce fairness through the framework of Algorithmic Collective Action, where a coordinated minority group strategically relabels its own data to enhance fairness, without altering the firm's training process. We propose three practical, model-agnostic methods to approximate ideal relabeling and validate them on real-world datasets. Our findings show that a subgroup of the minority can substantially reduce unfairness with a small impact on the overall prediction error.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2508.15374",
      "arxivId": "2508.15374",
      "url": "https://www.semanticscholar.org/paper/3a800a8a75a996bece545e7507dea863cf43fe24",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.15374"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8f80771c9c0bca637f0879a52f98b8a02597902c",
      "title": "Mitigating Demographic Bias in ImageNet: A Comprehensive Analysis of Disparities and Fairness in Deep Learning Models",
      "authors": [
        {
          "name": "C. K. Gitonga",
          "authorId": "102254785"
        },
        {
          "name": "Dennis Murithi",
          "authorId": "2357400957"
        },
        {
          "name": "Edna Chebet",
          "authorId": "2357406071"
        }
      ],
      "year": 2025,
      "abstract": "Deep learning has transformed artificial intelligence (AI), yet fairness concerns persist due to biases in training datasets. ImageNet, a key dataset in computer vision, contains demographic imbalances in its \u201cperson\u201d categories, raising concerns about biased AI models. This study is to examine these biases, evaluate their impact on model performance, and implement fairness aware mitigation strategies. Using a fine-tuned EfficientNet-B0 model, we achieved 98.44% accuracy. Subgroup analysis revealed higher error rates for darker-skinned individuals and women compared to lighter-skinned individuals and men. Mitigation techniques, including data augmentation and re-sampling, improved fairness metrics by 1.4% for underrepresented groups. Confidence analysis showed 99.25% accuracy for predictions with over 80% confidence. To enhance reproducibility, we deployed our demographic bias detection model on Hugging Face Spaces. The study\u2019s limitations include a focus on \u201cperson\u201d categories, computational constraints, and potential annotation biases. Future research should extend fairness-aware interventions across diverse datasets.",
      "citationCount": 1,
      "doi": "10.24018/ejai.2025.4.2.51",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/8f80771c9c0bca637f0879a52f98b8a02597902c",
      "venue": "European Journal of Artificial Intelligence and Machine Learning",
      "journal": {
        "name": "European Journal of Artificial Intelligence and Machine Learning"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "afce1a9a921101cdeb1d0c9730b873859b25b57c",
      "title": "Evaluating the impact of data biases on algorithmic fairness and clinical utility of machine learning models for prolonged opioid use prediction",
      "authors": [
        {
          "name": "Behzad Naderalvojoud",
          "authorId": "2150096"
        },
        {
          "name": "Catherine M. Curtin",
          "authorId": "2250815427"
        },
        {
          "name": "Steven M. Asch",
          "authorId": "2316631050"
        },
        {
          "name": "Keith Humphreys",
          "authorId": "2279417411"
        },
        {
          "name": "Tina Hernandez-Boussard",
          "authorId": "2359225046"
        }
      ],
      "year": 2025,
      "abstract": "Abstract Objectives The growing use of machine learning (ML) in healthcare raises concerns about how data biases affect real-world model performance. While existing frameworks evaluate algorithmic fairness, they often overlook the impact of bias on generalizability and clinical utility, which are critical for safe deployment. Building on prior methods, this study extends bias analysis to include clinical utility, addressing a key gap between fairness evaluation and decision-making. Materials and Methods We applied a 3-phase evaluation to a previously developed model predicting prolonged opioid use (POU), validated on Veterans Health Administration (VHA) data. The analysis included internal and external validation, model retraining on VHA data, and subgroup evaluation across demographic, vulnerable, risk, and comorbidity groups. We assessed performance using area under the receiver operating characteristic curve (AUROC), calibration, and decision curve analysis, incorporating standardized net-benefits to evaluate clinical utility alongside fairness and generalizability. Results The internal cohort (N\u2009=\u200941\u2009929) had a 14.7% POU prevalence, compared to 34.3% in the external VHA cohort (N\u2009=\u2009397\u2009150). The model\u2019s AUROC decreased from 0.74 in the internal test cohort to 0.70 in the full external cohort. Subgroup-level performance averaged 0.69 (SD\u2009=\u20090.01), showing minimal deviation from the external cohort overall. Retraining on VHA data improved AUROCs to 0.82. Clinical utility analysis showed systematic shifts in net-benefit across threshold probabilities. Discussion While the POU model showed generalizability and fairness internally, external validation and retraining revealed performance and utility shifts across subgroups. Conclusion Population-specific biases affect clinical utility\u2014an often-overlooked dimension in fairness evaluation\u2014a key need to ensure equitable benefits across diverse patient groups.",
      "citationCount": 1,
      "doi": "10.1093/jamiaopen/ooaf115",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/afce1a9a921101cdeb1d0c9730b873859b25b57c",
      "venue": "JAMIA Open",
      "journal": {
        "name": "JAMIA Open",
        "volume": "8"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8f37cef0a468bb66ca76e1cf5cdb8e898b05311d",
      "title": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach",
      "authors": [
        {
          "name": "Yi-Cheng Lin",
          "authorId": "2284761645"
        },
        {
          "name": "Huang-Cheng Chou",
          "authorId": "35725460"
        },
        {
          "name": "Hung-yi Lee",
          "authorId": "2305556858"
        }
      ],
      "year": 2025,
      "abstract": "While subgroup disparities and performance bias are increasingly studied in computational research, fairness in categorical Speech Emotion Recognition (SER) remains underexplored. Existing methods often rely on explicit demographic labels, which are difficult to obtain due to privacy concerns. To address this limitation, we introduce an Implicit Demography Inference (IDI) module that leverages pseudo-labeling from a pre-trained model and unsupervised learning using k-means clustering to mitigate bias in SER. Our experiments show that pseudo-labeling IDI reduces subgroup disparities, improving fairness metrics by over 28% with less than a 2% decrease in SER accuracy. Also, the unsupervised IDI yields more than a 4.6% improvement in fairness metrics with a drop of less than 3.6% in SER performance. Further analyses reveal that the unsupervised IDI consistently mitigates race and age disparities, demonstrating its potential when explicit demographic information is unavailable.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2505.14449",
      "arxivId": "2505.14449",
      "url": "https://www.semanticscholar.org/paper/8f37cef0a468bb66ca76e1cf5cdb8e898b05311d",
      "venue": "Interspeech",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.14449"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ba6e1afe8a5fcaedb18a15cc148b3ea27e9f0bfa",
      "title": "An External Fairness Evaluation of LinkedIn Talent Search",
      "authors": [
        {
          "name": "Tina Behzad",
          "authorId": "2392722586"
        },
        {
          "name": "Siddartha Devic",
          "authorId": "117403158"
        },
        {
          "name": "Vatsal Sharan",
          "authorId": "2798845"
        },
        {
          "name": "Aleksandra Korolova",
          "authorId": "2316636550"
        },
        {
          "name": "David Kempe",
          "authorId": "2284065558"
        }
      ],
      "year": 2025,
      "abstract": "We conduct an independent, third-party audit for bias of LinkedIn's Talent Search ranking system, focusing on potential ranking bias across two attributes: gender and race. To do so, we first construct a dataset of rankings produced by the system, collecting extensive Talent Search results across a diverse set of occupational queries. We then develop a robust labeling pipeline that infers the two demographic attributes of interest for the returned users. To evaluate potential biases in the collected dataset of real-world rankings, we utilize two exposure disparity metrics: deviation from group proportions and MinSkew. Our analysis reveals an under-representation of minority groups in early ranks across many queries. We further examine potential causes of this disparity, and discuss why they may be difficult or, in some cases, impossible to fully eliminate among the early ranks of queries. Beyond static metrics, we also investigate the concept of subgroup fairness over time, highlighting temporal disparities in exposure and retention, which are often more difficult to audit for in practice. In employer recruiting platforms such as LinkedIn Talent Search, the persistence of a particular candidate over multiple days in the ranking can directly impact the probability that the given candidate is selected for opportunities. Our analysis reveals demographic disparities in this temporal stability, with some groups experiencing greater volatility in their ranked positions than others. We contextualize all our findings alongside LinkedIn's published self-audits of its Talent Search system and reflect on the methodological constraints of a black-box external evaluation, including limited observability and noisy demographic inference.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2511.10752",
      "url": "https://www.semanticscholar.org/paper/ba6e1afe8a5fcaedb18a15cc148b3ea27e9f0bfa",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "5d14808ffffb9add548be4c1e74ed0022d4c2734",
      "title": "CAN ARTIFICIAL INTELLIGENCE (AI) JUDGE REFLECTION? VALIDITY, RELIABILITY, AND FAIRNESS OF GENERATIVE AI-ASSISTED ASSESSMENT IN POSTGRADUATE HEALTH PROFESSIONS EDUCATION",
      "authors": [
        {
          "name": "B. Jamil",
          "authorId": "66992170"
        },
        {
          "name": "Nowshad Asim",
          "authorId": "14503924"
        }
      ],
      "year": 2025,
      "abstract": "ABSTRACT\nOBJECTIVES\nThis study aimed to evaluate the role of GenAI in assessing reflective writing among Master of Health Professions Education (MHPE) students by comparing GenAI scores with those of human raters, examining subgroup fairness, exploring stakeholder perceptions, and proposing governance recommendations.\nMETHODOLOGY\nA sequential mixed-methods study was conducted in an MHPE programme at Khyber Medical University, Pakistan. In Phase I, 120 Gibbs-structured reflections from 40 students were scored by three trained faculty raters and a GPT-4-level GenAI model using an eight-dimensional rubric. Inter-rater reliability, AI-human agreement, and subgroup differences by gender, discipline, and career stage were examined. In Phase II, semi-structured interviews were conducted with 10 MHPE students and the three faculty raters. Data were analysed using reflexive thematic analysis and integrated with quantitative results.\nRESULTSHuman scoring demonstrated strong reliability (ICC = .82). GenAI showed high alignment with human ratings for surface-level dimensions such as clarity and language mechanics (r = .81-.84), but only modest agreement for higher-order reflective constructs including feelings, analysis, and conclusion (r = .49-.59). Exploratory subgroup analyses revealed no statistically significant differences in AI-human discrepancies. However, qualitative accounts highlighted concerns about linguistic and cultural fairness. Participants valued AI for efficient, organised feedback but consistently emphasised its inability to interpret emotional nuance, contextual meaning, or developmental trajectories. Faculty stressed the irreplaceability of human judgment and the need for transparent governance and fairness monitoring.\nCONCLUSION\nGenAI can effectively support the assessment of structural and linguistic aspects of reflective writing but remains limited in evaluating deeper reflective constructs central to postgraduate learning. Ethical and educationally sound integration requires hybrid human-AI approaches in which AI provides formative support while human evaluators retain primary responsibility for interpretive judgment, fairness oversight, and professional mentorship. GenAI should supplement, not replace, human assessment of reflective writing.",
      "citationCount": 0,
      "doi": "10.37762/jgmds.13-1.835",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/5d14808ffffb9add548be4c1e74ed0022d4c2734",
      "venue": "Journal of Gandhara Medical and Dental Science",
      "journal": {
        "name": "Journal of Gandhara Medical and Dental Science"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5a3780296d90ae27ffdcd031b9d8497356e08cba",
      "title": "Prompt Fairness: Sub-group Disparities in LLMs",
      "authors": [
        {
          "name": "Meiyu Zhong",
          "authorId": "2301152566"
        },
        {
          "name": "N. Teku",
          "authorId": "71090789"
        },
        {
          "name": "Ravi Tandon",
          "authorId": "2301151119"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs), though shown to be effective in many applications, can vary significantly in their response quality. In this paper, we investigate this problem of prompt fairness: specifically, the phrasing of a prompt by different users/styles, despite the same question being asked in principle, may elicit different responses from an LLM. To quantify this disparity, we propose to use information-theoretic metrics that can capture two dimensions of bias: subgroup sensitivity, the variability of responses within a subgroup and cross group consistency, the variability of responses across subgroups. Our analysis reveals that certain subgroups exhibit both higher internal variability and greater divergence from others. Our empirical analysis reveals that certain demographic sub groups experience both higher internal variability and greater divergence from others, indicating structural inequities in model behavior. To mitigate these disparities, we propose practical interventions, including majority voting across multiple generations and prompt neutralization, which together improve response stability and enhance fairness across user populations. In the experiments, we observe clear prompt sensitivity disparities across demographic subgroups: before mitigation, cross-group divergence values reach 0.28 and typically fall in the from 0.14 to 0.22 range. After applying our neutralization and multi generation strategy, these divergences consistently decrease, with the largest gap reduced to 0.22 and many distances falling to 0.17 or below, indicating more stable and consistent outputs across subgroups.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2511.19956",
      "url": "https://www.semanticscholar.org/paper/5a3780296d90ae27ffdcd031b9d8497356e08cba",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "d0c6f465b0ce4c69fe4a0e82c8fdf3d33c99e1a3",
      "title": "APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness",
      "authors": [
        {
          "name": "Priyobrata Mondal",
          "authorId": "2306608534"
        },
        {
          "name": "Faizanuddin Ansari",
          "authorId": "2226529323"
        },
        {
          "name": "Swagatam Das",
          "authorId": "2267691301"
        }
      ],
      "year": 2025,
      "abstract": "Ensuring fairness in machine learning models is critical, especially when biases compound across intersecting protected attributes like race, gender, and age. While existing methods address fairness for single attributes, they fail to capture the nuanced, multiplicative biases faced by intersectional subgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first framework to explicitly model intersectional fairness as a joint optimization problem over the Cartesian product of sensitive attributes. APFEx combines three key innovations- (1) an adaptive multi-objective optimizer that dynamically switches between Pareto cone projection, gradient weighting, and exploration strategies to navigate fairness-accuracy trade-offs, (2) differentiable intersectional fairness metrics enabling gradient-based optimization of non-smooth subgroup disparities, and (3) theoretical guarantees of convergence to Pareto-optimal solutions. Experiments on four real-world datasets demonstrate APFEx's superiority, reducing fairness violations while maintaining competitive accuracy. Our work bridges a critical gap in fair ML, providing a scalable, model-agnostic solution for intersectional fairness.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2509.13908",
      "arxivId": "2509.13908",
      "url": "https://www.semanticscholar.org/paper/d0c6f465b0ce4c69fe4a0e82c8fdf3d33c99e1a3",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.13908"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "21ec95cf3fd6621ecb1c6343131b1684e74d856c",
      "title": "On the Role of Calibration in Benchmarking Algorithmic Fairness for Skin Cancer Detection",
      "authors": [
        {
          "name": "Brandon Dominique",
          "authorId": "2389622403"
        },
        {
          "name": "Prudence Lam",
          "authorId": "2387212913"
        },
        {
          "name": "N. Kurtansky",
          "authorId": "1879519396"
        },
        {
          "name": "Jochen Weber",
          "authorId": "49751538"
        },
        {
          "name": "Kivan\u00e7 K\u00f6se",
          "authorId": "2381729842"
        },
        {
          "name": "Veronica Rotemberg",
          "authorId": "2253156732"
        },
        {
          "name": "Jennifer Dy",
          "authorId": "2389456206"
        }
      ],
      "year": 2025,
      "abstract": "Artificial Intelligence (AI) models have demonstrated expert-level performance in melanoma detection, yet their clinical adoption is hindered by performance disparities across demographic subgroups such as gender, race, and age. Previous efforts to benchmark the performance of AI models have primarily focused on assessing model performance using group fairness metrics that rely on the Area Under the Receiver Operating Characteristic curve (AUROC), which does not provide insights into a model\u2019s ability to provide accurate estimates. In line with clinical assessments, this paper addresses this gap by incorporating calibration as a complementary benchmarking metric to AUROC-based fairness metrics. Calibration evaluates the alignment between predicted probabilities and observed event rates, offering deeper insights into subgroup biases. We assess the performance of the leading skin cancer detection algorithm of the ISIC 2020 Challenge on the ISIC 2020 Challenge dataset and the PROVE-AI dataset, and compare it with the second- and third-place models, focusing on subgroups defined by sex, race (Fitzpatrick Skin Tone), and age. Our findings reveal that while existing models enhance discriminative accuracy, they often over-diagnose risk and exhibit calibration issues when applied to new datasets. This study underscores the necessity for comprehensive model auditing strategies and extensive metadata collection to achieve equitable AI-driven healthcare solutions.",
      "citationCount": 0,
      "doi": "10.59275/j.melba.2025-ae66",
      "arxivId": "2511.07700",
      "url": "https://www.semanticscholar.org/paper/21ec95cf3fd6621ecb1c6343131b1684e74d856c",
      "venue": "Machine Learning for Biomedical Imaging",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.07700"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e6000cfae9fb67b9327e874d8701625dbc8275d7",
      "title": "Alternative Fairness and Accuracy Optimization in Criminal Justice",
      "authors": [
        {
          "name": "Shaolong Wu",
          "authorId": "2391368669"
        },
        {
          "name": "James Blume",
          "authorId": "2390935998"
        },
        {
          "name": "Geshi Yeung",
          "authorId": "2390934900"
        }
      ],
      "year": 2025,
      "abstract": "Algorithmic fairness has grown rapidly as a research area, yet key concepts remain unsettled, especially in criminal justice. We review group, individual, and process fairness and map the conditions under which they conflict. We then develop a simple modification to standard group fairness. Rather than exact parity across protected groups, we minimize a weighted error loss while keeping differences in false negative rates within a small tolerance. This makes solutions easier to find, can raise predictive accuracy, and surfaces the ethical choice of error costs. We situate this proposal within three classes of critique: biased and incomplete data, latent affirmative action, and the explosion of subgroup constraints. Finally, we offer a practical framework for deployment in public decision systems built on three pillars: need-based decisions, Transparency and accountability, and narrowly tailored definitions and solutions. Together, these elements link technical design to legitimacy and provide actionable guidance for agencies that use risk assessment and related tools.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2511.04505",
      "arxivId": "2511.04505",
      "url": "https://www.semanticscholar.org/paper/e6000cfae9fb67b9327e874d8701625dbc8275d7",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.04505"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "0de033fd2c3190d1c7aeae12055799d900780178",
      "title": "Assessing Algorithm Fairness Requires Adjustment for Risk Distribution Differences: Re-Considering the Equal Opportunity Criterion",
      "authors": [
        {
          "name": "Sarah E. Hegarty",
          "authorId": "2347702363"
        },
        {
          "name": "Kristin A Linn",
          "authorId": "2333555995"
        },
        {
          "name": "Hong Zhang",
          "authorId": "2348212954"
        },
        {
          "name": "Stephanie Teeple",
          "authorId": "2347693767"
        },
        {
          "name": "Paul S. Albert",
          "authorId": "2347701653"
        },
        {
          "name": "Ravi B. Parikh",
          "authorId": "2333553714"
        },
        {
          "name": "Katherine Courtright",
          "authorId": "2347702344"
        },
        {
          "name": "David M. Kent",
          "authorId": "2347698295"
        },
        {
          "name": "Jinbo Chen",
          "authorId": "2340351894"
        }
      ],
      "year": 2025,
      "abstract": "The proliferation of algorithm-assisted decision making has prompted calls for careful assessment of algorithm fairness. One popular fairness metric, equal opportunity, demands parity in true positive rates (TPRs) across different population subgroups. However, we highlight a critical but overlooked weakness in this measure: at a given decision threshold, TPRs vary when the underlying risk distribution varies across subgroups, even if the model equally captures the underlying risks. Failure to account for variations in risk distributions may lead to misleading conclusions on performance disparity. To address this issue, we introduce a novel metric called adjusted TPR (aTPR), which modifies subgroup-specific TPRs to reflect performance relative to the risk distribution in a common reference subgroup. Evaluating fairness using aTPRs promotes equal treatment for equal risk by reflecting whether individuals with similar underlying risks have similar opportunities of being identified as high risk by the model, regardless of subgroup membership. We demonstrate our method through numerical experiments that explore a range of differential calibration relationships and in a real-world data set that predicts 6-month mortality risk in an in-patient sample in order to increase timely referrals for palliative care consultations.",
      "citationCount": 0,
      "doi": "10.1101/2025.01.31.25321489",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/0de033fd2c3190d1c7aeae12055799d900780178",
      "venue": "medRxiv",
      "journal": {
        "name": "medRxiv"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "75775954b7325ce7e2bad921bb5b56ff189a6749",
      "title": "Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising",
      "authors": [
        {
          "name": "Tomasz Szanda\u0142a",
          "authorId": "2310131633"
        },
        {
          "name": "Fatima Ezzeddine",
          "authorId": "2120285260"
        },
        {
          "name": "Natalia Rusin",
          "authorId": "2310125520"
        },
        {
          "name": "Silvia Giordano",
          "authorId": "2162250937"
        },
        {
          "name": "Omran Ayoub",
          "authorId": "2372691876"
        }
      ],
      "year": 2025,
      "abstract": "Artificial Intelligence-generated content has become increasingly popular, yet its malicious use, particularly the deepfakes, poses a serious threat to public trust and discourse. While deepfake detection methods achieve high predictive performance, they often exhibit biases across demographic attributes such as ethnicity and gender. In this work, we tackle the challenge of fair deepfake detection, aiming to mitigate these biases while maintaining robust detection capabilities. To this end, we propose a novel post-processing approach, referred to as Fairness-Oriented Final Layer Input Prioritising (Fair-FLIP), that reweights a trained model's final-layer inputs to reduce subgroup disparities, prioritising those with low variability while demoting highly variable ones. Experimental results comparing Fair-FLIP to both the baseline (without fairness-oriented de-biasing) and state-of-the-art approaches show that Fair-FLIP can enhance fairness metrics by up to 30 % while maintaining baseline accuracy, with only a negligible reduction of 0.25 %. Code is available on Github: https://github.com/szandala/fair-deepfake-detection-toolbox",
      "citationCount": 0,
      "doi": "10.1109/SDS66131.2025.00021",
      "arxivId": "2507.08912",
      "url": "https://www.semanticscholar.org/paper/75775954b7325ce7e2bad921bb5b56ff189a6749",
      "venue": "Swiss Conference on Data Science",
      "journal": {
        "name": "2025 IEEE Swiss Conference on Data Science (SDS)",
        "pages": "103-110"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "16bfc612397bfa553168987bd25ec935736853e4",
      "title": "Exploring Multi-Task Learning for Fairness in Machine Learning Regression",
      "authors": [
        {
          "name": "Bruno Pires",
          "authorId": "2396884963"
        },
        {
          "name": "Luiz Leduino",
          "authorId": "2397099568"
        },
        {
          "name": "Lilian Berton",
          "authorId": "2237897967"
        }
      ],
      "year": 2025,
      "abstract": "Ensuring fairness in machine learning is a critical concern for high-stakes domains, yet most fairness-aware Multi-Task Learning (MTL) frameworks overlook regression problems in favor of classification. This work extends these techniques to regression, proposing a novel MTL framework that optimizes for equitable continuous outcomes across demographic subgroups. Our method dynamically reweights task-specific gradients during training to reduce disparities without compromising predictive accuracy. Evaluated on two real-world datasets, our approach, in the best scenarios, reduces subgroup disparity by up to 94.9% while also improving overall regression performance by up to 32.6%. These findings highlight the significant potential of fairness-aware MTL for creating more inclusive and responsible machine learning applications in sensitive domains.",
      "citationCount": 0,
      "doi": "10.5753/eniac.2025.12440",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/16bfc612397bfa553168987bd25ec935736853e4",
      "venue": "Anais do XXII Encontro Nacional de Intelig\u00eancia Artificial e Computacional (ENIAC 2025)",
      "journal": {
        "name": "Anais do XXII Encontro Nacional de Intelig\u00eancia Artificial e Computacional (ENIAC 2025)"
      },
      "publicationTypes": null
    },
    {
      "paperId": "53d017329deee45a02af420abbd6dd9517e12025",
      "title": "Evaluating algorithmic fairness of machine learning models in predicting underweight, overweight, and adiposity across socioeconomic and caste groups in India: evidence from the longitudinal ageing study in India",
      "authors": [
        {
          "name": "John Tayu Lee",
          "authorId": "2370600868"
        },
        {
          "name": "Sheng Hui Hsu",
          "authorId": "2370559366"
        },
        {
          "name": "Vincent Cheng-Sheng Li",
          "authorId": "2374024587"
        },
        {
          "name": "Kanya Anindya",
          "authorId": "1753109320"
        },
        {
          "name": "Meng-Huan Chen",
          "authorId": "2370777819"
        },
        {
          "name": "Charlotte Wang",
          "authorId": "2370600292"
        },
        {
          "name": "Toby Kai-Bo Shen",
          "authorId": "2374030857"
        },
        {
          "name": "Valerie Tzu Ning Liu",
          "authorId": "2394582095"
        },
        {
          "name": "Hsiao-Hui Chen",
          "authorId": "2370857127"
        },
        {
          "name": "R. Atun",
          "authorId": "40144701"
        }
      ],
      "year": 2025,
      "abstract": "Machine learning (ML) models are increasingly applied to predict body mass index (BMI) and related outcomes, yet their fairness across socioeconomic and caste groups remains uncertain, particularly in contexts of structural inequality. Using nationally representative data from more than 55,000 adults aged 45 years and older in the Longitudinal Ageing Study in India (LASI), we evaluated the accuracy and fairness of multiple ML algorithms\u2014including Random Forest, XGBoost, Gradient Boosting, LightGBM, Deep Neural Networks, and Deep Cross Networks\u2014alongside logistic regression for predicting underweight, overweight, and central adiposity. Models were trained on 80% of the data and tested on 20%, with performance assessed using AUROC, accuracy, sensitivity, specificity, and precision. Fairness was evaluated through subgroup analyses across socioeconomic and caste groups and equity-based metrics such as Equalized Odds and Demographic Parity. Feature importance was examined using SHAP values, and bias-mitigation methods were implemented at pre-processing, in-processing, and post-processing stages. Tree-based models, particularly LightGBM and Gradient Boosting, achieved the highest AUROC values (0.79\u20130.84). Incorporating socioeconomic and health-related variables improved prediction, but fairness gaps persisted: performance declined for scheduled tribes and lower socioeconomic groups. SHAP analyses identified grip strength, gender, and residence as key drivers of prediction differences. Among mitigation strategies, Reject Option Classification and Equalized Odds Post-processing moderately reduced subgroup disparities but sometimes decreased overall performance, whereas other approaches yielded minimal gains. ML models can effectively predict obesity and adiposity risk in India, but addressing bias is essential for equitable application. Continued refinement of fairness-aware ML methods is needed to support inclusive and effective public-health decision-making.",
      "citationCount": 0,
      "doi": "10.1371/journal.pdig.0000951",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/53d017329deee45a02af420abbd6dd9517e12025",
      "venue": "PLOS Digital Health",
      "journal": {
        "name": "PLOS Digital Health",
        "volume": "4"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b599198cb6aca5c90eaec7ca02ec5ca6b16a712e",
      "title": "Mapping the Tradeoffs and Limitations of Algorithmic Fairness",
      "authors": [
        {
          "name": "Etam Benger",
          "authorId": "6585023"
        },
        {
          "name": "Katrina Ligett",
          "authorId": "2265980745"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.4230/LIPIcs.FORC.2025.19",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/b599198cb6aca5c90eaec7ca02ec5ca6b16a712e",
      "venue": "Symposium on Foundations of Responsible Computing",
      "journal": {
        "pages": "19:1-19:20"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0a49c157f2565a95187108b07ad2d8231bd77e86",
      "title": "A Feminist Account of Intersectional Algorithmic Fairness",
      "authors": [
        {
          "name": "Marie Mirsch",
          "authorId": "2377073672"
        },
        {
          "name": "Laila Wegner",
          "authorId": "2331068415"
        },
        {
          "name": "Jonas Strube",
          "authorId": "2376541652"
        },
        {
          "name": "C. Leicht-Scholten",
          "authorId": "2347733474"
        }
      ],
      "year": 2025,
      "abstract": "Intersectionality has profoundly influenced research and political action by revealing how interconnected systems of privilege and oppression influence lived experiences, yet its integration into algorithmic fairness research remains limited. Existing approaches often rely on single-axis or formal subgroup frameworks that risk oversimplifying social realities and neglecting structural inequalities. We propose Substantive Intersectional Algorithmic Fairness, extending Green's (2022) notion of substantive algorithmic fairness with insights from intersectional feminist theory. Building on this foundation, we introduce ten desiderata within the ROOF methodology to guide the design, assessment, and deployment of algorithmic systems in ways that address systemic inequities while mitigating harms to intersectionally marginalized communities. Rather than prescribing fixed operationalizations, these desiderata encourage reflection on assumptions of neutrality, the use of protected attributes, the inclusion of multiply marginalized groups, and enhancing algorithmic systems'potential. Our approach emphasizes that fairness cannot be separated from social context, and that in some cases, principled non-deployment may be necessary. By bridging computational and social science perspectives, we provide actionable guidance for more equitable, inclusive, and context-sensitive intersectional algorithmic practices.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.17944",
      "arxivId": "2508.17944",
      "url": "https://www.semanticscholar.org/paper/0a49c157f2565a95187108b07ad2d8231bd77e86",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.17944"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2d01951524540c11069d0cf15ee9c0e6ffdd05e3",
      "title": "Refocusing Algorithmic Fairness on Feature-Level Bias: A Diagnostic Approach Using Dutch EHR Data",
      "authors": [
        {
          "name": "C. S. Parsons",
          "authorId": "2391757648"
        },
        {
          "name": "S.-A. M. Girwar",
          "authorId": "2391753626"
        },
        {
          "name": "S. Azimi",
          "authorId": "2391753643"
        },
        {
          "name": "M. R. Spruit",
          "authorId": "2391754367"
        },
        {
          "name": "M. R. Haas",
          "authorId": "2391754032"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1101/2025.11.09.25339863",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/2d01951524540c11069d0cf15ee9c0e6ffdd05e3",
      "venue": "medRxiv",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "34351116622302b657f72320b4839166a26a63fe",
      "title": "Federated Multi-Omics Transformers: Subgroup-Fair Precision Medicine Across Borders",
      "authors": [
        {
          "name": "Murali Krishna Pasupuleti",
          "authorId": "2295364655"
        }
      ],
      "year": 2025,
      "abstract": "Abstract:\nWe present Federated Multi-Omics Transformers (FMOT), a framework for cross-border precision medicine that fuses genomics, transcriptomics, proteomics, and imaging under privacy-preserving federated learning. FMOT introduces a subgroup-fairness head with equalized-odds regularization and auditable training. Synthetic multi-country experiments show monotonic AUROC gains, shrinking EO gaps, and manageable latency across rounds. We provide a mathematical formulation, trade-off analysis, and a governance-ready pipeline.\nKeywords: Federated learning, multi-omics, transformer, subgroup fairness, equalized odds, cross-border compliance.",
      "citationCount": 0,
      "doi": "10.62311/nesx/rp-30102025-31-42",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/34351116622302b657f72320b4839166a26a63fe",
      "venue": "International Journal of Academic and Industrial Research Innovations(IJAIRI)",
      "journal": {
        "name": "International Journal of Academic and Industrial Research Innovations(IJAIRI)"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 50,
  "errors": []
}
