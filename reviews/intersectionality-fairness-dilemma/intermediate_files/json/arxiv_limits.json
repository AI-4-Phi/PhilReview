{
  "status": "success",
  "source": "arxiv",
  "query": "all:fairness formalization limits AND cat:cs.CY",
  "results": [
    {
      "arxiv_id": "2601.05240",
      "title": "Robust Reasoning as a Symmetry-Protected Topological Phase",
      "authors": [
        "Ilmo Sung"
      ],
      "abstract": "Large language models suffer from \"hallucinations\"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a \"Metric Phase,\" where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic \"mass gap,\" maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \\times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\\times$ beyond training ($L=50 \\to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI",
        "hep-th"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.05240v1",
      "url": "https://arxiv.org/abs/2601.05240"
    },
    {
      "arxiv_id": "2601.05228",
      "title": "A Geometric Definition of the Integral and Applications",
      "authors": [
        "Joshua Lackman"
      ],
      "abstract": "The standard definition of integration of differential forms is based on local coordinates and partitions of unity. This definition is mostly a formality and not used used in explicit computations or approximation schemes. We present a definition of the integral that uses triangulations instead. Our definition is a coordinate-free version of the standard definition of the Riemann integral on $\\mathbb{R}^n$ and we argue that it is the natural definition in the contexts of Lie algebroids, stochastic integration and quantum field theory, where path integrals are defined using lattices. In particular, our definition naturally incorporates the different stochastic integrals, which involve integration over H\u00f6lder continuous paths. Furthermore, our definition is well-adapted to establishing integral identities from their combinatorial counterparts. Our construction is based on the observation that, in great generality, the things that are integrated are determined by cochains on the pair groupoid. Abstractly, our definition uses the van Est map to lift a differential form to the pair groupoid. Our construction suggests a generalization of the fundamental theorem of calculus which we prove: the singular cohomology and de Rham cohomology cap products of a cocycle with the fundamental class are equal.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "math.DG",
      "categories": [
        "math.DG",
        "math-ph",
        "math.PR",
        "math.SG"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.05228v1",
      "url": "https://arxiv.org/abs/2601.05228"
    },
    {
      "arxiv_id": "2601.05226",
      "title": "Fast convergence of Majorana Propagation for weakly interacting fermions",
      "authors": [
        "Giorgio Facelli",
        "Hamza Fawzi",
        "Omar Fawzi"
      ],
      "abstract": "Simulating the time dynamics of an observable under Hamiltonian evolution is one of the most promising candidates for quantum advantage as we do not expect efficient classical algorithms for this problem except in restricted settings. Here, we introduce such a setting by showing that Majorana Propagation, a simple algorithm combining Trotter steps and truncations, efficiently finds a low-degree approximation of the time-evolved observable as soon as such an approximation exists. This provides the first provable guarantee about Majorana Propagation for Hamiltonian evolution. As an application of this result, we prove that Majorana Propagation can efficiently simulate the time dynamics of any sparse quartic Hamiltonian up to time $t_{\\text{max}}(u)$ depending on the interaction strength $u$. For a time horizon $t \\leq t_{\\text{max}}(u)$, the runtime of the algorithm is $N^{O(\\log(t/\\varepsilon))}$ where $N$ is the number of Majorana modes and $\\varepsilon$ is the error measured in the normalized Frobenius norm. Importantly, in the limit of small $u$, $t_{\\text{max}}(u)$ goes to $+\\infty$, formalizing the intuition that the algorithm is accurate at all times when the Hamiltonian is quadratic.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.05226v1",
      "url": "https://arxiv.org/abs/2601.05226"
    },
    {
      "arxiv_id": "2601.05217",
      "title": "A complete characterization of testable hypotheses",
      "authors": [
        "Martin Larsson",
        "Johannes Ruf",
        "Aaditya Ramdas"
      ],
      "abstract": "We revisit a fundamental question in hypothesis testing: given two sets of probability measures $\\mathcal{P}$ and $\\mathcal{Q}$, when does a nontrivial (i.e.\\ strictly unbiased) test for $\\mathcal{P}$ against $\\mathcal{Q}$ exist? Le~Cam showed that, when $\\mathcal{P}$ and $\\mathcal{Q}$ have a common dominating measure, a test that has power exceeding its level by more than $\\varepsilon$ exists if and only if the convex hulls of $\\mathcal{P}$ and $\\mathcal{Q}$ are separated in total variation distance by more than $\\varepsilon$. The requirement of a dominating measure is frequently violated in nonparametric statistics. In a passing remark, Le~Cam described an approach to address more general scenarios, but he stopped short of stating a formal theorem. This work completes Le~Cam's program, by presenting a matching necessary and sufficient condition for testability: for the aforementioned theorem to hold without assumptions, one must take the closures of the convex hulls of $\\mathcal{P}$ and $\\mathcal{Q}$ in the space of bounded finitely additive measures. We provide simple elucidating examples, and elaborate on various subtle measure theoretic and topological points regarding compactness and achievability.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "cs.IT",
        "math.PR"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.05217v1",
      "url": "https://arxiv.org/abs/2601.05217"
    },
    {
      "arxiv_id": "2601.05212",
      "title": "FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching",
      "authors": [
        "Danilo Danese",
        "Angela Lombardi",
        "Matteo Attimonelli",
        "Giuseppe Fasano",
        "Tommaso Di Noia"
      ],
      "abstract": "Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.05212v1",
      "url": "https://arxiv.org/abs/2601.05212"
    },
    {
      "arxiv_id": "2601.05173",
      "title": "Information-Theoretic Limits on Exact Subgraph Alignment Problem",
      "authors": [
        "Chun Hei Michael Shiu",
        "Hei Victor Cheng",
        "Lele Wang"
      ],
      "abstract": "The graph alignment problem aims to identify the vertex correspondence between two correlated graphs. Most existing studies focus on the scenario in which the two graphs share the same vertex set. However, in many real-world applications, such as computer vision, social network analysis, and bioinformatics, the task often involves locating a small graph pattern within a larger graph. Existing graph alignment algorithms and analysis cannot directly address these scenarios because they are not designed to identify the specific subset of vertices where the small graph pattern resides within the larger graph. Motivated by this limitation, we introduce the subgraph alignment problem, which seeks to recover both the vertex set and/or the vertex correspondence of a small graph pattern embedded in a larger graph. In the special case where the small graph pattern is an induced subgraph of the larger graph and both the vertex set and correspondence are to be recovered, the problem reduces to the subgraph isomorphism problem, which is NP-complete in the worst case. In this paper, we formally formulate the subgraph alignment problem by proposing the Erdos-Renyi subgraph pair model together with some appropriate recovery criterion. We then establish almost-tight information-theoretic results for the subgraph alignment problem and present some novel approaches for the analysis.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.05173v1",
      "url": "https://arxiv.org/abs/2601.05173"
    },
    {
      "arxiv_id": "2601.05157",
      "title": "Learning Mixture Models via Efficient High-dimensional Sparse Fourier Transforms",
      "authors": [
        "Alkis Kalavasis",
        "Pravesh K. Kothari",
        "Shuchen Li",
        "Manolis Zampetakis"
      ],
      "abstract": "In this work, we give a ${\\rm poly}(d,k)$ time and sample algorithm for efficiently learning the parameters of a mixture of $k$ spherical distributions in $d$ dimensions. Unlike all previous methods, our techniques apply to heavy-tailed distributions and include examples that do not even have finite covariances. Our method succeeds whenever the cluster distributions have a characteristic function with sufficiently heavy tails. Such distributions include the Laplace distribution but crucially exclude Gaussians.   All previous methods for learning mixture models relied implicitly or explicitly on the low-degree moments. Even for the case of Laplace distributions, we prove that any such algorithm must use super-polynomially many samples. Our method thus adds to the short list of techniques that bypass the limitations of the method of moments.   Somewhat surprisingly, our algorithm does not require any minimum separation between the cluster means. This is in stark contrast to spherical Gaussian mixtures where a minimum $\\ell_2$-separation is provably necessary even information-theoretically [Regev and Vijayaraghavan '17]. Our methods compose well with existing techniques and allow obtaining ''best of both worlds\" guarantees for mixtures where every component either has a heavy-tailed characteristic function or has a sub-Gaussian tail with a light-tailed characteristic function.   Our algorithm is based on a new approach to learning mixture models via efficient high-dimensional sparse Fourier transforms. We believe that this method will find more applications to statistical estimation. As an example, we give an algorithm for consistent robust mean estimation against noise-oblivious adversaries, a model practically motivated by the literature on multiple hypothesis testing. It was formally proposed in a recent Master's thesis by one of the authors, and has already inspired follow-up works.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS",
        "cs.LG",
        "stat.ML"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.05157v1",
      "url": "https://arxiv.org/abs/2601.05157"
    },
    {
      "arxiv_id": "2601.05131",
      "title": "Simulation of noisy quantum circuits using frame representations",
      "authors": [
        "Janek Denzler",
        "Jose Carrasco",
        "Jens Eisert",
        "Tommaso Guaita"
      ],
      "abstract": "One of the core research questions in the theory of quantum computing is to find out to what precise extent the classical simulation of a noisy quantum circuits is possible and where potential quantum advantages can set in. In this work, we introduce a unified framework for the classical simulation of quantum circuits based on frame theory, encompassing and generalizing a broad class of existing simulation strategies. Within this framework, the computational cost of a simulation algorithm is determined by the one-norm of an associated quasi-probability distribution, providing a common quantitative measure across different simulation approaches. This enables a comprehensive perspective on common methods for the simulation of noisy circuits based on different quantum resources, such as entanglement or non-stabilizerness. It further provides a clear scheme for generating novel classical simulation algorithms. Indeed, by exploring different choices of frames within this formalism and resorting to tools of convex optimization, we are able not only to obtain new insights and improved bounds for existing methods -- such as stabilizer state simulation or Pauli back-propagation -- but also to discover a new approach with an improved performance based on a generalization of the Pauli frame. We, thereby, show that classical simulation techniques can directly benefit from a perspective -- that of frames -- that goes beyond the traditional classification of quantum resources.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.05131v1",
      "url": "https://arxiv.org/abs/2601.05131"
    },
    {
      "arxiv_id": "2601.05075",
      "title": "SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment",
      "authors": [
        "Ziyang Chen",
        "Zhenxuan Huang",
        "Yile Wang",
        "Weiqin Wang",
        "Lu Yin",
        "Hui Huang"
      ],
      "abstract": "Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.05075v1",
      "url": "https://arxiv.org/abs/2601.05075"
    },
    {
      "arxiv_id": "2601.05073",
      "title": "Milestones over Outcome: Unlocking Geometric Reasoning with Sub-Goal Verifiable Reward",
      "authors": [
        "Jianlong Chen",
        "Daocheng Fu",
        "Shengze Xu",
        "Jiawei Chen",
        "Yuan Feng",
        "Yue Yang",
        "Junchi Yan",
        "Hongyuan Zha",
        "Renqiu Xia"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) struggle with complex geometric reasoning, largely because \"black box\" outcome-based supervision fails to distinguish between lucky guesses and rigorous deduction. To address this, we introduce a paradigm shift towards subgoal-level evaluation and learning. We first construct GeoGoal, a benchmark synthesized via a rigorous formal verification data engine, which converts abstract proofs into verifiable numeric subgoals. This structure reveals a critical divergence between reasoning quality and outcome accuracy. Leveraging this, we propose the Sub-Goal Verifiable Reward (SGVR) framework, which replaces sparse signals with dense rewards based on the Skeleton Rate. Experiments demonstrate that SGVR not only enhances geometric performance (+9.7%) but also exhibits strong generalization, transferring gains to general math (+8.0%) and other general reasoning tasks (+2.8%), demonstrating broad applicability across diverse domains.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.05073v1",
      "url": "https://arxiv.org/abs/2601.05073"
    },
    {
      "arxiv_id": "2601.05064",
      "title": "Electrically Switchable Flat Band in Two-Dimensional Electron Gases under Nonuniform Magnetic Fields",
      "authors": [
        "You-Ting Huang",
        "Chao-Cheng Kaun",
        "Ching-Hao Chang"
      ],
      "abstract": "Flat bands are associated with a range of desirable physical phenomena and potential applications, including enhanced superconducting tendencies due to the high density of states, strongly correlated phases such as quantum Hall states. Systems in which flat bands can be switched or tuned are therefore of particular interest. In this study, we analyze the electronic structure of two-dimensional electron gases (2DEGs) subjected to a linearly increasing magnetic-field dipole together with a transverse electric field, using the operator formalism of the quantum harmonic oscillator. When the electric field magnitude is tuned to a sequence of discrete values, different levels of energy bands are flattened. Moreover, at a specific electric field strength, the ground-state wave function admits an exact closed-form solution that can be understood through the magnetic drifts cancellation in the classical electrodynamics. We also demonstrate two distinct transmission properties, the quantized Hall conductance and the enhanced density of states, of the electrically switchable flat band. These findings establish a new route toward magnetoelectric band engineering and electrically guided transport in low-dimensional systems.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.05064v1",
      "url": "https://arxiv.org/abs/2601.05064"
    },
    {
      "arxiv_id": "2601.05051",
      "title": "Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence",
      "authors": [
        "Jennifer D'Souza",
        "Soren Auer",
        "Eleni Poupaki",
        "Alex Watkins",
        "Anjana Devi",
        "Riikka L. Puurunen",
        "Bora Karasulu",
        "Adrie Mackus",
        "Erwin Kessels"
      ],
      "abstract": "Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.DL",
        "cs.IT"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.05051v1",
      "url": "https://arxiv.org/abs/2601.05051"
    },
    {
      "arxiv_id": "2601.05049",
      "title": "How to Set the Learning Rate for Large-Scale Pre-training?",
      "authors": [
        "Yunhua Zhou",
        "Shuhao Xing",
        "Junhao Huang",
        "Xipeng Qiu",
        "Qipeng Guo"
      ],
      "abstract": "Optimal configuration of the learning rate (LR) is a fundamental yet formidable challenge in large-scale pre-training. Given the stringent trade-off between training costs and model performance, the pivotal question is whether the optimal LR can be accurately extrapolated from low-cost experiments. In this paper, we formalize this investigation into two distinct research paradigms: Fitting and Transfer. Within the Fitting Paradigm, we innovatively introduce a Scaling Law for search factor, effectively reducing the search complexity from O(n^3) to O(n*C_D*C_\u03b7) via predictive modeling. Within the Transfer Paradigm, we extend the principles of $\u03bc$Transfer to the Mixture of Experts (MoE) architecture, broadening its applicability to encompass model depth, weight decay, and token horizons. By pushing the boundaries of existing hyperparameter research in terms of scale, we conduct a comprehensive comparison between these two paradigms. Our empirical results challenge the scalability of the widely adopted $\u03bc$ Transfer in large-scale pre-training scenarios. Furthermore, we provide a rigorous analysis through the dual lenses of training stability and feature learning to elucidate the underlying reasons why module-wise parameter tuning underperforms in large-scale settings. This work offers systematic practical guidelines and a fresh theoretical perspective for optimizing industrial-level pre-training.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.05049v1",
      "url": "https://arxiv.org/abs/2601.05049"
    },
    {
      "arxiv_id": "2601.04977",
      "title": "On the Definition and Detection of Cherry-Picking in Counterfactual Explanations",
      "authors": [
        "James Hinns",
        "Sofie Goethals",
        "Stephan Van der Veeken",
        "Theodoros Evgeniou",
        "David Martens"
      ],
      "abstract": "Counterfactual explanations are widely used to communicate how inputs must change for a model to alter its prediction. For a single instance, many valid counterfactuals can exist, which leaves open the possibility for an explanation provider to cherry-pick explanations that better suit a narrative of their choice, highlighting favourable behaviour and withholding examples that reveal problematic behaviour. We formally define cherry-picking for counterfactual explanations in terms of an admissible explanation space, specified by the generation procedure, and a utility function. We then study to what extent an external auditor can detect such manipulation. Considering three levels of access to the explanation process: full procedural access, partial procedural access, and explanation-only access, we show that detection is extremely limited in practice. Even with full procedural access, cherry-picked explanations can remain difficult to distinguish from non cherry-picked explanations, because the multiplicity of valid counterfactuals and flexibility in the explanation specification provide sufficient degrees of freedom to mask deliberate selection. Empirically, we demonstrate that this variability often exceeds the effect of cherry-picking on standard counterfactual quality metrics such as proximity, plausibility, and sparsity, making cherry-picked explanations statistically indistinguishable from baseline explanations. We argue that safeguards should therefore prioritise reproducibility, standardisation, and procedural constraints over post-hoc detection, and we provide recommendations for algorithm developers, explanation providers, and auditors.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.04977v1",
      "url": "https://arxiv.org/abs/2601.04977"
    },
    {
      "arxiv_id": "2601.04917",
      "title": "Homeostasis Under Technological Transition: How High-Friction Universities Adapt Through Early Filtering Rather Than Reconfiguration",
      "authors": [
        "H. R. Paz"
      ],
      "abstract": "Universities are widely expected to respond to technological transitions through rapid reconfiguration of programme demand and curricular supply. Using four decades of longitudinal administrative cohorts (1980-2019) from a large public university, we examine whether technological change is translated into observable shifts in programme hierarchy, or instead absorbed by institutional mechanisms that preserve structural stability. We show that programme rankings by entrant volume remain remarkably stable over time, while the translation of technological transitions into enrolment composition occurs with substantial delay. Short-run adjustment appears primarily in early persistence dynamics: attrition reacts sooner than choice, and \"growth\" in entrants can coexist with declining early survival - producing false winners in which expansion is decoupled from persistence. Macroeconomic volatility amplifies attrition and compresses between-programme differences, masking technological signals that would otherwise be interpreted as preference shifts. To explain why stability dominates responsiveness, we situate these patterns within nationally regulated constraints governing engineering education - minimum total hours and mandated practice intensity - which materially limit the speed of curricular adaptation (Ministerio de Educacion, 2021; Ley de Educacion Superior, 1995). National system metrics further support the plausibility of a high-friction equilibrium in which large inflows coexist with standardised outputs (Secretaria de Politicas Universitarias [SPU], 2022). These findings suggest that apparent rigidity is not an anomaly but the predictable outcome of a system optimised for stability over responsiveness.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "physics.soc-ph",
      "categories": [
        "physics.soc-ph",
        "cs.CY"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.04917v1",
      "url": "https://arxiv.org/abs/2601.04917"
    },
    {
      "arxiv_id": "2601.04904",
      "title": "Parallel Quadratic Selected Inversion in Quantum Transport Simulation",
      "authors": [
        "Vincent Maillou",
        "Matthias Bollhofer",
        "Olaf Schenk",
        "Alexandros Nikolaos Ziogas",
        "Mathieu Luisier"
      ],
      "abstract": "Driven by Moore's Law, the dimensions of transistors have been pushed down to the nanometer scale. Advanced quantum transport (QT) solvers are required to accurately simulate such nano-devices. The non-equilibrium Green's function (NEGF) formalism lends itself optimally to these tasks, but it is computationally very intensive, involving the selected inversion (SI) of matrices and the selected solution of quadratic matrix (SQ) equations. Existing algorithms to tackle these numerical problems are ideally suited to GPU acceleration, e.g., the so-called recursive Green's function (RGF) technique, but they are typically sequential, require block-tridiagonal (BT) matrices as inputs, and their implementation has been so far restricted to shared memory parallelism, thus limiting the achievable device sizes. To address these shortcomings, we introduce distributed methods that build on RGF and enable parallel selected inversion and selected solution of the quadratic matrix equation. We further extend them to handle BT matrices with arrowhead, which allows for the investigation of multi-terminal transistor structures. We evaluate the performance of our approach on a real dataset from the QT simulation of a nano-ribbon transistor and compare it with the sparse direct package PARDISO. When scaling to 16 GPUs, our fused SI and SQ solver is 5.2x faster than the SI module of PARDISO applied to a device 16x shorter. These results highlight the potential of our method to accelerate NEGF-based nano-device simulations.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.PF"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.04904v1",
      "url": "https://arxiv.org/abs/2601.04904"
    },
    {
      "arxiv_id": "2601.04895",
      "title": "DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation",
      "authors": [
        "Renzhao Liang",
        "Jingru Chen",
        "Bo Jia",
        "Bo Deng",
        "Chenggang Xie",
        "Yidong Wang",
        "Ke Jin",
        "Xin Wang",
        "Linfeng Zhang",
        "Cunxiang Wang"
      ],
      "abstract": "Evaluating large language models (LLMs) is increasingly confounded by \\emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \\textbf{DVD} (\\textbf{D}etection via \\textbf{V}ariance of generation \\textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \\emph{memory-adherence} state and a \\emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \\textbf{DVD} consistently outperforms perplexity-based, Min-$k$\\%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.04895v1",
      "url": "https://arxiv.org/abs/2601.04895"
    },
    {
      "arxiv_id": "2601.04887",
      "title": "Flexible Manufacturing Systems Intralogistics: Dynamic Optimization of AGVs and Tool Sharing Using Coloured-Timed Petri Nets and Actor-Critic RL with Actions Masking",
      "authors": [
        "Sofiene Lassoued",
        "Laxmikant Shrikant Bahetic",
        "Nathalie Wei\u00df-Borkowskib",
        "Stefan Lierc",
        "Andreas Schwunga"
      ],
      "abstract": "Flexible Manufacturing Systems (FMS) are pivotal in optimizing production processes in today's rapidly evolving manufacturing landscape. This paper advances the traditional job shop scheduling problem by incorporating additional complexities through the simultaneous integration of automated guided vehicles (AGVs) and tool-sharing systems. We propose a novel approach that combines Colored-Timed Petri Nets (CTPNs) with actor-critic model-based reinforcement learning (MBRL), effectively addressing the multifaceted challenges associated with FMS. CTPNs provide a formal modeling structure and dynamic action masking, significantly reducing the action search space, while MBRL ensures adaptability to changing environments through the learned policy. Leveraging the advantages of MBRL, we incorporate a lookahead strategy for optimal positioning of AGVs, improving operational efficiency. Our approach was evaluated on small-sized public benchmarks and a newly developed large-scale benchmark inspired by the Taillard benchmark. The results show that our approach matches traditional methods on smaller instances and outperforms them on larger ones in terms of makespan while achieving a tenfold reduction in computation time. To ensure reproducibility, we propose a gym-compatible environment and an instance generator. Additionally, an ablation study evaluates the contribution of each framework component to its overall performance.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "doi": "10.1016/j.jmsy.2025.06.017",
      "journal_ref": "Journal of Manufacturing Systems Journal of Manufacturing Systems Volume 82, October 2025, Pages 405-419",
      "pdf_url": "https://arxiv.org/pdf/2601.04887v1",
      "url": "https://arxiv.org/abs/2601.04887"
    },
    {
      "arxiv_id": "2601.04842",
      "title": "Intelligent resource allocation in wireless networks via deep reinforcement learning",
      "authors": [
        "Marie Diane Iradukunda",
        "Chabi F. El\u00e9gb\u00e9d\u00e9",
        "Ya\u00e9 Ulrich Gaba"
      ],
      "abstract": "This study addresses the challenge of optimal power allocation in stochastic wireless networks by employing a Deep Reinforcement Learning (DRL) framework. Specifically, we design a Deep Q-Network (DQN) agent capable of learning adaptive power control policies directly from channel state observations, effectively bypassing the need for explicit system models. We formulate the resource allocation problem as a Markov Decision Process (MDP) and benchmark the proposed approach against classical heuristics, including fixed allocation, random assignment, and the theoretical water-filling algorithm. Empirical results demonstrate that the DQN agent achieves a system throughput of 3.88 Mbps, effectively matching the upper limit of the water fill, while outperforming the random and fixed allocation strategies by approximately 73% and 27%, respectively. Moreover, the agent exhibits emergent fairness, maintaining a Jain's Index of 0.91, and successfully optimizes the trade-off between spectral efficiency and energy consumption. These findings substantiate the efficacy of model-free DRL as a robust and scalable solution for resource management in next-generation communication systems.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.NI",
      "categories": [
        "cs.NI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.04842v1",
      "url": "https://arxiv.org/abs/2601.04842"
    },
    {
      "arxiv_id": "2601.04814",
      "title": "The Rezk Completion for Elementary Topoi",
      "authors": [
        "Kobe Wullaert",
        "Niels van der Weide"
      ],
      "abstract": "The development of category theory in univalent foundations and the formalization thereof is an active field of research. Categories in that setting are often assumed to be univalent which means that identities and isomorphisms of objects coincide. One consequence hereof is that equivalences and identities coincide for univalent categories and that structure on univalent categories transfers along equivalences. However, constructions such as the Kleisli category, the Karoubi envelope, and the tripos-to-topos construction, do not necessarily give univalent categories. To deal with that problem, one uses the Rezk completion, which completes a category into a univalent one. However, to use the Rezk completion when considering categories with structure, one also needs to show that the Rezk completion inherits the structure from the original category. In this work, we present a modular framework for lifting the Rezk completion from categories to categories with structure. We demonstrate the modularity of our framework by lifting the Rezk completion from categories to elementary topoi in manageable steps.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.LO",
      "categories": [
        "cs.LO",
        "math.CT"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.04814v1",
      "url": "https://arxiv.org/abs/2601.04814"
    },
    {
      "arxiv_id": "2601.04813",
      "title": "Proof of Commitment: A Human-Centric Resource for Permissionless Consensus",
      "authors": [
        "Homayoun Maleki",
        "Nekane Sainz",
        "Jon Legarda"
      ],
      "abstract": "Permissionless consensus protocols require a scarce resource to regulate leader election and provide Sybil resistance. Existing paradigms such as Proof of Work and Proof of Stake instantiate this scarcity through parallelizable resources like computation or capital. Once acquired, these resources can be subdivided across many identities at negligible marginal cost, making linear Sybil cost fundamentally unattainable.   We introduce Proof of Commitment (PoCmt), a consensus primitive grounded in a non-parallelizable resource: real-time human engagement. Validators maintain a commitment state capturing cumulative human effort, protocol participation, and online availability. Engagement is enforced through a Human Challenge Oracle that issues identity-bound, time-sensitive challenges, limiting the number of challenges solvable within each human window.   Under this model, sustaining multiple active identities requires proportional human-time effort. We establish a cost-theoretic separation showing that protocols based on parallelizable resources admit zero marginal Sybil cost, whereas PoCmt enforces a strictly linear cost profile. Using a weighted-backbone analysis, we show that PoCmt achieves safety, liveness, and commitment-proportional fairness under partial synchrony.   Simulations complement the analysis by isolating human-time capacity as the sole adversarial bottleneck and validating the predicted commitment drift and fairness properties. These results position PoCmt as a new point in the consensus design space, grounding permissionless security in sustained human effort rather than computation or capital.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.04813v1",
      "url": "https://arxiv.org/abs/2601.04813"
    },
    {
      "arxiv_id": "2601.04806",
      "title": "Bound state solutions with a linear combination of Yuakawa plus four-parameter diatomic potentials using path integral approach: Thermodynamic properties",
      "authors": [
        "Mohamed Am\u00e9ziane Sadoun",
        "Redouane Zamoum",
        "Abdellah Touati"
      ],
      "abstract": "In this paper, we investigate the approximate analytical bound states with a linear combination of two diatomic molecule potentials, Yukawa and four parameters potentials, within the framework of the path integral formalism. With the help of an appropriate approximation to evaluate the centrifugal term, the energy spectrum and the normalized wave functions of the bound states are derived from the poles of Green's function and its residues. The partition function and other thermodynamic properties were obtained using the compact form of the energy equation.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "math-ph",
        "physics.atom-ph"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.04806v1",
      "url": "https://arxiv.org/abs/2601.04806"
    },
    {
      "arxiv_id": "2601.04775",
      "title": "Towards a Unified Theoretical Framework for Self-Supervised MRI Reconstruction",
      "authors": [
        "Siying Xu",
        "Kerstin Hammernik",
        "Daniel Rueckert",
        "Sergios Gatidis",
        "Thomas K\u00fcstner"
      ],
      "abstract": "The demand for high-resolution, non-invasive imaging continues to drive innovation in magnetic resonance imaging (MRI), yet prolonged acquisition times hinder accessibility and real-time applications. While deep learning-based reconstruction methods have accelerated MRI, their predominant supervised paradigm depends on fully-sampled reference data that are challenging to acquire. Recently, self-supervised learning (SSL) approaches have emerged as promising alternatives, but most are empirically designed and fragmented. Therefore, we introduce UNITS (Unified Theory for Self-supervision), a general framework for self-supervised MRI reconstruction. UNITS unifies prior SSL strategies within a common formalism, enabling consistent interpretation and systematic benchmarking. We prove that SSL can achieve the same expected performance as supervised learning. Under this theoretical guarantee, we introduce sampling stochasticity and flexible data utilization, which improve network generalization under out-of-domain distributions and stabilize training. Together, these contributions establish UNITS as a theoretical foundation and a practical paradigm for interpretable, generalizable, and clinically applicable self-supervised MRI reconstruction.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.04775v1",
      "url": "https://arxiv.org/abs/2601.04775"
    },
    {
      "arxiv_id": "2601.04758",
      "title": "PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks",
      "authors": [
        "Yehoon Jang",
        "Chaewon Lee",
        "Hyun-seok Min",
        "Sungchul Choi"
      ],
      "abstract": "The Patent Trial and Appeal Board (PTAB) of the USPTO adjudicates thousands of ex parte appeals each year, requiring the integration of technical understanding and legal reasoning. While large language models (LLMs) are increasingly applied in patent and legal practice, their use has remained limited to lightweight tasks, with no established means of systematically evaluating their capacity for structured legal reasoning in the patent domain. In this work, we introduce PILOT-Bench, the first PTAB-centric benchmark that aligns PTAB decisions with USPTO patent data at the case-level and formalizes three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. We evaluate a diverse set of closed-source (commercial) and open-source LLMs and conduct analyses across multiple perspectives, including input-variation settings, model families, and error tendencies. Notably, on the Issue Type task, closed-source models consistently exceed 0.75 in Micro-F1 score, whereas the strongest open-source model (Qwen-8B) achieves performance around 0.56, highlighting a substantial gap in reasoning capabilities. PILOT-Bench establishes a foundation for the systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All data, code, and benchmark resources are available at https://github.com/TeamLab/pilot-bench.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.04758v1",
      "url": "https://arxiv.org/abs/2601.04758"
    },
    {
      "arxiv_id": "2601.04739",
      "title": "Generalised Quantifiers Based on Rabin-Mostowski Index",
      "authors": [
        "Denis Kuperberg",
        "Damian Niwi\u0144ski",
        "Pawe\u0142 Parys",
        "Micha\u0142 Skrzypczak"
      ],
      "abstract": "In this work we introduce new generalised quantifiers which allow us to express the Rabin-Mostowski index of automata. Our main results study expressive power and decidability of the monadic second-order (MSO) logic extended with these quantifiers. We study these problems in the realm of both $\u03c9$-words and infinite trees. As it turns out, the pictures in these two cases are very different. In the case of $\u03c9$-words the new quantifiers can be effectively expressed in pure MSO logic. In contrast, in the case of infinite trees, addition of these quantifiers leads to an undecidable formalism.   To realise index-quantifier elimination, we consider the extension of MSO by game quantifiers. As a tool, we provide a specific quantifier-elimination procedure for them. Moreover, we introduce a novel construction of transducers realising strategies in $\u03c9$-regular games with monadic parameters.",
      "published": "2026-01-08",
      "updated": "2026-01-08",
      "primary_category": "cs.LO",
      "categories": [
        "cs.LO",
        "cs.FL"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2601.04739v1",
      "url": "https://arxiv.org/abs/2601.04739"
    }
  ],
  "count": 25,
  "errors": []
}
