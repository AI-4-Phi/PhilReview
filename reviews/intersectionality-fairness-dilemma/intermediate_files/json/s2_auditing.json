{
  "status": "success",
  "source": "semantic_scholar",
  "query": "fairness auditing intersectionality",
  "results": [
    {
      "paperId": "016e0544babf77e31e1f648b36fa7e8860f47600",
      "title": "Reliable fairness auditing with semi-supervised inference",
      "authors": [
        {
          "name": "Jianhui Gao",
          "authorId": "2306244777"
        },
        {
          "name": "Jessica L. Gronsbell",
          "authorId": "14929566"
        }
      ],
      "year": 2025,
      "abstract": "Machine learning (ML) models often exhibit bias that can exacerbate inequities in biomedical applications. Fairness auditing, the process of evaluating a model's performance across subpopulations, is critical for identifying and mitigating these biases. However, such audits typically rely on large volumes of labeled data, which are costly and labor-intensive to obtain. To address this challenge, we introduce $\\textit{Infairness}$, a unified framework for auditing a wide range of fairness criteria using semi-supervised inference. Our approach combines a small labeled dataset with a large unlabeled dataset by imputing missing outcomes via regression with carefully selected nonlinear basis functions. We show that our proposed estimator is (i) consistent regardless of whether the ML or imputation models are correctly specified and (ii) more efficient than standard supervised estimation with the labeled data when the imputation model is correctly specified. Through extensive simulations, we also demonstrate that Infairness consistently achieves higher precision than supervised estimation. In a real-world application of phenotyping depression from electronic health records data, Infairness reduces variance by up to 64% compared to supervised estimation, underscoring its value for reliable fairness auditing with limited labeled data.",
      "citationCount": 1,
      "doi": null,
      "arxivId": "2505.12181",
      "url": "https://www.semanticscholar.org/paper/016e0544babf77e31e1f648b36fa7e8860f47600",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "8901f7d42f98cec9faf195afe5c0a025b374b97c",
      "title": "'I Know You Are Discriminatory!': Automated Substantiating for Individual Fairness Auditing of AI Systems",
      "authors": [
        {
          "name": "Yuanhao Liu",
          "authorId": "2372420720"
        },
        {
          "name": "Qi Cao",
          "authorId": "2057766936"
        },
        {
          "name": "Huawei Shen",
          "authorId": "2256603895"
        },
        {
          "name": "Kaike Zhang",
          "authorId": "1923171"
        },
        {
          "name": "Yunfan Wu",
          "authorId": "2144364879"
        },
        {
          "name": "Xueqi Cheng",
          "authorId": "2110251463"
        }
      ],
      "year": 2025,
      "abstract": "Artificial intelligence (AI) systems are playing an increasingly crucial role in people's lives, and their frequent unfair behaviors raise concerns about fairness. To unveil the unfairness in AI systems, researchers conduct fairness auditing on these systems. However, existing fairness auditing works often focus on group fairness while ignoring discriminatory phenomena among individuals. To unearth discriminatory phenomena against individuals within AI systems, this paper proposes an individual fairness auditing framework, termed ''substantiating'', which can identify discrimination instances within AI systems by constructing individual samples. To construct these samples for substantiating, auditors often have to rely on subjective prior knowledge, lacking guidelines on how to construct unfair samples. To address this issue, this paper introduces two categories of automated sample generation methods that can rapidly find unfair samples within a limited number of queries to the system and demonstrate their effectiveness through experiments. This paper evaluates the proposed auditing framework among three categories of stakeholders in AI fairness: auditors, AI model developers, and non-technical personnel. The research findings point out their demand for individual fairness audits of AI systems and highlight how the framework supports a reliable and convenient individual fairness audit.",
      "citationCount": 0,
      "doi": "10.1145/3757414",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/8901f7d42f98cec9faf195afe5c0a025b374b97c",
      "venue": "Proc. ACM Hum. Comput. Interact.",
      "journal": {
        "name": "Proceedings of the ACM on Human-Computer Interaction",
        "pages": "1 - 38",
        "volume": "9"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e1e7da0bb232715eb95814052508d84e7fc6cbd2",
      "title": "Fairness Auditing of Tabular Machine Learning via AIME: Exposing Dataset Bias with Groupwise \u0394\u2011Importance",
      "authors": [
        {
          "name": "Takafumi Nakanishi",
          "authorId": "2240189730"
        },
        {
          "name": "Ponlawat Chophuk",
          "authorId": "30767016"
        },
        {
          "name": "Krisana Chinnasarn",
          "authorId": "2296275128"
        }
      ],
      "year": 2025,
      "abstract": "This study proposes a practical workflow for fairness auditing in tabular machine learning (Tabular ML) by combining group-wise global importance differences (\u0394-importance) using Approximate Inverse Model Explanations (AIME) with concise fairness metrics (Demographic Parity [DP: selection rate and 4/5 rule] and Equalized Odds [EO: TPR/FPR difference]). Using the Adult (UCI Census Income) dataset, we trained a LightGBM model with sensitive attributes (gender and race) excluded from the training features and evaluated it at a threshold of 0.5. The overall performance was 0.875, the ROC-AUC was 0.929, and the selection rate was 0.200. From a fairness perspective, the gender selection rates were male (0.257) and female (0.084), with a four-fifths ratio of 0.328, and the EO had a TPR difference of 0.072 and an FPR difference of 0.058. In terms of race, the maximum selection rate was 0.238 (Asian-Pacific Islander) and the minimum was 0.073 (Other), with a four-fifths ratio of 0.308 and EO of TPR difference of 0.161/FPR difference of 0.088, indicating a significant deviation between the groups. By calculating the overall and group-specific importance using AIME in a consistent procedure and extracting \u0394-importance, we can specifically identify which features contribute relatively differently across groups, enabling explanation-guided mitigation based on explanations, such as preprocessing, feature design, threshold adjustment, calibration, and constrained learning. The proposed workflow is not specific to AIME and can be replaced or used in combination with other XAI metrics, such as SHAP, Permutation Importance, and SAGE. Limitations include dependence on operational thresholds and group sizes, estimation uncertainty, and the impossibility of reconciling fairness concepts. Nevertheless, this method is useful for visualizing data-driven biases and explicitly revealing their \"breakdown,\" demonstrating its practical applicability as a fairness audit template for Tabular ML.",
      "citationCount": 0,
      "doi": "10.1109/ICSEC67360.2025.11298030",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e1e7da0bb232715eb95814052508d84e7fc6cbd2",
      "venue": "International Computer Science and Engineering Conference",
      "journal": {
        "name": "2025 29th International Computer Science and Engineering Conference (ICSEC)",
        "pages": "77-84"
      },
      "publicationTypes": [
        "Conference"
      ]
    },
    {
      "paperId": "1c5573082f03732383b17ee1b4a5354baad76a33",
      "title": "Integrating Group and Individual Fairness in Clinical AI: A Post-Hoc, Model-Agnostic Framework for Fairness Auditing",
      "authors": [
        {
          "name": "\u2020. JavenXu",
          "authorId": "2379186826"
        },
        {
          "name": "Yeon-Mi Hwang",
          "authorId": "2379929939"
        },
        {
          "name": "In\u00b4es Dormoy",
          "authorId": "2379175425"
        },
        {
          "name": "Serena Liang",
          "authorId": "2379289586"
        },
        {
          "name": "Malvika Pillai",
          "authorId": "2282472554"
        },
        {
          "name": "Catherine M. Curtin",
          "authorId": "2250815427"
        },
        {
          "name": "Tina Hernandez-Boussard",
          "authorId": "2359225046"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1101/2025.09.03.25334999",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/1c5573082f03732383b17ee1b4a5354baad76a33",
      "venue": "medRxiv",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "07129a72f24367ddac018620f3f24452a91aee3a",
      "title": "Fairness Auditing of Limited-Access Models Using Protected Datasets",
      "authors": [
        {
          "name": "Zhaohui Zhu",
          "authorId": "2354707780"
        },
        {
          "name": "Marc A. Kastner",
          "authorId": "31549937"
        },
        {
          "name": "Shin\u2019ichi Satoh",
          "authorId": "2261398214"
        }
      ],
      "year": 2025,
      "abstract": "Fairness auditing of machine learning models is essential for ensuring equitable treatment across demographic groups, while real-world scenarios often involve strict privacy and access constraints. In this work, we address a practical yet underexplored scenario in which an auditor holds a protected private dataset for evaluation, while the uncooperative target model offers only limited access. Conventional evaluation methods typically rely on either revealing the private dataset or requiring extensive cooperation, which is prevented in the scenario. To overcome this challenge, we propose a novel fairness auditing framework based on model behavioral comparison. Our method constructs a pool of reference models with controlled demographic biases trained on the private dataset, and an embedding space representing fairness similarity. The fairness of the target model is estimated based on the proximity to reference models in the embedding space. We further develop an efficient probe sampling strategy that significantly reduces the number of queries to the target model while maintaining estimation accuracy. We demonstrate the effectiveness of our framework on facial age estimation models and gender classification models, evaluating both gender and racial bias. Evaluation on internally trained models and external pre-trained models is carried out using FairFace and UTKFace datasets to simulate the private dataset and probe data. The experimental results show that our framework achieves close estimation with low mean absolute errors to the ground-truth fairness metrics. Compared to baselines based on annotated public datasets or leaking private data, our framework achieves accurate and efficient fairness evaluation while preserving data confidentiality and operating under limited model access. This framework enables practical, privacy-preserving fairness auditing for real-world scenarios involving regulatory and certification contexts where mutual distrust between auditors and service providers creates operational constraints.",
      "citationCount": 0,
      "doi": "10.1109/ACCESS.2025.3641692",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/07129a72f24367ddac018620f3f24452a91aee3a",
      "venue": "IEEE Access",
      "journal": {
        "name": "IEEE Access",
        "pages": "208511-208532",
        "volume": "13"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "099390b8e9512aa968ce28d5b18efcf26e853756",
      "title": "Interactive Fairness Auditing: Leveraging AVOIR for Dynamic Evaluation and Mitigation",
      "authors": [
        {
          "name": "Amin Meghrazi",
          "authorId": "2193622395"
        },
        {
          "name": "Pranav Maneriker",
          "authorId": "8394636"
        },
        {
          "name": "Swati Padhee",
          "authorId": "2367240396"
        },
        {
          "name": "Srinivasan Parthasarathy",
          "authorId": "2323369106"
        }
      ],
      "year": 2025,
      "abstract": "Ensuring fairness in high-stakes machine learning (ML) applications remains a significant challenge. We introduce an interactive, Streamlit-based User Interface (UI) for AVOIR, an automated fairness monitoring framework. This UI allows users to select fairness metrics (e.g., Demographic Parity, Equalized Odds) and explore dataset attributes through dynamic visualizations. The UI supports iterative refinement, enabling users to define fairness constraints in a domain-specific language (DSL), evaluate models, and address violations via interactive charts. Leveraging AVOIR's inference and optimization capabilities, it provides probabilistic guarantees and detects biases at runtime. We demonstrate its utility on multiple datasets, showing how fairness violations are identified and mitigated. By combining declarative fairness specifications, rigorous optimization, and intuitive visualizations, our UI empowers stakeholders to deploy ML models responsibly.",
      "citationCount": 0,
      "doi": "10.1145/3722212.3725108",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/099390b8e9512aa968ce28d5b18efcf26e853756",
      "venue": "SIGMOD Conference Companion",
      "journal": {
        "name": "Companion of the 2025 International Conference on Management of Data"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "0701e93a564f0dd437da53fc6222fd62526b4e35",
      "title": "The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies",
      "authors": [
        {
          "name": "Disa Sariola",
          "authorId": "1742175969"
        },
        {
          "name": "Patrick Button",
          "authorId": "2372961751"
        },
        {
          "name": "Aron Culotta",
          "authorId": "2326205300"
        },
        {
          "name": "Nicholas Mattei",
          "authorId": "2326204995"
        }
      ],
      "year": 2025,
      "abstract": "Artificial intelligence systems, especially those using machine learning, are being deployed in domains from hiring to loan issuance in order to automate these complex decisions. Judging both the effectiveness and fairness of these AI systems, and their human decision making counterpart, is a complex and important topic studied across both computational and social sciences. Within machine learning, a common way to address bias in downstream classifiers is to resample the training data to offset disparities. For example, if hiring rates vary by some protected class, then one may equalize the rate within the training set to alleviate bias in the resulting classifier. While simple and seemingly effective, these methods have typically only been evaluated using data obtained through convenience samples, introducing selection bias and label bias into metrics. Within the social sciences, psychology, public health, and medicine, audit studies, in which fictitious ``testers''(e.g., resumes, emails, patient actors) are sent to subjects (e.g., job openings, businesses, doctors) in randomized control trials, provide high quality data that support rigorous estimates of discrimination. In this paper, we investigate how data from audit studies can be used to improve our ability to both train and evaluate automated hiring algorithms. We find that such data reveals cases where the common fairness intervention method of equalizing base rates across classes appears to achieve parity using traditional measures, but in fact has roughly 10% disparity when measured appropriately. We additionally introduce interventions based on individual treatment effect estimation methods that further reduce algorithmic discrimination using this data.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2507.02152",
      "arxivId": "2507.02152",
      "url": "https://www.semanticscholar.org/paper/0701e93a564f0dd437da53fc6222fd62526b4e35",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.02152"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d8307732b7537b31e4be0de0354d5259dd9e081f",
      "title": "AutoFairML: An Automated Middleware for Fairness Auditing in Real-world AI Pipelines",
      "authors": [
        {
          "name": "Dimitrios Tomaras",
          "authorId": "2082489302"
        },
        {
          "name": "V. Kalogeraki",
          "authorId": "1685532"
        },
        {
          "name": "Christos Varytimidis",
          "authorId": "2339777593"
        },
        {
          "name": "Evangelos C. Anagnostopoulos",
          "authorId": "2013638"
        },
        {
          "name": "Rahul Nair",
          "authorId": "2293536427"
        },
        {
          "name": "Elizabeth M. Daly",
          "authorId": "2269143190"
        },
        {
          "name": "Jakub Marecek",
          "authorId": "2395939024"
        },
        {
          "name": "Dimitrios Gunopulos",
          "authorId": "2303655763"
        }
      ],
      "year": 2025,
      "abstract": "The great proliferation of Artificial Intelligence in a wide variety of application domains has produced an imminent need to interpret the output of such models, especially for evaluating their fairness. Such domains may include human resources management for evaluating individual and group fairness, advertising and financial technologies, where fairness can have significant impact. However, existing tools like AIF360 and AIX360, although important for helping practitioners in industrial AI pipelines, have significant shortcomings. Such tools suffer from poor scalability and developers cannot directly benefit from, since there is no middleware available to support and take advantage of modern AI infrastructures. In this work, we present AutoFairML, an automated middleware able to leverage existing tools to detect bias in AI/ML models, examine trade-offs between different measures of fairness, certify their fairness a priori and provide explanations for the models. This represents a significant improvement in the usability and the real-world applicability of specialized fairness and explainability toolkits such as AIF360 and AIX360. Our experimental evaluation using real-world pipelines and data highlights its merits in terms of practicality and efficiency.",
      "citationCount": 0,
      "doi": "10.1109/ICDCSW63273.2025.00050",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/d8307732b7537b31e4be0de0354d5259dd9e081f",
      "venue": "International Conference on Distributed Computing Systems Workshops",
      "journal": {
        "name": "2025 IEEE 45th International Conference on Distributed Computing Systems Workshops (ICDCSW)",
        "pages": "261-266"
      },
      "publicationTypes": [
        "Conference"
      ]
    },
    {
      "paperId": "ccceae73374a416e3bb1ebb880f47dea1710baf2",
      "title": "Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks",
      "authors": [
        {
          "name": "V. Lafargue",
          "authorId": "2355085220"
        },
        {
          "name": "Adriana Laurindo Monteiro",
          "authorId": "2373614976"
        },
        {
          "name": "Emmanuelle Claeys",
          "authorId": "2373623278"
        },
        {
          "name": "Laurent Risser",
          "authorId": "2257296532"
        },
        {
          "name": "J. Loubes",
          "authorId": "144736569"
        }
      ],
      "year": 2025,
      "abstract": "Proving the compliance of AI algorithms has become an important challenge with the growing deployment of such algorithms for real-life applications. Inspecting possible biased behaviors is mandatory to satisfy the constraints of the regulations of the EU Artificial Intelligence's Act. Regulation-driven audits increasingly rely on global fairness metrics, with Disparate Impact being the most widely used. Yet such global measures depend highly on the distribution of the sample on which the measures are computed. We investigate first how to manipulate data samples to artificially satisfy fairness criteria, creating minimally perturbed datasets that remain statistically indistinguishable from the original distribution while satisfying prescribed fairness constraints. Then we study how to detect such manipulation. Our analysis (i) introduces mathematically sound methods for modifying empirical distributions under fairness constraints using entropic or optimal transport projections, (ii) examines how an auditee could potentially circumvent fairness inspections, and (iii) offers recommendations to help auditors detect such data manipulations. These results are validated through experiments on classical tabular datasets in bias detection.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2507.20708",
      "arxivId": "2507.20708",
      "url": "https://www.semanticscholar.org/paper/ccceae73374a416e3bb1ebb880f47dea1710baf2",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.20708"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b80e474b4a81f5cca1894ebfd6eaa7ac39d2f26c",
      "title": "Fairness Auditing with Multi-Agent Collaboration",
      "authors": [
        {
          "name": "M. Vos",
          "authorId": "2315137"
        },
        {
          "name": "Akash Dhasade",
          "authorId": "1588512022"
        },
        {
          "name": "Jade Garcia Bourr'ee",
          "authorId": "2218064133"
        },
        {
          "name": "Anne-Marie Kermarrec",
          "authorId": "1723331"
        },
        {
          "name": "E. L. Merrer",
          "authorId": "2561410"
        },
        {
          "name": "Benoit Rottembourg",
          "authorId": "2283935314"
        },
        {
          "name": "Gilles Tr\u00e9dan",
          "authorId": "2348944"
        }
      ],
      "year": 2024,
      "abstract": "Existing work in fairness auditing assumes that each audit is performed independently. In this paper, we consider multiple agents working together, each auditing the same platform for different tasks. Agents have two levers: their collaboration strategy, with or without coordination beforehand, and their strategy for sampling appropriate data points. We theoretically compare the interplay of these levers. Our main findings are that (i) collaboration is generally beneficial for accurate audits, (ii) basic sampling methods often prove to be effective, and (iii) counter-intuitively, extensive coordination on queries often deteriorates audits accuracy as the number of agents increases. Experiments on three large datasets confirm our theoretical results. Our findings motivate collaboration during fairness audits of platforms that use ML models for decision-making.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2402.08522",
      "arxivId": "2402.08522",
      "url": "https://www.semanticscholar.org/paper/b80e474b4a81f5cca1894ebfd6eaa7ac39d2f26c",
      "venue": "European Conference on Artificial Intelligence",
      "journal": {
        "pages": "1116-1123"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "ddbd4f7f3eaba7d83ee3dc15643dcc43d46196ec",
      "title": "Peer-induced Fairness: A Causal Approach for Algorithmic Fairness Auditing",
      "authors": [
        {
          "name": "Shiqi Fang",
          "authorId": "2316256158"
        },
        {
          "name": "Zexun Chen",
          "authorId": "2316167412"
        },
        {
          "name": "Jake Ansell",
          "authorId": "2316127931"
        }
      ],
      "year": 2024,
      "abstract": "With the European Union's Artificial Intelligence Act taking effect on 1 August 2024, high-risk AI applications must adhere to stringent transparency and fairness standards. This paper addresses a crucial question: how can we scientifically audit algorithmic fairness? Current methods typically remain at the basic detection stage of auditing, without accounting for more complex scenarios. We propose a novel framework, ``peer-induced fairness'', which combines the strengths of counterfactual fairness and peer comparison strategy, creating a reliable and robust tool for auditing algorithmic fairness. Our framework is universal, adaptable to various domains, and capable of handling different levels of data quality, including skewed distributions. Moreover, it can distinguish whether adverse decisions result from algorithmic discrimination or inherent limitations of the subjects, thereby enhancing transparency. This framework can serve as both a self-assessment tool for AI developers and an external assessment tool for auditors to ensure compliance with the EU AI Act. We demonstrate its utility in small and medium-sized enterprises access to finance, uncovering significant unfairness-41.51% of micro-firms face discrimination compared to non-micro firms. These findings highlight the framework's potential for broader applications in ensuring equitable AI-driven decision-making.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2408.02558",
      "arxivId": "2408.02558",
      "url": "https://www.semanticscholar.org/paper/ddbd4f7f3eaba7d83ee3dc15643dcc43d46196ec",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2408.02558"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d6a7e69149d6269ab72f17367c3c3734c791dd03",
      "title": "FAMEWS: a Fairness Auditing tool for Medical Early-Warning Systems",
      "authors": [
        {
          "name": "M. Hoche",
          "authorId": "2283266303"
        },
        {
          "name": "O. Mineeva",
          "authorId": "2283261622"
        },
        {
          "name": "M. Burger",
          "authorId": "2283270594"
        },
        {
          "name": "A. Blasimme",
          "authorId": "6475221"
        },
        {
          "name": "G. Ra\u00cc\u0088tsch",
          "authorId": "2283267305"
        }
      ],
      "year": 2024,
      "abstract": "Machine learning applications hold promise to aid clinicians in a wide range of clinical tasks, from diagnosis to prognosis, treatment, and patient monitoring. These potential applications are accompanied by a surge of ethical concerns surrounding the use of Machine Learning (ML) models in healthcare, especially regarding fairness and non-discrimination. While there is an increasing number of regulatory policies to ensure the ethical and safe integration of such systems, the translation from policies to practices remains an open challenge. Algorithmic frameworks, aiming to bridge this gap, should be tailored to the application to enable the translation from fundamental human-right principles into accurate statistical analysis, capturing the inherent complexity and risks associated with the system. In this work, we propose a set of fairness impartial checks especially adapted to ML early-warning systems in the medical context, comprising on top of standard fairness metrics, an analysis of clinical outcomes, and a screening of potential sources of bias in the pipeline. Our analysis is further fortified by the inclusion of event-based and prevalence-corrected metrics, as well as statistical tests to measure biases. Additionally, we emphasize the importance of considering subgroups beyond the conventional demographic attributes. Finally, to facilitate operationalization, we present an open-source tool FAMEWS to generate comprehensive fairness reports. These reports address the diverse needs and interests of the stakeholders involved in integrating ML into medical practice. The use of FAMEWS has the potential to reveal critical insights that might otherwise remain obscured. This can lead to improved model design, which in turn may translate into enhanced health outcomes.",
      "citationCount": 2,
      "doi": "10.1101/2024.02.08.24302458",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/d6a7e69149d6269ab72f17367c3c3734c791dd03",
      "venue": "medRxiv",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "b6cd3b7f9dbee9d0b55340175dc00d29012e9974",
      "title": "Statistical Inference for Fairness Auditing",
      "authors": [
        {
          "name": "John J. Cherian",
          "authorId": "1879342053"
        },
        {
          "name": "E. Cand\u00e8s",
          "authorId": "2006869"
        }
      ],
      "year": 2023,
      "abstract": "Before deploying a black-box model in high-stakes problems, it is important to evaluate the model's performance on sensitive subpopulations. For example, in a recidivism prediction task, we may wish to identify demographic groups for which our prediction model has unacceptably high false positive rates or certify that no such groups exist. In this paper, we frame this task, often referred to as\"fairness auditing,\"in terms of multiple hypothesis testing. We show how the bootstrap can be used to simultaneously bound performance disparities over a collection of groups with statistical guarantees. Our methods can be used to flag subpopulations affected by model underperformance, and certify subpopulations for which the model performs adequately. Crucially, our audit is model-agnostic and applicable to nearly any performance metric or group fairness criterion. Our methods also accommodate extremely rich -- even infinite -- collections of subpopulations. Further, we generalize beyond subpopulations by showing how to assess performance over certain distribution shifts. We test the proposed methods on benchmark datasets in predictive inference and algorithmic fairness and find that our audits can provide interpretable and trustworthy guarantees.",
      "citationCount": 16,
      "doi": "10.48550/arXiv.2305.03712",
      "arxivId": "2305.03712",
      "url": "https://www.semanticscholar.org/paper/b6cd3b7f9dbee9d0b55340175dc00d29012e9974",
      "venue": "Journal of machine learning research",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2305.03712"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "59f12d02da6549464aadb185ed615df195529f32",
      "title": "Online Fairness Auditing through Iterative Refinement",
      "authors": [
        {
          "name": "Pranav Maneriker",
          "authorId": "8394636"
        },
        {
          "name": "Codi Burley",
          "authorId": "65727106"
        },
        {
          "name": "Srinivas Parthasarathy",
          "authorId": "2739353"
        }
      ],
      "year": 2023,
      "abstract": "A sizable proportion of deployed machine learning models make their decisions in a black-box manner. Such decision-making procedures are susceptible to intrinsic biases, which has led to a call for accountability in deployed decision systems. In this work, we investigate mechanisms that help audit claimed mathematical guarantees of the fairness of such systems. We construct AVOIR, a system that reduces the number of observations required for the runtime monitoring of probabilistic assertions over fairness metrics specified on decision functions associated with black-box AI models. AVOIR provides an adaptive process that automates the inference of probabilistic guarantees associated with estimating a wide range of fairness metrics. In addition, AVOIR enables the exploration of fairness violations aligned with governance and regulatory requirements. We conduct case studies with fairness metrics on three different datasets and demonstrate how AVOIR can help detect and localize fairness violations and ameliorate the issues with faulty fairness metric design.",
      "citationCount": 16,
      "doi": "10.1145/3580305.3599454",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/59f12d02da6549464aadb185ed615df195529f32",
      "venue": "Knowledge Discovery and Data Mining",
      "journal": {
        "name": "Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
      ]
    },
    {
      "paperId": "12b1f49ba04077cd6cc1d5489a69298a1272b932",
      "title": "More or less discrimination? Practical feasibility of fairness auditing of technologies for personnel selection",
      "authors": [
        {
          "name": "Helena Mihaljevi\u0107",
          "authorId": "51160916"
        },
        {
          "name": "Ivana M\u00fcller",
          "authorId": "2068636498"
        },
        {
          "name": "Katja Dill",
          "authorId": "2184554551"
        },
        {
          "name": "Aysel Yollu-Tok",
          "authorId": "2079778237"
        },
        {
          "name": "Maximilian von Grafenstein",
          "authorId": "2388522991"
        }
      ],
      "year": 2023,
      "abstract": "The use of technologies in personnel selection has come under increased scrutiny in recent years, revealing their potential to amplify existing inequalities in recruitment processes. To date, however, there has been a lack of comprehensive assessments of respective discriminatory potentials and no legal or practical standards have been explicitly established for fairness auditing. The current proposal of the Artificial Intelligence Act classifies numerous applications in personnel selection and recruitment as high-risk technologies, and while it requires quality standards to protect the fundamental rights of those involved, particularly during development, it does not provide concrete guidance on how to ensure this, especially once the technologies are commercially available. We argue that comprehensive and reliable auditing of personnel selection technologies must be contextual, that is, embedded in existing processes and based on real data, as well as participative, involving various stakeholders beyond technology vendors and customers, such as advocacy organizations and researchers. We propose an architectural draft that employs a data trustee to provide independent, fiduciary management of personal and corporate data to audit the fairness of technologies used in personnel selection. Drawing on a case study conducted with two state-owned companies in Berlin, Germany, we discuss challenges and approaches related to suitable fairness metrics, operationalization of vague concepts such as migration* and applicable legal foundations that can be utilized to overcome the fairness-privacy-dilemma arising from uncertainties associated with current laws. We highlight issues that require further interdisciplinary research to enable a prototypical implementation of the auditing concept in the mid-term.",
      "citationCount": 11,
      "doi": "10.1007/s00146-023-01726-w",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/12b1f49ba04077cd6cc1d5489a69298a1272b932",
      "venue": "Ai & Society",
      "journal": {
        "name": "AI & SOCIETY",
        "pages": "2507 - 2523",
        "volume": "39"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f91a2c7c07df043db9c551167c59ecfd3c2ee955",
      "title": "Fairness as a Service (FaaS): verifiable and privacy-preserving fairness auditing of machine learning systems",
      "authors": [
        {
          "name": "Ehsan Toreini",
          "authorId": "2405396"
        },
        {
          "name": "M. Mehrnezhad",
          "authorId": "2747390"
        },
        {
          "name": "A. Moorsel",
          "authorId": "9072212"
        }
      ],
      "year": 2023,
      "abstract": "Providing trust in machine learning (ML) systems and their fairness is a socio-technical challenge, and while the use of ML continues to rise, there is lack of adequate processes and governance practices to assure their fairness. In this paper, we propose FaaS, a novel privacy-preserving, end-to-end verifiable solution, that audits the algorithmic fairness of ML systems. FaaS offers several features, which are absent from previous designs. The FAAS protocol is model-agnostic and independent of specific fairness metrics and can be utilised as a service by multiple stakeholders. FAAS uses zero knowledge proofs to assure the well-formedness of the cryptograms and provenance in the steps of the protocol. We implement a proof of concept of the FaaS architecture and protocol using off-the-shelf hardware, software, and datasets and run experiments to demonstrate its practical feasibility and to analyse its performance and scalability. Our experiments confirm that our proposed protocol is scalable to large-scale auditing scenarios\u00a0(e.g. over 1000 participants) and secure against various attack vectors.",
      "citationCount": 8,
      "doi": "10.1007/s10207-023-00774-z",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f91a2c7c07df043db9c551167c59ecfd3c2ee955",
      "venue": "International Journal of Information Security",
      "journal": {
        "name": "International Journal of Information Security",
        "pages": "1-17",
        "volume": ""
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c6d8d4030569f60b5df610d2fac965ef479434cc",
      "title": "Fairness Auditing, Explanation and Debiasing in Linguistic Data and Language Models",
      "authors": [
        {
          "name": "Marta Marchiori Manerba",
          "authorId": "2121386115"
        }
      ],
      "year": 2023,
      "abstract": null,
      "citationCount": 1,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/c6d8d4030569f60b5df610d2fac965ef479434cc",
      "venue": "xAI",
      "journal": {
        "pages": "241-248"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "bcd14b0e80f9ed5fe5ba1d0f3f772ea5c84748ed",
      "title": "Fairness Auditing in Urban Decisions using LP-based Data Combination",
      "authors": [
        {
          "name": "Jingyi Yang",
          "authorId": "2220590220"
        },
        {
          "name": "Joel Miller",
          "authorId": "2138727315"
        },
        {
          "name": "Mesrob I. Ohannessian",
          "authorId": "3150499"
        }
      ],
      "year": 2023,
      "abstract": "Auditing for fairness often requires relying on a secondary source, e.g., Census data, to inform about protected attributes. To avoid making assumptions about an overarching model that ties such information to the primary data source, a recent line of work has suggested finding the entire range of possible fairness valuations consistent with both sources. Though attractive, the current form of this methodology relies on rigid analytical expressions and lacks the ability to handle continuous decisions, e.g., metrics of urban services. We show that, in such settings, directly adapting these expressions can lead to loose and even vacuous results, particularly on just how fair the audited decisions may be. If used, the audit would be perceived more optimistically than it ought to be. We propose a linear programming formulation to handle continuous decisions, by finding the empirical fairness range when statistical parity is measured through the Kolmogorov-Smirnov distance. The size of this problem is linear in the number of data points and efficiently solvable. We analyze this approach and give finite-sample guarantees to the resulting fairness valuation. We then apply it to synthetic data and to 311 Chicago City Services data, and demonstrate its ability to reveal small but detectable bounds on fairness.",
      "citationCount": 1,
      "doi": "10.1145/3593013.3594118",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/bcd14b0e80f9ed5fe5ba1d0f3f772ea5c84748ed",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "3583a25b27e4eb83540d0492b4ea0152e6d7b5d6",
      "title": "Change-Relaxed Active Fairness Auditing",
      "authors": [
        {
          "name": "Augustin Godinot",
          "authorId": "2239127073"
        },
        {
          "name": "E. L. Merrer",
          "authorId": "2561410"
        },
        {
          "name": "C. Penzo",
          "authorId": "2239127137"
        },
        {
          "name": "F. Ta\u00efani",
          "authorId": "2239126888"
        },
        {
          "name": "G. Tr\u00e9dan",
          "authorId": "2239126883"
        }
      ],
      "year": 2023,
      "abstract": null,
      "citationCount": 2,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/3583a25b27e4eb83540d0492b4ea0152e6d7b5d6",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "7093729d440b4f457152f5a17e00cdd37d3fffa8",
      "title": "Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data",
      "authors": [
        {
          "name": "Chih-Cheng Rex Yuan",
          "authorId": "2320790052"
        },
        {
          "name": "Bow-Yaw Wang",
          "authorId": "2321155890"
        }
      ],
      "year": 2025,
      "abstract": "Fairness auditing of AI systems can identify and quantify biases. However, traditional auditing using real-world data raises security and privacy concerns. It exposes auditors to security risks as they become custodians of sensitive information and targets for cyberattacks. Privacy risks arise even without direct breaches, as data analyses can inadvertently expose confidential information. To address these, we propose a framework that leverages differentially private synthetic data to audit the fairness of AI systems. By applying privacy-preserving mechanisms, it generates synthetic data that mirrors the statistical properties of the original dataset while ensuring privacy. This method balances the goal of rigorous fairness auditing and the need for strong privacy protections. Through experiments on real datasets like Adult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real data. By analyzing the alignment and discrepancies between these metrics, we assess the capacity of synthetic data to preserve the fairness properties of real data. Our results demonstrate the framework's ability to enable meaningful fairness evaluations while safeguarding sensitive information, proving its applicability across critical and sensitive domains.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2504.21634",
      "arxivId": "2504.21634",
      "url": "https://www.semanticscholar.org/paper/7093729d440b4f457152f5a17e00cdd37d3fffa8",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.21634"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "720d8143d865921ca3eae2ffd34da763c57f05d9",
      "title": "A Proof System with Causal Labels (Part I): checking Individual Fairness and Intersectionality",
      "authors": [
        {
          "name": "Leonardo Ceragioli",
          "authorId": "1583469148"
        },
        {
          "name": "G. Primiero",
          "authorId": "2614511"
        }
      ],
      "year": 2025,
      "abstract": "In this article we propose an extension to the typed natural deduction calculus TNDPQ to model verification of individual fairness and intersectionality in probabilistic classifiers. Their interpretation is obtained by formulating specific conditions for the application of the structural rule of Weakening. Such restrictions are given by causal labels used to check for conditional independence between protected and target variables.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2507.14650",
      "arxivId": "2507.14650",
      "url": "https://www.semanticscholar.org/paper/720d8143d865921ca3eae2ffd34da763c57f05d9",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.14650"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c9ee9951999d4b80ab96185bd608338740a7ceff",
      "title": "Active Fairness Auditing",
      "authors": [
        {
          "name": "Tom Yan",
          "authorId": "12082007"
        },
        {
          "name": "Chicheng Zhang",
          "authorId": "40201729"
        }
      ],
      "year": 2022,
      "abstract": "The fast spreading adoption of machine learning (ML) by companies across industries poses significant regulatory challenges. One such challenge is scalability: how can regulatory bodies efficiently audit these ML models, ensuring that they are fair? In this paper, we initiate the study of query-based auditing algorithms that can estimate the demographic parity of ML models in a query-efficient manner. We propose an optimal deterministic algorithm, as well as a practical randomized, oracle-efficient algorithm with comparable guarantees. Furthermore, we make inroads into understanding the optimal query complexity of randomized active fairness estimation algorithms. Our first exploration of active fairness estimation aims to put AI governance on firmer theoretical foundations.",
      "citationCount": 30,
      "doi": "10.48550/arXiv.2206.08450",
      "arxivId": "2206.08450",
      "url": "https://www.semanticscholar.org/paper/c9ee9951999d4b80ab96185bd608338740a7ceff",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2206.08450"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "07961ad92808ecb1db46206cf21f69e999453ca3",
      "title": "Fairness is in the Details : Face Dataset Auditing",
      "authors": [
        {
          "name": "V. Lafargue",
          "authorId": "2355085220"
        },
        {
          "name": "E. Claeys",
          "authorId": "2355084913"
        },
        {
          "name": "J. Loubes",
          "authorId": "144736569"
        }
      ],
      "year": 2025,
      "abstract": "Auditing involves verifying the proper implementation of a given policy. As such, auditing is essential for ensuring compliance with the principles of fairness, equity, and transparency mandated by the European Union's AI Act. Moreover, biases present during the training phase of a learning system can persist in the modeling process and result in discrimination against certain subgroups of individuals when the model is deployed in production. Assessing bias in image datasets is a particularly complex task, as it first requires a feature extraction step, then to consider the extraction's quality in the statistical tests. This paper proposes a robust methodology for auditing image datasets based on so-called\"sensitive\"features, such as gender, age, and ethnicity. The proposed methodology consists of both a feature extraction phase and a statistical analysis phase. The first phase introduces a novel convolutional neural network (CNN) architecture specifically designed for extracting sensitive features with a limited number of manual annotations. The second phase compares the distributions of sensitive features across subgroups using a novel statistical test that accounts for the imprecision of the feature extraction model. Our pipeline constitutes a comprehensive and fully automated methodology for dataset auditing. We illustrate our approach using two manually annotated datasets. The code and datasets are available at github.com/ValentinLafargue/FairnessDetails.",
      "citationCount": 0,
      "doi": "10.1007/978-3-032-06129-4_18",
      "arxivId": "2504.08396",
      "url": "https://www.semanticscholar.org/paper/07961ad92808ecb1db46206cf21f69e999453ca3",
      "venue": "ECML/PKDD",
      "journal": {
        "pages": "299-315"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f240825e618621eff36510d916b23f6c06af680d",
      "title": "Auditing and instructing text-to-image generation models on fairness",
      "authors": [
        {
          "name": "Felix Friedrich",
          "authorId": "2055616945"
        },
        {
          "name": "Manuel Brack",
          "authorId": "2166299958"
        },
        {
          "name": "Lukas Struppek",
          "authorId": "2140415179"
        },
        {
          "name": "Dominik Hintersdorf",
          "authorId": "2140396795"
        },
        {
          "name": "P. Schramowski",
          "authorId": "40896023"
        },
        {
          "name": "Sasha Luccioni",
          "authorId": "2165225321"
        },
        {
          "name": "K. Kersting",
          "authorId": "2066493115"
        }
      ],
      "year": 2024,
      "abstract": "Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases during the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias in any direction based on human instructions yielding arbitrary proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, requiring no data filtering nor additional training.",
      "citationCount": 25,
      "doi": "10.1007/s43681-024-00531-5",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f240825e618621eff36510d916b23f6c06af680d",
      "venue": "AI and Ethics",
      "journal": {
        "name": "Ai and Ethics",
        "pages": "2103 - 2123",
        "volume": "5"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6780c5d4f93167f629104ab1f0654c8758d8dc80",
      "title": "Legal but Unfair: Auditing the Impact of Data Minimization on Fairness and Accuracy Trade-off in Recommender Systems",
      "authors": [
        {
          "name": "Salvatore Bufi",
          "authorId": "2240565226"
        },
        {
          "name": "Vincenzo Paparella",
          "authorId": "2125373186"
        },
        {
          "name": "V. W. Anelli",
          "authorId": "2431124"
        },
        {
          "name": "Tommaso Di Noia",
          "authorId": "27334490"
        }
      ],
      "year": 2025,
      "abstract": "Data minimization, required by recent data privacy regulations, is crucial for user privacy, but its impact on recommender systems remains largely unclear. The core problem lies in the fact that reducing or altering the training data of these systems can drastically affect their performance. While previous research has explored how data minimization affects recommendation accuracy, a critical gap remains: How does data minimization impact consumers\u2019 and providers\u2019 fairness? This study addresses this gap by systematically examining how data minimization influences multiple objectives in recommender systems, i.e., the trade-offs between accuracy, user fairness, and provider fairness. Our investigation includes (i) an analysis of how the data minimization strategies affect RS performance across these objectives, (ii) an assessment of data minimization techniques to determine those that can balance better the trade-off among the considered objectives, and (iii) an evaluation of the robustness of different recommendation models under diverse minimization strategies to identify those that best maintain performance. The findings reveal that data minimization can sometimes undermine provider fairness, albeit enhancing group-based consumer fairness to the detriment of accuracy. Additionally, different strategies can offer diverse trade-offs for the assessed objectives. The source code supporting this study is available at https://github.com/salvatore-bufi/DataMinimizationFairness.",
      "citationCount": 2,
      "doi": "10.1145/3699682.3728356",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6780c5d4f93167f629104ab1f0654c8758d8dc80",
      "venue": "User Modeling, Adaptation, and Personalization",
      "journal": {
        "name": "Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "b6bea3dd6812f520deeb59b3bf239aabee40f682",
      "title": "Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges",
      "authors": [
        {
          "name": "Ariel Lubonja",
          "authorId": "2400141720"
        },
        {
          "name": "P. Bassi",
          "authorId": "138550191"
        },
        {
          "name": "Wenxuan Li",
          "authorId": "2275761765"
        },
        {
          "name": "Hualin Qiao",
          "authorId": "2217339629"
        },
        {
          "name": "Randal Burns",
          "authorId": "2400137581"
        },
        {
          "name": "Alan L. Yuille",
          "authorId": "2253485882"
        },
        {
          "name": "Zongwei Zhou",
          "authorId": "2365063713"
        }
      ],
      "year": 2025,
      "abstract": "Open challenges have become the de facto standard for comparative ranking of medical AI methods. Despite their importance, medical AI leaderboards exhibit three persistent limitations: (1) score gaps are rarely tested for statistical significance, so rank stability is unknown; (2) single averaged metrics are applied to every organ, hiding clinically important boundary errors; (3) performance across intersecting demographics is seldom reported, masking fairness and equity gaps. We introduce RankInsight, an open-source toolkit that seeks to address these limitations. RankInsight (1) computes pair-wise significance maps that show the nnU-Net family outperforms Vision-Language and MONAI submissions with high statistical certainty; (2) recomputes leaderboards with organ-appropriate metrics, reversing the order of the top four models when Dice is replaced by NSD for tubular structures; and (3) audits intersectional fairness, revealing that more than half of the MONAI-based entries have the largest gender-race discrepancy on our proprietary Johns Hopkins Hospital dataset. The RankInsight toolkit is publicly released and can be directly applied to past, ongoing, and future challenges. It enables organizers and participants to publish rankings that are statistically sound, clinically meaningful, and demographically fair.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2512.19091",
      "url": "https://www.semanticscholar.org/paper/b6bea3dd6812f520deeb59b3bf239aabee40f682",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "9a6018899460df8cf0eb050e5ff968183246ce6b",
      "title": "Fairness in Predictive Marketing: Auditing and Mitigating Demographic Bias in Machine Learning for Customer Targeting",
      "authors": [
        {
          "name": "Sayee Phaneendhar Pasupuleti",
          "authorId": "2375463354"
        },
        {
          "name": "Jagadeesh Kola",
          "authorId": "2379222343"
        },
        {
          "name": "Sai Phaneendra Manikantesh Kodete",
          "authorId": "2379223099"
        },
        {
          "name": "Sree Harsha Palli",
          "authorId": "2370337261"
        }
      ],
      "year": 2025,
      "abstract": "As organizations increasingly turn to machine learning for customer segmentation and targeted marketing, concerns about fairness and algorithmic bias have become more urgent. This study presents a comprehensive fairness audit and mitigation framework for predictive marketing models using the Bank Marketing dataset. We train logistic regression and random forest classifiers to predict customer subscription behavior and evaluate their performance across key demographic groups, including age, education, and job type. Using model explainability techniques such as SHAP and fairness metrics including disparate impact and true positive rate parity, we uncover notable disparities in model behavior that could result in discriminatory targeting. We implement three mitigation strategies\u2014reweighing, threshold adjustment, and feature exclusion\u2014and assess their effectiveness in improving fairness while preserving business-relevant performance metrics. Among these, reweighing produced the most balanced outcome, raising the Disparate Impact Ratio for older individuals from 0.65 to 0.82 and reducing the true positive rate parity gap by over 40%, with only a modest decline in precision (from 0.78 to 0.76). We propose a replicable workflow for embedding fairness auditing into enterprise BI systems and highlight the strategic importance of ethical AI practices in building accountable and inclusive marketing technologies. technologies.",
      "citationCount": 0,
      "doi": "10.3390/analytics4040026",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/9a6018899460df8cf0eb050e5ff968183246ce6b",
      "venue": "Analytics",
      "journal": {
        "name": "Analytics"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "974fc89edf9729efee42e9a60d0bffdeab0ffa85",
      "title": "Web-Based Platform for Fairness and Privacy Auditing of Machine Learning Models",
      "authors": [
        {
          "name": "Ilhama Novruzova",
          "authorId": "2329395209"
        },
        {
          "name": "Natavan Hasanova",
          "authorId": "2206099401"
        },
        {
          "name": "Khalid Mammadov",
          "authorId": "2329393530"
        }
      ],
      "year": 2025,
      "abstract": "For machine learning (ML) models, especially those applied in sensitive domains such as healthcare, law enforcement, finance, and education, high performance alone is not sufficient. These models must also address concerns related to fairness and privacy preservation. We present VerifAI, a software platform that conducts an extensive analysis of fairness, utility, and privacy risks at the subpopulation level, revealing disparities that are often hidden in aggregate evaluations. VerifAI is the first platform that allows users to evaluate the privacy and fairness of ML models without writing any code, while also providing a comprehensive comparison of these metrics on a single interface.",
      "citationCount": 0,
      "doi": "10.1109/AICT67988.2025.11268553",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/974fc89edf9729efee42e9a60d0bffdeab0ffa85",
      "venue": "Advanced Industrial Conference on Telecommunications",
      "journal": {
        "name": "2025 IEEE 19th International Conference on Application of Information and Communication Technologies (AICT)",
        "pages": "1-7"
      },
      "publicationTypes": [
        "Conference"
      ]
    },
    {
      "paperId": "b28474f96c2704f53e7358fbbe558e414ddc43ab",
      "title": "Auditing the Fairness of AI-Detection Tools: A Comparative Study of ESL, Published, and AI-Generated Texts and Their Misclassification Risks",
      "authors": [
        {
          "name": "R. Lege",
          "authorId": "3177575"
        }
      ],
      "year": 2025,
      "abstract": "This study investigated the classification fairness at the threshold level of four commercially available AI detection tools on the Internet: Copyleaks, ZeroGPT, Scribbr, and Quillbot Premium. The research included the submission of three distinct chunks of texts (N=1212) of between 400-5oo words for evaluation. The writing texts came from fully AI-generated examples (N=307), prompted between 2024 and 2025, and published human-written texts (N=302), and ESL graduate student texts (N=303) written before 2021. The texts were analyzed using binary classification thresholds to determine how the three free devices (Copyleaks, ZeroGPT, Scribbr) and the one paid service (QPremium) performed when checking for potentially AI-generated material in each of the writing examples. The study employed a performance metrics to illustrate the issue with threshold application in such devices. The research included the use of the Chi-square test of independence as well as other inferential statistics to assess inter-detector consistency and potential bias patterns. The results indicated that such devices perform well in identifying AI-generated text written artificially; however, significant disparities emerged in the misclassification of human texts. In particular, AI detectors disproportionally flagged ESL writing with false positives. Such findings illustrate the importance of such fairness audits in assessing the linguistic sensitivity in such tools, especially in the educational setting, where misclassification can have academic or reputational consequences.",
      "citationCount": 0,
      "doi": "10.22161/ijtle.4.5.5",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/b28474f96c2704f53e7358fbbe558e414ddc43ab",
      "venue": "International Journal of Teaching, Learning and Education",
      "journal": {
        "name": "International Journal of Teaching, Learning and Education"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "3245cf52b2412d36eb26380c61b63a04d62b249b",
      "title": "Auditing and Validating Fairness and Ethics in Machine Learning Systems",
      "authors": [
        {
          "name": "Disa Sariola",
          "authorId": "1742175969"
        }
      ],
      "year": 2025,
      "abstract": "Ethics are embedded in the choices we make every day,\nwhether they are in our personal interactions or when\ndesigning computational systems. Ethics should be seen as\nnot just a philosophical pursuit or a legal construct of\njurisprudence , instead it is a framework for operating and\ndecision-making. In the context of machine learning,\nethical concerns emerge in subtle but rife ways: from how\ndata is collected and labeled, to who defines the\ncontextual fairness. After all, the word 'fair' can mean\ndifferent things; conforming with the established rules or\nmarked by impartiality and honesty. But, what does it mean\nin the context of computational frameworks? Despite the\nfrequent framing of ethical concerns and fairness as\ntechnical problems, issues like fairness are rooted in\nvalue judgments made by real humans. Decisions about\ninclusion, categorization, and weighting reflect the\nassumptions and priorities of the people behind the\nalgorithms. At scale, these decisions don\u2019t disappear into\nabstraction. Instead, they shape outcomes, influencing who\nbenefits from, or is harmed by, automated systems. While\nethical theory and legal scholarship have long tackled\nquestions of justice and fairness, computer science often\ntreats these topics as secondary to performance metrics or\nefficiency. These disconnects risk minimizing the\nreal-world stakes of algorithmic decision-making. My\nresearch aims to bridge this gap by examining how fairness\ninterventions can be examined from a broader perspective\nthrough the lens of algorithmic auditing. I aim to study\nhow fairness interventions efforts align with the stated\ngoals of the audit framework, and whether they meaningfully\naddress the inequities they aim to solve. The goal should\nnot be to reduce fairness to a checkbox, but to understand\nit in the context of the data and the broader society the\nsystem is embedded within.",
      "citationCount": 0,
      "doi": "10.1609/aies.v8i3.36795",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/3245cf52b2412d36eb26380c61b63a04d62b249b",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "42264c564392a8eb888fcb08d1058598eca9660c",
      "title": "Examining Ethical AI Auditing Dashboards: Enhancing Fairness and Accountability in U.S. High-Stakes Decisions",
      "authors": [
        {
          "name": "M. Masudi",
          "authorId": "2385658666"
        }
      ],
      "year": 2025,
      "abstract": "This study looks at special tools called ethical AI auditing dashboards, systems that help detect and fix unfair bias in\nartificial intelligence (AI), especially in important areas like lending, hiring, and law enforcement in the U.S.\nAs AI is used more often in making big decisions, it\u2019s important to make sure those decisions are fair, transparent, and\naccountable. Using real data from 2025, the study shows how well these dashboards work in real-life situations by analyzing how\nthey catch and reduce bias.\nFive types of graphs (pie chart, heat map, bar graph, line graph, and scatter plot) are used to show:\n\uf0b7 How much bias exists in different industries,\n\uf0b7 How well the dashboards reduce unfair treatment, and\n\uf0b7 How quickly they respond to problems.\nThe results show that AI systems without auditing tools often produce unfair results, especially against groups that have been\ndiscriminated against in the past. But when these dashboards are used, the systems become much fairer and more compliant\nwith the law.\nThis paper also connects the research to U.S. laws and goalslike civil rights, economic fairness, innovation, and public trust. It\nargues that these auditing dashboards are not just helpful tech tools but are critical for national progress and fairness.",
      "citationCount": 0,
      "doi": "10.22214/ijraset.2025.75017",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/42264c564392a8eb888fcb08d1058598eca9660c",
      "venue": "International Journal for Research in Applied Science and Engineering Technology",
      "journal": {
        "name": "International Journal for Research in Applied Science and Engineering Technology"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2435004629c5c09ac5fd7c43326abf4691260dca",
      "title": "Fair Play for Individuals, Foul Play for Groups? Auditing Anonymization's Impact on ML Fairness",
      "authors": [
        {
          "name": "H\u00e9ber H. Arcolezi",
          "authorId": "1393192257"
        },
        {
          "name": "Mina Alishahi",
          "authorId": "2360717078"
        },
        {
          "name": "Adda-Akram Bendoukha",
          "authorId": "2171674605"
        },
        {
          "name": "N. Kaaniche",
          "authorId": "3267285"
        }
      ],
      "year": 2025,
      "abstract": "Machine learning (ML) algorithms are heavily based on the availability of training data, which, depending on the domain, often includes sensitive information about data providers. This raises critical privacy concerns. Anonymization techniques have emerged as a practical solution to address these issues by generalizing features or suppressing data to make it more difficult to accurately identify individuals. Although recent studies have shown that privacy-enhancing technologies can influence ML predictions across different subgroups, thus affecting fair decision-making, the specific effects of anonymization techniques, such as $k$-anonymity, $\\ell$-diversity, and $t$-closeness, on ML fairness remain largely unexplored. In this work, we systematically audit the impact of anonymization techniques on ML fairness, evaluating both individual and group fairness. Our quantitative study reveals that anonymization can degrade group fairness metrics by up to fourfold. Conversely, similarity-based individual fairness metrics tend to improve under stronger anonymization, largely as a result of increased input homogeneity. By analyzing varying levels of anonymization across diverse privacy settings and data distributions, this study provides critical insights into the trade-offs between privacy, fairness, and utility, offering actionable guidelines for responsible AI development. Our code is publicly available at: https://github.com/hharcolezi/anonymity-impact-fairness.",
      "citationCount": 0,
      "doi": "10.3233/faia250909",
      "arxivId": "2505.07985",
      "url": "https://www.semanticscholar.org/paper/2435004629c5c09ac5fd7c43326abf4691260dca",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.07985"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "52c8497b23321c0b12bf363b242e808cd682baf6",
      "title": "PrivFair: a Library for Privacy-Preserving Fairness Auditing",
      "authors": [
        {
          "name": "Sikha Pentyala",
          "authorId": "2047996928"
        },
        {
          "name": "David Melanson",
          "authorId": "2101316778"
        },
        {
          "name": "Martine De Cock",
          "authorId": "33330805"
        },
        {
          "name": "G. Farnadi",
          "authorId": "2086602"
        }
      ],
      "year": 2022,
      "abstract": "Machine learning (ML) has become prominent in applications that directly affect people's quality of life, including in healthcare, justice, and finance. ML models have been found to exhibit discrimination based on sensitive attributes such as gender, race, or disability. Assessing if an ML model is free of bias remains challenging to date, and by definition has to be done with sensitive user characteristics that are subject of anti-discrimination and data protection law. Existing libraries for fairness auditing of ML models offer no mechanism to protect the privacy of the audit data. We present PrivFair, a library for privacy-preserving fairness audits of ML models. Through the use of Secure Multiparty Computation (MPC), PrivFair protects the confidentiality of the model under audit and the sensitive data used for the audit, hence it supports scenarios in which a proprietary classifier owned by a company is audited using sensitive audit data from an external investigator. We demonstrate the use of PrivFair for group fairness auditing with tabular data or image data, without requiring the investigator to disclose their data to anyone in an unencrypted manner, or the model owner to reveal their model parameters to anyone in plaintext.",
      "citationCount": 7,
      "doi": null,
      "arxivId": "2202.04058",
      "url": "https://www.semanticscholar.org/paper/52c8497b23321c0b12bf363b242e808cd682baf6",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2202.04058"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ccf7b168c82d4104b36bfcf1a9cfa232b834a657",
      "title": "Destination (Un)Known: Auditing Bias and Fairness in LLM-Based Travel Recommendations",
      "authors": [
        {
          "name": "Hristo Andreev",
          "authorId": "2198561849"
        },
        {
          "name": "P. Kosmas",
          "authorId": "94721458"
        },
        {
          "name": "Antonios D. Livieratos",
          "authorId": "2338312958"
        },
        {
          "name": "A. Theocharous",
          "authorId": "2020070"
        },
        {
          "name": "Anastasios Zopiatis",
          "authorId": "2006826"
        }
      ],
      "year": 2025,
      "abstract": "Large language-model chatbots such as ChatGPT and DeepSeek are quickly gaining traction as an easy, first-stop tool for trip planning because they offer instant, conversational advice that once required sifting through multiple websites or guidebooks. Yet little is known about the biases that shape the destination suggestions these systems provide. This study conducts a controlled, persona-based audit of the two models, generating 6480 recommendations for 216 traveller profiles that vary by origin country, age, gender identity and trip theme. Six observable bias families (popularity, geographic, cultural, stereotype, demographic and reinforcement) are quantified using tourism rankings, Hofstede scores, a 150-term clich\u00e9 lexicon and information-theoretic distance measures. Findings reveal measurable bias in every bias category. DeepSeek is more likely than ChatGPT to suggest off-list cities and recommends domestic travel more often, while both models still favour mainstream destinations. DeepSeek also points users toward culturally more distant destinations on all six Hofstede dimensions and employs a denser, superlative-heavy clich\u00e9 register; ChatGPT shows wider lexical variety but remains strongly promotional. Demographic analysis uncovers moderate gender gaps and extreme divergence for non-binary personas, tempered by a \u201cprotective\u201d tendency to guide non-binary travellers toward countries with higher LGBTQI acceptance. Reinforcement bias is minimal, with over 90 percent of follow-up suggestions being novel in both systems. These results confirm that unconstrained LLMs are not neutral filters but active amplifiers of structural imbalances. The paper proposes a public-interest re-ranking layer, hosted by a body such as UN Tourism, that balances exposure fairness, seasonality smoothing, low-carbon routing, cultural congruence, safety safeguards and stereotype penalties, transforming conversational AI from an opaque gatekeeper into a sustainability-oriented travel recommendation tool.",
      "citationCount": 1,
      "doi": "10.3390/ai6090236",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/ccf7b168c82d4104b36bfcf1a9cfa232b834a657",
      "venue": "Applied Informatics",
      "journal": {
        "name": "AI"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "fa1ece069a14b88be133136294d0b98a4d6364bb",
      "title": "Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening",
      "authors": [
        {
          "name": "Kevin T Webster",
          "authorId": "2372772420"
        }
      ],
      "year": 2025,
      "abstract": "The increasing use of generative AI for resume screening is predicated on the assumption that it offers an unbiased alternative to biased human decision-making. However, this belief fails to address a critical question: are these AI systems fundamentally competent at the evaluative tasks they are meant to perform? This study investigates the question of competence through a two-part audit of eight major AI platforms. Experiment 1 confirmed complex, contextual racial and gender biases, with some models penalizing candidates merely for the presence of demographic signals. Experiment 2, which evaluated core competence, provided a critical insight: some models that appeared unbiased were, in fact, incapable of performing a substantive evaluation, relying instead on superficial keyword matching. This paper introduces the\"Illusion of Neutrality\"to describe this phenomenon, where an apparent lack of bias is merely a symptom of a model's inability to make meaningful judgments. This study recommends that organizations and regulators adopt a dual-validation framework, auditing AI hiring tools for both demographic bias and demonstrable competence to ensure they are both equitable and effective.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2507.11548",
      "arxivId": "2507.11548",
      "url": "https://www.semanticscholar.org/paper/fa1ece069a14b88be133136294d0b98a4d6364bb",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.11548"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c0d76685c1a4deb44ad25b37d87cb7665facf0db",
      "title": "Sublinear Algorithms for Wasserstein and Total Variation Distances: Applications to Fairness and Privacy Auditing",
      "authors": [
        {
          "name": "Debabrota Basu",
          "authorId": "2257212923"
        },
        {
          "name": "Debarshi Chanda",
          "authorId": "2325097999"
        }
      ],
      "year": 2025,
      "abstract": "Resource-efficiently computing representations of probability distributions and the distances between them while only having access to the samples is a fundamental and useful problem across mathematical sciences. In this paper, we propose a generic framework to learn the probability and cumulative distribution functions (PDFs and CDFs) of a sub-Weibull, i.e. almost any light- or heavy-tailed, distribution while the samples from it arrive in a stream. The idea is to reduce these problems into estimating the frequency of an \\textit{appropriately chosen subset} of the support of a \\textit{properly discretised distribution}. We leverage this reduction to compute mergeable summaries of distributions from the stream of samples while requiring only sublinear space relative to the number of observed samples. This allows us to estimate Wasserstein and Total Variation (TV) distances between any two distributions while samples arrive in streams and from multiple sources. Our algorithms significantly improves on the existing methods for distance estimation incurring super-linear time and linear space complexities, and further extend the mergeable summaries framework to continuous distributions with possibly infinite support. Our results are tight with respect to the existing lower bounds for bounded discrete distributions. In addition, we leverage our proposed estimators of Wasserstein and TV distances to tightly audit the fairness and privacy of algorithms. We empirically demonstrate the efficiency of proposed algorithms across synthetic and real-world datasets.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2503.07775",
      "arxivId": "2503.07775",
      "url": "https://www.semanticscholar.org/paper/c0d76685c1a4deb44ad25b37d87cb7665facf0db",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.07775"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f065072c961028004a2029fadb4402085084d535",
      "title": "The Intersectionality Problem for Algorithmic Fairness",
      "authors": [
        {
          "name": "Johannes Himmelreich",
          "authorId": "2329187919"
        },
        {
          "name": "Arbie Hsu",
          "authorId": "2329185104"
        },
        {
          "name": "Kristian Lum",
          "authorId": "2329185564"
        },
        {
          "name": "Ellen Veomett",
          "authorId": "2307085482"
        }
      ],
      "year": 2024,
      "abstract": "A yet unmet challenge in algorithmic fairness is the problem of intersectionality, that is, achieving fairness across the intersection of multiple groups -- and verifying that such fairness has been attained. Because intersectional groups tend to be small, verifying whether a model is fair raises statistical as well as moral-methodological challenges. This paper (1) elucidates the problem of intersectionality in algorithmic fairness, (2) develops desiderata to clarify the challenges underlying the problem and guide the search for potential solutions, (3) illustrates the desiderata and potential solutions by sketching a proposal using simple hypothesis testing, and (4) evaluates, partly empirically, this proposal against the proposed desiderata.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2411.02569",
      "arxivId": "2411.02569",
      "url": "https://www.semanticscholar.org/paper/f065072c961028004a2029fadb4402085084d535",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2411.02569"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4b94c7dd43def3c8d2c297e4c9bf47ba96482951",
      "title": "Ethical Requirements for Achieving Fairness in Radiology Machine Learning: An Intersectionality and Social Embeddedness Approach",
      "authors": [
        {
          "name": "Dessislava S. Fessenko",
          "authorId": "123691602"
        }
      ],
      "year": 2024,
      "abstract": "Radiodiagnostics by machine-learning (ML) systems is often perceived as objective and fair. It may, however, exhibit bias towards certain patient sub-groups. The typical reasons for this are the selection of disease features for ML systems to screen, that ML systems learn from human clinical judgements, which are often biased, and that fairness in ML is often inappropriately conceptualized as \u201cequality\u201d. ML systems with such parameters fail to accurately diagnose and address patients\u2019 actual health needs and how they depend on patients\u2019 social identities (i.e. intersectionality) and broader social conditions (i.e. embeddedness). This paper explores the ethical obligations to ensure fairness of ML systems precisely in light of patients\u2019 intersectionality and the social embeddedness of their health. The paper proposes a set of interventions to tackle these issues. It recommended a paradigm shift in the development of ML systems that enables them to screen both endogenous disease causes and the health effects of patients\u2019 relevant underlying (e.g. socioeconomic) circumstances. The paper proposes a framework of ethical requirements for instituting this shift and further ensuring fairness. The requirements center patients\u2019 intersectionality and the social embeddedness of their health most notably through (i) integrating in ML systems adequate measurable medical indicators of the health impact of patients\u2019 circumstances, (ii) ethically sourced, diverse, representative and correct patient data concerning relevant disease features and medical indicators, and (iii) iterative socially sensitive co-exploration and co-design of datasets and ML systems involving all relevant stakeholders.",
      "citationCount": 1,
      "doi": "10.18785/jhe.2001.04",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/4b94c7dd43def3c8d2c297e4c9bf47ba96482951",
      "venue": "Journal of Health Ethics",
      "journal": {
        "name": "Journal of Health Ethics"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "44362349ef6d6ad6eaa879e23a9efaacb7b187bf",
      "title": "Ensuring Fairness with Transparent Auditing of Quantitative Bias in AI Systems",
      "authors": [
        {
          "name": "Chih-Cheng Rex Yuan",
          "authorId": "2320790052"
        },
        {
          "name": "Bow-Yaw Wang",
          "authorId": "2321155890"
        }
      ],
      "year": 2024,
      "abstract": "With the rapid advancement of AI, there is a growing trend to integrate AI into decision-making processes. However, AI systems may exhibit biases that lead decision-makers to draw unfair conclusions. Notably, the COMPAS system used in the American justice system to evaluate recidivism was found to favor racial majority groups; specifically, it violates a fairness standard called equalized odds. Various measures have been proposed to assess AI fairness. We present a framework for auditing AI fairness, involving third-party auditors and AI system providers, and we have created a tool to facilitate systematic examination of AI systems. The tool is open-sourced and publicly available. Unlike traditional AI systems, we advocate a transparent white-box and statistics-based approach. It can be utilized by third-party auditors, AI developers, or the general public for reference when judging the fairness criterion of AI systems.",
      "citationCount": 4,
      "doi": "10.23919/PNC63053.2024.10697374",
      "arxivId": "2409.06708",
      "url": "https://www.semanticscholar.org/paper/44362349ef6d6ad6eaa879e23a9efaacb7b187bf",
      "venue": "2024 Pacific Neighborhood Consortium Annual Conference and Joint Meetings (PNC)",
      "journal": {
        "name": "2024 Pacific Neighborhood Consortium Annual Conference and Joint Meetings (PNC)",
        "pages": "25-32"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "48422ab36d7e0f284af1b0d5afb6257c43ed13b2",
      "title": "Polynomial time auditing of statistical subgroup fairness for Gaussian data",
      "authors": [
        {
          "name": "Daniel Hsu",
          "authorId": "2281824349"
        },
        {
          "name": "Jizhou Huang",
          "authorId": "2281902958"
        },
        {
          "name": "Brendan Juba",
          "authorId": "2281826656"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.48550/arXiv.2401.16439",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/48422ab36d7e0f284af1b0d5afb6257c43ed13b2",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2401.16439"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 40,
  "errors": []
}
