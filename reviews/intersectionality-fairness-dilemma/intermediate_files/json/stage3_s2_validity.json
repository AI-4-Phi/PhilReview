{
  "status": "success",
  "source": "semantic_scholar",
  "query": "validity fairness metrics what do metrics measure",
  "results": [
    {
      "paperId": "ba76b32aea84d314f453c930356362698a71aa1f",
      "title": "From Clinical Trials to Real-World Impact: Introducing a Computational Framework to Detect Endpoint Bias in Opioid Use Disorder Research.",
      "authors": [
        {
          "name": "Gabriel J Odom",
          "authorId": "2193971718"
        },
        {
          "name": "Laura Brandt",
          "authorId": "2238559575"
        },
        {
          "name": "Aaron Marker",
          "authorId": "2397324290"
        },
        {
          "name": "Salvatore Giorgi",
          "authorId": "2337400563"
        },
        {
          "name": "Ganesh Jainarain",
          "authorId": "2397326308"
        },
        {
          "name": "H. A. Schwartz",
          "authorId": "2376210381"
        },
        {
          "name": "Larry Au",
          "authorId": "2005072796"
        },
        {
          "name": "Clinton Castro",
          "authorId": "2287987502"
        }
      ],
      "year": 2025,
      "abstract": "INTRODUCTION\nClinical trial endpoints are a 'finite sequence of instructions to perform a task' (measure treatment effectiveness), making them algorithms. Consequently, they may exhibit algorithmic bias: internal and external performance can vary across demographic groups, impacting fairness, validity and clinical decision-making.\n\n\nMETHODS\nWe developed the open-source Detecting Algorithmic Bias (DAB) Pipeline in Python to identify endpoint 'performance variance'-a specific algorithmic bias-as the proportion of minority participants changes. This pipeline assesses internal performance (on demographically matched test data) and external performance (on demographically diverse validation data) using metrics including F1 scores and area under the receiver operating characteristic curve (AUROC). We applied it to representative opioid use disorder (OUD) trial endpoints.\n\n\nRESULTS\nF1 scores remained stable across minority representation levels, suggesting consistency in precision-recall balance (F1) despite demographic shifts. Conversely, AUROC measures were more sensitive, revealing significant performance variance. Training on demographically homogeneous populations boosted internal performance (accuracy within similar cohorts) but critically compromised external generalisability (accuracy within diverse cohorts). This pattern reveals an 'endpoint bias trade-off': optimising performance for homogeneous populations vs. having generalisable performance for the real world.\n\n\nDISCUSSION AND CONCLUSIONS\nConsistently performing endpoints for one demographic profile may lose generalisability during population shifts, potentially introducing endpoint bias. Increasing minority representation in the training data consistently improved generalisability. The endpoint bias trade-off reinforces the importance of diverse recruitment in OUD trials. The DAB Pipeline helps researchers systematically pinpoint when an endpoint may suffer 'performance variance' (i.e., bias). As an open-source tool, it promotes transparent endpoint evaluation and supports selecting demographically invariant OUD endpoints.",
      "citationCount": 0,
      "doi": "10.1111/dar.70085",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/ba76b32aea84d314f453c930356362698a71aa1f",
      "venue": "Drug and Alcohol Review",
      "journal": {
        "name": "Drug and alcohol review",
        "pages": "\n          e70085\n        ",
        "volume": "45 1"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e7093de7baa3d837d715e298370c521ca3d3ee1a",
      "title": "How a socio-technical approach to AI auditing can change how we understand and measure fairness in machine learning systems",
      "authors": [
        {
          "name": "G. G. Clavell",
          "authorId": "2101451"
        },
        {
          "name": "Ariane Aumaitre",
          "authorId": "2317026989"
        },
        {
          "name": "T. Calders",
          "authorId": "2067055961"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e7093de7baa3d837d715e298370c521ca3d3ee1a",
      "venue": "AIMMES",
      "journal": null,
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1d18347a505d31d7ad59c66a32f0c536623ff1d1",
      "title": "Distributive Justice as the Foundational Premise of Fair ML: Unification, Extension, and Interpretation of Group Fairness Metrics",
      "authors": [
        {
          "name": "J. Baumann",
          "authorId": "2058322501"
        },
        {
          "name": "Corinna Hertweck",
          "authorId": "1785374742"
        },
        {
          "name": "M. Loi",
          "authorId": "151501082"
        },
        {
          "name": "Christoph Heitz",
          "authorId": "151473392"
        }
      ],
      "year": 2022,
      "abstract": "Group fairness metrics are an established way of assessing the fairness of prediction-based decision-making systems. However, these metrics are still insufficiently linked to philosophical theories, and their moral meaning is often unclear. In this paper, we propose a comprehensive framework for group fairness metrics, which links them to more theories of distributive justice. The different group fairness metrics differ in their choices about how to measure the benefit or harm of a decision for the affected individuals, and what moral claims to benefits are assumed. Our unifying framework reveals the normative choices associated with standard group fairness metrics and allows an interpretation of their moral substance. In addition, this broader view provides a structure for the expansion of standard fairness metrics that we find in the literature. This expansion allows addressing several criticisms of standard group fairness metrics, specifically: (1) they are parity-based, i.e., they demand some form of equality between groups, which may sometimes be detrimental to marginalized groups; (2) they only compare decisions across groups but not the resulting consequences for these groups; and (3) the full breadth of the distributive justice literature is not sufficiently represented.",
      "citationCount": 11,
      "doi": "10.48550/arXiv.2206.02897",
      "arxivId": "2206.02897",
      "url": "https://www.semanticscholar.org/paper/1d18347a505d31d7ad59c66a32f0c536623ff1d1",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2206.02897"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a51cbfa482ed9837994435c7d12ffc101df221a0",
      "title": "Do Automatic Factuality Metrics Measure Factuality? A Critical Evaluation",
      "authors": [
        {
          "name": "S. Ramprasad",
          "authorId": "98806251"
        },
        {
          "name": "Byron C. Wallace",
          "authorId": "2282965150"
        }
      ],
      "year": 2024,
      "abstract": "Modern LLMs can now produce highly readable abstractive summaries, to the point that traditional automated metrics for evaluating summary quality, such as ROUGE, have saturated. However, LLMs still sometimes introduce inaccuracies into summaries, i.e., information inconsistent with or unsupported by the corresponding source. Measuring the occurrence of these often subtle factual inconsistencies automatically has proved challenging. This in turn has motivated development of metrics intended to measure the factual consistency of generated summaries against sources. But are these approaches measuring what they purport to? Or are they mostly exploiting artifacts? In this work, we stress test a range of automatic factuality metrics, including specialized models and LLM-based prompting methods, to probe what they actually capture. Using a shallow classifier to separate ``easy''examples for factual evaluation where surface features suffice from ``hard''cases requiring deeper reasoning, we find that all metrics show substantial performance drops on the latter. Furthermore, some metrics are more sensitive to benign, fact-preserving edits than to factual corrections. Building on this observation, we demonstrate that most automatic factuality metrics can be gamed, i.e., their scores can be artificially inflated by appending innocuous, content-free sentences to summaries. Among the metrics tested, the prompt based ChatGPT-DA approach is the most robust and reliable. However, this comes with a notable caveat: Prompting LLMs to assess factuality may overly rely on their parametric knowledge rather than the provided reference when making judgments. Taken together, our findings call into question the reliability of current factuality metrics and prompt a broader reflection on what these metrics are truly measuring.",
      "citationCount": 8,
      "doi": "10.48550/arXiv.2411.16638",
      "arxivId": "2411.16638",
      "url": "https://www.semanticscholar.org/paper/a51cbfa482ed9837994435c7d12ffc101df221a0",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2411.16638"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e163a71a6def72e27cb2f51fe21b9382051cbab5",
      "title": "Revisiting Group Fairness Metrics: The Effect of Networks",
      "authors": [
        {
          "name": "Anay Mehrotra",
          "authorId": "152276166"
        },
        {
          "name": "Jeffery Sachs",
          "authorId": "144275165"
        },
        {
          "name": "L. E. Celis",
          "authorId": "144776615"
        }
      ],
      "year": 2022,
      "abstract": "An increasing amount of work studies fairness in socio-technical settings from a computational perspective. This work has introduced a variety of metrics to measure fairness in different settings. Most of these metrics, however, do not account for the interactions between individuals or evaluate any underlying network's effect on the outcomes measured. While a wide body of work studies the organization of individuals into a network structure and how individuals access resources in networks, the impact of network structure on fairness has been largely unexplored. We introduce templates for group fairness metrics that account for network structure. More specifically, we present two types of group fairness metrics that measure distinct yet complementary forms of bias in networks. The first type of metric evaluates how access to others in the network is distributed across groups. The second type of metric evaluates how groups distribute their interactions across other groups, and hence captures inter-group biases. We find that ignoring the network can lead to spurious fairness evaluations by either not capturing imbalances in influence and reach illuminated by the first type of metric, or by overlooking interaction biases as evaluated by the second type of metric. Our empirical study illustrates these pronounced differences between network and non-network evaluations of fairness.",
      "citationCount": 8,
      "doi": "10.1145/3555100",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e163a71a6def72e27cb2f51fe21b9382051cbab5",
      "venue": "Proc. ACM Hum. Comput. Interact.",
      "journal": {
        "name": "Proceedings of the ACM on Human-Computer Interaction",
        "pages": "1 - 29",
        "volume": "6"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d9424371662717c8981eef3d501d7ce59c66ce77",
      "title": "On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations",
      "authors": [
        {
          "name": "Yang Trista Cao",
          "authorId": "48696491"
        },
        {
          "name": "Yada Pruksachatkun",
          "authorId": "100984698"
        },
        {
          "name": "Kai-Wei Chang",
          "authorId": "2782886"
        },
        {
          "name": "Rahul Gupta",
          "authorId": "2139538015"
        },
        {
          "name": "Varun Kumar",
          "authorId": "40574366"
        },
        {
          "name": "J. Dhamala",
          "authorId": "3475586"
        },
        {
          "name": "A. Galstyan",
          "authorId": "143728483"
        }
      ],
      "year": 2022,
      "abstract": "Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.",
      "citationCount": 104,
      "doi": "10.48550/arXiv.2203.13928",
      "arxivId": "2203.13928",
      "url": "https://www.semanticscholar.org/paper/d9424371662717c8981eef3d501d7ce59c66ce77",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2203.13928"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "82a1b6e4a84a8481d9ea89ad58728973ee4a0e6a",
      "title": "Scoping Fairness Objectives and Identifying Fairness Metrics for Recommender Systems: The Practitioners\u2019 Perspective",
      "authors": [
        {
          "name": "Jessie J. Smith",
          "authorId": "2109847276"
        },
        {
          "name": "Lex Beattie",
          "authorId": "2178708575"
        },
        {
          "name": "H. Cramer",
          "authorId": "39649662"
        }
      ],
      "year": 2023,
      "abstract": "Measuring and assessing the impact and \u201cfairness\u2019\u2019 of recommendation algorithms is central to responsible recommendation efforts. However, the complexity of fairness definitions and the proliferation of fairness metrics in research literature have led to a complex decision-making space. This environment makes it challenging for practitioners to operationalize and pick metrics that work within their unique context. This suggests that practitioners require more decision-making support, but it is not clear what type of support would be beneficial. We conducted a literature review of 24 papers to gather metrics introduced by the research community for measuring fairness in recommendation and ranking systems. We organized these metrics into a \u2018decision-tree style\u2019 support framework designed to help practitioners scope fairness objectives and identify fairness metrics relevant to their recommendation domain and application context. To explore the feasibility of this approach, we conducted 15 semi-structured interviews using this framework to assess which challenges practitioners may face when scoping fairness objectives and metrics for their system, and which further support may be needed beyond such tools.",
      "citationCount": 26,
      "doi": "10.1145/3543507.3583204",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/82a1b6e4a84a8481d9ea89ad58728973ee4a0e6a",
      "venue": "The Web Conference",
      "journal": {
        "name": "Proceedings of the ACM Web Conference 2023"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "52629ce70c164d4096c2afafb060bdb47fe9fc07",
      "title": "Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics",
      "authors": [
        {
          "name": "Debjani Saha",
          "authorId": "6972492"
        },
        {
          "name": "Candice Schumann",
          "authorId": "29326342"
        },
        {
          "name": "Duncan C. McElfresh",
          "authorId": "9561277"
        },
        {
          "name": "John P. Dickerson",
          "authorId": "1718974"
        },
        {
          "name": "Michelle L. Mazurek",
          "authorId": "37176218"
        },
        {
          "name": "Michael Carl Tschantz",
          "authorId": "2877185"
        }
      ],
      "year": 2019,
      "abstract": null,
      "citationCount": 69,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/52629ce70c164d4096c2afafb060bdb47fe9fc07",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "pages": "8377-8387"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "f4e766da620d7d80f25152e12b0cf03e76bcdcc2",
      "title": "On the Choice of Fairness: Finding Representative Fairness Metrics for a Given Context",
      "authors": [
        {
          "name": "H. Anahideh",
          "authorId": "2126533826"
        },
        {
          "name": "Nazanin Nezami",
          "authorId": "2029527236"
        },
        {
          "name": "Abolfazl Asudeh",
          "authorId": "1717283"
        }
      ],
      "year": 2021,
      "abstract": null,
      "citationCount": 6,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f4e766da620d7d80f25152e12b0cf03e76bcdcc2",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2109.05697"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "75aaedff55137923718ca01909666c802854e073",
      "title": "You cannot improve what you do not measure: A triangulation study of software security metrics",
      "authors": [
        {
          "name": "Arina Kudriavtseva",
          "authorId": "2192822239"
        },
        {
          "name": "Olga Gadyatskaya",
          "authorId": "2292056209"
        }
      ],
      "year": 2024,
      "abstract": "When organizations invest in security, they need to monitor if their security program is effective and helps them remediate vulnerabilities. For this purpose, many organizations collect security metrics. In this paper, we investigate the current state-of-the-art and state-of-practice of security metrics used to measure security across all phases of software development lifecycle (SDLC). The study focused on gaining multiple perspectives on software security measurement. To this end, we performed a triangulation study that compared security metrics proposed in the academic literature, metrics mentioned in grey literature aimed at software practitioners, and metrics elicited in a focus group workshop with secure software engineering experts. Our study reports two critical insights. First, our results reveal a significant discrepancy in the utilization of metrics across the different SDLC stages. While the academic literature proposes a comprehensive spectrum, encompassing metrics for both early and late SDLC phases, industry predominantly focuses on the later SDLC stages. This highlights an industry-wide tendency to prioritize security measurement later in the software development process, potentially overlooking early-stage concerns. Second, our study sheds light on the practitioners' dissatisfaction with the current security metrics. This dissatisfaction highlights the industry's need for more nuanced and effective metrics that can offer both quantitative and qualitative insights to assess security of a software development program.",
      "citationCount": 6,
      "doi": "10.1145/3605098.3635892",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/75aaedff55137923718ca01909666c802854e073",
      "venue": "ACM Symposium on Applied Computing",
      "journal": {
        "name": "Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "0b2c1dd60b982829449d8c96bed3aecdb88ff2df",
      "title": "Reliability and Validity of Image-Based and Self-Reported Skin Phenotype Metrics",
      "authors": [
        {
          "name": "John J. Howard",
          "authorId": "22024318"
        },
        {
          "name": "Yevgeniy B. Sirotin",
          "authorId": "2733054"
        },
        {
          "name": "Jerry L. Tipton",
          "authorId": "81453626"
        },
        {
          "name": "A. Vemury",
          "authorId": "52182134"
        }
      ],
      "year": 2021,
      "abstract": "With increasing adoption of face recognition systems, it is important to ensure adequate performance of these technologies across demographic groups, such as race, age, and gender. Recently, phenotypes such as skin tone, have been proposed as superior alternatives to traditional race categories when exploring performance differentials. However, there is little consensus regarding how to appropriately measure skin tone in evaluations of biometric performance or in AI more broadly. Biometric researchers have estimated skin tone, most notably focusing on face area lightness measures (FALMs) using automated color analysis or Fitzpatrick Skin Types (FST). These estimates have generally been based on the same images used to assess biometric performance, which are often collected using unknown and varied devices, at unknown and varied times, and under unknown and varied environmental conditions. In this study, we explore the relationship between FALMs estimated from images and ground-truth skin readings collected using a colormeter device specifically designed to measure human skin. FALMs estimated from different images of the same individual varied significantly relative to ground-truth FALMs. This variation was only reduced by greater control of acquisition (camera, background, and environmental conditions). Next, we compare ground-truth FALMs to FST categories obtained using the standard, in-person, medical survey. We found that there was relatively little change in ground-truth FALMs across different FST category and that FST correlated more with self-reported race than with ground-truth FALMs. These findings show FST is poorly predictive of skin tone and should not be used as such in evaluations of computer vision applications. Finally, using modeling, we show that when face recognition performance is driven by FALMs and independent of race, noisy FALM estimates can lead to erroneous selection of race as a key correlate of biometric performance. These results demonstrate that measures of skin type for biometric performance evaluations must come from objective, characterized, and controlled sources. Further, despite this being a currently practiced approach, estimating FST categories and FALMs from uncontrolled imagery does not provide an appropriate measure of skin tone.",
      "citationCount": 32,
      "doi": "10.1109/tbiom.2021.3123550",
      "arxivId": "2106.11240",
      "url": "https://www.semanticscholar.org/paper/0b2c1dd60b982829449d8c96bed3aecdb88ff2df",
      "venue": "IEEE Transactions on Biometrics Behavior and Identity Science",
      "journal": {
        "name": "IEEE Transactions on Biometrics, Behavior, and Identity Science",
        "pages": "550-560",
        "volume": "3"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "cb91f670ff0653480501647a8e306c44d94b082e",
      "title": "How Do Algorithmic Fairness Metrics Align with Human Judgement? A Mixed-Initiative System for Contextualized Fairness Assessment",
      "authors": [
        {
          "name": "Rares Constantin",
          "authorId": "2197172415"
        },
        {
          "name": "Moritz D\u00fcck",
          "authorId": "2197167458"
        },
        {
          "name": "Anton Alexandrov",
          "authorId": "2196145936"
        },
        {
          "name": "Patrik Mato\u0161evi\u0107",
          "authorId": "2197170951"
        },
        {
          "name": "Daphna Keidar",
          "authorId": "2000487110"
        },
        {
          "name": "Mennatallah El-Assady",
          "authorId": "1401917601"
        }
      ],
      "year": 2022,
      "abstract": "Fairness evaluation presents a challenging problem in machine learning, and is usually restricted to the exploration of various metrics that attempt to quantify algorithmic fairness. However, due to cultural and perceptual biases, such metrics are often not powerful enough to accurately capture what people perceive as fair or unfair. To close the gap between human judgement and automated fairness evaluation, we develop a mixed-initiative system named FairAlign, where laypeople assess the fairness of different classification models by analyzing expressive and interactive visualizations of data. Using the aggregated qualitative feedback, data scientists and machine learning experts can examine the similarities and the differences between predefined fairness metrics and human judgement in a contextualized setting. To validate the utility of our system, we conducted a small study on a socially relevant classification task, where six people were asked to assess the fairness of multiple prediction models using the provided visualizations. The results show that our platform is able to give valuable guidance for model evaluation in case of otherwise contradicting and indecisive metrics for algorithmic fairness.",
      "citationCount": 3,
      "doi": "10.1109/TREX57753.2022.00005",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/cb91f670ff0653480501647a8e306c44d94b082e",
      "venue": "2022 IEEE Workshop on TRust and EXpertise in Visual Analytics (TREX)",
      "journal": {
        "name": "2022 IEEE Workshop on TRust and EXpertise in Visual Analytics (TREX)",
        "pages": "1-7"
      },
      "publicationTypes": null
    },
    {
      "paperId": "47c204b26467a6c86fd4ad34bc11fc77940c1af1",
      "title": "Fairness in machine learning: against false positive rate equality as a measure of fairness",
      "authors": [
        {
          "name": "R. Long",
          "authorId": "30535644"
        }
      ],
      "year": 2020,
      "abstract": "\nAs machine learning informs increasingly consequential decisions, different metrics have been proposed for measuring algorithmic bias or unfairness. Two popular \u201cfairness measures\u201d are calibration and equality of false positive rate. Each measure seems intuitively important, but notably, it is usually impossible to satisfy both measures. For this reason, a large literature in machine learning speaks of a \u201cfairness tradeoff\u201d between these two measures. This framing assumes that both measures are, in fact, capturing something important. To date, philosophers have seldom examined this crucial assumption, and examined to what extent each measure actually tracks a normatively important property. This makes this inevitable statistical conflict \u2013 between calibration and false positive rate equality \u2013 an important topic for ethics. In this paper, I give an ethical framework for thinking about these measures and argue that, contrary to initial appearances, false positive rate equality is in fact morally irrelevant and does not measure fairness.",
      "citationCount": 41,
      "doi": "10.1163/17455243-20213439",
      "arxivId": "2007.02890",
      "url": "https://www.semanticscholar.org/paper/47c204b26467a6c86fd4ad34bc11fc77940c1af1",
      "venue": "Journal of Moral Philosophy",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2007.02890"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "058dee85d522f6565fe1502cafcf9a5e3f6a6f0e",
      "title": "Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models",
      "authors": [
        {
          "name": "Pieter Delobelle",
          "authorId": "150258834"
        },
        {
          "name": "E. Tokpo",
          "authorId": "2145259446"
        },
        {
          "name": "T. Calders",
          "authorId": "1709830"
        },
        {
          "name": "Bettina Berendt",
          "authorId": "2990203"
        }
      ],
      "year": 2022,
      "abstract": "An increasing awareness of biased patterns in natural language processing resources such as BERT has motivated many metrics to quantify \u2018bias\u2019 and \u2018fairness\u2019 in these resources. However, comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the literature on fairness metrics for pre-trained language models and experimentally evaluate compatibility, including both biases in language models and in their downstream tasks. We do this by combining traditional literature survey, correlation analysis and empirical evaluations. We find that many metrics are not compatible with each other and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. We also see no tangible evidence of intrinsic bias relating to extrinsic bias. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective. To improve future comparisons and fairness evaluations, we recommend to avoid embedding-based metrics and focus on fairness evaluations in downstream tasks.",
      "citationCount": 93,
      "doi": "10.18653/v1/2022.naacl-main.122",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/058dee85d522f6565fe1502cafcf9a5e3f6a6f0e",
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "journal": {
        "pages": "1693-1706"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "aeeb0641ee52607c58826fafdcdb97aebf7e235c",
      "title": "Addressing multiple metrics of group fairness in data-driven decision making",
      "authors": [
        {
          "name": "M. Miron",
          "authorId": "144057160"
        },
        {
          "name": "Song\u00fcl Tolan",
          "authorId": "12092873"
        },
        {
          "name": "Emilia G\u00f3mez",
          "authorId": "1740615089"
        },
        {
          "name": "C. Castillo",
          "authorId": "2065333595"
        }
      ],
      "year": 2020,
      "abstract": "The Fairness, Accountability, and Transparency in Machine Learning (FAT-ML) literature proposes a varied set of group fairness metrics to measure discrimination against socio-demographic groups that are characterized by a protected feature, such as gender or race.Such a system can be deemed as either fair or unfair depending on the choice of the metric. Several metrics have been proposed, some of them incompatible with each other.We do so empirically, by observing that several of these metrics cluster together in two or three main clusters for the same groups and machine learning methods. In addition, we propose a robust way to visualize multidimensional fairness in two dimensions through a Principal Component Analysis (PCA) of the group fairness metrics. Experimental results on multiple datasets show that the PCA decomposition explains the variance between the metrics with one to three components.",
      "citationCount": 9,
      "doi": null,
      "arxivId": "2003.04794",
      "url": "https://www.semanticscholar.org/paper/aeeb0641ee52607c58826fafdcdb97aebf7e235c",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2003.04794"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "39aa25abb5ea0037cb09993da4c1aa081d6d06f8",
      "title": "Toward Valid Measurement Of (Un)fairness For Generative AI: A Proposal For Systematization Through The Lens Of Fair Equality of Chances",
      "authors": [
        {
          "name": "K. Truong",
          "authorId": "2372638663"
        },
        {
          "name": "Annette Zimmermann",
          "authorId": "2373046081"
        },
        {
          "name": "Hoda Heidari",
          "authorId": "2256993437"
        }
      ],
      "year": 2025,
      "abstract": "Disparities in the societal harms and impacts of Generative AI (GenAI) systems highlight the critical need for effective unfairness measurement approaches. While numerous benchmarks exist, designing valid measurements requires proper systematization of the unfairness construct. However, this process is often neglected, resulting in metrics that may mischaracterize unfairness by overlooking contextual nuances, thereby compromising the validity of the resulting measurements. Building on established (un)fairness measurement frameworks for predictive AI, this paper focuses on assessing and improving the validity of the measurement task. By extending existing conceptual work in political philosophy, we propose a novel framework for evaluating GenAI unfairness measurement through the lens of the Fair Equality of Chances framework. Our framework decomposes unfairness into three core constituents: the harm or benefit resulting from the system outcomes, morally arbitrary factors that should not lead to inequality in the distribution of harm or benefit, and the morally decisive factors, which distinguish subsets that can justifiably receive different treatments. By examining fairness through this structured lens, we integrate diverse notions of (un)fairness while accounting for the contextual dynamics that shape GenAI outcomes. We analyze factors contributing to each component and the appropriate processes to systematize and measure each in turn. This work establishes a foundation for developing more valid (un)fairness measurements for GenAI systems.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2507.04641",
      "arxivId": "2507.04641",
      "url": "https://www.semanticscholar.org/paper/39aa25abb5ea0037cb09993da4c1aa081d6d06f8",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.04641"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "ca6789df4ce150fec1898820a7e2ecc38e2e01da",
      "title": "LLMs on Trial: Evaluating Judicial Fairness for Large Language Models",
      "authors": [
        {
          "name": "Yiran Hu",
          "authorId": "2212045428"
        },
        {
          "name": "Zongyue Xue",
          "authorId": "2253400746"
        },
        {
          "name": "Haitao Li",
          "authorId": "2108590438"
        },
        {
          "name": "Siyuan Zheng",
          "authorId": "2374022118"
        },
        {
          "name": "Qingjing Chen",
          "authorId": "2347043166"
        },
        {
          "name": "Shaochun Wang",
          "authorId": "2373694579"
        },
        {
          "name": "Xihan Zhang",
          "authorId": "2373532621"
        },
        {
          "name": "Ning Zheng",
          "authorId": "2307916600"
        },
        {
          "name": "Yun Liu",
          "authorId": "2238545223"
        },
        {
          "name": "Qingyao Ai",
          "authorId": "2256982003"
        },
        {
          "name": "Yiqun Liu",
          "authorId": "2260835922"
        },
        {
          "name": "Charles L.A. Clarke",
          "authorId": "2351797159"
        },
        {
          "name": "Weixing Shen",
          "authorId": "2211946754"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) are increasingly used in high-stakes fields where their decisions impact rights and equity. However, LLMs'judicial fairness and implications for social justice remain underexplored. When LLMs act as judges, the ability to fairly resolve judicial issues is a prerequisite to ensure their trustworthiness. Based on theories of judicial fairness, we construct a comprehensive framework to measure LLM fairness, leading to a selection of 65 labels and 161 corresponding values. Applying this framework to the judicial system, we compile an extensive dataset, JudiFair, comprising 177,100 unique case facts. To achieve robust statistical inference, we develop three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and introduce a method to assess the overall fairness of multiple LLMs across various labels. Through experiments with 16 LLMs, we uncover pervasive inconsistency, bias, and imbalanced inaccuracy across models, underscoring severe LLM judicial unfairness. Particularly, LLMs display notably more pronounced biases on demographic labels, with slightly less bias on substance labels compared to procedure ones. Interestingly, increased inconsistency correlates with reduced biases, but more accurate predictions exacerbate biases. While we find that adjusting the temperature parameter can influence LLM fairness, model size, release date, and country of origin do not exhibit significant effects on judicial fairness. Accordingly, we introduce a publicly available toolkit containing all datasets and code, designed to support future research in evaluating and improving LLM fairness.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2507.10852",
      "arxivId": "2507.10852",
      "url": "https://www.semanticscholar.org/paper/ca6789df4ce150fec1898820a7e2ecc38e2e01da",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.10852"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "9b828aab5b242ee9c8699d4b215c71fbd7349361",
      "title": "The Neutrality Fallacy: When Algorithmic Fairness Interventions are (Not) Positive Action",
      "authors": [
        {
          "name": "Hilde J. P. Weerts",
          "authorId": "32345170"
        },
        {
          "name": "Rapha\u00eble Xenidis",
          "authorId": "115134985"
        },
        {
          "name": "Fabien Tarissan",
          "authorId": "1724314"
        },
        {
          "name": "Henrik Palmer Olsen",
          "authorId": "2297186269"
        },
        {
          "name": "Mykola Pechenizkiy",
          "authorId": "1691997"
        }
      ],
      "year": 2024,
      "abstract": "Various metrics and interventions have been developed to identify and mitigate unfair outputs of machine learning systems. While individuals and organizations have an obligation to avoid discrimination, the use of fairness-aware machine learning interventions has also been described as amounting to \u2018algorithmic positive action\u2019 under European Union (EU) non-discrimination law. As the Court of Justice of the European Union has been strict when it comes to assessing the lawfulness of positive action, this would impose a significant legal burden on those wishing to implement fair-ml interventions. In this paper, we propose that algorithmic fairness interventions often should be interpreted as a means to prevent discrimination, rather than a measure of positive action. Specifically, we suggest that this category mistake can often be attributed to neutrality fallacies: faulty assumptions regarding the neutrality of (fairness-aware) algorithmic decision-making. Our findings raise the question of whether a negative obligation to refrain from discrimination is sufficient in the context of algorithmic decision-making. Consequently, we suggest moving away from a duty to \u2018not do harm\u2019 towards a positive obligation to actively \u2018do no harm\u2019 as a more adequate framework for algorithmic decision-making and fair ml-interventions.",
      "citationCount": 9,
      "doi": "10.1145/3630106.3659025",
      "arxivId": "2404.12143",
      "url": "https://www.semanticscholar.org/paper/9b828aab5b242ee9c8699d4b215c71fbd7349361",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle"
      ]
    },
    {
      "paperId": "2ae72dde6506f9119ef38aff355bf7a918827bf0",
      "title": "Do internal software quality tools measure validated metrics?",
      "authors": [
        {
          "name": "May E. Nilson",
          "authorId": "93049918"
        },
        {
          "name": "Vard Antinyan",
          "authorId": "144869889"
        },
        {
          "name": "Lucas Gren",
          "authorId": "2385125"
        }
      ],
      "year": 2019,
      "abstract": "Internal software quality determines the maintainability of the software product and influences the quality in use. There is a plethora of metrics which purport to measure the internal quality of software, and these metrics are offered by static software analysis tools. To date, a number of reports have assessed the validity of these metrics. No data are available, however, on whether metrics offered by the tools are somehow validated in scientific studies. The current study covers this gap by providing data on which tools and how many validated metrics are provided. The results show that a range of metrics that the tools provided do not seem to be validated in the literature and that only a small percentage of metrics are validated in the provided tools.",
      "citationCount": 9,
      "doi": "10.1007/978-3-030-35333-9_50",
      "arxivId": "1909.09682",
      "url": "https://www.semanticscholar.org/paper/2ae72dde6506f9119ef38aff355bf7a918827bf0",
      "venue": "International Conference on Product Focused Software Process Improvement",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/1909.09682"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f69e12842637282b1c8136c5142aff899b9e37be",
      "title": "Reducing Population-level Inequality Can Improve Demographic Group Fairness: a Twitter Case Study",
      "authors": [
        {
          "name": "Avijit Ghosh",
          "authorId": "2257400786"
        },
        {
          "name": "Tomo Lazovich",
          "authorId": "2272887308"
        },
        {
          "name": "Kristian Lum",
          "authorId": "2320798724"
        },
        {
          "name": "Chris L. Wilson",
          "authorId": "2111228581"
        }
      ],
      "year": 2024,
      "abstract": "Many existing fairness metrics measure group-wise demographic disparities in system behavior or model performance. Calculating these metrics requires access to demographic information, which, in industrial settings, is often unavailable. By contrast, economic inequality metrics, such as the Gini coefficient, require no demographic data to measure. However, reductions in economic inequality do not necessarily correspond to reductions in demographic disparities. In this paper, we empirically explore the relationship between demographic-free inequality metrics -- such as the Gini coefficient -- and standard demographic bias metrics that measure group-wise model performance disparities specifically in the case of engagement inequality on Twitter. We analyze tweets from 174K users over the duration of 2021 and find that demographic-free impression inequality metrics are positively correlated with gender, race, and age disparities in the average case, and weakly (but still positively) correlated with demographic bias in the worst case. We therefore recommend inequality metrics as a potentially useful proxy measure of average group-wise disparities, especially in cases where such disparities cannot be measured directly. Based on these results, we believe they can be used as part of broader efforts to improve fairness between demographic groups in scenarios like content recommendation on social media.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2409.08135",
      "arxivId": "2409.08135",
      "url": "https://www.semanticscholar.org/paper/f69e12842637282b1c8136c5142aff899b9e37be",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2409.08135"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7a6386a97e06a5b19ea832f886d55764f4d6e7c4",
      "title": "Estimating Group Fairness Using Pairwise Similarity",
      "authors": [
        {
          "name": "Adisak Supeesun",
          "authorId": "9581023"
        },
        {
          "name": "Jakarin Chawachat",
          "authorId": "2076046"
        },
        {
          "name": "Vacharapat Mettanant",
          "authorId": "51907402"
        },
        {
          "name": "Jittat Fakcharoenphol",
          "authorId": "1762245"
        }
      ],
      "year": 2024,
      "abstract": "Ensuring group fairness in machine learning models requires measuring model fairness performance under various metrics, typically defined under explicit $g$ roup membership information. In FAccT'23, Liu, Do, Usunier, and Nickel proposed a \u201cgroup-free\u201d fairness measurement based on homophily in social networks. They derived the measurement based on generalized entropy indices, typically used to measure inequality in economics, using pairwise similarity estimation from social connections. This measurement has one major drawback, i.e., the unfairness quantity is group-weighted. While this gives a number that represents the overall picture, it might not give enough protection for minority groups. We generalize their approach based on pairwise similarity and define relaxed group fairness measurements based on many standard group fairness metrics. We provide theoretical justifications by proving that these measurements are indeed relaxations of standard ones, and demonstrate the practicality of our approach with experiments.",
      "citationCount": 0,
      "doi": "10.1109/JCSSE61278.2024.10613692",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7a6386a97e06a5b19ea832f886d55764f4d6e7c4",
      "venue": "International Joint Conference on Computer Science and Software Engineering",
      "journal": {
        "name": "2024 21st International Joint Conference on Computer Science and Software Engineering (JCSSE)",
        "pages": "343-350"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "93d30163c48e3ef77ec6527c6170d14a705ad4af",
      "title": "Choosing the right (HR) metrics: digital data for capturing team proactivity and determinants of content validity",
      "authors": [
        {
          "name": "Greta Ontrup",
          "authorId": "69877281"
        },
        {
          "name": "Pia Sophie Schempp",
          "authorId": "2140199809"
        },
        {
          "name": "A. Kluge",
          "authorId": "2221155"
        }
      ],
      "year": 2021,
      "abstract": "PurposeThe purpose of this paper is to explore how positive organizational behaviors, specifically team proactivity, can be captured through digital data and what determines content validity of these data. The aim is to enable scientifically rigorous HR analytics projects for measuring and managing organizational behavior.Design/methodology/approachResults are derived from interview data (N\u00a0=\u00a024) with team members, HR professionals and consultants of HR software.FindingsBased on inductive qualitative content analysis, the authors clustered six data types generated/recorded by 13 different technological applications that were proposed to be informative of team proactivity. Four determinants of content validity were derived.Practical implicationsThe overview of technological applications and resulting data types can stimulate diverse HR analytics projects, which can contribute to organizational performance. The authors suggest ways to control for the threats to content validity in the design of HR analytics or research projects.Originality/valueHR analytics projects in the application field of managing organizational behavior are rare. This paper provides starting points for choosing data to measure team proactivity as one form of organizational behavior and guidelines for ensuring their validity.",
      "citationCount": 12,
      "doi": "10.1108/joepp-03-2021-0064",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/93d30163c48e3ef77ec6527c6170d14a705ad4af",
      "venue": "Journal of Organizational Effectiveness",
      "journal": {
        "name": "Journal of Organizational Effectiveness: People and Performance"
      },
      "publicationTypes": [
        "Review"
      ]
    },
    {
      "paperId": "4d947e5d29e3c05c5c71820c2a381187f58551b8",
      "title": "Development and application of novel performance validity metrics for computerized neurocognitive batteries",
      "authors": [
        {
          "name": "J. C. Scott",
          "authorId": "2235032328"
        },
        {
          "name": "T. Moore",
          "authorId": "21858452"
        },
        {
          "name": "D. Roalf",
          "authorId": "2877005"
        },
        {
          "name": "T. Satterthwaite",
          "authorId": "2213003"
        },
        {
          "name": "D. Wolf",
          "authorId": "33506831"
        },
        {
          "name": "Allison M Port",
          "authorId": "16326558"
        },
        {
          "name": "Ellyn R. Butler",
          "authorId": "47512325"
        },
        {
          "name": "K. Ruparel",
          "authorId": "2023951"
        },
        {
          "name": "C. Nievergelt",
          "authorId": "4352061"
        },
        {
          "name": "V. Risbrough",
          "authorId": "6160867"
        },
        {
          "name": "D. Baker",
          "authorId": "39763523"
        },
        {
          "name": "R. Gur",
          "authorId": "2406788"
        },
        {
          "name": "R. Gur",
          "authorId": "144762538"
        }
      ],
      "year": 2021,
      "abstract": "Abstract Objectives: Data from neurocognitive assessments may not be accurate in the context of factors impacting validity, such as disengagement, unmotivated responding, or intentional underperformance. Performance validity tests (PVTs) were developed to address these phenomena and assess underperformance on neurocognitive tests. However, PVTs can be burdensome, rely on cutoff scores that reduce information, do not examine potential variations in task engagement across a battery, and are typically not well-suited to acquisition of large cognitive datasets. Here we describe the development of novel performance validity measures that could address some of these limitations by leveraging psychometric concepts using data embedded within the Penn Computerized Neurocognitive Battery (PennCNB). Methods: We first developed these validity measures using simulations of invalid response patterns with parameters drawn from real data. Next, we examined their application in two large, independent samples: 1) children and adolescents from the Philadelphia Neurodevelopmental Cohort (n = 9498); and 2) adult servicemembers from the Marine Resiliency Study-II (n = 1444). Results: Our performance validity metrics detected patterns of invalid responding in simulated data, even at subtle levels. Furthermore, a combination of these metrics significantly predicted previously established validity rules for these tests in both developmental and adult datasets. Moreover, most clinical diagnostic groups did not show reduced validity estimates. Conclusions: These results provide proof-of-concept evidence for multivariate, data-driven performance validity metrics. These metrics offer a novel method for determining the performance validity for individual neurocognitive tests that is scalable, applicable across different tests, less burdensome, and dimensional. However, more research is needed into their application.",
      "citationCount": 7,
      "doi": "10.1017/S1355617722000893",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/4d947e5d29e3c05c5c71820c2a381187f58551b8",
      "venue": "Journal of the International Neuropsychological Society",
      "journal": {
        "name": "Journal of the International Neuropsychological Society",
        "pages": "789 - 797",
        "volume": "29"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d984e5d42a251798f03ba62fc30fe0fa321f651c",
      "title": "Ethical AI on the Waitlist: Group Fairness Evaluation of LLM-Aided Organ Allocation",
      "authors": [
        {
          "name": "Hannah Murray",
          "authorId": "2354177895"
        },
        {
          "name": "Brian Hyeongseok Kim",
          "authorId": "2354183427"
        },
        {
          "name": "Isabelle Lee",
          "authorId": "2267033397"
        },
        {
          "name": "Jason Byun",
          "authorId": "2354167362"
        },
        {
          "name": "Dani Yogatama",
          "authorId": "2261674070"
        },
        {
          "name": "Evi Micha",
          "authorId": "2354178219"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) are becoming ubiquitous, promising automation even in high-stakes scenarios. However, existing evaluation methods often fall short -- benchmarks saturate, accuracy-based metrics are overly simplistic, and many inherently ambiguous problems lack a clear ground truth. Given these limitations, evaluating fairness becomes complex. To address this, we reframe fairness evaluation using Borda scores, a method from voting theory, as a nuanced yet interpretable metric for measuring fairness. Using organ allocation as a case study, we introduce two tasks: (1) Choose-One and (2) Rank-All. In Choose-One, LLMs select a single candidate for a kidney, and we assess fairness across demographics using proportional parity. In Rank-All, LLMs rank all candidates for a kidney, reflecting real-world allocation processes. Since traditional fairness metrics do not account for ranking, we propose a novel application of Borda scoring to capture biases. Our findings highlight the potential of voting-based metrics to provide a richer, more multifaceted evaluation of LLM fairness.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2504.03716",
      "arxivId": "2504.03716",
      "url": "https://www.semanticscholar.org/paper/d984e5d42a251798f03ba62fc30fe0fa321f651c",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.03716"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7f51bb3a2647a5c2df19289026eb93c10b7cd69b",
      "title": "Joint Evaluation of Fairness and Relevance in Recommender Systems with Pareto Frontier",
      "authors": [
        {
          "name": "Theresia Veronika Rampisela",
          "authorId": "2264993806"
        },
        {
          "name": "Tuukka Ruotsalo",
          "authorId": "2084101"
        },
        {
          "name": "Maria Maistro",
          "authorId": "1954475"
        },
        {
          "name": "Christina Lioma",
          "authorId": "2276440036"
        }
      ],
      "year": 2025,
      "abstract": "Fairness and relevance are two important aspects of recommender systems (RSs). Typically, they are evaluated either (i) separately by individual measures of fairness and relevance, or (ii) jointly using a single measure that accounts for fairness with respect to relevance. However, approach (i) often does not provide a reliable joint estimate of the goodness of the models, as it has two different best models: one for fairness and another for relevance. Approach (ii) is also problematic because these measures tend to be ad-hoc and do not relate well to traditional relevance measures, like NDCG. Motivated by this, we present a new approach for jointly evaluating fairness and relevance in RSs: Distance to Pareto Frontier (DPFR). Given some user-item interaction data, we compute their Pareto frontier for a pair of existing relevance and fairness measures, and then use the distance from the frontier as a measure of the jointly achievable fairness and relevance. Our approach is modular and intuitive as it can be computed with existing measures. Experiments with 4 RS models, 3 re-ranking strategies, and 6 datasets show that existing metrics have inconsistent associations with our Pareto-optimal solution, making DPFR a more robust and theoretically well-founded joint measure for assessing fairness and relevance. Our code: https://github.com/theresiavr/DPFR-recsys-evaluation",
      "citationCount": 2,
      "doi": "10.1145/3696410.3714589",
      "arxivId": "2502.11921",
      "url": "https://www.semanticscholar.org/paper/7f51bb3a2647a5c2df19289026eb93c10b7cd69b",
      "venue": "The Web Conference",
      "journal": {
        "name": "Proceedings of the ACM on Web Conference 2025"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
      ]
    },
    {
      "paperId": "f6cf7114745495f384df06c821bac190dfaa89f4",
      "title": "Cauchy-Schwarz Fairness Regularizer",
      "authors": [
        {
          "name": "Yezi Liu",
          "authorId": "2319672217"
        },
        {
          "name": "Hanning Chen",
          "authorId": "2241550254"
        },
        {
          "name": "Wenjun Huang",
          "authorId": "2280107063"
        },
        {
          "name": "Yang Ni",
          "authorId": "2290732265"
        },
        {
          "name": "Mohsen Imani",
          "authorId": "2238651307"
        }
      ],
      "year": 2025,
      "abstract": "Group fairness in machine learning is often enforced by adding a regularizer that reduces the dependence between model predictions and sensitive attributes. However, existing regularizers are built on heterogeneous distance measures and design choices, which makes their behavior hard to reason about and their performance inconsistent across tasks. This raises a basic question: what properties make a good fairness regularizer? We address this question by first organizing existing in-process methods into three families: (i) matching prediction statistics across sensitive groups, (ii) aligning latent representations, and (iii) directly minimizing dependence between predictions and sensitive attributes. Through this lens, we identify desirable properties of the underlying distance measure, including tight generalization bounds, robustness to scale differences, and the ability to handle arbitrary prediction distributions. Motivated by these properties, we propose a Cauchy-Schwarz (CS) fairness regularizer that penalizes the empirical CS divergence between prediction distributions conditioned on sensitive groups. Under a Gaussian comparison, we show that CS divergence yields a tighter bound than Kullback-Leibler divergence, Maximum Mean Discrepancy, and the mean disparity used in Demographic Parity, and we discuss how these advantages translate to a distribution-free, kernel-based estimator that naturally extends to multiple sensitive attributes. Extensive experiments on four tabular benchmarks and one image dataset demonstrate that the proposed CS regularizer consistently improves Demographic Parity and Equal Opportunity metrics while maintaining competitive accuracy, and achieves a more stable utility-fairness trade-off across hyperparameter settings compared to prior regularizers.",
      "citationCount": 1,
      "doi": null,
      "arxivId": "2512.09467",
      "url": "https://www.semanticscholar.org/paper/f6cf7114745495f384df06c821bac190dfaa89f4",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "3e65f572322e192fe36ae52a8a7f025b0685dfc6",
      "title": "Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
      "authors": [
        {
          "name": "Su Lin Blodgett",
          "authorId": "3422038"
        },
        {
          "name": "Gilsinia Lopez",
          "authorId": "2057983441"
        },
        {
          "name": "Alexandra Olteanu",
          "authorId": "2064011617"
        },
        {
          "name": "Robert Sim",
          "authorId": "1562202621"
        },
        {
          "name": "Hanna M. Wallach",
          "authorId": "1831395"
        }
      ],
      "year": 2021,
      "abstract": "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system\u2019s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens\u2014originating from the social sciences\u2014to inventory a range of pitfalls that threaten these benchmarks\u2019 validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.",
      "citationCount": 346,
      "doi": "10.18653/v1/2021.acl-long.81",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/3e65f572322e192fe36ae52a8a7f025b0685dfc6",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "1004-1015"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "2d23402001da59509b1bc8ee629491e33f5cbbbf",
      "title": "Measuring what Matters: Construct Validity in Large Language Model Benchmarks",
      "authors": [
        {
          "name": "Andrew M. Bean",
          "authorId": "2242554313"
        },
        {
          "name": "R. Kearns",
          "authorId": "2348544925"
        },
        {
          "name": "Angelika Romanou",
          "authorId": "1910588458"
        },
        {
          "name": "Franziska Sofia Hafner",
          "authorId": "2205250896"
        },
        {
          "name": "Harry Mayne",
          "authorId": "2290011658"
        },
        {
          "name": "Jan Batzner",
          "authorId": "2366021434"
        },
        {
          "name": "Negar Foroutan",
          "authorId": "9737058"
        },
        {
          "name": "Chris Schmitz",
          "authorId": "2366011151"
        },
        {
          "name": "Karolina Korgul",
          "authorId": "2257035147"
        },
        {
          "name": "Hunar Batra",
          "authorId": "2290487774"
        },
        {
          "name": "Oishi Deb",
          "authorId": "2273066972"
        },
        {
          "name": "Emma Beharry",
          "authorId": "2391520984"
        },
        {
          "name": "Cornelius Emde",
          "authorId": "153438331"
        },
        {
          "name": "Thomas Foster",
          "authorId": "2345922826"
        },
        {
          "name": "Anna Gausen",
          "authorId": "2275054359"
        },
        {
          "name": "Mar\u00eda Grandury",
          "authorId": "2176184513"
        },
        {
          "name": "Simeng Han",
          "authorId": "3226782"
        },
        {
          "name": "Valentin Hofmann",
          "authorId": "2325952857"
        },
        {
          "name": "Lujain Ibrahim",
          "authorId": "2296990193"
        },
        {
          "name": "Hazel Kim",
          "authorId": "2401590388"
        },
        {
          "name": "Hannah Rose Kirk",
          "authorId": "90729626"
        },
        {
          "name": "Fangru Lin",
          "authorId": "2279899894"
        },
        {
          "name": "Gabrielle Kaili-May Liu",
          "authorId": "2383414185"
        },
        {
          "name": "Lennart Luettgau",
          "authorId": "2372808848"
        },
        {
          "name": "Jabez Magomere",
          "authorId": "2305622172"
        },
        {
          "name": "Jonathan Rystr\u00f8m",
          "authorId": "2346109360"
        },
        {
          "name": "Anna Sotnikova",
          "authorId": "2289611422"
        },
        {
          "name": "Yushi Yang",
          "authorId": "2316173013"
        },
        {
          "name": "Yilun Zhao",
          "authorId": "46316984"
        },
        {
          "name": "Adel Bibi",
          "authorId": "2257303816"
        },
        {
          "name": "A. Bosselut",
          "authorId": "2284866282"
        },
        {
          "name": "Ronald Clark",
          "authorId": "2333506799"
        },
        {
          "name": "Arman Cohan",
          "authorId": "2266838179"
        },
        {
          "name": "Jakob Foerster",
          "authorId": "2345921728"
        },
        {
          "name": "Yarin Gal",
          "authorId": "2315116895"
        },
        {
          "name": "Scott A. Hale",
          "authorId": "1741886127"
        },
        {
          "name": "Inioluwa Deborah Raji",
          "authorId": "81316798"
        },
        {
          "name": "Chris Summerfield",
          "authorId": "2343744579"
        },
        {
          "name": "Philip H. S. Torr",
          "authorId": "2282534002"
        },
        {
          "name": "C. Ududec",
          "authorId": "11285234"
        },
        {
          "name": "Luc Rocher",
          "authorId": "21687784"
        },
        {
          "name": "Adam Mahdi",
          "authorId": "2380689923"
        }
      ],
      "year": 2025,
      "abstract": "Evaluating large language models (LLMs) is crucial for both assessing their capabilities and identifying safety or robustness issues prior to deployment. Reliably measuring abstract and complex phenomena such as'safety'and'robustness'requires strong construct validity, that is, having measures that represent what matters to the phenomenon. With a team of 29 expert reviewers, we conduct a systematic review of 445 LLM benchmarks from leading conferences in natural language processing and machine learning. Across the reviewed articles, we find patterns related to the measured phenomena, tasks, and scoring metrics which undermine the validity of the resulting claims. To address these shortcomings, we provide eight key recommendations and detailed actionable guidance to researchers and practitioners in developing LLM benchmarks.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2511.04703",
      "arxivId": "2511.04703",
      "url": "https://www.semanticscholar.org/paper/2d23402001da59509b1bc8ee629491e33f5cbbbf",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.04703"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "754290d634d20833c4f597c798e5670c35bbe838",
      "title": "Sustainable Development of Digital Financial Infrastructure: Evaluating Data Quality and Information Fairness in Cryptocurrency Exchanges",
      "authors": [
        {
          "name": "Jun-Kuan Tsao",
          "authorId": "2396232982"
        },
        {
          "name": "Chiang-Yu Cheng",
          "authorId": "2395988767"
        }
      ],
      "year": 2025,
      "abstract": "The quality of digital financial infrastructure directly impacts the fairness of information access for market participants, which is fundamental to the sustainable development of the entire financial ecosystem. As cryptocurrency markets operate globally 24/7 without interruption, ensuring equitable access to high-quality data is essential for promoting responsible investment and reducing market inequality. This study establishes a standardized data quality assessment framework for three major cryptocurrency exchanges to explore how data quality differences affect information fairness. Through systematic data collection conducted every 60 seconds for eight consecutive hours, 463 valid Bitcoin price data samples were successfully obtained, covering major Asian and European trading sessions. The research constructs a comprehensive Data Quality Index integrating three core dimensions of consistency, freshness, and availability metrics. The consistency component measures price synchronization between platforms by calculating relative price differences with a coefficient of 50,000 to capture micro-level variations. The freshness indicator evaluates information timeliness by measuring delays between API request time and exchange data timestamps using a stringent 0.2-second threshold. The availability metric assesses service reliability through success rate calculations. The final composite index employs weighted averaging with 40% for consistency, 40% for freshness, and 20% for availability. Results reveal that Binance ranks first with 92.50 points, followed by Bitget at 88.61 points and MEXC at 85.36 points, with the 7.14-point gap directly impacting information fairness. This study provides quantitative assessment tools for building sustainable digital financial ecosystems, supporting UN Sustainable Development Goals 9.1 and 10.5.",
      "citationCount": 0,
      "doi": "10.63665/gjis.v1.30",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/754290d634d20833c4f597c798e5670c35bbe838",
      "venue": "Glovento Journal of Integrated Studies",
      "journal": {
        "name": "Glovento Journal of Integrated Studies"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "48f88ef2876679410d8a67911759de2a29741f40",
      "title": "Navigating fairness measures and trade-offs",
      "authors": [
        {
          "name": "Stefan Buijsman",
          "authorId": "2051359358"
        }
      ],
      "year": 2023,
      "abstract": "To monitor and prevent bias in AI systems, we can use a wide range of (statistical) fairness measures. However, it is mathematically impossible to optimize all of these measures at the same time. In addition, optimizing a fairness measure often greatly reduces the accuracy of the system (Kozodoi et al., Eur J Oper Res 297:1083\u20131094, 2022). As a result, we need a substantive theory that informs us how to make these decisions and for what reasons. I show that by using Rawls\u2019 notion of justice as fairness, we can create a basis for navigating fairness measures and the accuracy trade-off. In particular, this leads to a principled choice focusing on both the most vulnerable groups and the type of fairness measure that has the biggest impact on that group. This also helps to close part of the gap between philosophical accounts of distributive justice and the fairness literature that has been observed by (Kuppler et al. Distributive justice and fairness metrics in automated decision-making: How much overlap is there? arXiv preprint arXiv:2105.01441, 2021), and to operationalise the value of fairness.",
      "citationCount": 15,
      "doi": "10.1007/s43681-023-00318-0",
      "arxivId": "2307.08484",
      "url": "https://www.semanticscholar.org/paper/48f88ef2876679410d8a67911759de2a29741f40",
      "venue": "AI and Ethics",
      "journal": {
        "name": "AI and Ethics",
        "pages": "1323 - 1334",
        "volume": "4"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 30,
  "errors": []
}
