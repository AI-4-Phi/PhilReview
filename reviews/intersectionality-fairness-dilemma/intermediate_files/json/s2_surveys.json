{
  "status": "success",
  "source": "semantic_scholar",
  "query": "fairness survey machine learning",
  "results": [
    {
      "paperId": "0090023afc66cd2741568599057f4e82b566137c",
      "title": "A Survey on Bias and Fairness in Machine Learning",
      "authors": [
        {
          "name": "Ninareh Mehrabi",
          "authorId": "51997673"
        },
        {
          "name": "Fred Morstatter",
          "authorId": "2775559"
        },
        {
          "name": "N. Saxena",
          "authorId": "51884035"
        },
        {
          "name": "Kristina Lerman",
          "authorId": "1782658"
        },
        {
          "name": "A. Galstyan",
          "authorId": "143728483"
        }
      ],
      "year": 2019,
      "abstract": "With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",
      "citationCount": 5220,
      "doi": "10.1145/3457607",
      "arxivId": "1908.09635",
      "url": "https://www.semanticscholar.org/paper/0090023afc66cd2741568599057f4e82b566137c",
      "venue": "ACM Computing Surveys",
      "journal": {
        "name": "ACM Computing Surveys (CSUR)",
        "pages": "1 - 35",
        "volume": "54"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "a8acd277f45bfe5b30123c38f73fcdf9dfe9390e",
      "title": "Privacy and Fairness in Machine Learning: A Survey",
      "authors": [
        {
          "name": "Sina Shaham",
          "authorId": "40221713"
        },
        {
          "name": "Arash Hajisafi",
          "authorId": "2209222310"
        },
        {
          "name": "Min Quan",
          "authorId": "2155747501"
        },
        {
          "name": "Dinh C. Nguyen",
          "authorId": "123547055"
        },
        {
          "name": "Bhaskar Krishnamachari",
          "authorId": "2174866863"
        },
        {
          "name": "Charith Peris",
          "authorId": "2341825754"
        },
        {
          "name": "Gabriel Ghinita",
          "authorId": "2182067502"
        },
        {
          "name": "Cyrus Shahabi",
          "authorId": "2258957849"
        },
        {
          "name": "P. Pathirana",
          "authorId": "2301180"
        }
      ],
      "year": 2025,
      "abstract": "Privacy and fairness are two crucial pillars of responsible artificial intelligence (AI) and trustworthy machine learning (ML). Each objective has been independently studied in the literature with the aim of reducing utility loss in achieving them. Despite the significant interest attracted from both academia and industry, there remains an immediate demand for more in-depth research to unravel how these two objectives can be simultaneously integrated into ML models. As opposed to well-accepted trade-offs, i.e., privacy-utility and fairness-utility, the interrelation between privacy and fairness is not well-understood. While some works suggest a trade-off between the two objective functions, there are others that demonstrate the alignment of these functions in certain scenarios. To fill this research gap, we provide a thorough review of privacy and fairness in ML, including supervised, unsupervised, semisupervised, and reinforcement learning. After examining and consolidating the literature on both objectives, we present a holistic survey on the impact of privacy on fairness, the impact of fairness on privacy, existing architectures, their interaction in application domains, and algorithms that aim to achieve both objectives while minimizing the utility sacrificed. Finally, we identify research challenges in achieving concurrently privacy and fairness in ML, particularly focusing on large language models.",
      "citationCount": 4,
      "doi": "10.1109/TAI.2025.3531326",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a8acd277f45bfe5b30123c38f73fcdf9dfe9390e",
      "venue": "IEEE Transactions on Artificial Intelligence",
      "journal": {
        "name": "IEEE Transactions on Artificial Intelligence",
        "pages": "1706-1726",
        "volume": "6"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "2c4817a327cc5e6cba4116a4ea2e748ec3f2bb80",
      "title": "Fairness in Graph Learning Augmented with Machine Learning: A Survey",
      "authors": [
        {
          "name": "Renqiang Luo",
          "authorId": "2298756911"
        },
        {
          "name": "Ziqi Xu",
          "authorId": "2358600235"
        },
        {
          "name": "Xikun Zhang",
          "authorId": "2358293062"
        },
        {
          "name": "Qing Qing",
          "authorId": "2358260525"
        },
        {
          "name": "Huafei Huang",
          "authorId": "2358447272"
        },
        {
          "name": "Enyan Dai",
          "authorId": "2358259422"
        },
        {
          "name": "Zhe Wang",
          "authorId": "2358407534"
        },
        {
          "name": "Bo Yang",
          "authorId": "2358568706"
        }
      ],
      "year": 2025,
      "abstract": "Augmenting specialised machine learning techniques into traditional graph learning models has achieved notable success across various domains, including federated graph learning, dynamic graph learning, and graph transformers. However, the intricate mechanisms of these specialised techniques introduce significant challenges in maintaining model fairness, potentially resulting in discriminatory outcomes in high-stakes applications such as recommendation systems, disaster response, criminal justice, and loan approval. This paper systematically examines the unique fairness challenges posed by Graph Learning augmented with Machine Learning (GL-ML). It highlights the complex interplay between graph learning mechanisms and machine learning techniques, emphasising how the augmentation of machine learning both enhances and complicates fairness. Additionally, we explore four critical techniques frequently employed to improve fairness in GL-ML methods. By thoroughly investigating the root causes and broader implications of fairness challenges in this rapidly evolving field, this work establishes a robust foundation for future research and innovation in GL-ML fairness.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2504.21296",
      "arxivId": "2504.21296",
      "url": "https://www.semanticscholar.org/paper/2c4817a327cc5e6cba4116a4ea2e748ec3f2bb80",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.21296"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "92accefd1d8e19e4498641d21eeb047082b90b8d",
      "title": "Distributed machine learning for next-generation communication networks: A survey on privacy, fairness, efficiency, and trade-offs",
      "authors": [
        {
          "name": "Wenqi Zhang",
          "authorId": "2378189038"
        },
        {
          "name": "D. Zhan",
          "authorId": "3431709"
        },
        {
          "name": "Haining Yu",
          "authorId": "2378568062"
        },
        {
          "name": "Langtong Zhang",
          "authorId": "2378950661"
        },
        {
          "name": "Bei Zhao",
          "authorId": "2378196549"
        },
        {
          "name": "Xuetao Du",
          "authorId": "2378190209"
        },
        {
          "name": "Zhihong Tian",
          "authorId": "2379617067"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.1016/j.inffus.2025.103657",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/92accefd1d8e19e4498641d21eeb047082b90b8d",
      "venue": "Information Fusion",
      "journal": {
        "name": "Inf. Fusion",
        "pages": "103657",
        "volume": "126"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "395ecfdf331573206d0d682dbbbc563ab796e464",
      "title": "Long-Term Fairness Inquiries and Pursuits in Machine Learning: A Survey of Notions, Methods, and Challenges",
      "authors": [
        {
          "name": "Usman Gohar",
          "authorId": "1386345955"
        },
        {
          "name": "Zeyu Tang",
          "authorId": "2125563094"
        },
        {
          "name": "Jialu Wang",
          "authorId": "46583583"
        },
        {
          "name": "Kun Zhang",
          "authorId": "2309259181"
        },
        {
          "name": "Peter Spirtes",
          "authorId": "2295467970"
        },
        {
          "name": "Yang Liu",
          "authorId": "2268439518"
        },
        {
          "name": "Lu Cheng",
          "authorId": "2331366892"
        }
      ],
      "year": 2024,
      "abstract": "The widespread integration of Machine Learning systems in daily life, particularly in high-stakes domains, has raised concerns about the fairness implications. While prior works have investigated static fairness measures, recent studies reveal that automated decision-making has long-term implications and that off-the-shelf fairness approaches may not serve the purpose of achieving long-term fairness. Additionally, the existence of feedback loops and the interaction between models and the environment introduces additional complexities that may deviate from the initial fairness goals. In this survey, we review existing literature on long-term fairness from different perspectives and present a taxonomy for long-term fairness studies. We highlight key challenges and consider future research directions, analyzing both current issues and potential further explorations.",
      "citationCount": 6,
      "doi": "10.48550/arXiv.2406.06736",
      "arxivId": "2406.06736",
      "url": "https://www.semanticscholar.org/paper/395ecfdf331573206d0d682dbbbc563ab796e464",
      "venue": "Trans. Mach. Learn. Res.",
      "journal": {
        "name": "Trans. Mach. Learn. Res.",
        "volume": "2025"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "d37abf344455cdcf992ebe1eb4cb27f8e3f53992",
      "title": "Connecting algorithmic fairness to quality dimensions in machine learning in official statistics and survey production",
      "authors": [
        {
          "name": "Patrick Oliver Schenk",
          "authorId": "2219040479"
        },
        {
          "name": "Christoph Kern",
          "authorId": "2284067045"
        }
      ],
      "year": 2024,
      "abstract": "National Statistical Organizations (NSOs) increasingly draw on Machine Learning (ML) to improve the timeliness and cost-effectiveness of their products. When introducing ML solutions, NSOs must ensure that high standards with respect to robustness, reproducibility, and accuracy are upheld as codified, e.g., in the Quality Framework for Statistical Algorithms (QF4SA; Yung et al. 2022, Statistical Journal of the IAOS). At the same time, a growing body of research focuses on fairness as a pre-condition of a safe deployment of ML to prevent disparate social impacts in practice. However, fairness has not yet been explicitly discussed as a quality aspect in the context of the application of ML at NSOs. We employ the QF4SA quality framework and present a mapping of its quality dimensions to algorithmic fairness. We thereby extend the QF4SA framework in several ways: First, we investigate the interaction of fairness with each of these quality dimensions. Second, we argue for fairness as its own, additional quality dimension, beyond what is contained in the QF4SA so far. Third, we emphasize and explicitly address data, both on its own and its interaction with applied methodology. In parallel with empirical illustrations, we show how our mapping can contribute to methodology in the domains of official statistics, algorithmic fairness, and trustworthy machine learning. Little to no prior knowledge of ML, fairness, and quality dimensions in official statistics is required as we provide introductions to these subjects. These introductions are also targeted to the discussion of quality dimensions and fairness.",
      "citationCount": 4,
      "doi": "10.1007/s11943-024-00344-2",
      "arxivId": "2402.09328",
      "url": "https://www.semanticscholar.org/paper/d37abf344455cdcf992ebe1eb4cb27f8e3f53992",
      "venue": "AStA Wirtschafts- und Sozialstatistisches Archiv",
      "journal": {
        "name": "AStA Wirtschafts- und Sozialstatistisches Archiv",
        "pages": "131 - 184",
        "volume": "18"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "fee8f63972906214b77f16cfeca0b93ee8f36ba2",
      "title": "Fairness in Machine Learning: A Survey",
      "authors": [
        {
          "name": "Simon Caton",
          "authorId": "2452441"
        },
        {
          "name": "C. Haas",
          "authorId": "152864672"
        }
      ],
      "year": 2020,
      "abstract": "When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.",
      "citationCount": 793,
      "doi": "10.1145/3616865",
      "arxivId": "2010.04053",
      "url": "https://www.semanticscholar.org/paper/fee8f63972906214b77f16cfeca0b93ee8f36ba2",
      "venue": "ACM Computing Surveys",
      "journal": {
        "name": "ACM Computing Surveys",
        "pages": "1 - 38",
        "volume": "56"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "2563d796b24e4fe1cc3ceed99bb9b2deb23edf46",
      "title": "Fair Machine Learning in Healthcare: A Survey",
      "authors": [
        {
          "name": "Qizhang Feng",
          "authorId": "2151233715"
        },
        {
          "name": "Mengnan Du",
          "authorId": "2258705352"
        },
        {
          "name": "Na Zou",
          "authorId": "2281308239"
        },
        {
          "name": "Xia Hu",
          "authorId": "2276449057"
        }
      ],
      "year": 2025,
      "abstract": "The digitization of healthcare data coupled with advances in computational capabilities has propelled the adoption of machine learning (ML) in healthcare. However, these methods can perpetuate or even exacerbate existing disparities, leading to fairness concerns such as the unequal distribution of resources and diagnostic inaccuracies among different demographic groups. Addressing these fairness problems is paramount to prevent further entrenchment of social injustices. In this survey, we analyze the intersection of fairness in ML and healthcare disparities. We adopt a framework based on the principles of distributive justice to categorize fairness concerns into two distinct classes: equal allocation and equal performance. We provide a critical review of the associated fairness metrics from a ML standpoint and examine biases and mitigation strategies across the stages of the ML lifecycle, discussing the relationship between biases and their countermeasures. The article concludes with a discussion on the pressing challenges that remain unaddressed in ensuring fairness in healthcare ML and proposes several new research directions that hold promise for developing ethical and equitable ML applications in healthcare.",
      "citationCount": 11,
      "doi": "10.1109/TAI.2024.3361836",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/2563d796b24e4fe1cc3ceed99bb9b2deb23edf46",
      "venue": "IEEE Transactions on Artificial Intelligence",
      "journal": {
        "name": "IEEE Transactions on Artificial Intelligence",
        "pages": "493-507",
        "volume": "6"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "a286ae7200a238770746969a8daeb9092f51ae36",
      "title": "Taming the Triangle: On the Interplays Between Fairness, Interpretability, and Privacy in Machine Learning",
      "authors": [
        {
          "name": "Julien Ferry",
          "authorId": "153242948"
        },
        {
          "name": "Ulrich A\u00efvodji",
          "authorId": "2279845359"
        },
        {
          "name": "S\u00e9bastien Gambs",
          "authorId": "2334007268"
        },
        {
          "name": "Marie-Jos\u00e9 Huguet",
          "authorId": "32237525"
        },
        {
          "name": "Mohamed Siala",
          "authorId": "2319420291"
        }
      ],
      "year": 2025,
      "abstract": "Machine learning techniques are increasingly used for high\u2010stakes decision\u2010making, such as college admissions, loan attribution, or recidivism prediction. Thus, it is crucial to ensure that the models learnt can be audited or understood by human users, do not create or reproduce discrimination or bias and do not leak sensitive information regarding their training data. Indeed, interpretability, fairness, and privacy are key requirements for the development of responsible machine learning, and all three have been studied extensively during the last decade. However, they were mainly considered in isolation, while in practice they interplay with each other, either positively or negatively. In this survey paper, we review the literature on the interactions between these three desiderata. More precisely, for each pairwise interaction, we summarize the identified synergies and tensions. These findings highlight several fundamental theoretical and empirical conflicts, while also demonstrating that jointly considering these different requirements is challenging when one aims at preserving a high level of utility. To solve this issue, we also discuss possible conciliation mechanisms, showing that a careful design can enable to successfully handle these different concerns in practice.",
      "citationCount": 4,
      "doi": "10.1111/coin.70113",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a286ae7200a238770746969a8daeb9092f51ae36",
      "venue": "International Conference on Climate Informatics",
      "journal": {
        "name": "Computational Intelligence",
        "volume": "41"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "f3c006b136105aae45c47df4c80a047dadf5052c",
      "title": "Secure Multi-Party Computation for Machine Learning: A Survey",
      "authors": [
        {
          "name": "Ian Zhou",
          "authorId": "1516202151"
        },
        {
          "name": "Farzad Tofigh",
          "authorId": "30609985"
        },
        {
          "name": "Massimo Piccardi",
          "authorId": "2276718962"
        },
        {
          "name": "M. Abolhasan",
          "authorId": "1776795"
        },
        {
          "name": "D. Franklin",
          "authorId": "34704346"
        },
        {
          "name": "J. Lipman",
          "authorId": "143861150"
        }
      ],
      "year": 2024,
      "abstract": "Machine learning is a powerful technology for extracting information from data of diverse nature and origin. As its deployment increasingly depends on data from multiple entities, ensuring privacy for these contributors becomes paramount for the integrity and fairness of machine learning endeavors. This review looks into the recent advancements in secure multi-party computation (SMPC) for machine learning, a pivotal technology championing data privacy. We evaluate these applications from various aspects, including security models, requirements, system types, and service models, aligning with the IEEE\u2019s recommended practices for SMPC. Broadly, SMPC systems are divided into two categories: homomorphic-based systems, which facilitate computations on encrypted data, ensuring data remains confidential, and secret sharing-based systems, which disseminate data across parties in fragmented shares. Our literature analysis highlights certain gaps, such as security requisites, streamlined information exchange, incentive structures, data authenticity, and operational efficiency. Recognizing these challenges lead to envisioning a holistic SMPC protocol tailored for machine learning applications.",
      "citationCount": 78,
      "doi": "10.1109/ACCESS.2024.3388992",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f3c006b136105aae45c47df4c80a047dadf5052c",
      "venue": "IEEE Access",
      "journal": {
        "name": "IEEE Access",
        "pages": "53881-53899",
        "volume": "12"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "aeaffe41e8ee8adc7aa2b58c45202b8c930ebc19",
      "title": "A fairness-aware machine learning framework for maternal health in Ghana: integrating explainability, bias mitigation, and causal inference for ethical AI deployment",
      "authors": [
        {
          "name": "Augustus Osborne",
          "authorId": "1490672816"
        },
        {
          "name": "Kobloobase O. Usani",
          "authorId": "2396705168"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1186/s13040-025-00505-1",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/aeaffe41e8ee8adc7aa2b58c45202b8c930ebc19",
      "venue": "BioData Mining",
      "journal": {
        "name": "BioData Mining",
        "volume": "19"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "35ba67f49e33b5391f5712e9b0457b5ffa6a7f3f",
      "title": "A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges",
      "authors": [
        {
          "name": "Usman Gohar",
          "authorId": "1386345955"
        },
        {
          "name": "Lu Cheng",
          "authorId": "144842921"
        }
      ],
      "year": 2023,
      "abstract": "The widespread adoption of Machine Learning systems, especially in more decision-critical applications such as criminal sentencing and bank loans, has led to increased concerns about fairness implications. Algorithms and metrics have been developed to mitigate and measure these discriminations. More recently, works have identified a more challenging form of bias called intersectional bias, which encompasses multiple sensitive attributes, such as race and gender, together. In this survey, we review the state-of-the-art in intersectional fairness. We present a taxonomy for intersectional notions of fairness and mitigation. Finally, we identify the key challenges and provide researchers with guidelines for future directions.",
      "citationCount": 57,
      "doi": "10.24963/ijcai.2023/742",
      "arxivId": "2305.06969",
      "url": "https://www.semanticscholar.org/paper/35ba67f49e33b5391f5712e9b0457b5ffa6a7f3f",
      "venue": "International Joint Conference on Artificial Intelligence",
      "journal": {
        "pages": "6619-6627"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "ebb0df8229e7dcf47f79c96e77b37e6fa3dae485",
      "title": "A survey on datasets for fairness\u2010aware machine learning",
      "authors": [
        {
          "name": "Tai Le Quy",
          "authorId": "1488731837"
        },
        {
          "name": "Arjun Roy",
          "authorId": "47762613"
        },
        {
          "name": "Vasileios Iosifidis",
          "authorId": "3176896"
        },
        {
          "name": "Eirini Ntoutsi",
          "authorId": "1804618"
        }
      ],
      "year": 2021,
      "abstract": "As decision\u2010making increasingly relies on machine learning (ML) and (big) data, the issue of fairness in data\u2010driven artificial intelligence systems is receiving increasing attention from both research and industry. A large variety of fairness\u2010aware ML solutions have been proposed which involve fairness\u2010related interventions in the data, learning algorithms, and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real\u2010world datasets used for fairness\u2010aware ML. We focus on tabular data as the most common data representation for fairness\u2010aware ML. We start our analysis by identifying relationships between the different attributes, particularly with respect to protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate interesting relationships using exploratory analysis.",
      "citationCount": 327,
      "doi": "10.1002/widm.1452",
      "arxivId": "2110.00530",
      "url": "https://www.semanticscholar.org/paper/ebb0df8229e7dcf47f79c96e77b37e6fa3dae485",
      "venue": "WIREs Data Mining Knowl. Discov.",
      "journal": {
        "name": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",
        "volume": "12"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "13d17338ba4b992fd5c0ab16725bbec5f25641ba",
      "title": "Trustworthy Machine Learning via Memorization and the Granular Long-Tail: A Survey on Interactions, Tradeoffs, and Beyond",
      "authors": [
        {
          "name": "Qiongxiu Li",
          "authorId": "2334530019"
        },
        {
          "name": "Xiaoyu Luo",
          "authorId": "2334696963"
        },
        {
          "name": "Yiyi Chen",
          "authorId": "2263765891"
        },
        {
          "name": "Johannes Bjerva",
          "authorId": "3336895"
        }
      ],
      "year": 2025,
      "abstract": "The role of memorization in machine learning (ML) has garnered significant attention, particularly as modern models are empirically observed to memorize fragments of training data. Previous theoretical analyses, such as Feldman's seminal work, attribute memorization to the prevalence of long-tail distributions in training data, proving it unavoidable for samples that lie in the tail of the distribution. However, the intersection of memorization and trustworthy ML research reveals critical gaps. While prior research in memorization in trustworthy ML has solely focused on class imbalance, recent work starts to differentiate class-level rarity from atypical samples, which are valid and rare intra-class instances. However, a critical research gap remains: current frameworks conflate atypical samples with noisy and erroneous data, neglecting their divergent impacts on fairness, robustness, and privacy. In this work, we conduct a thorough survey of existing research and their findings on trustworthy ML and the role of memorization. More and beyond, we identify and highlight uncharted gaps and propose new revenues in this research direction. Since existing theoretical and empirical analyses lack the nuances to disentangle memorization's duality as both a necessity and a liability, we formalize three-level long-tail granularity - class imbalance, atypicality, and noise - to reveal how current frameworks misapply these levels, perpetuating flawed solutions. By systematizing this granularity, we draw a roadmap for future research. Trustworthy ML must reconcile the nuanced trade-offs between memorizing atypicality for fairness assurance and suppressing noise for robustness and privacy guarantee. Redefining memorization via this granularity reshapes the theoretical foundation for trustworthy ML, and further affords an empirical prerequisite for models that align performance with societal trust.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2503.07501",
      "arxivId": "2503.07501",
      "url": "https://www.semanticscholar.org/paper/13d17338ba4b992fd5c0ab16725bbec5f25641ba",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.07501"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "0b0915f26c701ff03c8dd4d52934a62b45c67fd2",
      "title": "Fairness in Machine Learning: A Survey Part 1",
      "authors": [
        {
          "name": "Karen Roberts-Licklider",
          "authorId": "2298041231"
        },
        {
          "name": "Theodore B. Trafalis",
          "authorId": "2337205607"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/978-3-031-81010-7_16",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/0b0915f26c701ff03c8dd4d52934a62b45c67fd2",
      "venue": "Symposium on Designing Interactive Systems",
      "journal": {
        "pages": "251-272"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "73b54779db6ec233103f37b05d91d9168e9cc22b",
      "title": "The Survey of Machine Learning Techniques in PUBG: Strategies for AI and Player\\'s Gameplay",
      "authors": [
        {
          "name": "Nikhil Jadhav",
          "authorId": "2351151588"
        }
      ],
      "year": 2025,
      "abstract": "This research paper delves into the integration of machine learning (ML) techniques within PlayerUnknown's\nBattlegrounds (PUBG), focusing on strategies that enhance artificial intelligence (AI) performance and player interaction. By\nanalyzing large-scale gameplay datasets, the study evaluates the effectiveness of generative AI frameworks and reinforcement\nlearning algorithms. Key findings highlight improvements in AI adaptability, strategic decision-making, and player satisfaction.\nThe research identifies gaps in real-time AI adaptability, trust-building in high-stakes environments, and fairness in gameplay,\naiming to advance AI capabilities in multiplayer gaming scenarios.",
      "citationCount": 0,
      "doi": "10.22214/ijraset.2025.67470",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/73b54779db6ec233103f37b05d91d9168e9cc22b",
      "venue": "International Journal for Research in Applied Science and Engineering Technology",
      "journal": {
        "name": "International Journal for Research in Applied Science and Engineering Technology"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "e140384789f4dbe7f1e7e876c6713d2a0795cc22",
      "title": "Holistic Survey of Privacy and Fairness in Machine Learning",
      "authors": [
        {
          "name": "Sina Shaham",
          "authorId": "40221713"
        },
        {
          "name": "Arash Hajisafi",
          "authorId": "2209222310"
        },
        {
          "name": "Min Quan",
          "authorId": "2155747501"
        },
        {
          "name": "Dinh C. Nguyen",
          "authorId": "123547055"
        },
        {
          "name": "Bhaskar Krishnamachari",
          "authorId": "2174866863"
        },
        {
          "name": "Charith Peris",
          "authorId": "102648923"
        },
        {
          "name": "Gabriel Ghinita",
          "authorId": "3277872"
        },
        {
          "name": "C. Shahabi",
          "authorId": "1773086"
        },
        {
          "name": "P. Pathirana",
          "authorId": "2301180"
        }
      ],
      "year": 2023,
      "abstract": "Privacy and fairness are two crucial pillars of responsible Artificial Intelligence (AI) and trustworthy Machine Learning (ML). Each objective has been independently studied in the literature with the aim of reducing utility loss in achieving them. Despite the significant interest attracted from both academia and industry, there remains an immediate demand for more in-depth research to unravel how these two objectives can be simultaneously integrated into ML models. As opposed to well-accepted trade-offs, i.e., privacy-utility and fairness-utility, the interrelation between privacy and fairness is not well-understood. While some works suggest a trade-off between the two objective functions, there are others that demonstrate the alignment of these functions in certain scenarios. To fill this research gap, we provide a thorough review of privacy and fairness in ML, including supervised, unsupervised, semi-supervised, and reinforcement learning. After examining and consolidating the literature on both objectives, we present a holistic survey on the impact of privacy on fairness, the impact of fairness on privacy, existing architectures, their interaction in application domains, and algorithms that aim to achieve both objectives while minimizing the utility sacrificed. Finally, we identify research challenges in achieving privacy and fairness concurrently in ML, particularly focusing on large language models.",
      "citationCount": 9,
      "doi": "10.48550/arXiv.2307.15838",
      "arxivId": "2307.15838",
      "url": "https://www.semanticscholar.org/paper/e140384789f4dbe7f1e7e876c6713d2a0795cc22",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2307.15838"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "8bb0357270f27b848a979f3e42ea35cd973bf2a5",
      "title": "Mitigating Sociodemographic Bias in Opioid Use Disorder Prediction: Fairness-Aware Machine Learning Framework",
      "authors": [
        {
          "name": "Mohammad Yaseliani",
          "authorId": "2170259246"
        },
        {
          "name": "Md. Noor-E-Alam",
          "authorId": "2300732507"
        },
        {
          "name": "M. Hasan",
          "authorId": "2295515451"
        }
      ],
      "year": 2024,
      "abstract": "Background Opioid use disorder (OUD) is a critical public health crisis in the United States, affecting >5.5 million Americans in 2021. Machine learning has been used to predict patient risk of incident OUD. However, little is known about the fairness and bias of these predictive models. Objective The aims of this study are two-fold: (1) to develop a machine learning bias mitigation algorithm for sociodemographic features and (2) to develop a fairness-aware weighted majority voting (WMV) classifier for OUD prediction. Methods We used the 2020 National Survey on Drug and Health data to develop a neural network (NN) model using stochastic gradient descent (SGD; NN-SGD) and an NN model using Adam (NN-Adam) optimizers and evaluated sociodemographic bias by comparing the area under the curve values. A bias mitigation algorithm, based on equality of odds, was implemented to minimize disparities in specificity and recall. Finally, a WMV classifier was developed for fairness-aware prediction of OUD. To further analyze bias detection and mitigation, we did a 1-N matching of OUD to non-OUD cases, controlling for socioeconomic variables, and evaluated the performance of the proposed bias mitigation algorithm and WMV classifier. Results Our bias mitigation algorithm substantially reduced bias with NN-SGD, by 21.66% for sex, 1.48% for race, and 21.04% for income, and with NN-Adam by 16.96% for sex, 8.87% for marital status, 8.45% for working condition, and 41.62% for race. The fairness-aware WMV classifier achieved a recall of 85.37% and 92.68% and an accuracy of 58.85% and 90.21% using NN-SGD and NN-Adam, respectively. The results after matching also indicated remarkable bias reduction with NN-SGD and NN-Adam, respectively, as follows: sex (0.14% vs 0.97%), marital status (12.95% vs 10.33%), working condition (14.79% vs 15.33%), race (60.13% vs 41.71%), and income (0.35% vs 2.21%). Moreover, the fairness-aware WMV classifier achieved high performance with a recall of 100% and 85.37% and an accuracy of 73.20% and 89.38% using NN-SGD and NN-Adam, respectively. Conclusions The application of the proposed bias mitigation algorithm shows promise in reducing sociodemographic bias, with the WMV classifier confirming bias reduction and high performance in OUD prediction.",
      "citationCount": 5,
      "doi": "10.2196/55820",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/8bb0357270f27b848a979f3e42ea35cd973bf2a5",
      "venue": "JMIR AI",
      "journal": {
        "name": "JMIR AI",
        "volume": "3"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "660f9fa75c325a6c3850a76cd10edb84468beb85",
      "title": "In-Processing Modeling Techniques for Machine Learning Fairness: A Survey",
      "authors": [
        {
          "name": "Mingyang Wan",
          "authorId": "101486331"
        },
        {
          "name": "D. Zha",
          "authorId": "1759658"
        },
        {
          "name": "Ninghao Liu",
          "authorId": "2257675416"
        },
        {
          "name": "Na Zou",
          "authorId": "2257261491"
        }
      ],
      "year": 2022,
      "abstract": "Machine learning models are becoming pervasive in high-stakes applications. Despite their clear benefits in terms of performance, the models could show discrimination against minority groups and result in fairness issues in a decision-making process, leading to severe negative impacts on the individuals and the society. In recent years, various techniques have been developed to mitigate the unfairness for machine learning models. Among them, in-processing methods have drawn increasing attention from the community, where fairness is directly taken into consideration during model design to induce intrinsically fair models and fundamentally mitigate fairness issues in outputs and representations. In this survey, we review the current progress of in-processing fairness mitigation techniques. Based on where the fairness is achieved in the model, we categorize them into explicit and implicit methods, where the former directly incorporates fairness metrics in training objectives, and the latter focuses on refining latent representation learning. Finally, we conclude the survey with a discussion of the research challenges in this community to motivate future exploration.",
      "citationCount": 112,
      "doi": "10.1145/3551390",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/660f9fa75c325a6c3850a76cd10edb84468beb85",
      "venue": "ACM Transactions on Knowledge Discovery from Data",
      "journal": {
        "name": "ACM Transactions on Knowledge Discovery from Data",
        "pages": "1 - 27",
        "volume": "17"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "46a908eac1f4e6bdbce377ea84a8743eb0d5a826",
      "title": "Unveiling farmers\u2019 perceptions: a citizen science and machine learning approach to exploring drivers in the adequacy and fairness of water systems",
      "authors": [
        {
          "name": "Vasilios Plakandaras",
          "authorId": "3215432"
        },
        {
          "name": "F. Khadim",
          "authorId": "30966103"
        },
        {
          "name": "Vassiliki Kazana",
          "authorId": "1856448"
        },
        {
          "name": "E. Anagnostou",
          "authorId": "31634517"
        },
        {
          "name": "A. Bagtzoglou",
          "authorId": "2941280"
        }
      ],
      "year": 2024,
      "abstract": "Agricultural production sustains a large part of the population in the Great South area (sub-Saharan Africa, South America and South Asia), relying heavily on rainfed agriculture and partly on reservoir-based irrigation schemes. This study evaluates the effects of citizen science approaches in shaping the farmers\u2019 perceptions towards adequacy of the quantity of the provided water and the fairness of the irrigation distribution system in the area, using as a case study a project implemented in the Upper Blue Nile (UBN) region of Ethiopia in two irrigated communities. Harnessing the analytical power of machine learning models in extracting patterns from data, the informational content of social surveys coupled with hydrological data for the survey region from a calibrated MODFLOW-NWT groundwater (GW) model, we draw inferences on the importance of socioeconomic rather than hydrological variables as drivers in agricultural decisions about crop selection and planting period, underscoring those factors as potential criteria in drawing successful agricultural policies for crop yield optimization in the Great South area. This study extends the existing literature towards understanding the interplay between people and water under a qualitative framework with distinct policy implications.",
      "citationCount": 2,
      "doi": "10.1088/2515-7620/ada1ac",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/46a908eac1f4e6bdbce377ea84a8743eb0d5a826",
      "venue": "Environmental Research Communications",
      "journal": {
        "name": "Environmental Research Communications",
        "volume": "7"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "c8cfe7331dce017758f41c4b7006db3264890937",
      "title": "Perceptions of the Fairness Impacts of Multiplicity in Machine Learning",
      "authors": [
        {
          "name": "Anna P. Meyer",
          "authorId": "2146994380"
        },
        {
          "name": "Yea-Seul Kim",
          "authorId": "2282278991"
        },
        {
          "name": "Loris D'antoni",
          "authorId": "1397413848"
        },
        {
          "name": "Aws Albarghouthi",
          "authorId": "1893193"
        }
      ],
      "year": 2024,
      "abstract": "Machine learning (ML) is increasingly used in high-stakes settings, yet multiplicity \u2013 the existence of multiple good models \u2013 means that some predictions are essentially arbitrary. ML researchers and philosophers posit that multiplicity poses a fairness risk, but no studies have investigated whether stakeholders agree. In this work, we conduct a survey to see how multiplicity impacts lay stakeholders\u2019 \u2013 i.e., decision subjects\u2019 \u2013 perceptions of ML fairness, and which approaches to address multiplicity they prefer. We investigate how these perceptions are modulated by task characteristics (e.g., stakes and uncertainty). Survey respondents think that multiplicity threatens the fairness of model outcomes, but not the appropriateness of using the model, even though existing work suggests the opposite. Participants are strongly against resolving multiplicity by using a single model (effectively ignoring multiplicity) or by randomizing the outcomes. Our results indicate that model developers should be intentional about dealing with multiplicity in order to maintain fairness.",
      "citationCount": 2,
      "doi": "10.1145/3706598.3713524",
      "arxivId": "2409.12332",
      "url": "https://www.semanticscholar.org/paper/c8cfe7331dce017758f41c4b7006db3264890937",
      "venue": "International Conference on Human Factors in Computing Systems",
      "journal": {
        "name": "Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "26bfb84f8da2497ead59b1c2dc0692085cfc5ead",
      "title": "Assured, Explainable, And Auditable AI For High-Stakes Decisions: A Survey Of Trustworthy Machine Learning In Mission-Critical Systems",
      "authors": [
        {
          "name": "Yesu Vara Prasad Kollipara",
          "authorId": "2390834083"
        }
      ],
      "year": 2025,
      "abstract": "Deployment of artificial intelligence in mission-critical domains healthcare, criminal justice, finance, and public administration, demands systems that withstand legal, ethical, and reliability scrutiny. This survey synthesizes techniques that transform black-box models into accountable decision aids. Post-hoc explanation methods, including feature attribution and counterfactual reasoning, are contrasted with intrinsically interpretable architectures and causal frameworks that support real-world interventions. Uncertainty quantification through conformal prediction and calibrated probabilistic outputs bounds error in safety-critical workflows, while fairness auditing across protected groups employs metrics and bias mitigation strategies to navigate accuracy-equity trade-offs. Operational assurance mechanisms, dataset shift detection, continuous monitoring, model versioning, rollback protocols, and red-team evaluation \u2014are mapped to emerging risk-management and documentation frameworks such as model cards and system cards. Open challenges include scaling explainability to foundation models, multi-objective optimization balancing competing desiderata, and aligning machine-generated rationale with human cognitive processes in consequential decisions. The synthesis establishes a comprehensive agenda for building AI systems that support verifiable, responsible choices where failure carries unacceptable consequences.",
      "citationCount": 0,
      "doi": "10.63278/jicrcr.vi.3392",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/26bfb84f8da2497ead59b1c2dc0692085cfc5ead",
      "venue": "Journal of International Crisis and Risk Communication Research",
      "journal": {
        "name": "Journal of International Crisis and Risk Communication Research"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "079ee98678d337458551618f00739d5a78d51a22",
      "title": "A Survey on Data Quality Dimensions and Tools for Machine Learning Invited Paper",
      "authors": [
        {
          "name": "Yuhan Zhou",
          "authorId": "2309114134"
        },
        {
          "name": "Fengjiao Tu",
          "authorId": "2153734083"
        },
        {
          "name": "Kewei Sha",
          "authorId": "2367038756"
        },
        {
          "name": "Junhua Ding",
          "authorId": "2295893424"
        },
        {
          "name": "Haihua Chen",
          "authorId": "2309146718"
        }
      ],
      "year": 2024,
      "abstract": "Machine learning (ML) technologies have become substantial in practically all aspects of our society, and data quality (DQ) is critical for the performance, fairness, robustness, safety, and scalability of ML models. With the large and complex data in data-centric AI, traditional methods like exploratory data analysis (EDA) and cross-validation (CV) face challenges, highlighting the importance of mastering DQ tools. In this survey, we review 17 DQ evaluation and improvement tools in the last 5 years. By introducing the DQ dimensions, metrics, and main functions embedded in these tools, we compare their strengths and limitations and propose a roadmap for developing open-source DQ tools for ML. Based on the discussions on the chal-lenges and emerging trends, we further highlight the potential applications of large language models (LLMs) and generative AI in DQ evaluation and improvement for ML. We believe this comprehensive survey can enhance understanding of DQ in ML and could drive progress in data-centric AI. A complete list of the literature investigated in this surveyis available on GitHub at: https://github.com/haihua0913/awesome-dq4ml..",
      "citationCount": 14,
      "doi": "10.1109/AITest62860.2024.00023",
      "arxivId": "2406.19614",
      "url": "https://www.semanticscholar.org/paper/079ee98678d337458551618f00739d5a78d51a22",
      "venue": "International Conference on Artificial Intelligence Testing",
      "journal": {
        "name": "2024 IEEE International Conference on Artificial Intelligence Testing (AITest)",
        "pages": "120-131"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "ebf3990dd51e1cc90e2df0828e349fd80cea6d3e",
      "title": "What-is and How-to for Fairness in Machine Learning: A Survey, Reflection, and Perspective",
      "authors": [
        {
          "name": "Zeyu Tang",
          "authorId": "2125563094"
        },
        {
          "name": "Jiji Zhang",
          "authorId": "2108025301"
        },
        {
          "name": "Kun Zhang",
          "authorId": "2119017174"
        }
      ],
      "year": 2022,
      "abstract": "We review and reflect on fairness notions proposed in machine learning literature and make an attempt to draw connections to arguments in moral and political philosophy, especially theories of justice. We survey dynamic fairness inquiries and further consider the long-term impact induced by current prediction and decision. We present a flowchart that encompasses implicit assumptions and expected outcomes of different fairness inquiries on the data-generating process, the predicted outcome, and the induced impact, respectively. We demonstrate the importance of matching the mission (what kind of fairness to enforce) and the means (which appropriate fairness spectrum to analyze) to fulfill the intended purpose.",
      "citationCount": 35,
      "doi": "10.1145/3597199",
      "arxivId": "2206.04101",
      "url": "https://www.semanticscholar.org/paper/ebf3990dd51e1cc90e2df0828e349fd80cea6d3e",
      "venue": "ACM Computing Surveys",
      "journal": {
        "name": "ACM Computing Surveys",
        "pages": "1 - 37",
        "volume": "55"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "76a000275033cb9cc3e4550d3924a1884d2e4340",
      "title": "Multimodal Machine Learning in Mental Health: A Survey of Data, Algorithms, and Challenges",
      "authors": [
        {
          "name": "Zahraa Al Sahili",
          "authorId": "2175555681"
        },
        {
          "name": "Ioannis Patras",
          "authorId": "2306252618"
        },
        {
          "name": "Matthew Purver",
          "authorId": "2265965933"
        }
      ],
      "year": 2024,
      "abstract": "Multimodal machine learning (MML) is rapidly reshaping the way mental-health disorders are detected, characterized, and longitudinally monitored. Whereas early studies relied on isolated data streams -- such as speech, text, or wearable signals -- recent research has converged on architectures that integrate heterogeneous modalities to capture the rich, complex signatures of psychiatric conditions. This survey provides the first comprehensive, clinically grounded synthesis of MML for mental health. We (i) catalog 26 public datasets spanning audio, visual, physiological signals, and text modalities; (ii) systematically compare transformer, graph, and hybrid-based fusion strategies across 28 models, highlighting trends in representation learning and cross-modal alignment. Beyond summarizing current capabilities, we interrogate open challenges: data governance and privacy, demographic and intersectional fairness, evaluation explainability, and the complexity of mental health disorders in multimodal settings. By bridging methodological innovation with psychiatric utility, this survey aims to orient both ML researchers and mental-health practitioners toward the next generation of trustworthy, multimodal decision-support systems.",
      "citationCount": 9,
      "doi": "10.48550/arXiv.2407.16804",
      "arxivId": "2407.16804",
      "url": "https://www.semanticscholar.org/paper/76a000275033cb9cc3e4550d3924a1884d2e4340",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2407.16804"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "59033c5ce8e3b87bfcb86aa237747c5367586400",
      "title": "A Survey on Fairness for Machine Learning on Graphs",
      "authors": [
        {
          "name": "Manvi Choudhary",
          "authorId": "2007596562"
        },
        {
          "name": "Charlotte Laclau",
          "authorId": "1947597"
        },
        {
          "name": "C. Largeron",
          "authorId": "1725600"
        }
      ],
      "year": 2022,
      "abstract": "Nowadays, the analysis of complex phenomena modeled by graphs plays a crucial role in many real-world application domains where decisions can have a strong societal impact. However, numerous studies and papers have recently revealed that machine learning models could lead to potential disparate treatment between individuals and unfair outcomes. In that context, algorithmic contributions for graph mining are not spared by the problem of fairness and present some specific challenges related to the intrinsic nature of graphs: (1) graph data is non-IID, and this assumption may invalidate many existing studies in fair machine learning, (2) suited metric definitions to assess the different types of fairness with relational data and (3) algorithmic challenge on the difficulty of finding a good trade-off between model accuracy and fairness. This survey is the first one dedicated to fairness for relational data. It aims to present a comprehensive review of state-of-the-art techniques in fairness on graph mining and identify the open challenges and future trends. In particular, we start by presenting several sensible application domains and the associated graph mining tasks with a focus on edge prediction and node classification in the sequel. We also recall the different metrics proposed to evaluate potential bias at different levels of the graph mining process; then we provide a comprehensive overview of recent contributions in the domain of fair machine learning for graphs, that we classify into pre-processing, in-processing and post-processing models. We also propose to describe existing graph data, synthetic and real-world benchmarks. Finally, we present in detail five potential promising directions to advance research in studying algorithmic fairness on graphs.",
      "citationCount": 32,
      "doi": "10.48550/arXiv.2205.05396",
      "arxivId": "2205.05396",
      "url": "https://www.semanticscholar.org/paper/59033c5ce8e3b87bfcb86aa237747c5367586400",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2205.05396"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "d36d2fa6ea9a096f65d3bd75b2c0e5e6f59ad28f",
      "title": "Fairness-aware machine learning engineering: how far are we?",
      "authors": [
        {
          "name": "Carmine Ferrara",
          "authorId": "2267911659"
        },
        {
          "name": "Giulia Sellitto",
          "authorId": "2150963873"
        },
        {
          "name": "F. Ferrucci",
          "authorId": "1760777"
        },
        {
          "name": "Fabio Palomba",
          "authorId": "34946033"
        },
        {
          "name": "Andrea De Lucia",
          "authorId": "72187755"
        }
      ],
      "year": 2023,
      "abstract": "Machine learning is part of the daily life of people and companies worldwide. Unfortunately, bias in machine learning algorithms risks unfairly influencing the decision-making process and reiterating possible discrimination. While the interest of the software engineering community in software fairness is rapidly increasing, there is still a lack of understanding of various aspects connected to fair machine learning engineering, i.e., the software engineering process involved in developing fairness-critical machine learning systems. Questions connected to the practitioners\u2019 awareness and maturity about fairness, the skills required to deal with the matter, and the best development phase(s) where fairness should be faced more are just some examples of the knowledge gaps currently open. In this paper, we provide insights into how fairness is perceived and managed in practice, to shed light on the instruments and approaches that practitioners might employ to properly handle fairness. We conducted a survey with 117 professionals who shared their knowledge and experience highlighting the relevance of fairness in practice, and the skills and tools required to handle it. The key results of our study show that fairness is still considered a second-class quality aspect in the development of artificial intelligence systems. The building of specific methods and development environments, other than automated validation tools, might help developers to treat fairness throughout the software lifecycle and revert this trend.",
      "citationCount": 44,
      "doi": "10.1007/s10664-023-10402-y",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/d36d2fa6ea9a096f65d3bd75b2c0e5e6f59ad28f",
      "venue": "Empirical Software Engineering",
      "journal": {
        "name": "Empirical Software Engineering",
        "volume": "29"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "7d23163b118ac71f48d6ef501651578849e012bf",
      "title": "A Survey on Optimization and Machine -learning-based Fair Decision Making in Healthcare",
      "authors": [
        {
          "name": "Z. Chen",
          "authorId": "2274313993"
        },
        {
          "name": "W. J. Marrero",
          "authorId": "2274185953"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.1101/2024.03.16.24304403",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7d23163b118ac71f48d6ef501651578849e012bf",
      "venue": "medRxiv",
      "journal": null,
      "publicationTypes": [
        "Review"
      ]
    },
    {
      "paperId": "564ee18a1afd3016618fe0df2271e924a77b1fc6",
      "title": "Modeling Techniques for Machine Learning Fairness: A Survey",
      "authors": [
        {
          "name": "Mingyang Wan",
          "authorId": "101486331"
        },
        {
          "name": "D. Zha",
          "authorId": "1759658"
        },
        {
          "name": "Ninghao Liu",
          "authorId": "47717322"
        },
        {
          "name": "Na Zou",
          "authorId": "49648991"
        }
      ],
      "year": 2021,
      "abstract": "Machine learning models are becoming pervasive in high-stakes applications. Despite their clear benefits in terms of performance, the models could show discrimination against minority groups and result in fairness issues in a decision-making process, leading to severe negative impacts on the individuals and the society. In recent years, various techniques have been developed to mitigate the unfairness for machine learning models. Among them, in-processing methods have drawn increasing attention from the community, where fairness is directly taken into consideration during model design to induce intrinsically fair models and fundamentally mitigate fairness issues in outputs and representations. In this survey, we review the current progress of in-processing fairness mitigation techniques. Based on where the fairness is achieved in the model, we categorize them into explicit and implicit methods, where the former directly incorporates fairness metrics in training objectives, and the latter focuses on refining latent representation learning. Finally, we conclude the survey with a discussion of the research challenges in this community to motivate future exploration.",
      "citationCount": 39,
      "doi": null,
      "arxivId": "2111.03015",
      "url": "https://www.semanticscholar.org/paper/564ee18a1afd3016618fe0df2271e924a77b1fc6",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2111.03015"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "5752891be570cdfddad5f2649c6619f14b70d309",
      "title": "Survey on Causal-based Machine Learning Fairness Notions",
      "authors": [
        {
          "name": "K. Makhlouf",
          "authorId": "8356809"
        },
        {
          "name": "Sami Zhioua",
          "authorId": "1722898"
        },
        {
          "name": "C. Palamidessi",
          "authorId": "1722055"
        }
      ],
      "year": 2020,
      "abstract": "Addressing the problem of fairness is crucial to safely use machine learning algorithms to support decisions with a critical impact on people's lives such as job hiring, child maltreatment, disease diagnosis, loan granting, etc. Several notions of fairness have been defined and examined in the past decade, such as, statistical parity and equalized odds. The most recent fairness notions, however, are causal-based and reflect the now widely accepted idea that using causality is necessary to appropriately address the problem of fairness. This paper examines an exhaustive list of causal-based fairness notions, in particular their applicability in real-world scenarios. As the majority of causal-based fairness notions are defined in terms of non-observable quantities (e.g. interventions and counterfactuals), their applicability depends heavily on the identifiability of those quantities from observational data. In this paper, we compile the most relevant identifiability criteria for the problem of fairness from the extensive literature on identifiability theory. These criteria are then used to decide about the applicability of causal-based fairness notions in concrete discrimination scenarios.",
      "citationCount": 98,
      "doi": null,
      "arxivId": "2010.09553",
      "url": "https://www.semanticscholar.org/paper/5752891be570cdfddad5f2649c6619f14b70d309",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2010.09553"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    }
  ],
  "count": 30,
  "errors": []
}
