{
  "status": "success",
  "source": "semantic_scholar",
  "query": "operationalization fairness machine learning",
  "results": [
    {
      "paperId": "f00206d28a81a8ac9d153b7a0ad5d736396b6db4",
      "title": "Identifying Key Predictors of Students\u2019 Competency Achievement Using Machine Learning Models: A Bioengineering Case Study",
      "authors": [
        {
          "name": "Julio C\u00e9sar Quintana-Zaez",
          "authorId": "2383193754"
        },
        {
          "name": "Patricia V\u00e1zquez-Villegas",
          "authorId": "1399534227"
        },
        {
          "name": "Danilo Vald\u00e9s-Ram\u00edrez",
          "authorId": "1390016331"
        }
      ],
      "year": 2025,
      "abstract": "Competency-based education (CBE) in higher education demands interpretable and scalable tools to monitor student progress. Current studies on CBE have used small samples in short evaluation periods or have not used machine learning or explainability of the results. This study introduces a robust analytical pipeline that integrates correlation analysis, Factor Analysis of Mixed Data, and explainable machine learning to predict competency achievement in bioengineering programs. Using over 300,000 evaluations from a private Mexican university, Random Forest model achieved outstanding predictive performance in a Stratified 10-fold Cross-validation experiment (AUC = 0.9604\u20130.9653), outperforming deep neural networks for One-Class Classification in highly imbalanced data. Model interpretability using SHAP highlighted academic and course-related variables, rather than demographic factors, as the strongest predictors, reinforcing the fairness of the evaluation process. This work advances the operationalization of explainable AI in CBE, contributing to the emerging vision of Data-Based Education by providing actionable insights for curriculum design, academic advising, and institutional policy.",
      "citationCount": 2,
      "doi": "10.1109/ACCESS.2025.3613250",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f00206d28a81a8ac9d153b7a0ad5d736396b6db4",
      "venue": "IEEE Access",
      "journal": {
        "name": "IEEE Access",
        "pages": "168855-168877",
        "volume": "13"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "31384d09c7e179d2dc935578c773d5c1cf35627e",
      "title": "Action-guidance and AI ethics: the case of fair machine learning",
      "authors": [
        {
          "name": "Otto Sahlgren",
          "authorId": "2320675295"
        }
      ],
      "year": 2024,
      "abstract": "A prominent approach to implementing AI ethics involves translating ethical principles, such as fairness and transparency, into practical frameworks and tools that responsible agents, such as ML developers, can use to ensure that machine learning systems act according to the relevant principles. Fair machine learning research exemplifies this approach by producing frameworks and software toolkits that responsible agents could apply to align machine learning systems with principles such as fairness, equality, and justice. However, the application of available frameworks and tools has proven challenging both due to ambiguous operationalization of the relevant principles and many real-life obstacles that agents face in the context of machine learning system design and development, such as lack of access to proper evaluation data. This article conceptualizes these problems as instances of a more general \u201caction-guidance gap\u201d in AI ethics. The article addresses the action-guidance gap by outlining a philosophical account of action-guidance that can be used to identify and address problems related to the specification and practical implementation of AI ethics principles. Centering on fair machine learning practice as a case example, the article presents a set of detailed requirements for action-guidance in fair machine learning practice which explain problems that previous studies have identified with regard to the real-life application of fair machine learning frameworks and tools. Paving a way forward, the article presents theoretical and practical lessons for ensuring action-guidance in fairness-sensitive design, with implications for AI ethics more generally.",
      "citationCount": 2,
      "doi": "10.1007/s43681-024-00437-2",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/31384d09c7e179d2dc935578c773d5c1cf35627e",
      "venue": "AI and Ethics",
      "journal": {
        "name": "AI and Ethics",
        "pages": "1019 - 1031",
        "volume": "5"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "111b83c0c0553cb3d960af6ae74aee6567f30fe6",
      "title": "MACHINE LEARNING AND SECURE DATA PIPELINE FRAMEWORKS FOR IMPROVING PATIENT SAFETY WITHIN U.S. ELECTRONIC HEALTH RECORD SYSTEMS",
      "authors": [
        {
          "name": "Md. Jobayer Ibne Saidur",
          "authorId": "2370374069"
        },
        {
          "name": "Aditya Dhanekula",
          "authorId": "2396099133"
        }
      ],
      "year": 2024,
      "abstract": "This study examined how secure data pipelines operated as quantitative determinants of machine learning (ML) reliability and patient safety outcomes in Electronic Health Record (EHR) environments among U.S. healthcare providers. A retrospective, multi-site quantitative design was applied to de-identified EHR encounter streams, linking provider-level pipeline maturity (confidentiality, integrity, availability, and data quality) with ML safety-prediction performance and EHR-derived safety endpoints. The methodology and findings were grounded in an overall review of spanning secure healthcare data pipelines, EHR-driven ML safety prediction, cybersecurity incident impacts, fairness in clinical ML, and EHR safety endpoint operationalization; this number should match the total reported across the methods and findings sections of the article. The analytic cohort comprised 12 providers contributing 184,732 adult encounters, with a median of 14,980 encounters per provider. Pipeline maturity demonstrated cross-site differentiation (mean = 72.8, SD = 9.4), accompanied by measurable variability in quality and security indicators: missingness averaged 6.4% (SD = 3.1), timestamp misalignment averaged 3.8 per 1,000 events (SD = 1.9), unit harmonization errors averaged 5.6 per 10,000 labs (SD = 2.4), audit-log completeness averaged 91.5% (SD = 4.7), encryption coverage averaged 94.2% (SD = 3.9), and downtime averaged 2.7 hours per quarter (SD = 1.4). ML models for safety prediction showed stable reliability across endpoints, with mean discrimination AUROC = 0.84 (SD = 0.03), calibration slope = 0.97 (SD = 0.06), false-alarm burden = 14.9 alerts per 100 (SD = 3.5), lead-time advantage for deterioration alerts = 3.6 hours (SD = 1.1), and cross-provider transportability loss \u0394AUROC = 0.04 (SD = 0.02). Safety outcomes occurred at clinically meaningful rates: preventable adverse drug events = 1.9%, abnormal-result follow-up delays = 7.6%, deterioration/failure-to-rescue events = 2.4%, and hospital-acquired harms = 3.1%. Multilevel regression indicated that higher pipeline maturity predicted lower composite harm incidence (\u03b2 = \u22120.21, p < .001), while ML reliability independently reduced harms (\u03b2 = \u22120.18, p < .001). Mediation analysis showed a significant indirect pathway through ML reliability (indirect effect = \u22120.09, p = .002) alongside a remaining direct maturity effect (\u03b2 = \u22120.11, p = .007). Moderation tests indicated stronger maturity-to-reliability effects under higher interoperability (interaction \u03b2 = 0.14, p = .019). Overall, the results demonstrated a statistically linked infrastructure\u2013analytics pathway through which secure pipelines enhanced ML reliability and corresponded to lower patient-harm burdens in EHR-driven care.",
      "citationCount": 0,
      "doi": "10.63125/nb2c1f86",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/111b83c0c0553cb3d960af6ae74aee6567f30fe6",
      "venue": "American Journal of Interdisciplinary Studies",
      "journal": {
        "name": "American Journal of Interdisciplinary Studies"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "0a87276c78982365f176947c2cc2d9a9c4158ad2",
      "title": "FairExpand: Individual Fairness on Graphs with Partial Similarity Information",
      "authors": [
        {
          "name": "Rebecca Salganik",
          "authorId": "2307470076"
        },
        {
          "name": "Yibin Wang",
          "authorId": "2400159785"
        },
        {
          "name": "Guillaume Salha-Galvan",
          "authorId": "2400146084"
        },
        {
          "name": "Jian Kang",
          "authorId": "2307569689"
        }
      ],
      "year": 2025,
      "abstract": "Individual fairness, which requires that similar individuals should be treated similarly by algorithmic systems, has become a central principle in fair machine learning. Individual fairness has garnered traction in graph representation learning due to its practical importance in high-stakes Web areas such as user modeling, recommender systems, and search. However, existing methods assume the existence of predefined similarity information over all node pairs, an often unrealistic requirement that prevents their operationalization in practice. In this paper, we assume the similarity information is only available for a limited subset of node pairs and introduce FairExpand, a flexible framework that promotes individual fairness in this more realistic partial information scenario. FairExpand follows a two-step pipeline that alternates between refining node representations using a backbone model (e.g., a graph neural network) and gradually propagating similarity information, which allows fairness enforcement to effectively expand to the entire graph. Extensive experiments show that FairExpand consistently enhances individual fairness while preserving performance, making it a practical solution for enabling graph-based individual fairness in real-world applications with partial similarity information.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2512.18180",
      "url": "https://www.semanticscholar.org/paper/0a87276c78982365f176947c2cc2d9a9c4158ad2",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "74e9bc44dfb1a8d14d63692144c633d69dfeb4fc",
      "title": "A Clearer View on Fairness: Visual and Formal Representations for Comparative Analysis",
      "authors": [
        {
          "name": "Julian Alfredo Mendez",
          "authorId": "2156864353"
        },
        {
          "name": "Timotheus Kampik",
          "authorId": "2437582"
        },
        {
          "name": "Andrea Aler Tubella",
          "authorId": "9937546"
        },
        {
          "name": "Virginia Dignum",
          "authorId": "2273814472"
        }
      ],
      "year": 2024,
      "abstract": "The opaque nature of machine learning systems has raised concerns about whether these systems can guarantee fairness. Furthermore, ensuring fair decision making requires the consideration of multiple perspectives on fairness.At the moment, there is no agreement on the definitions of fairness, achieving shared interpretations is difficult, and there is no unified formal language to describe them. Current definitions are implicit in the operationalization of systems, making their comparison difficult.In this paper, we propose a framework for specifying formal representations of fairness that allows instantiating, visualizing, and comparing different interpretations of fairness. Our framework provides a meta-model for comparative analysis. We present several examples that consider different definitions of fairness, as well as an open-source implementation that uses the object-oriented functional language Soda.",
      "citationCount": 0,
      "doi": "10.3384/ecp208013",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/74e9bc44dfb1a8d14d63692144c633d69dfeb4fc",
      "venue": "Scandinavian Conference on AI",
      "journal": {
        "pages": "112-120"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "72e4a298b49f8155150f449e63543cfc32c0c92d",
      "title": "Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare",
      "authors": [
        {
          "name": "Eran Tal",
          "authorId": "47653586"
        }
      ],
      "year": 2023,
      "abstract": "Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology \u2013 the science of measurement \u2013 suggests ways of counteracting target specification bias and avoiding its harmful consequences.",
      "citationCount": 14,
      "doi": "10.1145/3600211.3604678",
      "arxivId": "2308.02081",
      "url": "https://www.semanticscholar.org/paper/72e4a298b49f8155150f449e63543cfc32c0c92d",
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "5bd6933cd27697ffddc149acf5fb57c8342456c4",
      "title": "A Case Study in Fairness Evaluation: Current Limitations and Challenges for Human Pose Estimation",
      "authors": [
        {
          "name": "Julienne M LaChance",
          "authorId": "2253803972"
        },
        {
          "name": "William Thong",
          "authorId": "2238952987"
        },
        {
          "name": "Shruti Nagpal",
          "authorId": "1925017"
        },
        {
          "name": "Alice Xiang",
          "authorId": "2238953185"
        }
      ],
      "year": 2023,
      "abstract": null,
      "citationCount": 4,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/5bd6933cd27697ffddc149acf5fb57c8342456c4",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "9cb78d9c048423f41f7082bba87a636aed70d45a",
      "title": "Towards a Better Understanding of Evaluating Trustworthiness in AI Systems",
      "authors": [
        {
          "name": "Nils Kemmerzell",
          "authorId": "2258963785"
        },
        {
          "name": "Annika Schreiner",
          "authorId": "2258958798"
        },
        {
          "name": "Haroon Khalid",
          "authorId": "2349370298"
        },
        {
          "name": "Michael Schalk",
          "authorId": "2349367325"
        },
        {
          "name": "Letizia Bordoli",
          "authorId": "2349368273"
        }
      ],
      "year": 2025,
      "abstract": "With the increasing integration of artificial intelligence into various applications across industries, numerous institutions are striving to establish requirements for AI systems to be considered trustworthy, such as fairness, privacy, robustness, or transparency. For the implementation of Trustworthy AI into real-world applications, these requirements need to be operationalized, which includes evaluating the extent to which these criteria are fulfilled. This survey contributes to the discourse by outlining the current understanding of trustworthiness and its evaluation. Initially, existing evaluation frameworks are analyzed, from which common dimensions of trustworthiness are derived. For each dimension, the literature is surveyed for evaluation strategies, specifically focusing on quantitative metrics. By mapping these strategies to the machine learning lifecycle, an evaluation framework is derived, which can serve as a foundation towards the operationalization of Trustworthy AI.",
      "citationCount": 8,
      "doi": "10.1145/3721976",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/9cb78d9c048423f41f7082bba87a636aed70d45a",
      "venue": "ACM Computing Surveys",
      "journal": {
        "name": "ACM Computing Surveys",
        "pages": "1 - 38",
        "volume": "57"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "0090023afc66cd2741568599057f4e82b566137c",
      "title": "A Survey on Bias and Fairness in Machine Learning",
      "authors": [
        {
          "name": "Ninareh Mehrabi",
          "authorId": "51997673"
        },
        {
          "name": "Fred Morstatter",
          "authorId": "2775559"
        },
        {
          "name": "N. Saxena",
          "authorId": "51884035"
        },
        {
          "name": "Kristina Lerman",
          "authorId": "1782658"
        },
        {
          "name": "A. Galstyan",
          "authorId": "143728483"
        }
      ],
      "year": 2019,
      "abstract": "With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",
      "citationCount": 5220,
      "doi": "10.1145/3457607",
      "arxivId": "1908.09635",
      "url": "https://www.semanticscholar.org/paper/0090023afc66cd2741568599057f4e82b566137c",
      "venue": "ACM Computing Surveys",
      "journal": {
        "name": "ACM Computing Surveys (CSUR)",
        "pages": "1 - 35",
        "volume": "54"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "3db4933a10b124e26d7fe404da145292bd495b2a",
      "title": "FairXAI - A Taxonomy and Framework for Fairness and Explainability Synergy in Machine Learning",
      "authors": [
        {
          "name": "R. Ramachandranpillai",
          "authorId": "1392792012"
        },
        {
          "name": "Ricardo A. Baeza-Yates",
          "authorId": "2260060392"
        },
        {
          "name": "Fredrik Heintz",
          "authorId": "2259941780"
        }
      ],
      "year": 2025,
      "abstract": "Explainable artificial intelligence (XAI) and fair learning have made significant strides in various application domains, including criminal recidivism predictions, healthcare settings, toxic comment detection, automatic speech detection, recommendation systems, and image segmentation. However, these two fields have largely evolved independently. Recent studies have demonstrated that incorporating explanations into decision-making processes enhances the transparency and trustworthiness of AI systems. In light of this, our objective is to conduct a systematic review of FairXAI, which explores the interplay between fairness and explainability frameworks. To commence, we propose a taxonomy of FairXAI that utilizes XAI to mitigate and evaluate bias. This taxonomy will be a base for machine learning researchers operating in diverse domains. Additionally, we will undertake an extensive review of existing articles, taking into account factors such as the purpose of the interaction, target audience, and domain and context. Moreover, we outline an interaction framework for FairXAI considering various fairness perceptions and propose a FairXAI wheel that encompasses four core properties that must be verified and evaluated. This will serve as a practical tool for researchers and practitioners, ensuring the fairness and transparency of their AI systems. Furthermore, we will identify challenges and conflicts in the interactions between fairness and explainability, which could potentially pave the way for enhancing the responsibility of AI systems. As the inaugural review of its kind, we hope that this survey will inspire scholars to address these challenges by scrutinizing current research in their respective domains.",
      "citationCount": 8,
      "doi": "10.1109/TNNLS.2025.3528321",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/3db4933a10b124e26d7fe404da145292bd495b2a",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "journal": {
        "name": "IEEE Transactions on Neural Networks and Learning Systems",
        "pages": "9819-9836",
        "volume": "36"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "b1f552cb70b2d042ecc2c5cf6baf1e5e80b36181",
      "title": "Fairness issues, current approaches, and challenges in machine learning models",
      "authors": [
        {
          "name": "Tonni Das Jui",
          "authorId": "1453355651"
        },
        {
          "name": "Pablo Rivas",
          "authorId": "134599600"
        }
      ],
      "year": 2024,
      "abstract": "With the increasing influence of machine learning algorithms in decision-making processes, concerns about fairness have gained significant attention. This area now offers significant literature that is complex and hard to penetrate for newcomers to the domain. Thus, a mapping study of articles exploring fairness issues is a valuable tool to provide a general introduction to this field. Our paper presents a systematic approach for exploring existing literature by aligning their discoveries with predetermined inquiries and a comprehensive overview of diverse bias dimensions, encompassing training data bias, model bias, conflicting fairness concepts, and the absence of prediction transparency, as observed across several influential articles. To establish connections between fairness issues and various issue mitigation approaches, we propose a taxonomy of machine learning fairness issues and map the diverse range of approaches scholars developed to address issues. We briefly explain the responsible critical factors behind these issues in a graphical view with a discussion and also highlight the limitations of each approach analyzed in the reviewed articles. Our study leads to a discussion regarding the potential future direction in ML and AI fairness.",
      "citationCount": 45,
      "doi": "10.1007/s13042-023-02083-2",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/b1f552cb70b2d042ecc2c5cf6baf1e5e80b36181",
      "venue": "International Journal of Machine Learning and Cybernetics",
      "journal": {
        "name": "Int. J. Mach. Learn. Cybern.",
        "pages": "3095-3125",
        "volume": "15"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "def037852fea1e692c5b324aa4779500b7212b19",
      "title": "AI Ethics and Fairness in Machine Learning: Promoting Fairness in Critical Engineering Systems",
      "authors": [
        {
          "name": "Anil Kumar Jonnalagadda",
          "authorId": "2336805523"
        },
        {
          "name": "Prudhvi Naayini",
          "authorId": "2345573993"
        },
        {
          "name": "Srikanth Kamatala",
          "authorId": "2345457632"
        },
        {
          "name": "Praveen Kumar Myakala",
          "authorId": "2332100398"
        },
        {
          "name": "Chiranjeevi Bura",
          "authorId": "2332100344"
        }
      ],
      "year": 2025,
      "abstract": "The integration of AI and machine learning into engineering systems has revolutionized various domains, including autonomous systems, renewable energy management, and industrial automation. While these advancements have enhanced efficiency, predictive accuracy, and operational reliability, they have also introduced significant ethical challenges, particularly concerning fairness and bias in decision-making. This study investigates the ethical implications of AI in critical engineering systems, focusing on fairness in resource allocation, fault detection, and safety-critical applications. Three engineering datasets were employed to examine bias and fairness in machine learning models: the KITTI Vision Benchmark Suite for autonomous vehicles, NASA\u2019s C-MAPSS dataset for aerospace predictive maintenance, and the UCI Gas Sensor Array Drift dataset for industrial IoT applications. Ensemble methods such as Random Forest and LightGBM, known for their robustness in handling complex datasets, were used. Bias mitigation strategies, including reweighting, data augmentation, and fairness constraints, were applied to address disparities and ensure equitable outcomes. Results demonstrated substantial improvements in fairness metrics, with demographic parity, equal opportunity, and disparate impact showing an average enhancement of over 30% across datasets. Precision improved by 9.75%, recall increased by 17.71%, and AUC-ROC rose by 14.18%. Although accuracy exhibited a minor reduction (3% on average), these gains underscore the effectiveness of the mitigation techniques in achieving fairness while maintaining system performance. This study highlights the importance of integrating fairness into AI models for engineering systems, balancing performance with ethical considerations.",
      "citationCount": 5,
      "doi": "10.1109/ICTEST64710.2025.11042668",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/def037852fea1e692c5b324aa4779500b7212b19",
      "venue": "2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)",
      "journal": {
        "name": "2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)",
        "pages": "1-9",
        "volume": "1"
      },
      "publicationTypes": [
        "Conference"
      ]
    },
    {
      "paperId": "2ad6b8c806b81fd7e3efa39767429832b448902f",
      "title": "AI Ethics in Engineering: Enhancing Fairness in Machine Learning Models for Critical Systems",
      "authors": [
        {
          "name": "Vijayalaxmi Methuku",
          "authorId": "2348096820"
        },
        {
          "name": "Direesh Reddy Aunugu",
          "authorId": "2351213752"
        },
        {
          "name": "Anil Kumar Jonnalagadda",
          "authorId": "2336805523"
        },
        {
          "name": "Praveen Kumar Myakala",
          "authorId": "2332100398"
        }
      ],
      "year": 2025,
      "abstract": "The integration of artificial intelligence (AI) and machine learning (ML) into engineering systems has revolutionized industries such as autonomous vehicles, renewable energy management, and industrial automation. While these advancements have significantly improved efficiency, predictive accuracy, and operational reliability, they have also introduced ethical challenges, particularly concerning fairness and bias in decision-making. This study investigates the ethical implications of AI in critical engineering systems, focusing on fairness in resource allocation, fault detection, and safety-critical applications. Three engineering datasets were analyzed to evaluate bias and fairness in ML models: the KITTI Vision Benchmark Suite for autonomous vehicles, NASA\u2019s C-MAPSS dataset for aerospace predictive maintenance, and the UCI Gas Sensor Array Drift dataset for industrial IoT applications. Ensemble methods such as Random Forest and LightGBM were employed due to their robustness in handling complex datasets. Bias mitigation strategies, including reweighting, data augmentation, and fairness constraints, were applied to address disparities and ensure equitable outcomes. Results demonstrated significant improvements in fairness metrics, with demographic parity, equal opportunity, and disparate impact showing an average enhancement of over 30% across datasets. Precision improved by 9.75%, recall increased by 17.71%, and AUC-ROC rose by 14.18%. Although accuracy experienced a minor reduction (3% on average), these gains underscore the effectiveness of the mitigation techniques in achieving fairness while maintaining system performance. This study highlights the importance of integrating fairness into AI models for engineering systems, balancing performance with ethical considerations to ensure equitable and reliable outcomes in critical applications.",
      "citationCount": 3,
      "doi": "10.1109/ICCIES63851.2025.11032595",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/2ad6b8c806b81fd7e3efa39767429832b448902f",
      "venue": "2025 International Conference on Computational Innovations and Engineering Sustainability (ICCIES)",
      "journal": {
        "name": "2025 International Conference on Computational Innovations and Engineering Sustainability (ICCIES)",
        "pages": "1-7"
      },
      "publicationTypes": [
        "Conference"
      ]
    },
    {
      "paperId": "3eace7ee2ddfff1f0ce925645f13d0c44c1c016d",
      "title": "Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective",
      "authors": [
        {
          "name": "Firas Laakom",
          "authorId": "133961681"
        },
        {
          "name": "Haobo Chen",
          "authorId": "2311946720"
        },
        {
          "name": "Jurgen Schmidhuber",
          "authorId": "2311440772"
        },
        {
          "name": "Yuheng Bu",
          "authorId": "2281032938"
        }
      ],
      "year": 2025,
      "abstract": "Despite substantial progress in promoting fairness in high-stake applications using machine learning models, existing methods often modify the training process, such as through regularizers or other interventions, but lack formal guarantees that fairness achieved during training will generalize to unseen data. Although overfitting with respect to prediction performance has been extensively studied, overfitting in terms of fairness loss has received far less attention. This paper proposes a theoretical framework for analyzing fairness generalization error through an information-theoretic lens. Our novel bounding technique is based on Efron-Stein inequality, which allows us to derive tight information-theoretic fairness generalization bounds with both Mutual Information (MI) and Conditional Mutual Information (CMI). Our empirical results validate the tightness and practical relevance of these bounds across diverse fairness-aware learning algorithms. Our framework offers valuable insights to guide the design of algorithms improving fairness generalization.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2506.07861",
      "arxivId": "2506.07861",
      "url": "https://www.semanticscholar.org/paper/3eace7ee2ddfff1f0ce925645f13d0c44c1c016d",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.07861"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "ecdbafe24aefb2cddc25e48a58daa3388a6dce86",
      "title": "Unveiling Accuracy-Fairness Trade-Offs: Investigating Machine Learning Models in Student Performance Prediction",
      "authors": [
        {
          "name": "Raymond A. Opoku",
          "authorId": "2279030591"
        },
        {
          "name": "Bo Pei",
          "authorId": "2301938263"
        },
        {
          "name": "Wanli Xing",
          "authorId": "2305962188"
        }
      ],
      "year": 2025,
      "abstract": "While high-accuracy machine learning (ML) models for predicting student learning performance have been widely explored, their deployment in real educational settings can lead to unintended harm if the predictions are biased. This study systematically examines the trade-offs between prediction accuracy and fairness in ML models trained on the widely used Open University Learning Analytics Dataset (OULAD). We evaluated the relationship between model accuracy and fairness across various student demographic subgroups and investigated the extent to which fairness can be improved without significantly sacrificing accuracy. Our analysis revealed that standard ML models often exhibit bias; however, applying bias mitigation techniques can reduce these disparities while maintaining acceptable accuracy. Our findings emphasize the importance of auditing ML models for fairness to ensure that predictive insights are equitable across diverse student populations. We also discuss implications for best practices and challenges in achieving fair ML models for student performance prediction.",
      "citationCount": 1,
      "doi": "10.18608/jla.2025.8543",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/ecdbafe24aefb2cddc25e48a58daa3388a6dce86",
      "venue": "Journal of Learning Analytics",
      "journal": {
        "name": "J. Learn. Anal.",
        "pages": "125-139",
        "volume": "12"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "693584820e6fa31fc8f7431c9f78e147a059234a",
      "title": "On responsible machine learning datasets emphasizing fairness, privacy and regulatory norms with examples in biometrics and healthcare",
      "authors": [
        {
          "name": "S. Mittal",
          "authorId": "2073395889"
        },
        {
          "name": "K. Thakral",
          "authorId": "2066220945"
        },
        {
          "name": "Richa Singh",
          "authorId": "2041134713"
        },
        {
          "name": "M. Vatsa",
          "authorId": "2250562057"
        },
        {
          "name": "Tamar Glaser",
          "authorId": "2071335303"
        },
        {
          "name": "Cristian Canton Ferrer",
          "authorId": "2318034069"
        },
        {
          "name": "Tal Hassner",
          "authorId": "2256650936"
        }
      ],
      "year": 2024,
      "abstract": "Artificial Intelligence (AI) has seamlessly integrated into numerous scientific domains, catalysing unparalleled enhancements across a broad spectrum of tasks; however, its integrity and trustworthiness have emerged as notable concerns. The scientific community has focused on the development of trustworthy AI algorithms; however, machine learning and deep learning algorithms, popular in the AI community today, intrinsically rely on the quality of their training data. These algorithms are designed to detect patterns within the data, thereby learning the intended behavioural objectives. Any inadequacy in the data has the potential to translate directly into algorithms. In this study we discuss the importance of responsible machine learning datasets through the lens of fairness, privacy and regulatory compliance, and present a large audit of computer vision datasets. Despite the ubiquity of fairness and privacy challenges across diverse data domains, current regulatory frameworks primarily address human-centric data concerns. We therefore focus our discussion on biometric and healthcare datasets, although the principles we outline are broadly applicable across various domains. The audit is conducted through evaluation of the proposed responsible rubric. After surveying over 100 datasets, our detailed analysis of 60 distinct datasets highlights a universal susceptibility to fairness, privacy and regulatory compliance issues. This finding emphasizes the urgent need for revising dataset creation methodologies within the scientific community, especially in light of global advancements in data protection legislation. We assert that our study is critically relevant in the contemporary AI context, offering insights and recommendations that are both timely and essential for the ongoing evolution of AI technologies. There are pervasive concerns related to fairness, privacy and regulatory compliance in machine learning applications in healthcare, necessitating a reevaluation of dataset creation practices. Mittal et al. examine various computer vision datasets, providing insights to foster responsible AI development.",
      "citationCount": 28,
      "doi": "10.1038/s42256-024-00874-y",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/693584820e6fa31fc8f7431c9f78e147a059234a",
      "venue": "Nature Machine Intelligence",
      "journal": {
        "name": "Nature Machine Intelligence",
        "pages": "936 - 949",
        "volume": "6"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "e1e7da0bb232715eb95814052508d84e7fc6cbd2",
      "title": "Fairness Auditing of Tabular Machine Learning via AIME: Exposing Dataset Bias with Groupwise \u0394\u2011Importance",
      "authors": [
        {
          "name": "Takafumi Nakanishi",
          "authorId": "2240189730"
        },
        {
          "name": "Ponlawat Chophuk",
          "authorId": "30767016"
        },
        {
          "name": "Krisana Chinnasarn",
          "authorId": "2296275128"
        }
      ],
      "year": 2025,
      "abstract": "This study proposes a practical workflow for fairness auditing in tabular machine learning (Tabular ML) by combining group-wise global importance differences (\u0394-importance) using Approximate Inverse Model Explanations (AIME) with concise fairness metrics (Demographic Parity [DP: selection rate and 4/5 rule] and Equalized Odds [EO: TPR/FPR difference]). Using the Adult (UCI Census Income) dataset, we trained a LightGBM model with sensitive attributes (gender and race) excluded from the training features and evaluated it at a threshold of 0.5. The overall performance was 0.875, the ROC-AUC was 0.929, and the selection rate was 0.200. From a fairness perspective, the gender selection rates were male (0.257) and female (0.084), with a four-fifths ratio of 0.328, and the EO had a TPR difference of 0.072 and an FPR difference of 0.058. In terms of race, the maximum selection rate was 0.238 (Asian-Pacific Islander) and the minimum was 0.073 (Other), with a four-fifths ratio of 0.308 and EO of TPR difference of 0.161/FPR difference of 0.088, indicating a significant deviation between the groups. By calculating the overall and group-specific importance using AIME in a consistent procedure and extracting \u0394-importance, we can specifically identify which features contribute relatively differently across groups, enabling explanation-guided mitigation based on explanations, such as preprocessing, feature design, threshold adjustment, calibration, and constrained learning. The proposed workflow is not specific to AIME and can be replaced or used in combination with other XAI metrics, such as SHAP, Permutation Importance, and SAGE. Limitations include dependence on operational thresholds and group sizes, estimation uncertainty, and the impossibility of reconciling fairness concepts. Nevertheless, this method is useful for visualizing data-driven biases and explicitly revealing their \"breakdown,\" demonstrating its practical applicability as a fairness audit template for Tabular ML.",
      "citationCount": 0,
      "doi": "10.1109/ICSEC67360.2025.11298030",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e1e7da0bb232715eb95814052508d84e7fc6cbd2",
      "venue": "International Computer Science and Engineering Conference",
      "journal": {
        "name": "2025 29th International Computer Science and Engineering Conference (ICSEC)",
        "pages": "77-84"
      },
      "publicationTypes": [
        "Conference"
      ]
    },
    {
      "paperId": "68968dd49dde6e99226529e4caed6f3fb76da899",
      "title": "Fairness-Aware Classification Based on Rawlsian Veil of Ignorance: A Mathematical Framework for Bias Detection and Mitigation in Machine Learning",
      "authors": [
        {
          "name": "Lin Chen",
          "authorId": "2397387049"
        }
      ],
      "year": 2025,
      "abstract": "Machine learning's penetration into high-stakes decision-making\u2014credit approvals, healthcare triage, criminal risk assessment\u2014has amplified pre-existing societal inequities rather than ameliorating them. This study operationalizes John Rawls's \"veil of ignorance\" (1971) as a computational principle for binary classifiers, confronting a gap: most fairness metrics lack philosophical grounding while Rawlsian theories remain mathematically unformalized. Through three empirical phases\u2014(1) baseline logistic regression on full feature sets, (2) bias quantification via disaggregated metrics across protected groups, and (3) mitigation via pre-processing blindess and post-processing threshold optimization\u2014we demonstrate how ignorance of demographic attributes can be algorithmically imposed. Using the German Credit Dataset (n=1,000), we expose a 12.9% accuracy gap between gender groups in standard models. Our framework collapses demographic parity difference from 15.7% to 0.1% while paradoxically boosting accuracy by 2.8% (from 72.0% to 74.0%), challenging the fairness-accuracy sacrifice orthodoxy. Counterintuitively, naive feature removal worsened bias (+8.9%), only proxy-aware pruning achieved DPD reduction of 32.6%. These findings suggest that Rawlsian principles, when translated into constrained optimization, yield Pareto-superior solutions\u2014though we argue such technical fixes must complement, not substitute for, institutional reform.",
      "citationCount": 0,
      "doi": "10.54097/v1xa7p32",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/68968dd49dde6e99226529e4caed6f3fb76da899",
      "venue": "Academic Journal of Science and Technology",
      "journal": {
        "name": "Academic Journal of Science and Technology"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2ae9e442ab86d5cb8699dd3506252591e5e09c22",
      "title": "Perceived Fairness of the Machine Learning Development Process: Concept Scale Development",
      "authors": [
        {
          "name": "Anoop Mishra",
          "authorId": "2087235720"
        },
        {
          "name": "Deepak Khazanchi",
          "authorId": "2081924633"
        }
      ],
      "year": 2025,
      "abstract": "In machine learning (ML) applications, unfairness is triggered due to bias in the data, the data curation process, erroneous assumptions, and implicit bias rendered during the development process. It is also well-accepted by researchers that fairness in ML application development is highly subjective, with a lack of clarity of what it means from an ML development and implementation perspective. Thus, in this research, we investigate and formalize the notion of the perceived fairness of ML development from a sociotechnical lens. Our goal in this research is to understand the characteristics of perceived fairness in ML applications. We address this research goal using a three-pronged strategy: 1) conducting virtual focus groups with ML developers, 2) reviewing existing literature on fairness in ML, and 3) incorporating aspects of justice theory relating to procedural and distributive justice. Based on our theoretical exposition, we propose operational attributes of perceived fairness to be transparency, accountability, and representativeness. These are described in terms of multiple concepts that comprise each dimension of perceived fairness. We use this operationalization to empirically validate the notion of perceived fairness of machine learning (ML) applications from both the ML practioners and users perspectives. The multidimensional framework for perceived fairness offers a comprehensive understanding of perceived fairness, which can guide the creation of fair ML systems with positive implications for society and businesses.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2501.13421",
      "arxivId": "2501.13421",
      "url": "https://www.semanticscholar.org/paper/2ae9e442ab86d5cb8699dd3506252591e5e09c22",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2501.13421"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "f7903f6f35173c9c911e5ce936c8abe50c065a7f",
      "title": "AI for Robustness and Fairness: Addressing bias, fairness and robustness in machine learning algorithms",
      "authors": [
        {
          "name": "Gaurav Kashyap",
          "authorId": "2339823886"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.51219/jaimld/gaurav-kashyap/443",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f7903f6f35173c9c911e5ce936c8abe50c065a7f",
      "venue": "Journal of Artificial Intelligence, Machine Learning and Data Science",
      "journal": {
        "name": "Journal of Artificial Intelligence, Machine Learning and Data Science"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d8431e5536d975490db151a127265fd87e367745",
      "title": "What Is Fair? Defining Fairness in Machine Learning for Health",
      "authors": [
        {
          "name": "Jianhui Gao",
          "authorId": "2306244777"
        },
        {
          "name": "Benson Chou",
          "authorId": "2306257779"
        },
        {
          "name": "Z. McCaw",
          "authorId": "2299683429"
        },
        {
          "name": "Hilary Thurston",
          "authorId": "2306252273"
        },
        {
          "name": "Paul Varghese",
          "authorId": "2171475909"
        },
        {
          "name": "Chuan Hong",
          "authorId": "2307069108"
        },
        {
          "name": "Jessica L. Gronsbell",
          "authorId": "14929566"
        }
      ],
      "year": 2024,
      "abstract": "Ensuring that machine\u2010learning (ML) models are safe, effective, and equitable across all patients is critical for clinical decision\u2010making and for preventing the amplification of existing health disparities. In this work, we examine how fairness is conceptualized in ML for health, including why ML models may lead to unfair decisions and how fairness has been measured in diverse real\u2010world applications. We review commonly used fairness notions within group, individual, and causal\u2010based frameworks. We also discuss the outlook for future research and highlight opportunities and challenges in operationalizing fairness in health\u2010focused applications.",
      "citationCount": 10,
      "doi": "10.1002/sim.70234",
      "arxivId": "2406.09307",
      "url": "https://www.semanticscholar.org/paper/d8431e5536d975490db151a127265fd87e367745",
      "venue": "Statistics in Medicine",
      "journal": {
        "name": "Statistics in Medicine",
        "volume": "44"
      },
      "publicationTypes": [
        "Review",
        "JournalArticle"
      ]
    },
    {
      "paperId": "a2432b8e49be8e8ca656a27cc9eca0c47226296a",
      "title": "A Multi-Objective Framework for Balancing Fairness and Accuracy in Debiasing Machine Learning Models",
      "authors": [
        {
          "name": "Rashmi Nagpal",
          "authorId": "2312605811"
        },
        {
          "name": "Ariba Khan",
          "authorId": "2108514656"
        },
        {
          "name": "Mihir Borkar",
          "authorId": "2322192065"
        },
        {
          "name": "Amar Gupta",
          "authorId": "2143747303"
        }
      ],
      "year": 2024,
      "abstract": "Machine learning algorithms significantly impact decision-making in high-stakes domains, necessitating a balance between fairness and accuracy. This study introduces an in-processing, multi-objective framework that leverages the Reject Option Classification (ROC) algorithm to simultaneously optimize fairness and accuracy while safeguarding protected attributes such as age and gender. Our approach seeks a multi-objective optimization solution that balances accuracy, group fairness loss, and individual fairness loss. The framework integrates fairness objectives without relying on a weighted summation method, instead focusing on directly optimizing the trade-offs. Empirical evaluations on publicly available datasets, including German Credit, Adult Income, and COMPAS, reveal several significant findings: the ROC-based approach demonstrates superior performance, achieving an accuracy of 94.29%, an individual fairness loss of 0.04, and a group fairness loss of 0.06 on the German Credit dataset. These results underscore the effectiveness of our framework, particularly the ROC component, in enhancing both the fairness and performance of machine learning models.",
      "citationCount": 8,
      "doi": "10.3390/make6030105",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a2432b8e49be8e8ca656a27cc9eca0c47226296a",
      "venue": "Machine Learning and Knowledge Extraction",
      "journal": {
        "name": "Mach. Learn. Knowl. Extr.",
        "pages": "2130-2148",
        "volume": "6"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c81a34828fe1bae635f309b636fad1953bfaf9ef",
      "title": "Data vs. Model Machine Learning Fairness Testing: An Empirical Study",
      "authors": [
        {
          "name": "Arumoy Shome",
          "authorId": "1380220231"
        },
        {
          "name": "Lu\u00eds Cruz",
          "authorId": "2267243294"
        },
        {
          "name": "Arie van Deursen",
          "authorId": "10734708"
        }
      ],
      "year": 2024,
      "abstract": "Although several fairness definitions and bias mitigation techniques exist in the literature, all existing solutions evaluate fairness of Machine Learning (ML) systems after the training stage. In this paper, we take the first steps towards evaluating a more holistic approach by testing for fairness both before and after model training. We evaluate the effectiveness of the proposed approach and position it within the ML development lifecycle, using an empirical analysis of the relationship between model dependent and independent fairness metrics. The study uses 2 fairness metrics, 4 ML algorithms, 5 real-world datasets and 1600 fairness evaluation cycles. We find a linear relationship between data and model fairness metrics when the distribution and the size of the training data changes. Our results indicate that testing for fairness prior to training can be a \u201ccheap\u201d and effective means of catching a biased data collection process early; detecting data drifts in production systems and minimising execution of full training cycles thus reducing development time and costs.",
      "citationCount": 6,
      "doi": "10.1145/3643786.3648022",
      "arxivId": "2401.07697",
      "url": "https://www.semanticscholar.org/paper/c81a34828fe1bae635f309b636fad1953bfaf9ef",
      "venue": "Workshop on Deep Learning for Testing and Testing for Deep Learning",
      "journal": {
        "name": "2024 IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest)",
        "pages": "1-8"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "577e4670dfdf8c4148f2412ce89f317fe802659e",
      "title": "A novel approach for assessing fairness in deployed machine learning algorithms",
      "authors": [
        {
          "name": "S. Uddin",
          "authorId": "1798025"
        },
        {
          "name": "Haohui Lu",
          "authorId": "2149891854"
        },
        {
          "name": "Ashfaqur Rahman",
          "authorId": "2284217160"
        },
        {
          "name": "Junbin Gao",
          "authorId": "2261897593"
        }
      ],
      "year": 2024,
      "abstract": "Fairness in machine learning (ML) emerges as a critical concern as AI systems increasingly influence diverse aspects of society, from healthcare decisions to legal judgments. Many studies show evidence of unfair ML outcomes. However, the current body of literature lacks a statistically validated approach that can evaluate the fairness of a deployed ML algorithm against a dataset. A novel evaluation approach is introduced in this research based on k-fold cross-validation and statistical t-tests to assess the fairness of ML algorithms. This approach was exercised across five benchmark datasets using six classical ML algorithms. Considering four fair ML definitions guided by the current literature, our analysis showed that the same dataset generates a fair outcome for one ML algorithm but an unfair result for another. Such an observation reveals complex, context-dependent fairness issues in ML, complicated further by the varied operational mechanisms of the underlying ML models. Our proposed approach enables researchers to check whether deploying any ML algorithms against a protected attribute within datasets is fair. We also discuss the broader implications of the proposed approach, highlighting a notable variability in its fairness outcomes. Our discussion underscores the need for adaptable fairness definitions and the exploration of methods to enhance the fairness of ensemble approaches, aiming to advance fair ML practices and ensure equitable AI deployment across societal sectors.",
      "citationCount": 5,
      "doi": "10.1038/s41598-024-68651-w",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/577e4670dfdf8c4148f2412ce89f317fe802659e",
      "venue": "Scientific Reports",
      "journal": {
        "name": "Scientific Reports",
        "volume": "14"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "704b5a29ac8d7a264f67aadc582b47a9a411a721",
      "title": "Promoting Equity: Assessing Algorithmic Fairness in Machine Learning Approaches for Predicting Alzheimer's Disease",
      "authors": [
        {
          "name": "Kenna Zhang",
          "authorId": "2346360927"
        }
      ],
      "year": 2024,
      "abstract": "Alzheimer's disease (AD) presents a significant challenge for older adults in marginalized communities. The application of machine learning (ML) techniques through predictive models holds promise for enhancing the early detection and management of AD. However, it is essential to address potential biases within ML models that may contribute to or exacerbate existing disparities. This study delves into the algorithmic fairness of four distinct ML models, logistic regression (LR), random forest (RF), k-nearest neighbors (KNN), and multilayer perceptron (MLP), in predicting the Alzheimer's disease. The assessment of fairness spans race, ethnicity, and gender subgroups, employing major key measurements: demographic parity, equalized odds, and disparate impact. While all four ML models exhibited strong overall performance, disparities emerged across race and ethnicity subgroups. Participants with Hispanic ethnicity background experienced lower sensitivity compared to their Non-Hispanic counterparts. Similar patterns of decreased sensitivity were observed for non-white participants in contrast to Non-Hispanic White individuals. Prediction performance dropped when demographic features was included. Despite their aggregate accuracy, the ML models fell short in meeting fairness metrics, emphasizing the need to integrate fairness considerations in the phase ML models for AD prediction. Addressing these disparities is crucial for ensuring equitable outcomes in the utilization of predictive models within diverse populations.",
      "citationCount": 0,
      "doi": "10.1109/FMLDS63805.2024.00063",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/704b5a29ac8d7a264f67aadc582b47a9a411a721",
      "venue": "2024 IEEE International Conference on Future Machine Learning and Data Science (FMLDS)",
      "journal": {
        "name": "2024 IEEE International Conference on Future Machine Learning and Data Science (FMLDS)",
        "pages": "315-318"
      },
      "publicationTypes": [
        "Conference"
      ]
    },
    {
      "paperId": "35ddc9072c09ef7b06d1769aa5f15b06e3e1d039",
      "title": "Responsible Machine Learning in Student-Facing Applications: Bias Mitigation & Fairness Frameworks",
      "authors": [
        {
          "name": "Jayant Bhat",
          "authorId": "2401897756"
        }
      ],
      "year": 2024,
      "abstract": "The increasing deployment of machine learning (ML) in student-facing applications\u2014such as academic performance prediction, automated assessment, intelligent tutoring, and early-warning systems\u2014has amplified concerns about bias, fairness, and accountability in educational decision-making. As of 2024, educational institutions rely heavily on data-driven models to support high-impact student outcomes, making responsible ML practices essential for protecting equity and trust. This paper investigates algorithmic bias in student-facing ML systems and evaluates contemporary fairness frameworks and mitigation strategies across the full ML lifecycle. Using large-scale educational data, the study analyzes sources of bias arising from socioeconomic, demographic, and behavioral factors, and examines their influence on predictive models. The paper reviews group, individual, and counterfactual fairness metrics, highlighting practical trade-offs between fairness and predictive accuracy. A comprehensive responsible ML fairness framework is proposed, integrating privacy preservation, fairness-aware learning, continuous monitoring, and human-in-the-loop governance. Empirical evaluation on the STAAR dataset from Texas public schools (2012\u20132019, approximately five million students) demonstrates that bias mitigation techniques can improve fairness metrics by over 20% on average, while revealing inherent fairness\u2013accuracy and interpretability trade-offs. The findings emphasize that fairness is not a one-time intervention but a continuous operational requirement. This work provides actionable guidance for designing, deploying, and governing equitable ML systems in educational environments, aligning technical innovation with ethical responsibility and inclusive student outcomes.",
      "citationCount": 0,
      "doi": "10.63282/3117-5481/aijcst-v6i1p104",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/35ddc9072c09ef7b06d1769aa5f15b06e3e1d039",
      "venue": "American International Journal of Computer Science and Technology",
      "journal": {
        "name": "American International Journal of Computer Science and Technology"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "105912cee50f1e878e092a9d68c2e0af7f4968dc",
      "title": "Algorithmic fairness and bias mitigation for clinical machine learning with deep reinforcement learning",
      "authors": [
        {
          "name": "Jenny Yang",
          "authorId": "2109754263"
        },
        {
          "name": "A. Soltan",
          "authorId": "52279987"
        },
        {
          "name": "D. Eyre",
          "authorId": "2376251"
        },
        {
          "name": "D. Clifton",
          "authorId": "2059501659"
        }
      ],
      "year": 2023,
      "abstract": "As models based on machine learning continue to be developed for healthcare applications, greater effort is needed to ensure that these technologies do not reflect or exacerbate any unwanted or discriminatory biases that may be present in the data. Here we introduce a reinforcement learning framework capable of mitigating biases that may have been acquired during data collection. In particular, we evaluated our model for the task of rapidly predicting COVID-19 for patients presenting to hospital emergency departments and aimed to mitigate any site (hospital)-specific and ethnicity-based biases present in the data. Using a specialized reward function and training procedure, we show that our method achieves clinically effective screening performances, while significantly improving outcome fairness compared with current benchmarks and state-of-the-art machine learning methods. We performed external validation across three independent hospitals, and additionally tested our method on a patient intensive care unit discharge status task, demonstrating model generalizability. The tendency of machine learning algorithms to learn biases from training data calls for methods to mitigate unfairness before deployment to healthcare and other applications. Yang et al. propose a reinforcement-learning-based method for algorithmic bias mitigation and demonstrate it on COVID-19 screening and patient discharge prediction tasks.",
      "citationCount": 73,
      "doi": "10.1038/s42256-023-00697-3",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/105912cee50f1e878e092a9d68c2e0af7f4968dc",
      "venue": "Nature Machine Intelligence",
      "journal": {
        "name": "Nature Machine Intelligence",
        "pages": "884 - 894",
        "volume": "5"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0d3b64836fb307a4352e9b942dce0e9e1dd16458",
      "title": "On The Impact of Machine Learning Randomness on Group Fairness",
      "authors": [
        {
          "name": "Prakhar Ganesh",
          "authorId": "51236762"
        },
        {
          "name": "Hong Chang",
          "authorId": "2116284897"
        },
        {
          "name": "Martin Strobel",
          "authorId": "35149351"
        },
        {
          "name": "R. Shokri",
          "authorId": "2520493"
        }
      ],
      "year": 2023,
      "abstract": "Statistical measures for group fairness in machine learning reflect the gap in performance of algorithms across different groups. These measures, however, exhibit a high variance between different training instances, which makes them unreliable for empirical evaluation of fairness. What causes this high variance? We investigate the impact on group fairness of different sources of randomness in training neural networks. We show that the variance in group fairness measures is rooted in the high volatility of the learning process on under-represented groups. Further, we recognize the dominant source of randomness as the stochasticity of data order during training. Based on these findings, we show how one can control group-level accuracy (i.e., model fairness), with high efficiency and negligible impact on the model\u2019s overall performance, by simply changing the data order for a single epoch.",
      "citationCount": 39,
      "doi": "10.1145/3593013.3594116",
      "arxivId": "2307.04138",
      "url": "https://www.semanticscholar.org/paper/0d3b64836fb307a4352e9b942dce0e9e1dd16458",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle"
      ]
    },
    {
      "paperId": "f64670a5f54fcce339a916497a001cbf02a9a04f",
      "title": "A Review on Fairness in Machine Learning",
      "authors": [
        {
          "name": "Dana Pessach",
          "authorId": "2130148626"
        },
        {
          "name": "E. Shmueli",
          "authorId": "1824816"
        }
      ],
      "year": 2022,
      "abstract": "An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence and machine learning (ML) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans, and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop ML algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision making may be inherently prone to unfairness, even when there is no intention for it. This article presents an overview of the main concepts of identifying, measuring, and improving algorithmic fairness when using ML algorithms, focusing primarily on classification tasks. The article begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process, and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, toward a better understanding of which mechanisms should be used in different scenarios. The article ends by reviewing several emerging research sub-fields of algorithmic fairness, beyond classification.",
      "citationCount": 591,
      "doi": "10.1145/3494672",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f64670a5f54fcce339a916497a001cbf02a9a04f",
      "venue": "ACM Computing Surveys",
      "journal": {
        "name": "ACM Computing Surveys (CSUR)",
        "pages": "1 - 44",
        "volume": "55"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "5077f51744f1163275998659f25f47c7d78b33c0",
      "title": "Fairness in machine learning from the perspective of sociology of statistics: How machine learning is becoming scientific by turning its back on metrological realism",
      "authors": [
        {
          "name": "Bilel Benbouzid",
          "authorId": "2070696486"
        }
      ],
      "year": 2023,
      "abstract": "We argue in this article that the integration of fairness into machine learning, or FairML, is a valuable exemplar of the politics of statistics and their ongoing transformations. Classically, statisticians sought to eliminate any trace of politics from their measurement tools. But data scientists who are developing predictive machines for social applications \u2013 are inevitably confronted with the problem of fairness. They thus face two difficult and often distinct types of demands: first, for reliable computational techniques, and second, for transparency, given the constructed, politically situated nature of quantification operations. We begin by socially localizing the formation of FairML as a field of research and describing the associated epistemological framework. We then examine how researchers simultaneously think the mathematical and social construction of approaches to machine learning, following controversies around fairness metrics and their status. Thirdly and finally, we show that FairML approaches tend towards a specific form of objectivity, \u201ctrained judgement,\u201d which is based on a reasonably partial justification from the designer of the machine \u2013 which itself comes to be politically situated as a result.",
      "citationCount": 9,
      "doi": "10.1145/3593013.3593974",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/5077f51744f1163275998659f25f47c7d78b33c0",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle"
      ]
    },
    {
      "paperId": "e58b71c6fb5c4293e83ad186fad459bc4f6de921",
      "title": "FairCompass: Operationalizing Fairness in Machine Learning",
      "authors": [
        {
          "name": "Jessica Liu",
          "authorId": "2276665273"
        },
        {
          "name": "Huaming Chen",
          "authorId": "49177531"
        },
        {
          "name": "Jun Shen",
          "authorId": "2237849897"
        },
        {
          "name": "Kim-Kwang Raymond Choo",
          "authorId": "2275597561"
        }
      ],
      "year": 2023,
      "abstract": "As artificial intelligence (AI) increasingly becomes an integral part of our societal and individual activities, there is a growing imperative to develop responsible AI solutions. Despite a diverse assortment of machine learning fairness solutions is proposed in the literature, there is reportedly a lack of practical implementation of these tools in real-world applications. Industry experts have participated in thorough discussions on the challenges associated with operationalizing fairness in the development of machine learning-empowered solutions, in which a shift toward human-centred approaches is promptly advocated to mitigate the limitations of existing techniques. In this work, we propose a human-in-the-loop approach for fairness auditing, presenting a mixed visual analytical system (hereafter referred to as \u201cFairCompass\u201d), which integrates both subgroup discovery technique and the decision tree-based schema for end users. Moreover, we innovatively integrate an exploration, guidance, and informed analysis loop, to facilitate the use of the knowledge generation model for visual analytics in FairCompass. We evaluate the effectiveness of FairCompass for fairness auditing in a real-world scenario, and the findings demonstrate the system's potential for real-world deployability. We anticipate this work will address the current gaps in research for fairness and facilitate the operationalization of fairness in machine learning systems.",
      "citationCount": 8,
      "doi": "10.1109/TAI.2023.3348429",
      "arxivId": "2312.16726",
      "url": "https://www.semanticscholar.org/paper/e58b71c6fb5c4293e83ad186fad459bc4f6de921",
      "venue": "IEEE Transactions on Artificial Intelligence",
      "journal": {
        "name": "IEEE Transactions on Artificial Intelligence",
        "pages": "281-291",
        "volume": "6"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f3d096d0e299fd79d0f4364992dc849741123c80",
      "title": "On The Fairness Impacts of Hardware Selection in Machine Learning",
      "authors": [
        {
          "name": "Sree Harsha Nelaturu",
          "authorId": "1388047904"
        },
        {
          "name": "Nishaanth Kanna Ravichandran",
          "authorId": "2271649487"
        },
        {
          "name": "Cuong Tran",
          "authorId": "2055333757"
        },
        {
          "name": "Sara Hooker",
          "authorId": "2304752569"
        },
        {
          "name": "Ferdinando Fioretto",
          "authorId": "2141569789"
        }
      ],
      "year": 2023,
      "abstract": "In the machine learning ecosystem, hardware selection is often regarded as a mere utility, overshadowed by the spotlight on algorithms and data. This oversight is particularly problematic in contexts like ML-as-a-service platforms, where users often lack control over the hardware used for model deployment. How does the choice of hardware impact generalization properties? This paper investigates the influence of hardware on the delicate balance between model performance and fairness. We demonstrate that hardware choices can exacerbate existing disparities, attributing these discrepancies to variations in gradient flows and loss surfaces across different demographic groups. Through both theoretical and empirical analysis, the paper not only identifies the underlying factors but also proposes an effective strategy for mitigating hardware-induced performance imbalances.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2312.03886",
      "arxivId": "2312.03886",
      "url": "https://www.semanticscholar.org/paper/f3d096d0e299fd79d0f4364992dc849741123c80",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2312.03886"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "b41daf07f4cb04f494a0c7ce420afc68b3e6dbf5",
      "title": "Beyond bias and discrimination: redefining the AI ethics principle of fairness in healthcare machine-learning algorithms",
      "authors": [
        {
          "name": "B. Giovanola",
          "authorId": "93899248"
        },
        {
          "name": "S. Tiribelli",
          "authorId": "2139844938"
        }
      ],
      "year": 2022,
      "abstract": "The increasing implementation of and reliance on machine-learning (ML) algorithms to perform tasks, deliver services and make decisions in health and healthcare have made the need for fairness in ML, and more specifically in healthcare ML algorithms (HMLA), a very important and urgent task. However, while the debate on fairness in the ethics of artificial intelligence (AI) and in HMLA has grown significantly over the last decade, the very concept of fairness as an ethical value has not yet been sufficiently explored. Our paper aims to fill this gap and address the AI ethics principle of fairness from a conceptual standpoint, drawing insights from accounts of fairness elaborated in moral philosophy and using them to conceptualise fairness as an ethical value and to redefine fairness in HMLA accordingly. To achieve our goal, following a first section aimed at clarifying the background, methodology and structure of the paper, in the second section, we provide an overview of the discussion of the AI ethics principle of fairness in HMLA and show that the concept of fairness underlying this debate is framed in purely distributive terms and overlaps with non-discrimination, which is defined in turn as the absence of biases. After showing that this framing is inadequate, in the third section, we pursue an ethical inquiry into the concept of fairness and argue that fairness ought to be conceived of as an ethical value. Following a clarification of the relationship between fairness and non-discrimination, we show that the two do not overlap and that fairness requires much more than just non-discrimination. Moreover, we highlight that fairness not only has a distributive but also a socio-relational dimension. Finally, we pinpoint the constitutive components of fairness. In doing so, we base our arguments on a renewed reflection on the concept of respect, which goes beyond the idea of equal respect to include respect for individual persons. In the fourth section, we analyse the implications of our conceptual redefinition of fairness as an ethical value in the discussion of fairness in HMLA. Here, we claim that fairness requires more than non-discrimination and the absence of biases as well as more than just distribution; it needs to ensure that HMLA respects persons both as persons and as particular individuals. Finally, in the fifth section, we sketch some broader implications and show how our inquiry can contribute to making HMLA and, more generally, AI promote the social good and a fairer society.",
      "citationCount": 116,
      "doi": "10.1007/s00146-022-01455-6",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/b41daf07f4cb04f494a0c7ce420afc68b3e6dbf5",
      "venue": "Ai & Society",
      "journal": {
        "name": "Ai & Society",
        "pages": "549 - 563",
        "volume": "38"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "f0cc477e06c855a16321ac90dc55719a377d8799",
      "title": "Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems",
      "authors": [
        {
          "name": "Ioannis Pastaltzidis",
          "authorId": "2171362585"
        },
        {
          "name": "N. Dimitriou",
          "authorId": "1753998"
        },
        {
          "name": "K. Quezada-Tav\u00e1rez",
          "authorId": "2101048118"
        },
        {
          "name": "Stergios Aidinlis",
          "authorId": "72079363"
        },
        {
          "name": "Thomas Marquenie",
          "authorId": "31689335"
        },
        {
          "name": "Agata Gurzawska",
          "authorId": "118624225"
        },
        {
          "name": "D. Tzovaras",
          "authorId": "143636644"
        }
      ],
      "year": 2022,
      "abstract": "Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.",
      "citationCount": 44,
      "doi": "10.1145/3531146.3534644",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f0cc477e06c855a16321ac90dc55719a377d8799",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle"
      ]
    },
    {
      "paperId": "594e9d5c39ed84371bd240ccc9e7775803a6bded",
      "title": "Exploring How Machine Learning Practitioners (Try To) Use Fairness Toolkits",
      "authors": [
        {
          "name": "Wesley Hanwen Deng",
          "authorId": "2066620785"
        },
        {
          "name": "Manish Nagireddy",
          "authorId": "2163096175"
        },
        {
          "name": "M. S. Lee",
          "authorId": "1739477817"
        },
        {
          "name": "Jatinder Singh",
          "authorId": "2109471576"
        },
        {
          "name": "Zhiwei Steven Wu",
          "authorId": "1768074"
        },
        {
          "name": "Kenneth Holstein",
          "authorId": "2257955034"
        },
        {
          "name": "Haiyi Zhu",
          "authorId": "1742431"
        }
      ],
      "year": 2022,
      "abstract": "Recent years have seen the development of many open-source ML fairness toolkits aimed at helping ML practitioners assess and address unfairness in their systems. However, there has been little research investigating how ML practitioners actually use these toolkits in practice. In this paper, we conducted the first in-depth empirical exploration of how industry practitioners (try to) work with existing fairness toolkits. In particular, we conducted think-aloud interviews to understand how participants learn about and use fairness toolkits, and explored the generality of our findings through an anonymous online survey. We identified several opportunities for fairness toolkits to better address practitioner needs and scaffold them in using toolkits effectively and responsibly. Based on these findings, we highlight implications for the design of future open-source fairness toolkits that can support practitioners in better contextualizing, communicating, and collaborating around ML fairness efforts.",
      "citationCount": 112,
      "doi": "10.1145/3531146.3533113",
      "arxivId": "2205.06922",
      "url": "https://www.semanticscholar.org/paper/594e9d5c39ed84371bd240ccc9e7775803a6bded",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book",
        "Review"
      ]
    },
    {
      "paperId": "fee8f63972906214b77f16cfeca0b93ee8f36ba2",
      "title": "Fairness in Machine Learning: A Survey",
      "authors": [
        {
          "name": "Simon Caton",
          "authorId": "2452441"
        },
        {
          "name": "C. Haas",
          "authorId": "152864672"
        }
      ],
      "year": 2020,
      "abstract": "When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.",
      "citationCount": 793,
      "doi": "10.1145/3616865",
      "arxivId": "2010.04053",
      "url": "https://www.semanticscholar.org/paper/fee8f63972906214b77f16cfeca0b93ee8f36ba2",
      "venue": "ACM Computing Surveys",
      "journal": {
        "name": "ACM Computing Surveys",
        "pages": "1 - 38",
        "volume": "56"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "ac61014a786b8d3ca66a40b13dcd901db927077c",
      "title": "Do the Ends Justify the Means? Variation in the Distributive and Procedural Fairness of Machine Learning Algorithms",
      "authors": [
        {
          "name": "Lily Morse",
          "authorId": "81918527"
        },
        {
          "name": "M. Teodorescu",
          "authorId": "50021825"
        },
        {
          "name": "Yazeed Awwad",
          "authorId": "1934464783"
        },
        {
          "name": "Gerald C. Kane",
          "authorId": "2878729"
        }
      ],
      "year": 2021,
      "abstract": null,
      "citationCount": 62,
      "doi": "10.1007/s10551-021-04939-5",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/ac61014a786b8d3ca66a40b13dcd901db927077c",
      "venue": "Journal of Business Ethics",
      "journal": {
        "name": "Journal of Business Ethics",
        "pages": "1083-1095",
        "volume": "181"
      },
      "publicationTypes": null
    },
    {
      "paperId": "b75e3c5cc0d57866b178e62e159ab58d8c388f79",
      "title": "Multi-disciplinary fairness considerations in machine learning for clinical trials",
      "authors": [
        {
          "name": "Isabel Chien",
          "authorId": "145890038"
        },
        {
          "name": "Nina Deliu",
          "authorId": "51500221"
        },
        {
          "name": "Richard E. Turner",
          "authorId": "145369890"
        },
        {
          "name": "Adrian Weller",
          "authorId": "145689461"
        },
        {
          "name": "S. Villar",
          "authorId": "3194835"
        },
        {
          "name": "Niki Kilbertus",
          "authorId": "2346973067"
        }
      ],
      "year": 2022,
      "abstract": "While interest in the application of machine learning to improve healthcare has grown tremendously in recent years, a number of barriers prevent deployment in medical practice. A notable concern is the potential to exacerbate entrenched biases and existing health disparities in society. The area of fairness in machine learning seeks to address these issues of equity; however, appropriate approaches are context-dependent, necessitating domain-specific consideration. We focus on clinical trials, i.e., research studies conducted on humans to evaluate medical treatments. Clinical trials are a relatively under-explored application in machine learning for healthcare, in part due to complex ethical, legal, and regulatory requirements and high costs. Our aim is to provide a multi-disciplinary assessment of how fairness for machine learning fits into the context of clinical trials research and practice. We start by reviewing the current ethical considerations and guidelines for clinical trials and examine their relationship with common definitions of fairness in machine learning. We examine potential sources of unfairness in clinical trials, providing concrete examples, and discuss the role machine learning might play in either mitigating potential biases or exacerbating them when applied without care. Particular focus is given to adaptive clinical trials, which may employ machine learning. Finally, we highlight concepts that require further investigation and development, and emphasize new approaches to fairness that may be relevant to the design of clinical trials.",
      "citationCount": 26,
      "doi": "10.1145/3531146.3533154",
      "arxivId": "2205.08875",
      "url": "https://www.semanticscholar.org/paper/b75e3c5cc0d57866b178e62e159ab58d8c388f79",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "d3992c3d89e5ec05e0a6c96c4956f2ff9f7da023",
      "title": "Machine learning and algorithmic fairness in public and population health",
      "authors": [
        {
          "name": "Vishwali Mhasawade",
          "authorId": "51429443"
        },
        {
          "name": "Yuan Zhao",
          "authorId": "2110150937"
        },
        {
          "name": "R. Chunara",
          "authorId": "3144230"
        }
      ],
      "year": 2021,
      "abstract": null,
      "citationCount": 167,
      "doi": "10.1038/s42256-021-00373-4",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/d3992c3d89e5ec05e0a6c96c4956f2ff9f7da023",
      "venue": "Nature Machine Intelligence",
      "journal": {
        "name": "Nature Machine Intelligence",
        "pages": "659 - 666",
        "volume": "3"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6305ce02d4425d3f4764c941ab71134e9d55e92f",
      "title": "Equity in essence: a call for operationalising fairness in machine learning for healthcare",
      "authors": [
        {
          "name": "Judy Wawira Gichoya",
          "authorId": "2086837115"
        },
        {
          "name": "L. McCoy",
          "authorId": "1577224695"
        },
        {
          "name": "L. Celi",
          "authorId": "143605744"
        },
        {
          "name": "M. Ghassemi",
          "authorId": "2804918"
        }
      ],
      "year": 2021,
      "abstract": "\u00a9 Author(s) (or their employer(s)) 2021. Reuse permitted under CC BYNC. No commercial reuse. See rights and permissions. Published by BMJ. INTRODUCTION Machine learning for healthcare (MLHC) is at the juncture of leaping from the pages of journals and conference proceedings to clinical implementation at the bedside. Succeeding in this endeavour requires the synthesis of insights from both the machine learning and healthcare domains, in order to ensure that the unique characteristics of MLHC are leveraged to maximise benefits and minimise risks. An important part of this effort is establishing and formalising processes and procedures for characterising these tools and assessing their performance. Meaningful progress in this direction can be found in recently developed guidelines for the development of MLHC models, guidelines for the design and reporting of MLHC clinical trials, 3 and protocols for the regulatory assessment of MLHC tools. 5 But while such guidelines and protocols engage extensively with relevant technical considerations, engagement with issues of fairness, bias and unintended disparate impact is lacking. Such issues have taken on a place of prominence in the broader ML community, with recent work highlighting issues such as racial disparities in the accuracy of facial recognition and gender classification software, 10 gender bias in the output of natural language processing models 12 and racial bias in algorithms for bail and criminal sentencing. MLHC is not immune to these concerns, as seen in disparate outcomes from algorithms for allocating healthcare resources, 15 bias in language models developed on clinical notes and melanoma detection models developed primarily on images of lightcoloured skin. Within this paper, we will examine the inclusion of fairness in recent guidelines for MLHC model reporting, clinical trials and regulatory approval. We highlight opportunities to ensure that fairness is made fundamental to MLHC, and examine ways how this can be operationalised for the MLHC context.",
      "citationCount": 84,
      "doi": "10.1136/bmjhci-2020-100289",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6305ce02d4425d3f4764c941ab71134e9d55e92f",
      "venue": "BMJ Health & Care Informatics",
      "journal": {
        "name": "BMJ Health & Care Informatics",
        "volume": "28"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 40,
  "errors": []
}
