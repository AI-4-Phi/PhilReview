{
  "status": "success",
  "source": "semantic_scholar",
  "query": "multicalibration",
  "results": [
    {
      "paperId": "71d517ea75fac9f9fc62aefca4ad02eaa7cc0c76",
      "title": "Multicalibration for Confidence Scoring in LLMs",
      "authors": [
        {
          "name": "Gianluca Detommaso",
          "authorId": "2295667267"
        },
        {
          "name": "Martin Bertran",
          "authorId": "2295665717"
        },
        {
          "name": "Riccardo Fogliato",
          "authorId": "2295664744"
        },
        {
          "name": "Aaron Roth",
          "authorId": "2295665299"
        }
      ],
      "year": 2024,
      "abstract": "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
      "citationCount": 31,
      "doi": "10.48550/arXiv.2404.04689",
      "arxivId": "2404.04689",
      "url": "https://www.semanticscholar.org/paper/71d517ea75fac9f9fc62aefca4ad02eaa7cc0c76",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2404.04689"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "4c6bc7314d88ee1a16ed9d35e853f0e51cd42bce",
      "title": "Improved Bounds for Swap Multicalibration and Swap Omniprediction",
      "authors": [
        {
          "name": "Haipeng Luo",
          "authorId": "2304143210"
        },
        {
          "name": "Spandan Senapati",
          "authorId": "2194651488"
        },
        {
          "name": "Vatsal Sharan",
          "authorId": "2798845"
        }
      ],
      "year": 2025,
      "abstract": "In this paper, we consider the related problems of multicalibration -- a multigroup fairness notion and omniprediction -- a simultaneous loss minimization paradigm, both in the distributional and online settings. The recent work of Garg et al. (2024) raised the open problem of whether it is possible to efficiently achieve $O(\\sqrt{T})$ $\\ell_{2}$-multicalibration error against bounded linear functions. In this paper, we answer this question in a strongly affirmative sense. We propose an efficient algorithm that achieves $O(T^{\\frac{1}{3}})$ $\\ell_{2}$-swap multicalibration error (both in high probability and expectation). On propagating this bound onward, we obtain significantly improved rates for $\\ell_{1}$-swap multicalibration and swap omniprediction for a loss class of convex Lipschitz functions. In particular, we show that our algorithm achieves $O(T^{\\frac{2}{3}})$ $\\ell_{1}$-swap multicalibration and swap omniprediction errors, thereby improving upon the previous best-known bound of $O(T^{\\frac{7}{8}})$. As a consequence of our improved online results, we further obtain several improved sample complexity rates in the distributional setting. In particular, we establish a $O(\\varepsilon ^ {-3})$ sample complexity of efficiently learning an $\\varepsilon$-swap omnipredictor for the class of convex and Lipschitz functions, $O(\\varepsilon ^{-2.5})$ sample complexity of efficiently learning an $\\varepsilon$-swap agnostic learner for the squared loss, and $O(\\varepsilon ^ {-5}), O(\\varepsilon ^ {-2.5})$ sample complexities of learning $\\ell_{1}, \\ell_{2}$-swap multicalibrated predictors against linear functions, all of which significantly improve on the previous best-known bounds.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2505.20885",
      "arxivId": "2505.20885",
      "url": "https://www.semanticscholar.org/paper/4c6bc7314d88ee1a16ed9d35e853f0e51cd42bce",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.20885"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1d29ab84c638270868e511fe7240a2eba29b4e19",
      "title": "Discretization-free Multicalibration through Loss Minimization over Tree Ensembles",
      "authors": [
        {
          "name": "Hongyi Henry Jin",
          "authorId": "2363414319"
        },
        {
          "name": "Zijun Ding",
          "authorId": "2363673811"
        },
        {
          "name": "D. D. Ngo",
          "authorId": "2303842192"
        },
        {
          "name": "Zhiwei Steven Wu",
          "authorId": "2238047775"
        }
      ],
      "year": 2025,
      "abstract": "In recent years, multicalibration has emerged as a desirable learning objective for ensuring that a predictor is calibrated across a rich collection of overlapping subpopulations. Existing approaches typically achieve multicalibration by discretizing the predictor's output space and iteratively adjusting its output values. However, this discretization approach departs from the standard empirical risk minimization (ERM) pipeline, introduces rounding error and additional sensitive hyperparameter, and may distort the predictor's outputs in ways that hinder downstream decision-making. In this work, we propose a discretization-free multicalibration method that directly optimizes an empirical risk objective over an ensemble of depth-two decision trees. Our ERM approach can be implemented using off-the-shelf tree ensemble learning methods such as LightGBM. Our algorithm provably achieves multicalibration, provided that the data distribution satisfies a technical condition we term as loss saturation. Across multiple datasets, our empirical evaluation shows that this condition is always met in practice. Our discretization-free algorithm consistently matches or outperforms existing multicalibration approaches--even when evaluated using a discretization-based multicalibration metric that shares its discretization granularity with the baselines.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2505.17435",
      "arxivId": "2505.17435",
      "url": "https://www.semanticscholar.org/paper/1d29ab84c638270868e511fe7240a2eba29b4e19",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.17435"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a38a6f445a66c2d2255c0e718195e6f573921c47",
      "title": "Multiaccuracy and Multicalibration via Proxy Groups",
      "authors": [
        {
          "name": "Beepul Bharti",
          "authorId": "2176187091"
        },
        {
          "name": "M. Clemens-Sewall",
          "authorId": "2138581500"
        },
        {
          "name": "P. Yi",
          "authorId": "2055854318"
        },
        {
          "name": "Jeremias Sulam",
          "authorId": "2714145"
        }
      ],
      "year": 2025,
      "abstract": "As the use of predictive machine learning algorithms increases in high-stakes decision-making, it is imperative that these algorithms are fair across sensitive groups. However, measuring and enforcing fairness in real-world applications can be challenging due to the missing or incomplete sensitive group information. Proxy-sensitive attributes have been proposed as a practical and effective solution in these settings, but only for parity-based fairness notions. Knowing how to evaluate and control for fairness with missing sensitive group data for newer, different, and more flexible frameworks, such as multiaccuracy and multicalibration, remain unexplored. In this work, we address this gap by demonstrating that in the absence of sensitive group data, proxy-sensitive attributes can provably used to derive actionable upper bounds on the true multiaccuracy and multicalibration violations, providing insights into a predictive model's potential worst-case fairness violations. Additionally, we show that adjusting models to satisfy multiaccuracy and multicalibration across proxy-sensitive attributes can significantly mitigate these violations for the true, but unknown, sensitive groups. Through several experiments on real-world datasets, we illustrate that approximate multiaccuracy and multicalibration can be achieved even when sensitive group data is incomplete or unavailable.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2503.02870",
      "arxivId": "2503.02870",
      "url": "https://www.semanticscholar.org/paper/a38a6f445a66c2d2255c0e718195e6f573921c47",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.02870"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "9dabb370d4e8e12f3753ac228bb9ec228d65e659",
      "title": "When is Multicalibration Post-Processing Necessary?",
      "authors": [
        {
          "name": "Dutch Hansen",
          "authorId": "2305623214"
        },
        {
          "name": "Siddartha Devic",
          "authorId": "117403158"
        },
        {
          "name": "Preetum Nakkiran",
          "authorId": "2181918"
        },
        {
          "name": "Vatsal Sharan",
          "authorId": "2798845"
        }
      ],
      "year": 2024,
      "abstract": "Calibration is a well-studied property of predictors which guarantees meaningful uncertainty estimates. Multicalibration is a related notion -- originating in algorithmic fairness -- which requires predictors to be simultaneously calibrated over a potentially complex and overlapping collection of protected subpopulations (such as groups defined by ethnicity, race, or income). We conduct the first comprehensive study evaluating the usefulness of multicalibration post-processing across a broad set of tabular, image, and language datasets for models spanning from simple decision trees to 90 million parameter fine-tuned LLMs. Our findings can be summarized as follows: (1) models which are calibrated out of the box tend to be relatively multicalibrated without any additional post-processing; (2) multicalibration post-processing can help inherently uncalibrated models and large vision and language models; and (3) traditional calibration measures may sometimes provide multicalibration implicitly. More generally, we also distill many independent observations which may be useful for practical and effective applications of multicalibration post-processing in real-world contexts. We also release a python package implementing multicalibration algorithms, available via `pip install multicalibration'.",
      "citationCount": 15,
      "doi": "10.48550/arXiv.2406.06487",
      "arxivId": "2406.06487",
      "url": "https://www.semanticscholar.org/paper/9dabb370d4e8e12f3753ac228bb9ec228d65e659",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.06487"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6518eca0a74e4c2293d20bfb2674e546d9655276",
      "title": "Improved and Oracle-Efficient Online \u21131-Multicalibration",
      "authors": [
        {
          "name": "Rohan Ghuge",
          "authorId": "14626765"
        },
        {
          "name": "Vidya Muthukumar",
          "authorId": "145192874"
        },
        {
          "name": "Sahil Singla",
          "authorId": "2302802548"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 2,
      "doi": "10.48550/arXiv.2505.17365",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6518eca0a74e4c2293d20bfb2674e546d9655276",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.17365"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "5fa0fd11cc124f10d760cbdfa58a947557833ac7",
      "title": "Efficient Swap Multicalibration of Elicitable Properties",
      "authors": [
        {
          "name": "Lunjia Hu",
          "authorId": "2391647244"
        },
        {
          "name": "Haipeng Luo",
          "authorId": "2304143210"
        },
        {
          "name": "Spandan Senapati",
          "authorId": "2194651488"
        },
        {
          "name": "Vatsal Sharan",
          "authorId": "2798845"
        }
      ],
      "year": 2025,
      "abstract": "Multicalibration [HJKRR18] is an algorithmic fairness perspective that demands that the predictions of a predictor are correct conditional on themselves and membership in a collection of potentially overlapping subgroups of a population. The work of [NR23] established a surprising connection between multicalibration for an arbitrary property $\\Gamma$ (e.g., mean or median) and property elicitation: a property $\\Gamma$ can be multicalibrated if and only if it is elicitable, where elicitability is the notion that the true property value of a distribution can be obtained by solving a regression problem over the distribution. In the online setting, [NR23] proposed an inefficient algorithm that achieves $\\sqrt T$ $\\ell_2$-multicalibration error for a hypothesis class of group membership functions and an elicitable property $\\Gamma$, after $T$ rounds of interaction between a forecaster and adversary. In this paper, we generalize multicalibration for an elicitable property $\\Gamma$ from group membership functions to arbitrary bounded hypothesis classes and introduce a stronger notion -- swap multicalibration, following [GKR23]. Subsequently, we propose an oracle-efficient algorithm which, when given access to an online agnostic learner, achieves $T^{1/(r+1)}$ $\\ell_r$-swap multicalibration error with high probability (for $r\\ge2$) for a hypothesis class with bounded sequential Rademacher complexity and an elicitable property $\\Gamma$. For the special case of $r=2$, this implies an oracle-efficient algorithm that achieves $T^{1/3}$ $\\ell_2$-swap multicalibration error, which significantly improves on the previously established bounds for the problem [NR23, GMS25, LSS25a], and completely resolves an open question raised in [GJRR24] on the possibility of an oracle-efficient algorithm that achieves $\\sqrt{T}$ $\\ell_2$-mean multicalibration error by answering it in a strongly affirmative sense.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2511.04907",
      "arxivId": "2511.04907",
      "url": "https://www.semanticscholar.org/paper/5fa0fd11cc124f10d760cbdfa58a947557833ac7",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.04907"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "34b4f684ef6335444eb605c492f347fb29259a27",
      "title": "Improved and Oracle-Efficient Online $\\ell_1$-Multicalibration",
      "authors": [
        {
          "name": "Rohan Ghuge",
          "authorId": "14626765"
        },
        {
          "name": "Vidya Muthukumar",
          "authorId": "145192874"
        },
        {
          "name": "Sahil Singla",
          "authorId": "2302802548"
        }
      ],
      "year": 2025,
      "abstract": "We study \\emph{online multicalibration}, a framework for ensuring calibrated predictions across multiple groups in adversarial settings, across $T$ rounds. Although online calibration is typically studied in the $\\ell_1$ norm, prior approaches to online multicalibration have taken the indirect approach of obtaining rates in other norms (such as $\\ell_2$ and $\\ell_{\\infty}$) and then transferred these guarantees to $\\ell_1$ at additional loss. In contrast, we propose a direct method that achieves improved and oracle-efficient rates of $\\widetilde{\\mathcal{O}}(T^{-1/3})$ and $\\widetilde{\\mathcal{O}}(T^{-1/4})$ respectively, for online $\\ell_1$-multicalibration. Our key insight is a novel reduction of online \\(\\ell_1\\)-multicalibration to an online learning problem with product-based rewards, which we refer to as \\emph{online linear-product optimization} ($\\mathtt{OLPO}$). To obtain the improved rate of $\\widetilde{\\mathcal{O}}(T^{-1/3})$, we introduce a linearization of $\\mathtt{OLPO}$ and design a no-regret algorithm for this linearized problem. Although this method guarantees the desired sublinear rate (nearly matching the best rate for online calibration), it is computationally expensive when the group family \\(\\mathcal{H}\\) is large or infinite, since it enumerates all possible groups. To address scalability, we propose a second approach to $\\mathtt{OLPO}$ that makes only a polynomial number of calls to an offline optimization (\\emph{multicalibration evaluation}) oracle, resulting in \\emph{oracle-efficient} online \\(\\ell_1\\)-multicalibration with a rate of $\\widetilde{\\mathcal{O}}(T^{-1/4})$. Our framework also extends to certain infinite families of groups (e.g., all linear functions on the context space) by exploiting a $1$-Lipschitz property of the \\(\\ell_1\\)-multicalibration error with respect to \\(\\mathcal{H}\\).",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2505.17365",
      "url": "https://www.semanticscholar.org/paper/34b4f684ef6335444eb605c492f347fb29259a27",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "482b8909444c85bce107a25c0ec7b961b1192f5b",
      "title": "Auditability and the Landscape of Distance to Multicalibration",
      "authors": [
        {
          "name": "Nathan Derhake",
          "authorId": "2381372375"
        },
        {
          "name": "Siddartha Devic",
          "authorId": "117403158"
        },
        {
          "name": "Dutch Hansen",
          "authorId": "2305623214"
        },
        {
          "name": "Kuan Liu",
          "authorId": "2381775415"
        },
        {
          "name": "Vatsal Sharan",
          "authorId": "2798845"
        }
      ],
      "year": 2025,
      "abstract": "Calibration is a critical property for establishing the trustworthiness of predictors that provide uncertainty estimates. Multicalibration is a strengthening of calibration which requires that predictors be calibrated on a potentially overlapping collection of subsets of the domain. As multicalibration grows in popularity with practitioners, an essential question is: how do we measure how multicalibrated a predictor is? B{\\l}asiok et al. (2023) considered this question for standard calibration by introducing the distance to calibration framework (dCE) to understand how calibration metrics relate to each other and the ground truth. Building on the dCE framework, we consider the auditability of the distance to multicalibration of a predictor $f$. We begin by considering two natural generalizations of dCE to multiple subgroups: worst group dCE (wdMC), and distance to multicalibration (dMC). We argue that there are two essential properties of any multicalibration error metric: 1) the metric should capture how much $f$ would need to be modified in order to be perfectly multicalibrated; and 2) the metric should be auditable in an information theoretic sense. We show that wdMC and dMC each fail to satisfy one of these two properties, and that similar barriers arise when considering the auditability of general distance to multigroup fairness notions. We then propose two (equivalent) multicalibration metrics which do satisfy these requirements: 1) a continuized variant of dMC; and 2) a distance to intersection multicalibration, which leans on intersectional fairness desiderata. Along the way, we shed light on the loss-landscape of distance to multicalibration and the geometry of the set of perfectly multicalibrated predictors. Our findings may have implications for the development of stronger multicalibration algorithms as well as multigroup auditing more generally.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2509.16930",
      "arxivId": "2509.16930",
      "url": "https://www.semanticscholar.org/paper/482b8909444c85bce107a25c0ec7b961b1192f5b",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.16930"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6a6b00684511c39ba4247e8f9c4b98df45362251",
      "title": "MCGrad: Multicalibration at Web Scale",
      "authors": [
        {
          "name": "Lorenzo Perini",
          "authorId": "2367039296"
        },
        {
          "name": "Daniel Haimovich",
          "authorId": "50516758"
        },
        {
          "name": "Fridolin Linder",
          "authorId": "2275602211"
        },
        {
          "name": "Niek Tax",
          "authorId": "1836097"
        },
        {
          "name": "Dima Karamshuk",
          "authorId": "1929011982"
        },
        {
          "name": "Milan Vojnovic",
          "authorId": "2275618044"
        },
        {
          "name": "Nastaran Okati",
          "authorId": "41022166"
        },
        {
          "name": "P. Apostolopoulos",
          "authorId": "68974609"
        }
      ],
      "year": 2025,
      "abstract": "We propose MCGrad, a novel and scalable multicalibration algorithm. Multicalibration - calibration in sub-groups of the data - is an important property for the performance of machine learning-based systems. Existing multicalibration methods have thus far received limited traction in industry. We argue that this is because existing methods (1) require such subgroups to be manually specified, which ML practitioners often struggle with, (2) are not scalable, or (3) may harm other notions of model performance such as log loss and Area Under the Precision-Recall Curve (PRAUC). MCGrad does not require explicit specification of protected groups, is scalable, and often improves other ML evaluation metrics instead of harming them. MCGrad has been in production at Meta, and is now part of hundreds of production models. We present results from these deployments as well as results on public datasets.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2509.19884",
      "arxivId": "2509.19884",
      "url": "https://www.semanticscholar.org/paper/6a6b00684511c39ba4247e8f9c4b98df45362251",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.19884"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8132a9f5a4f340f19f0c1dac5112e037e61dc6ff",
      "title": "Multicalibration yields better matchings",
      "authors": [
        {
          "name": "Riccardo Colini Baldeschi",
          "authorId": "115724783"
        },
        {
          "name": "Simone Di Gregorio",
          "authorId": "2382764053"
        },
        {
          "name": "Simone Fioravanti",
          "authorId": "2392723281"
        },
        {
          "name": "Federico Fusco",
          "authorId": "2260339744"
        },
        {
          "name": "Ido Guy",
          "authorId": "2273936168"
        },
        {
          "name": "Daniel Haimovich",
          "authorId": "50516758"
        },
        {
          "name": "Stefano Leonardi",
          "authorId": "2287831845"
        },
        {
          "name": "Fridolin Linder",
          "authorId": "2275602211"
        },
        {
          "name": "Lorenzo Perini",
          "authorId": "2367039296"
        },
        {
          "name": "M. Russo",
          "authorId": "32680837"
        },
        {
          "name": "Niek Tax",
          "authorId": "1836097"
        }
      ],
      "year": 2025,
      "abstract": "Consider the problem of finding the best matching in a weighted graph where we only have access to predictions of the actual stochastic weights, based on an underlying context. If the predictor is the Bayes optimal one, then computing the best matching based on the predicted weights is optimal. However, in practice, this perfect information scenario is not realistic. Given an imperfect predictor, a suboptimal decision rule may compensate for the induced error and thus outperform the standard optimal rule. In this paper, we propose multicalibration as a way to address this problem. This fairness notion requires a predictor to be unbiased on each element of a family of protected sets of contexts. Given a class of matching algorithms $\\mathcal C$ and any predictor $\\gamma$ of the edge-weights, we show how to construct a specific multicalibrated predictor $\\hat \\gamma$, with the following property. Picking the best matching based on the output of $\\hat \\gamma$ is competitive with the best decision rule in $\\mathcal C$ applied onto the original predictor $\\gamma$. We complement this result by providing sample complexity bounds.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2511.11413",
      "url": "https://www.semanticscholar.org/paper/8132a9f5a4f340f19f0c1dac5112e037e61dc6ff",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "8f9716f5cfc63f69c34d3b060d3bcf238e607401",
      "title": "Multicalibration for LLM-based Code Generation",
      "authors": [
        {
          "name": "Viola Campos",
          "authorId": "2163452192"
        },
        {
          "name": "Robin Kuschnereit",
          "authorId": "2397489023"
        },
        {
          "name": "A. Ulges",
          "authorId": "1782440"
        }
      ],
      "year": 2025,
      "abstract": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2512.08810",
      "url": "https://www.semanticscholar.org/paper/8f9716f5cfc63f69c34d3b060d3bcf238e607401",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "03dd209a79b302cd7bc976c0bbe6761c152c05b3",
      "title": "Bridging Multicalibration and Out-of-distribution Generalization Beyond Covariate Shift",
      "authors": [
        {
          "name": "Jiayun Wu",
          "authorId": "2265797598"
        },
        {
          "name": "Jiashuo Liu",
          "authorId": "2120846500"
        },
        {
          "name": "Peng Cui",
          "authorId": "2300089202"
        },
        {
          "name": "Zhiwei Steven Wu",
          "authorId": "2238047775"
        }
      ],
      "year": 2024,
      "abstract": "We establish a new model-agnostic optimization framework for out-of-distribution generalization via multicalibration, a criterion that ensures a predictor is calibrated across a family of overlapping groups. Multicalibration is shown to be associated with robustness of statistical inference under covariate shift. We further establish a link between multicalibration and robustness for prediction tasks both under and beyond covariate shift. We accomplish this by extending multicalibration to incorporate grouping functions that consider covariates and labels jointly. This leads to an equivalence of the extended multicalibration and invariance, an objective for robust learning in existence of concept shift. We show a linear structure of the grouping function class spanned by density ratios, resulting in a unifying framework for robust learning by designing specific grouping functions. We propose MC-Pseudolabel, a post-processing algorithm to achieve both extended multicalibration and out-of-distribution generalization. The algorithm, with lightweight hyperparameters and optimization through a series of supervised regression steps, achieves superior performance on real-world datasets with distribution shift.",
      "citationCount": 11,
      "doi": "10.48550/arXiv.2406.00661",
      "arxivId": "2406.00661",
      "url": "https://www.semanticscholar.org/paper/03dd209a79b302cd7bc976c0bbe6761c152c05b3",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.00661"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "77d1cfd1aa1e63ef5ccd76a193161c6034c0a70b",
      "title": "Multicalibration in Fair Link Prediction",
      "authors": [
        {
          "name": "Manjish Pal",
          "authorId": "2331541575"
        },
        {
          "name": "Sandipan Sikdar",
          "authorId": "2632448"
        },
        {
          "name": "Niloy Ganguly",
          "authorId": "2265180746"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1109/tcss.2025.3600263",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/77d1cfd1aa1e63ef5ccd76a193161c6034c0a70b",
      "venue": "IEEE Transactions on Computational Social Systems",
      "journal": {
        "name": "IEEE Transactions on Computational Social Systems"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "0fff3b43bfe041111e75acf5341c9667568e32ce",
      "title": "Multicalibration as Boosting for Regression",
      "authors": [
        {
          "name": "Ira Globus-Harris",
          "authorId": "2085680234"
        },
        {
          "name": "Declan Harrison",
          "authorId": "2203431645"
        },
        {
          "name": "M. Kearns",
          "authorId": "119910493"
        },
        {
          "name": "Aaron Roth",
          "authorId": "2134931587"
        },
        {
          "name": "Jessica Sorrell",
          "authorId": "39715789"
        }
      ],
      "year": 2023,
      "abstract": "We study the connection between multicalibration and boosting for squared error regression. First we prove a useful characterization of multicalibration in terms of a ``swap regret'' like condition on squared error. Using this characterization, we give an exceedingly simple algorithm that can be analyzed both as a boosting algorithm for regression and as a multicalibration algorithm for a class H that makes use only of a standard squared error regression oracle for H. We give a weak learning assumption on H that ensures convergence to Bayes optimality without the need to make any realizability assumptions -- giving us an agnostic boosting algorithm for regression. We then show that our weak learning assumption on H is both necessary and sufficient for multicalibration with respect to H to imply Bayes optimality. We also show that if H satisfies our weak learning condition relative to another class C then multicalibration with respect to H implies multicalibration with respect to C. Finally we investigate the empirical performance of our algorithm experimentally using an open source implementation that we make available. Our code repository can be found at https://github.com/Declancharrison/Level-Set-Boosting.",
      "citationCount": 32,
      "doi": "10.48550/arXiv.2301.13767",
      "arxivId": "2301.13767",
      "url": "https://www.semanticscholar.org/paper/0fff3b43bfe041111e75acf5341c9667568e32ce",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "pages": "11459-11492"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "b6cddfcfed63fe3619a4a53aac3ca9aec237144b",
      "title": "Swap Agnostic Learning, or Characterizing Omniprediction via Multicalibration",
      "authors": [
        {
          "name": "Parikshit Gopalan",
          "authorId": "1718214"
        },
        {
          "name": "Michael P. Kim",
          "authorId": "2110079050"
        },
        {
          "name": "Omer Reingold",
          "authorId": "1746057"
        }
      ],
      "year": 2023,
      "abstract": "We introduce and study Swap Agnostic Learning. The problem can be phrased as a game between a predictor and an adversary: first, the predictor selects a hypothesis $h$; then, the adversary plays in response, and for each level set of the predictor $\\{x \\in \\mathcal{X} : h(x) = v\\}$ selects a (different) loss-minimizing hypothesis $c_v \\in \\mathcal{C}$; the predictor wins if $h$ competes with the adaptive adversary's loss. Despite the strength of the adversary, we demonstrate the feasibility Swap Agnostic Learning for any convex loss. Somewhat surprisingly, the result follows through an investigation into the connections between Omniprediction and Multicalibration. Omniprediction is a new notion of optimality for predictors that strengthtens classical notions such as agnostic learning. It asks for loss minimization guarantees (relative to a hypothesis class) that apply not just for a specific loss function, but for any loss belonging to a rich family of losses. A recent line of work shows that omniprediction is implied by multicalibration and related multi-group fairness notions. This unexpected connection raises the question: is multi-group fairness necessary for omniprediction? Our work gives the first affirmative answer to this question. We establish an equivalence between swap variants of omniprediction and multicalibration and swap agnostic learning. Further, swap multicalibration is essentially equivalent to the standard notion of multicalibration, so existing learning algorithms can be used to achieve any of the three notions. Building on this characterization, we paint a complete picture of the relationship between different variants of multi-group fairness, omniprediction, and Outcome Indistinguishability. This inquiry reveals a unified notion of OI that captures all existing notions of omniprediction and multicalibration.",
      "citationCount": 26,
      "doi": null,
      "arxivId": "2302.06726",
      "url": "https://www.semanticscholar.org/paper/b6cddfcfed63fe3619a4a53aac3ca9aec237144b",
      "venue": "Neural Information Processing Systems",
      "journal": null,
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "edc8a542bd4e404738baa7be86e7efd22cb2c256",
      "title": "Oracle Efficient Online Multicalibration and Omniprediction",
      "authors": [
        {
          "name": "Sumegha Garg",
          "authorId": "3433226"
        },
        {
          "name": "Christopher Jung",
          "authorId": "2069019232"
        },
        {
          "name": "Omer Reingold",
          "authorId": "1746057"
        },
        {
          "name": "Aaron Roth",
          "authorId": "1682008"
        }
      ],
      "year": 2023,
      "abstract": "A recent line of work has shown a surprising connection between multicalibration, a multi-group fairness notion, and omniprediction, a learning paradigm that provides simultaneous loss minimization guarantees for a large family of loss functions. Prior work studies omniprediction in the batch setting. We initiate the study of omniprediction in the online adversarial setting. Although there exist algorithms for obtaining notions of multicalibration in the online adversarial setting, unlike batch algorithms, they work only for small finite classes of benchmark functions $F$, because they require enumerating every function $f \\in F$ at every round. In contrast, omniprediction is most interesting for learning theoretic hypothesis classes $F$, which are generally continuously large. We develop a new online multicalibration algorithm that is well defined for infinite benchmark classes $F$, and is oracle efficient (i.e. for any class $F$, the algorithm has the form of an efficient reduction to a no-regret learning algorithm for $F$). The result is the first efficient online omnipredictor -- an oracle efficient prediction algorithm that can be used to simultaneously obtain no regret guarantees to all Lipschitz convex loss functions. For the class $F$ of linear functions, we show how to make our algorithm efficient in the worst case. Also, we show upper and lower bounds on the extent to which our rates can be improved: our oracle efficient algorithm actually promises a stronger guarantee called swap-omniprediction, and we prove a lower bound showing that obtaining $O(\\sqrt{T})$ bounds for swap-omniprediction is impossible in the online setting. On the other hand, we give a (non-oracle efficient) algorithm which can obtain the optimal $O(\\sqrt{T})$ omniprediction bounds without going through multicalibration, giving an information theoretic separation between these two solution concepts.",
      "citationCount": 27,
      "doi": "10.48550/arXiv.2307.08999",
      "arxivId": "2307.08999",
      "url": "https://www.semanticscholar.org/paper/edc8a542bd4e404738baa7be86e7efd22cb2c256",
      "venue": "ACM-SIAM Symposium on Discrete Algorithms",
      "journal": {
        "pages": "2725-2792"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "565377f83dd38dd080b675078e181cf34e5b13c9",
      "title": "Who's the (Multi-)Fairest of Them All: Rethinking Interpolation-Based Data Augmentation Through the Lens of Multicalibration",
      "authors": [
        {
          "name": "Karina Halevy",
          "authorId": "2106627578"
        },
        {
          "name": "Karly Hou",
          "authorId": "2335568210"
        },
        {
          "name": "Charumathi Badrinath",
          "authorId": "2224843950"
        }
      ],
      "year": 2024,
      "abstract": "Data augmentation methods, especially SoTA interpolation-based methods such as Fair Mixup, have been widely shown to increase model fairness. However, this fairness is evaluated on metrics that do not capture model uncertainty and on datasets with only one, relatively large, minority group. As a remedy, multicalibration has been introduced to measure fairness while accommodating uncertainty and accounting for multiple minority groups. However, existing methods of improving multicalibration involve reducing initial training data to create a holdout set for post-processing, which is not ideal when minority training data is already sparse. This paper uses multicalibration to more rigorously examine data augmentation for classification fairness. We stress-test four versions of Fair Mixup on two structured data classification problems with up to 81 marginalized groups, evaluating multicalibration violations and balanced accuracy. We find that on nearly every experiment, Fair Mixup worsens baseline performance and fairness, but the simple vanilla Mixup outperforms both Fair Mixup and the baseline, especially when calibrating on small groups. Combining vanilla Mixup with multicalibration post-processing, which enforces multicalibration through post-processing on a holdout set, further increases fairness.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2412.10575",
      "arxivId": "2412.10575",
      "url": "https://www.semanticscholar.org/paper/565377f83dd38dd080b675078e181cf34e5b13c9",
      "venue": "AAAI Conference on Artificial Intelligence",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2412.10575"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "52788d606ea3be913af36dbcb91a024ac1aed0ca",
      "title": "Multicalibration for Modeling Censored Survival Data with Universal Adaptability",
      "authors": [
        {
          "name": "Hanxuan Ye",
          "authorId": "2304399528"
        },
        {
          "name": "Hongzhe Li",
          "authorId": "2303410927"
        }
      ],
      "year": 2024,
      "abstract": "\n Traditional statistical and machine learning methods typically assume that the training and test data follow the same distribution. However, this assumption is frequently violated in real-world applications, where the training data in the source domain may under-represent specific subpopulations in the test data of the target domain. This paper addresses target-independent learning under covariate shift, focusing on multicalibration for survival probability and restricted mean survival time. A black-box post-processing boosting algorithm specifically designed for censored survival data is introduced. By leveraging pseudo-observations, our method produces a multicalibrated predictor that is competitive with inverse propensity score weighting in predicting the survival outcome in an unlabeled target domain, ensuring not only overall accuracy but also fairness across diverse subpopulations. Our theoretical analysis of pseudo-observations builds upon the functional delta method and the p-variational norm. The algorithm\u2019s sample complexity, convergence properties, and multicalibration guarantees for post-processed predictors are provided. Our results establish a fundamental connection between multicalibration and universal adaptability, demonstrating that our calibrated function is comparable to, or outperforms, the inverse propensity score weighting estimator. Extensive numerical simulations and a real-world case study on cardiovascular disease risk prediction using two large prospective cohort studies validate the effectiveness of our approach.",
      "citationCount": 0,
      "doi": "10.1093/biomet/asaf039",
      "arxivId": "2405.15948",
      "url": "https://www.semanticscholar.org/paper/52788d606ea3be913af36dbcb91a024ac1aed0ca",
      "venue": "Biometrika",
      "journal": {
        "name": "Biometrika"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "10d4841084f6801300f69d1b577436ccdcc3b43e",
      "title": "Characterizing the Distinguishability of Product Distributions through Multicalibration",
      "authors": [
        {
          "name": "Cassandra Marcussen",
          "authorId": "2333599017"
        },
        {
          "name": "Aaron Putterman",
          "authorId": "2154429279"
        },
        {
          "name": "Salil P. Vadhan",
          "authorId": "2227102503"
        }
      ],
      "year": 2024,
      "abstract": "Given a sequence of samples $x_1, \\dots , x_k$ promised to be drawn from one of two distributions $X_0, X_1$, a well-studied problem in statistics is to decide $\\textit{which}$ distribution the samples are from. Information theoretically, the maximum advantage in distinguishing the two distributions given $k$ samples is captured by the total variation distance between $X_0^{\\otimes k}$ and $X_1^{\\otimes k}$. However, when we restrict our attention to $\\textit{efficient distinguishers}$ (i.e., small circuits) of these two distributions, exactly characterizing the ability to distinguish $X_0^{\\otimes k}$ and $X_1^{\\otimes k}$ is more involved and less understood. In this work, we give a general way to reduce bounds on the computational indistinguishability of $X_0$ and $X_1$ to bounds on the $\\textit{information-theoretic}$ indistinguishability of some specific, related variables $\\widetilde{X}_0$ and $\\widetilde{X}_1$. As a consequence, we prove a new, tight characterization of the number of samples $k$ needed to efficiently distinguish $X_0^{\\otimes k}$ and $X_1^{\\otimes k}$ with constant advantage as \\[ k = \\Theta\\left(d_H^{-2}\\left(\\widetilde{X}_0, \\widetilde{X}_1\\right)\\right), \\] which is the inverse of the squared Hellinger distance $d_H$ between two distributions $\\widetilde{X}_0$ and $\\widetilde{X}_1$ that are computationally indistinguishable from $X_0$ and $X_1$. Likewise, our framework can be used to re-derive a result of Halevi and Rabin (TCC 2008) and Geier (TCC 2022), proving nearly-tight bounds on how computational indistinguishability scales with the number of samples for arbitrary product distributions.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2412.03562",
      "arxivId": "2412.03562",
      "url": "https://www.semanticscholar.org/paper/10d4841084f6801300f69d1b577436ccdcc3b43e",
      "venue": "Cybersecurity and Cyberforensics Conference",
      "journal": {
        "pages": "19:1-19:19"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "1b487d11f9c260067b6e7eb30d089fe534388aab",
      "title": "Loss minimization yields multicalibration for large neural networks",
      "authors": [
        {
          "name": "Jaroslaw Blasiok",
          "authorId": "2346983198"
        },
        {
          "name": "Parikshit Gopalan",
          "authorId": "1718214"
        },
        {
          "name": "Lunjia Hu",
          "authorId": "9560164"
        },
        {
          "name": "A. Kalai",
          "authorId": "2186481"
        },
        {
          "name": "Preetum Nakkiran",
          "authorId": "2181918"
        }
      ],
      "year": 2023,
      "abstract": "Multicalibration is a notion of fairness for predictors that requires them to provide calibrated predictions across a large set of protected groups. Multicalibration is known to be a distinct goal than loss minimization, even for simple predictors such as linear functions. In this work, we consider the setting where the protected groups can be represented by neural networks of size $k$, and the predictors are neural networks of size $n>k$. We show that minimizing the squared loss over all neural nets of size $n$ implies multicalibration for all but a bounded number of unlucky values of $n$. We also give evidence that our bound on the number of unlucky values is tight, given our proof technique. Previously, results of the flavor that loss minimization yields multicalibration were known only for predictors that were near the ground truth, hence were rather limited in applicability. Unlike these, our results rely on the expressivity of neural nets and utilize the representation of the predictor.",
      "citationCount": 16,
      "doi": "10.48550/arXiv.2304.09424",
      "arxivId": "2304.09424",
      "url": "https://www.semanticscholar.org/paper/1b487d11f9c260067b6e7eb30d089fe534388aab",
      "venue": "Information Technology Convergence and Services",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2304.09424"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "57e3e9dde87fd5f56249a8d4c6ac737a1f9e2ad9",
      "title": "HappyMap : A Generalized Multicalibration Method",
      "authors": [
        {
          "name": "Zhun Deng",
          "authorId": "10394991"
        },
        {
          "name": "C. Dwork",
          "authorId": "1781565"
        },
        {
          "name": "Linjun Zhang",
          "authorId": "2305743789"
        }
      ],
      "year": 2023,
      "abstract": "Multi-calibration is a powerful and evolving concept originating in the field of algorithmic fairness. For a predictor $f$ that estimates the outcome $y$ given covariates $x$, and for a function class $\\mathcal{C}$, multi-calibration requires that the predictor $f(x)$ and outcome $y$ are indistinguishable under the class of auditors in $\\mathcal{C}$. Fairness is captured by incorporating demographic subgroups into the class of functions~$\\mathcal{C}$. Recent work has shown that, by enriching the class $\\mathcal{C}$ to incorporate appropriate propensity re-weighting functions, multi-calibration also yields target-independent learning, wherein a model trained on a source domain performs well on unseen, future, target domains(approximately) captured by the re-weightings. Formally, multi-calibration with respect to $\\mathcal{C}$ bounds $\\big|\\mathbb{E}_{(x,y)\\sim \\mathcal{D}}[c(f(x),x)\\cdot(f(x)-y)]\\big|$ for all $c \\in \\mathcal{C}$. In this work, we view the term $(f(x)-y)$ as just one specific mapping, and explore the power of an enriched class of mappings. We propose \\textit{HappyMap}, a generalization of multi-calibration, which yields a wide range of new applications, including a new fairness notion for uncertainty quantification (conformal prediction), a novel technique for conformal prediction under covariate shift, and a different approach to analyzing missing data, while also yielding a unified understanding of several existing seemingly disparate algorithmic fairness notions and target-independent learning approaches. We give a single \\textit{HappyMap} meta-algorithm that captures all these results, together with a sufficiency condition for its success.",
      "citationCount": 21,
      "doi": "10.4230/LIPIcs.ITCS.2023.41",
      "arxivId": "2303.04379",
      "url": "https://www.semanticscholar.org/paper/57e3e9dde87fd5f56249a8d4c6ac737a1f9e2ad9",
      "venue": "Information Technology Convergence and Services",
      "journal": {
        "pages": "41:1-41:23"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8bfaf6c35a8f69f8bddfa946ad871c840c70feb5",
      "title": "Complexity-Theoretic Implications of Multicalibration",
      "authors": [
        {
          "name": "S'ilvia Casacuberta",
          "authorId": "2276608667"
        },
        {
          "name": "Cynthia Dwork",
          "authorId": "2256532690"
        },
        {
          "name": "Salil P. Vadhan",
          "authorId": "2227102503"
        }
      ],
      "year": 2023,
      "abstract": "We present connections between the recent literature on multigroup fairness for prediction algorithms and classical results in computational complexity. Multiaccurate predictors are correct in expectation on each member of an arbitrary collection of pre-specified sets. Multicalibrated predictors satisfy a stronger condition: they are calibrated on each set in the collection. Multiaccuracy is equivalent to a regularity notion for functions defined by Trevisan, Tulsiani, and Vadhan (2009). They showed that, given a class F of (possibly simple) functions, an arbitrarily complex function g can be approximated by a low-complexity function h that makes a small number of oracle calls to members of F, where the notion of approximation requires that h cannot be distinguished from g by members of F. This complexity-theoretic Regularity Lemma is known to have implications in different areas, including in complexity theory, additive number theory, information theory, graph theory, and cryptography. Starting from the stronger notion of multicalibration, we obtain stronger and more general versions of a number of applications of the Regularity Lemma, including the Hardcore Lemma, the Dense Model Theorem, and the equivalence of conditional pseudo-min-entropy and unpredictability. For example, we show that every boolean function (regardless of its hardness) has a small collection of disjoint hardcore sets, where the sizes of those hardcore sets are related to how balanced the function is on corresponding pieces of an efficient partition of the domain.",
      "citationCount": 14,
      "doi": "10.1145/3618260.3649748",
      "arxivId": "2312.17223",
      "url": "https://www.semanticscholar.org/paper/8bfaf6c35a8f69f8bddfa946ad871c840c70feb5",
      "venue": "Symposium on the Theory of Computing",
      "journal": {
        "name": "Proceedings of the 56th Annual ACM Symposium on Theory of Computing"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
      ]
    },
    {
      "paperId": "36cbc94ca9b1921c649c21a841b767208b594517",
      "title": "Low-Degree Multicalibration",
      "authors": [
        {
          "name": "Parikshit Gopalan",
          "authorId": "1718214"
        },
        {
          "name": "Michael P. Kim",
          "authorId": "2110079050"
        },
        {
          "name": "M. Singhal",
          "authorId": "118219937"
        },
        {
          "name": "Shengjia Zhao",
          "authorId": "3303970"
        }
      ],
      "year": 2022,
      "abstract": "Introduced as a notion of algorithmic fairness, multicalibration has proved to be a powerful and versatile concept with implications far beyond its original intent. This stringent notion -- that predictions be well-calibrated across a rich class of intersecting subpopulations -- provides its strong guarantees at a cost: the computational and sample complexity of learning multicalibrated predictors are high, and grow exponentially with the number of class labels. In contrast, the relaxed notion of multiaccuracy can be achieved more efficiently, yet many of the most desirable properties of multicalibration cannot be guaranteed assuming multiaccuracy alone. This tension raises a key question: Can we learn predictors with multicalibration-style guarantees at a cost commensurate with multiaccuracy? In this work, we define and initiate the study of Low-Degree Multicalibration. Low-Degree Multicalibration defines a hierarchy of increasingly-powerful multi-group fairness notions that spans multiaccuracy and the original formulation of multicalibration at the extremes. Our main technical contribution demonstrates that key properties of multicalibration, related to fairness and accuracy, actually manifest as low-degree properties. Importantly, we show that low-degree multicalibration can be significantly more efficient than full multicalibration. In the multi-class setting, the sample complexity to achieve low-degree multicalibration improves exponentially (in the number of classes) over full multicalibration. Our work presents compelling evidence that low-degree multicalibration represents a sweet spot, pairing computational and sample efficiency with strong fairness and accuracy guarantees.",
      "citationCount": 50,
      "doi": "10.48550/arXiv.2203.01255",
      "arxivId": "2203.01255",
      "url": "https://www.semanticscholar.org/paper/36cbc94ca9b1921c649c21a841b767208b594517",
      "venue": "Annual Conference Computational Learning Theory",
      "journal": {
        "pages": "3193-3234"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "916c816f16e4934e41f09a3ff81a10e5fc4bb459",
      "title": "Multicalibration: Calibration for the (Computationally-Identifiable) Masses",
      "authors": [
        {
          "name": "\u00darsula H\u00e9bert-Johnson",
          "authorId": "2346121057"
        },
        {
          "name": "Michael P. Kim",
          "authorId": "2110079050"
        },
        {
          "name": "Omer Reingold",
          "authorId": "1746057"
        },
        {
          "name": "G. Rothblum",
          "authorId": "2551824"
        }
      ],
      "year": 2018,
      "abstract": null,
      "citationCount": 426,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/916c816f16e4934e41f09a3ff81a10e5fc4bb459",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "pages": "1944-1953"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "c30d86403e9fcd985eabcd9b72cd896a24d2028b",
      "title": "The Statistical Scope of Multicalibration",
      "authors": [
        {
          "name": "Georgy Noarov",
          "authorId": "82274934"
        },
        {
          "name": "Aaron Roth",
          "authorId": "1682008"
        }
      ],
      "year": 2023,
      "abstract": null,
      "citationCount": 7,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/c30d86403e9fcd985eabcd9b72cd896a24d2028b",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "pages": "26283-26310"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "2b1d8adae7ebc367dd298c79c6158faf187256fc",
      "title": "The Scope of Multicalibration: Characterizing Multicalibration via Property Elicitation",
      "authors": [
        {
          "name": "Georgy Noarov",
          "authorId": "82274934"
        },
        {
          "name": "Aaron Roth",
          "authorId": "1682008"
        }
      ],
      "year": 2023,
      "abstract": "We make a connection between multicalibration and property elicitation and show that (under mild technical conditions) it is possible to produce a multicalibrated predictor for a continuous scalar distributional property $\\Gamma$ if and only if $\\Gamma$ is elicitable. On the negative side, we show that for non-elicitable continuous properties there exist simple data distributions on which even the true distributional predictor is not calibrated. On the positive side, for elicitable $\\Gamma$, we give simple canonical algorithms for the batch and the online adversarial setting, that learn a $\\Gamma$-multicalibrated predictor. This generalizes past work on multicalibrated means and quantiles, and in fact strengthens existing online quantile multicalibration results. To further counter-weigh our negative result, we show that if a property $\\Gamma^1$ is not elicitable by itself, but is elicitable conditionally on another elicitable property $\\Gamma^0$, then there is a canonical algorithm that jointly multicalibrates $\\Gamma^1$ and $\\Gamma^0$; this generalizes past work on mean-moment multicalibration. Finally, as applications of our theory, we provide novel algorithmic and impossibility results for fair (multicalibrated) risk assessment.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2302.08507",
      "arxivId": "2302.08507",
      "url": "https://www.semanticscholar.org/paper/2b1d8adae7ebc367dd298c79c6158faf187256fc",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2302.08507"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e582bdeb9f2614c1f8f26cf917081712af65d65a",
      "title": "Moment Multicalibration for Uncertainty Estimation",
      "authors": [
        {
          "name": "Christopher Jung",
          "authorId": "2069019232"
        },
        {
          "name": "Changhwa Lee",
          "authorId": "2155591369"
        },
        {
          "name": "Mallesh M. Pai",
          "authorId": "7242417"
        },
        {
          "name": "Aaron Roth",
          "authorId": "1682008"
        },
        {
          "name": "R. Vohra",
          "authorId": "2539286"
        }
      ],
      "year": 2020,
      "abstract": "We show how to achieve the notion of \"multicalibration\" from Hebert-Johnson et al. [2018] not just for means, but also for variances and other higher moments. Informally, it means that we can find regression functions which, given a data point, can make point predictions not just for the expectation of its label, but for higher moments of its label distribution as well-and those predictions match the true distribution quantities when averaged not just over the population as a whole, but also when averaged over an enormous number of finely defined subgroups. It yields a principled way to estimate the uncertainty of predictions on many different subgroups-and to diagnose potential sources of unfairness in the predictive power of features across subgroups. As an application, we show that our moment estimates can be used to derive marginal prediction intervals that are simultaneously valid as averaged over all of the (sufficiently large) subgroups for which moment multicalibration has been obtained.",
      "citationCount": 69,
      "doi": null,
      "arxivId": "2008.08037",
      "url": "https://www.semanticscholar.org/paper/e582bdeb9f2614c1f8f26cf917081712af65d65a",
      "venue": "Annual Conference Computational Learning Theory",
      "journal": {
        "pages": "2634-2678"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "062d10ee9432dac705061107d30b3c03f9c95165",
      "title": "Fair admission risk prediction with proportional multicalibration",
      "authors": [
        {
          "name": "W. L. Cava",
          "authorId": "2910714"
        },
        {
          "name": "Elle Lett",
          "authorId": "1879204208"
        },
        {
          "name": "Guangya Wan",
          "authorId": "2186405523"
        }
      ],
      "year": 2022,
      "abstract": "Fair calibration is a widely desirable fairness criteria in risk prediction contexts. One way to measure and achieve fair calibration is with multicalibration. Multicalibration constrains calibration error among flexibly-defined subpopulations while maintaining overall calibration. However, multicalibrated models can exhibit a higher percent calibration error among groups with lower base rates than groups with higher base rates. As a result, it is possible for a decision-maker to learn to trust or distrust model predictions for specific groups. To alleviate this, we propose proportional multicalibration, a criteria that constrains the percent calibration error among groups and within prediction bins. We prove that satisfying proportional multicalibration bounds a model's multicalibration as well its differential calibration, a fairness criteria that directly measures how closely a model approximates sufficiency. Therefore, proportionally calibrated models limit the ability of decision makers to distinguish between model performance on different patient groups, which may make the models more trustworthy in practice. We provide an efficient algorithm for post-processing risk prediction models for proportional multicalibration and evaluate it empirically. We conduct simulation studies and investigate a real-world application of PMC-postprocessing to prediction of emergency department patient admissions. We observe that proportional multicalibration is a promising criteria for controlling simultaneous measures of calibration fairness of a model over intersectional groups with virtually no cost in terms of classification performance.",
      "citationCount": 10,
      "doi": null,
      "arxivId": "2209.14613",
      "url": "https://www.semanticscholar.org/paper/062d10ee9432dac705061107d30b3c03f9c95165",
      "venue": "ACM Conference on Health, Inference, and Learning",
      "journal": {
        "name": "Proceedings of machine learning research",
        "pages": "\n          350-378\n        ",
        "volume": "209"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7615a91e9e70cb869dd470b8cb107b77354d3b31",
      "title": "Proportional Multicalibration",
      "authors": [
        {
          "name": "W. L. Cava",
          "authorId": "2910714"
        },
        {
          "name": "Elle Lett",
          "authorId": "1879204208"
        },
        {
          "name": "Guangya Wan",
          "authorId": "2186405523"
        }
      ],
      "year": 2022,
      "abstract": null,
      "citationCount": 4,
      "doi": "10.48550/arXiv.2209.14613",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7615a91e9e70cb869dd470b8cb107b77354d3b31",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2209.14613"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "374b77912fdf3dd640614aa7a97115b15ba72ded",
      "title": "An Exploration of Multicalibration Uniform Convergence Bounds",
      "authors": [
        {
          "name": "Harrison Rosenberg",
          "authorId": "51917955"
        },
        {
          "name": "Robi Bhattacharjee",
          "authorId": "84169327"
        },
        {
          "name": "Kassem Fawaz",
          "authorId": "1910642"
        },
        {
          "name": "S. Jha",
          "authorId": "1680133"
        }
      ],
      "year": 2022,
      "abstract": "Recent works have investigated the sample complexity necessary for fair machine learning. The most advanced of such sample complexity bounds are developed by analyzing multicalibration uniform convergence for a given predictor class. We present a framework which yields multicalibration error uniform convergence bounds by reparametrizing sample complexities for Empirical Risk Minimization (ERM) learning. From this framework, we demonstrate that multicalibration error exhibits dependence on the classifier architecture as well as the underlying data distribution. We perform an experimental evaluation to investigate the behavior of multicalibration error for different families of classifiers. We compare the results of this evaluation to multicalibration error concentration bounds. Our investigation provides additional perspective on both algorithmic fairness and multicalibration error convergence bounds. Given the prevalence of ERM sample complexity bounds, our proposed framework enables machine learning practitioners to easily understand the convergence behavior of multicalibration error for a myriad of classifier architectures.",
      "citationCount": 1,
      "doi": null,
      "arxivId": "2202.04530",
      "url": "https://www.semanticscholar.org/paper/374b77912fdf3dd640614aa7a97115b15ba72ded",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2202.04530"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2564832cf9087d629c96b3cee7ad56fffa2b6987",
      "title": "Sample Complexity of Uniform Convergence for Multicalibration",
      "authors": [
        {
          "name": "Eliran Shabat",
          "authorId": "1672900606"
        },
        {
          "name": "Lee Cohen",
          "authorId": "2067708059"
        },
        {
          "name": "Y. Mansour",
          "authorId": "144830983"
        }
      ],
      "year": 2020,
      "abstract": "There is a growing interest in societal concerns in machine learning systems, especially in fairness. Multicalibration gives a comprehensive methodology to address group fairness. In this work, we address the multicalibration error and decouple it from the prediction error. The importance of decoupling the fairness metric (multicalibration) and the accuracy (prediction error) is due to the inherent trade-off between the two, and the societal decision regarding the \"right tradeoff\" (as imposed many times by regulators). Our work gives sample complexity bounds for uniform convergence guarantees of multicalibration error, which implies that regardless of the accuracy, we can guarantee that the empirical and (true) multicalibration errors are close. We emphasize that our results: (1) are more general than previous bounds, as they apply to both agnostic and realizable settings, and do not rely on a specific type of algorithm (such as deferentially private), (2) improve over previous multicalibration sample complexity bounds and (3) implies uniform convergence guarantees for the classical calibration error.",
      "citationCount": 30,
      "doi": null,
      "arxivId": "2005.01757",
      "url": "https://www.semanticscholar.org/paper/2564832cf9087d629c96b3cee7ad56fffa2b6987",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2005.01757"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "346d55729c80dfdfb1d9148ba4d3d4ce414c4420",
      "title": "How Global Calibration Strengthens Multiaccuracy",
      "authors": [
        {
          "name": "S'ilvia Casacuberta",
          "authorId": "2356550170"
        },
        {
          "name": "Parikshit Gopalan",
          "authorId": "1718214"
        },
        {
          "name": "Varun Kanade",
          "authorId": "2253663523"
        },
        {
          "name": "Omer Reingold",
          "authorId": "2363877765"
        }
      ],
      "year": 2025,
      "abstract": "Multiaccuracy and multicalibration are multigroup fairness notions for prediction that have found numerous applications in learning and computational complexity. They can be achieved from a single learning primitive: weak agnostic learning. Here we investigate the power of multiaccuracy as a learning primitive, both with and without the additional assumption of calibration. We find that multiaccuracy in itself is rather weak, but that the addition of global calibration (this notion is called calibrated multiaccuracy) boosts its power substantially, enough to recover implications that were previously known only assuming the stronger notion of multicalibration. We give evidence that multiaccuracy might not be as powerful as standard weak agnostic learning, by showing that there is no way to post-process a multiaccurate predictor to get a weak learner, even assuming the best hypothesis has correlation $1/2$. Rather, we show that it yields a restricted form of weak agnostic learning, which requires some concept in the class to have correlation greater than $1/2$ with the labels. However, by also requiring the predictor to be calibrated, we recover not just weak, but strong agnostic learning. A similar picture emerges when we consider the derivation of hardcore measures from predictors satisfying multigroup fairness notions. On the one hand, while multiaccuracy only yields hardcore measures of density half the optimal, we show that (a weighted version of) calibrated multiaccuracy achieves optimal density. Our results yield new insights into the complementary roles played by multiaccuracy and calibration in each setting. They shed light on why multiaccuracy and global calibration, although not particularly powerful by themselves, together yield considerably stronger notions.",
      "citationCount": 6,
      "doi": "10.48550/arXiv.2504.15206",
      "arxivId": "2504.15206",
      "url": "https://www.semanticscholar.org/paper/346d55729c80dfdfb1d9148ba4d3d4ce414c4420",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.15206"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b8366a8cb47d79556359dc15c0ead168a69867cb",
      "title": "Generalized Venn and Venn-Abers Calibration with Applications in Conformal Prediction",
      "authors": [
        {
          "name": "L. Laan",
          "authorId": "151412676"
        },
        {
          "name": "Ahmed Alaa",
          "authorId": "2340683655"
        }
      ],
      "year": 2025,
      "abstract": "Ensuring model calibration is critical for reliable prediction, yet popular distribution-free methods such as histogram binning and isotonic regression offer only asymptotic guarantees. We introduce a unified framework for Venn and Venn-Abers calibration that extends Vovk's approach beyond binary classification to a broad class of prediction problems defined by generic loss functions. Our method transforms any perfectly in-sample calibrated predictor into a set-valued predictor that, in finite samples, outputs at least one marginally calibrated point prediction. These set predictions shrink asymptotically and converge to a conditionally calibrated prediction, capturing epistemic uncertainty. We further propose Venn multicalibration, a new approach for achieving finite-sample calibration across subpopulations. For quantile loss, our framework recovers group-conditional and multicalibrated conformal prediction as special cases and yields novel prediction intervals with quantile-conditional coverage.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2502.05676",
      "arxivId": "2502.05676",
      "url": "https://www.semanticscholar.org/paper/b8366a8cb47d79556359dc15c0ead168a69867cb",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.05676"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "c09785026e5e76265c896bbe4ec07cf55b9635b3",
      "title": "When does a predictor know its own loss?",
      "authors": [
        {
          "name": "Aravind Gollakota",
          "authorId": "1658704335"
        },
        {
          "name": "Parikshit Gopalan",
          "authorId": "1718214"
        },
        {
          "name": "Aayush Karan",
          "authorId": "2336746521"
        },
        {
          "name": "Charlotte Peale",
          "authorId": "2337686968"
        },
        {
          "name": "Udi Wieder",
          "authorId": "1753945"
        }
      ],
      "year": 2025,
      "abstract": "Given a predictor and a loss function, how well can we predict the loss that the predictor will incur on an input? This is the problem of loss prediction, a key computational task associated with uncertainty estimation for a predictor. In a classification setting, a predictor will typically predict a distribution over labels and hence have its own estimate of the loss that it will incur, given by the entropy of the predicted distribution. Should we trust this estimate? In other words, when does the predictor know what it knows and what it does not know? In this work we study the theoretical foundations of loss prediction. Our main contribution is to establish tight connections between nontrivial loss prediction and certain forms of multicalibration, a multigroup fairness notion that asks for calibrated predictions across computationally identifiable subgroups. Formally, we show that a loss predictor that is able to improve on the self-estimate of a predictor yields a witness to a failure of multicalibration, and vice versa. This has the implication that nontrivial loss prediction is in effect no easier or harder than auditing for multicalibration. We support our theoretical results with experiments that show a robust positive correlation between the multicalibration error of a predictor and the efficacy of training a loss predictor.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2502.20375",
      "arxivId": "2502.20375",
      "url": "https://www.semanticscholar.org/paper/c09785026e5e76265c896bbe4ec07cf55b9635b3",
      "venue": "Symposium on Foundations of Responsible Computing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.20375"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e4767e43ca0dc7a668ccaca25c3f6c68fba436ab",
      "title": "Kernel Multiaccuracy",
      "authors": [
        {
          "name": "Carol Xuan Long",
          "authorId": "2220214891"
        },
        {
          "name": "Wael Alghamdi",
          "authorId": "46976603"
        },
        {
          "name": "Alex Glynn",
          "authorId": "2310697251"
        },
        {
          "name": "Yixuan Wu",
          "authorId": "2293766957"
        },
        {
          "name": "F. Calmon",
          "authorId": "144717568"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 2,
      "doi": "10.4230/LIPIcs.FORC.2025.7",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e4767e43ca0dc7a668ccaca25c3f6c68fba436ab",
      "venue": "Symposium on Foundations of Responsible Computing",
      "journal": {
        "pages": "7:1-7:23"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b599198cb6aca5c90eaec7ca02ec5ca6b16a712e",
      "title": "Mapping the Tradeoffs and Limitations of Algorithmic Fairness",
      "authors": [
        {
          "name": "Etam Benger",
          "authorId": "6585023"
        },
        {
          "name": "Katrina Ligett",
          "authorId": "2265980745"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.4230/LIPIcs.FORC.2025.19",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/b599198cb6aca5c90eaec7ca02ec5ca6b16a712e",
      "venue": "Symposium on Foundations of Responsible Computing",
      "journal": {
        "pages": "19:1-19:20"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "62dc2b2c3e8228a31640bf8ce3d6ac345e44ea7d",
      "title": "Sample-Efficient Omniprediction for Proper Losses",
      "authors": [
        {
          "name": "Isaac Gibbs",
          "authorId": "2385556001"
        },
        {
          "name": "R. Tibshirani",
          "authorId": "2242472434"
        }
      ],
      "year": 2025,
      "abstract": "We consider the problem of constructing probabilistic predictions that lead to accurate decisions when employed by downstream users to inform actions. For a single decision maker, designing an optimal predictor is equivalent to minimizing a proper loss function corresponding to the negative utility of that individual. For multiple decision makers, our problem can be viewed as a variant of omniprediction in which the goal is to design a single predictor that simultaneously minimizes multiple losses. Existing algorithms for achieving omniprediction broadly fall into two categories: 1) boosting methods that optimize other auxiliary targets such as multicalibration and obtain omniprediction as a corollary, and 2) adversarial two-player game based approaches that estimate and respond to the ``worst-case\"loss in an online fashion. We give lower bounds demonstrating that multicalibration is a strictly more difficult problem than omniprediction and thus the former approach must incur suboptimal sample complexity. For the latter approach, we discuss how these ideas can be used to obtain a sample-efficient algorithm through an online-to-batch conversion. This conversion has the downside of returning a complex, randomized predictor. We improve on this method by designing a more direct, unrandomized algorithm that exploits structural elements of the set of proper losses.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.12769",
      "arxivId": "2510.12769",
      "url": "https://www.semanticscholar.org/paper/62dc2b2c3e8228a31640bf8ce3d6ac345e44ea7d",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.12769"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a71d289867db05ee5c1ad22d1ad77fbbe4090334",
      "title": "In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization",
      "authors": [
        {
          "name": "Thiti Suttaket",
          "authorId": "2178709207"
        },
        {
          "name": "Stanley Kok",
          "authorId": "2305228823"
        }
      ],
      "year": 2025,
      "abstract": "Survival analysis is an important problem in healthcare because it models the relationship between an individual's covariates and the onset time of an event of interest (e.g., death). It is important for survival models to be well-calibrated (i.e., for their predicted probabilities to be close to ground-truth probabilities) because badly calibrated systems can result in erroneous clinical decisions. Existing survival models are typically calibrated at the population level only, and thus run the risk of being poorly calibrated for one or more minority subpopulations. We propose a model called GRADUATE that achieves multicalibration by ensuring that all subpopulations are well-calibrated too. GRADUATE frames multicalibration as a constrained optimization problem, and optimizes both calibration and discrimination in-training to achieve a good balance between them. We mathematically prove that the optimization method used yields a solution that is both near-optimal and feasible with high probability. Empirical comparisons against state-of-the-art baselines on real-world clinical datasets demonstrate GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of the baselines vis-a-vis GRADUATE's strengths.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2507.02807",
      "arxivId": "2507.02807",
      "url": "https://www.semanticscholar.org/paper/a71d289867db05ee5c1ad22d1ad77fbbe4090334",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.02807"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8fd88a245d48666500f2a1b372932333e19f647c",
      "title": "Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks",
      "authors": [
        {
          "name": "Lujing Zhang",
          "authorId": "2300210720"
        },
        {
          "name": "Aaron Roth",
          "authorId": "2299940726"
        },
        {
          "name": "Linjun Zhang",
          "authorId": "2300021373"
        }
      ],
      "year": 2024,
      "abstract": "This paper introduces a framework for post-processing machine learning models so that their predictions satisfy multi-group fairness guarantees. Based on the celebrated notion of multicalibration, we introduce $(\\mathbf{s},\\mathcal{G}, \\alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for multi-dimensional mappings $\\mathbf{s}$, constraint set $\\mathcal{G}$, and a pre-specified threshold level $\\alpha$. We propose associated algorithms to achieve this notion in general settings. This framework is then applied to diverse scenarios encompassing different fairness concerns, including false negative rate control in image segmentation, prediction set conditional uncertainty quantification in hierarchical classification, and de-biased text generation in language models. We conduct numerical studies on several datasets and tasks.",
      "citationCount": 10,
      "doi": "10.48550/arXiv.2405.02225",
      "arxivId": "2405.02225",
      "url": "https://www.semanticscholar.org/paper/8fd88a245d48666500f2a1b372932333e19f647c",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2405.02225"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    }
  ],
  "count": 40,
  "errors": []
}
