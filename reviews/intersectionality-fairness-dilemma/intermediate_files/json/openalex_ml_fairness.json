{
  "status": "success",
  "source": "openalex",
  "query": "fairness machine learning",
  "results": [
    {
      "openalex_id": "W3043023066",
      "doi": "10.1016/j.jbi.2020.103621",
      "title": "An empirical characterization of fair machine learning for clinical risk prediction",
      "authors": [
        {
          "name": "Stephen Pfohl",
          "openalex_id": "A5021812637",
          "orcid": "https://orcid.org/0000-0003-0551-9664",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Agata Foryciarz",
          "openalex_id": "A5065839612",
          "orcid": "https://orcid.org/0000-0002-8968-5805",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Nigam H. Shah",
          "openalex_id": "A5041175834",
          "orcid": "https://orcid.org/0000-0001-9385-7158",
          "institutions": [
            "Stanford University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-11-18",
      "abstract": null,
      "cited_by_count": 131,
      "type": "article",
      "source": {
        "name": "Journal of Biomedical Informatics",
        "type": "journal",
        "issn": [
          "1532-0464",
          "1532-0480"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2007.10306"
      },
      "topics": [
        "Healthcare cost, quality, practices",
        "Health Systems, Economic Evaluations, Quality of Life",
        "Healthcare Policy and Management"
      ],
      "referenced_works_count": 205,
      "url": "https://openalex.org/W3043023066"
    },
    {
      "openalex_id": "W3206540055",
      "doi": "10.1287/mnsc.2021.4065",
      "title": "\u201cUn\u201dFair Machine Learning Algorithms",
      "authors": [
        {
          "name": "Runshan Fu",
          "openalex_id": "A5042980028",
          "orcid": "https://orcid.org/0000-0002-9455-624X",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Manmohan Aseri",
          "openalex_id": "A5047069572",
          "orcid": "https://orcid.org/0000-0001-6943-2432",
          "institutions": [
            "University of Pittsburgh"
          ]
        },
        {
          "name": "Param Vir Singh",
          "openalex_id": "A5044224562",
          "orcid": "https://orcid.org/0000-0002-0211-7849",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Kannan Srinivasan",
          "openalex_id": "A5038040329",
          "orcid": "https://orcid.org/0000-0002-5389-8569",
          "institutions": [
            "Carnegie Mellon University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-10-21",
      "abstract": "Ensuring fairness in algorithmic decision making is a crucial policy issue. Current legislation ensures fairness by barring algorithm designers from using demographic information in their decision making. As a result, to be legally compliant, the algorithms need to ensure equal treatment. However, in many cases, ensuring equal treatment leads to disparate impact particularly when there are differences among groups based on demographic classes. In response, several \u201cfair\u201d machine learning (ML) algorithms that require impact parity (e.g., equal opportunity) at the cost of equal treatment have recently been proposed to adjust for the societal inequalities. Advocates of fair ML propose changing the law to allow the use of protected class-specific decision rules. We show that the proposed fair ML algorithms that require impact parity, while conceptually appealing, can make everyone worse off, including the very class they aim to protect. Compared with the current law, which requires treatment parity, the fair ML algorithms, which require impact parity, limit the benefits of a more accurate algorithm for a firm. As a result, profit maximizing firms could underinvest in learning, that is, improving the accuracy of their machine learning algorithms. We show that the investment in learning decreases when misclassification is costly, which is exactly the case when greater accuracy is otherwise desired. Our paper highlights the importance of considering strategic behavior of stake holders when developing and evaluating fair ML algorithms. Overall, our results indicate that fair ML algorithms that require impact parity, if turned into law, may not be able to deliver some of the anticipated benefits. This paper was accepted by Kartik Hosanagar, information systems.",
      "cited_by_count": 66,
      "type": "article",
      "source": {
        "name": "Management Science",
        "type": "journal",
        "issn": [
          "0025-1909",
          "1526-5501"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 8,
      "url": "https://openalex.org/W3206540055"
    },
    {
      "openalex_id": "W4210736086",
      "doi": "10.1145/3494672",
      "title": "A Review on Fairness in Machine Learning",
      "authors": [
        {
          "name": "Dana Pessach",
          "openalex_id": "A5043139004",
          "orcid": "https://orcid.org/0000-0002-1806-0066",
          "institutions": [
            "Tel Aviv University"
          ]
        },
        {
          "name": "Erez Shmueli",
          "openalex_id": "A5075956753",
          "orcid": "https://orcid.org/0000-0003-3193-5768",
          "institutions": [
            "Tel Aviv University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-02-03",
      "abstract": "An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence and machine learning (ML) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans, and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop ML algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision making may be inherently prone to unfairness, even when there is no intention for it. This article presents an overview of the main concepts of identifying, measuring, and improving algorithmic fairness when using ML algorithms, focusing primarily on classification tasks. The article begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process, and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, toward a better understanding of which mechanisms should be used in different scenarios. The article ends by reviewing several emerging research sub-fields of algorithmic fairness, beyond classification.",
      "cited_by_count": 478,
      "type": "review",
      "source": {
        "name": "ACM Computing Surveys",
        "type": "journal",
        "issn": [
          "0360-0300",
          "1557-7341"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Artificial Intelligence in Healthcare and Education",
        "Privacy-Preserving Technologies in Data"
      ],
      "referenced_works_count": 164,
      "url": "https://openalex.org/W4210736086"
    },
    {
      "openalex_id": "W3092541244",
      "doi": "10.1145/3616865",
      "title": "Fairness in Machine Learning: A Survey",
      "authors": [
        {
          "name": "Simon Caton",
          "openalex_id": "A5021197359",
          "orcid": "https://orcid.org/0000-0001-9379-3879",
          "institutions": [
            "University College Dublin"
          ]
        },
        {
          "name": "Christian Haas",
          "openalex_id": "A5023136097",
          "orcid": "https://orcid.org/0000-0002-2690-5962",
          "institutions": [
            "Vienna University of Economics and Business",
            "University of Nebraska at Omaha"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-08-23",
      "abstract": "When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.",
      "cited_by_count": 375,
      "type": "review",
      "source": {
        "name": "ACM Computing Surveys",
        "type": "journal",
        "issn": [
          "0360-0300",
          "1557-7341"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3616865"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 414,
      "url": "https://openalex.org/W3092541244"
    },
    {
      "openalex_id": "W3035729345",
      "doi": "10.48550/arxiv.2006.08669",
      "title": "On Adversarial Bias and the Robustness of Fair Machine Learning",
      "authors": [
        {
          "name": "Hongyan Chang",
          "openalex_id": "A5091522099",
          "orcid": "https://orcid.org/0000-0002-0569-0173"
        },
        {
          "name": "Ta Duy Nguyen",
          "openalex_id": "A5026682474"
        },
        {
          "name": "Sasi Kumar Murakonda",
          "openalex_id": "A5002058716"
        },
        {
          "name": "Ehsan Kazemi",
          "openalex_id": "A5036259251",
          "orcid": "https://orcid.org/0000-0003-4866-7245"
        },
        {
          "name": "Reza Shokri",
          "openalex_id": "A5084892128",
          "orcid": "https://orcid.org/0000-0001-9816-0173"
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-06-15",
      "abstract": "Optimizing prediction accuracy can come at the expense of fairness. Towards minimizing discrimination against a group, fair machine learning algorithms strive to equalize the behavior of a model across different groups, by imposing a fairness constraint on models. However, we show that giving the same importance to groups of different sizes and distributions, to counteract the effect of bias in training data, can be in conflict with robustness. We analyze data poisoning attacks against group-based fair machine learning, with the focus on equalized odds. An adversary who can control sampling or labeling for a fraction of training data, can reduce the test accuracy significantly beyond what he can achieve on unconstrained models. Adversarial sampling and adversarial labeling attacks can also worsen the model's fairness gap on test data, even though the model satisfies the fairness constraint on training data. We analyze the robustness of fair machine learning through an empirical evaluation of attacks on multiple algorithms and benchmark datasets.",
      "cited_by_count": 37,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2006.08669"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 37,
      "url": "https://openalex.org/W3035729345"
    },
    {
      "openalex_id": "W3014590323",
      "doi": "10.1007/978-3-030-43883-8_7",
      "title": "Fairness in Machine Learning",
      "authors": [
        {
          "name": "Luca Oneto",
          "openalex_id": "A5045802198",
          "orcid": "https://orcid.org/0000-0002-8445-395X",
          "institutions": [
            "University of Genoa",
            "University of Pisa"
          ]
        },
        {
          "name": "Silvia Chiappa",
          "openalex_id": "A5087675089",
          "orcid": "https://orcid.org/0000-0002-1882-6842",
          "institutions": [
            "DeepMind (United Kingdom)"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-01",
      "abstract": null,
      "cited_by_count": 294,
      "type": "book-chapter",
      "source": {
        "name": "Studies in computational intelligence",
        "type": "book series",
        "issn": [
          "1860-949X",
          "1860-9503"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2012.15816"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Bayesian Modeling and Causal Inference"
      ],
      "referenced_works_count": 181,
      "url": "https://openalex.org/W3014590323"
    },
    {
      "openalex_id": "W2969896603",
      "doi": "10.1145/3457607",
      "title": "A Survey on Bias and Fairness in Machine Learning",
      "authors": [
        {
          "name": "Ninareh Mehrabi",
          "openalex_id": "A5056269049",
          "institutions": [
            "Integrated Systems Incorporation (United States)"
          ]
        },
        {
          "name": "Fred Morstatter",
          "openalex_id": "A5002709735",
          "orcid": "https://orcid.org/0000-0002-0247-4328",
          "institutions": [
            "Integrated Systems Incorporation (United States)"
          ]
        },
        {
          "name": "Nripsuta Ani Saxena",
          "openalex_id": "A5091625866",
          "orcid": "https://orcid.org/0009-0004-1121-5426",
          "institutions": [
            "Integrated Systems Incorporation (United States)"
          ]
        },
        {
          "name": "Kristina Lerman",
          "openalex_id": "A5049634383",
          "orcid": "https://orcid.org/0000-0002-5071-0575",
          "institutions": [
            "Integrated Systems Incorporation (United States)"
          ]
        },
        {
          "name": "Aram Galstyan",
          "openalex_id": "A5101715504",
          "orcid": "https://orcid.org/0000-0003-4215-0886",
          "institutions": [
            "Integrated Systems Incorporation (United States)"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-07-13",
      "abstract": "With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",
      "cited_by_count": 316,
      "type": "preprint",
      "source": {
        "name": "ACM Computing Surveys",
        "type": "journal",
        "issn": [
          "0360-0300",
          "1557-7341"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/1908.09635"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 148,
      "url": "https://openalex.org/W2969896603"
    },
    {
      "openalex_id": "W3033733989",
      "doi": "10.1145/3376898",
      "title": "A snapshot of the frontiers of fairness in machine learning",
      "authors": [
        {
          "name": "Alexandra Chouldechova",
          "openalex_id": "A5057438082",
          "orcid": "https://orcid.org/0000-0002-2337-9610",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Aaron Roth",
          "openalex_id": "A5057693522",
          "orcid": "https://orcid.org/0000-0002-0586-0515",
          "institutions": [
            "University of Pennsylvania"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-04-20",
      "abstract": "A group of industry, academic, and government experts convene in Philadelphia to explore the roots of algorithmic bias.",
      "cited_by_count": 326,
      "type": "article",
      "source": {
        "name": "Communications of the ACM",
        "type": "journal",
        "issn": [
          "0001-0782",
          "1557-7317"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Artificial Intelligence in Healthcare and Education",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 69,
      "url": "https://openalex.org/W3033733989"
    },
    {
      "openalex_id": "W3093194580",
      "doi": "10.48550/arxiv.2010.07389",
      "title": "Explainability for fair machine learning",
      "authors": [
        {
          "name": "Tom Begley",
          "openalex_id": "A5049071368",
          "institutions": [
            "Faculty (United Kingdom)"
          ]
        },
        {
          "name": "Tobias Schwedes",
          "openalex_id": "A5016531216",
          "institutions": [
            "Faculty (United Kingdom)"
          ]
        },
        {
          "name": "Christopher Frye",
          "openalex_id": "A5025519853",
          "institutions": [
            "Faculty (United Kingdom)"
          ]
        },
        {
          "name": "Ilya Feige",
          "openalex_id": "A5074723086",
          "institutions": [
            "University College London"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-10-14",
      "abstract": "As the decisions made or influenced by machine learning models increasingly impact our lives, it is crucial to detect, understand, and mitigate unfairness. But even simply determining what \"unfairness\" should mean in a given context is non-trivial: there are many competing definitions, and choosing between them often requires a deep understanding of the underlying task. It is thus tempting to use model explainability to gain insights into model fairness, however existing explainability tools do not reliably indicate whether a model is indeed fair. In this work we present a new approach to explaining fairness in machine learning, based on the Shapley value paradigm. Our fairness explanations attribute a model's overall unfairness to individual input features, even in cases where the model does not operate on sensitive attributes directly. Moreover, motivated by the linearity of Shapley explainability, we propose a meta algorithm for applying existing training-time fairness interventions, wherein one trains a perturbation to the original model, rather than a new model entirely. By explaining the original model, the perturbation, and the fair-corrected model, we gain insight into the accuracy-fairness trade-off that is being made by the intervention. We further show that this meta algorithm enjoys both flexibility and stability benefits with no loss in performance.",
      "cited_by_count": 21,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2010.07389"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 32,
      "url": "https://openalex.org/W3093194580"
    },
    {
      "openalex_id": "W4214835294",
      "doi": "10.1002/widm.1452",
      "title": "A survey on datasets for fairness-aware machine learning",
      "authors": [
        {
          "name": "Tai Le Quy",
          "openalex_id": "A5040114911",
          "orcid": "https://orcid.org/0000-0001-8512-5854",
          "institutions": [
            "L3S Research Center",
            "Leibniz University Hannover"
          ]
        },
        {
          "name": "Arjun Roy",
          "openalex_id": "A5078516991",
          "orcid": "https://orcid.org/0000-0002-4279-9442",
          "institutions": [
            "Leibniz University Hannover",
            "L3S Research Center",
            "Freie Universit\u00e4t Berlin"
          ]
        },
        {
          "name": "Vasileios Iosifidis",
          "openalex_id": "A5012928144",
          "orcid": "https://orcid.org/0000-0002-3005-4507",
          "institutions": [
            "Leibniz University Hannover",
            "L3S Research Center"
          ]
        },
        {
          "name": "Wenbin Zhang",
          "openalex_id": "A5100710814",
          "orcid": "https://orcid.org/0000-0003-3024-5415",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Eirini Ntoutsi",
          "openalex_id": "A5089247128",
          "orcid": "https://orcid.org/0000-0001-5729-1003",
          "institutions": [
            "Freie Universit\u00e4t Berlin"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-10-01",
      "abstract": "As decision-making increasingly relies on Machine Learning (ML) and (big) data, the issue of fairness in data-driven Artificial Intelligence (AI) systems is receiving increasing attention from both research and industry. A large variety of fairness-aware machine learning solutions have been proposed which involve fairness-related interventions in the data, learning algorithms and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real-world datasets used for fairness-aware machine learning. We focus on tabular data as the most common data representation for fairness-aware machine learning. We start our analysis by identifying relationships between the different attributes, particularly w.r.t. protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate the interesting relationships using exploratory analysis.",
      "cited_by_count": 214,
      "type": "article",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2110.00530"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Privacy-Preserving Technologies in Data"
      ],
      "referenced_works_count": 159,
      "url": "https://openalex.org/W4214835294"
    },
    {
      "openalex_id": "W3156313717",
      "doi": "10.3389/frai.2020.561802",
      "title": "Addressing Fairness, Bias, and Appropriate Use of Artificial Intelligence and Machine Learning in Global Health",
      "authors": [
        {
          "name": "R. Fletcher",
          "openalex_id": "A5065184377",
          "orcid": "https://orcid.org/0000-0001-8470-8417",
          "institutions": [
            "Massachusetts Institute of Technology",
            "University of Massachusetts Chan Medical School"
          ]
        },
        {
          "name": "Audace Nakeshimana",
          "openalex_id": "A5082614315",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Olusubomi Olubeko",
          "openalex_id": "A5030673309",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-04-15",
      "abstract": "In Low- and Middle- Income Countries (LMICs), machine learning (ML) and artificial intelligence (AI) offer attractive solutions to address the shortage of health care resources and improve the capacity of the local health care infrastructure. However, AI and ML should also be used cautiously, due to potential issues of fairness and algorithmic bias that may arise if not applied properly. Furthermore, populations in LMICs can be particularly vulnerable to bias and fairness in AI algorithms, due to a lack of technical capacity, existing social bias against minority groups, and a lack of legal protections. In order to address the need for better guidance within the context of global health, we describe three basic criteria (Appropriateness, Fairness, and Bias) that can be used to help evaluate the use of machine learning and AI systems: 1) APPROPRIATENESS is the process of deciding how the algorithm should be used in the local context, and properly matching the machine learning model to the target population; 2) BIAS is a systematic tendency in a model to favor one demographic group vs another, which can be mitigated but can lead to unfairness; and 3) FAIRNESS involves examining the impact on various demographic groups and choosing one of several mathematical definitions of group fairness that will adequately satisfy the desired set of legal, cultural, and ethical requirements. Finally, we illustrate how these principles can be applied using a case study of machine learning applied to the diagnosis and screening of pulmonary disease in Pune, India. We hope that these methods and principles can help guide researchers and organizations working in global health who are considering the use of machine learning and artificial intelligence.",
      "cited_by_count": 260,
      "type": "editorial",
      "source": {
        "name": "Frontiers in Artificial Intelligence",
        "type": "journal",
        "issn": [
          "2624-8212"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/pdf"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Ethics and Social Impacts of AI",
        "COVID-19 diagnosis using AI"
      ],
      "referenced_works_count": 45,
      "url": "https://openalex.org/W3156313717"
    },
    {
      "openalex_id": "W3184865800",
      "doi": "10.1038/s42256-021-00373-4",
      "title": "Machine learning and algorithmic fairness in public and population health",
      "authors": [
        {
          "name": "Vishwali Mhasawade",
          "openalex_id": "A5081852226",
          "orcid": "https://orcid.org/0000-0003-1269-7071",
          "institutions": [
            "New York University"
          ]
        },
        {
          "name": "Yuan Zhao",
          "openalex_id": "A5064463116",
          "orcid": "https://orcid.org/0000-0001-7997-5385",
          "institutions": [
            "New York University"
          ]
        },
        {
          "name": "Rumi Chunara",
          "openalex_id": "A5005061793",
          "orcid": "https://orcid.org/0000-0002-5346-7259",
          "institutions": [
            "New York University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-07-29",
      "abstract": null,
      "cited_by_count": 188,
      "type": "article",
      "source": {
        "name": "Nature Machine Intelligence",
        "type": "journal",
        "issn": [
          "2522-5839"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "bronze",
        "oa_url": "https://www.nature.com/articles/s42256-021-00373-4.pdf"
      },
      "topics": [
        "Health, Environment, Cognitive Aging",
        "Global Public Health Policies and Epidemiology",
        "Chronic Disease Management Strategies"
      ],
      "referenced_works_count": 101,
      "url": "https://openalex.org/W3184865800"
    },
    {
      "openalex_id": "W4367318855",
      "doi": "10.1038/s42256-023-00651-3",
      "title": "Translating intersectionality to fair machine learning in health sciences",
      "authors": [
        {
          "name": "Elle Lett",
          "openalex_id": "A5048124985",
          "orcid": "https://orcid.org/0000-0002-6590-7821",
          "institutions": [
            "Boston Children's Hospital",
            "University of Pennsylvania"
          ]
        },
        {
          "name": "William La Cava",
          "openalex_id": "A5002465837",
          "orcid": "https://orcid.org/0000-0002-1332-2960",
          "institutions": [
            "Harvard University",
            "Boston Children's Hospital"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-04-28",
      "abstract": null,
      "cited_by_count": 21,
      "type": "article",
      "source": {
        "name": "Nature Machine Intelligence",
        "type": "journal",
        "issn": [
          "2522-5839"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10437125/pdf/nihms-1897462.pdf"
      },
      "topics": [
        "Sex and Gender in Healthcare",
        "Gender Politics and Representation",
        "Diversity and Career in Medicine"
      ],
      "referenced_works_count": 19,
      "url": "https://openalex.org/W4367318855"
    },
    {
      "openalex_id": "W3023069697",
      "doi": "10.1016/s2589-7500(20)30065-0",
      "title": "Ethical limitations of algorithmic fairness solutions in health care machine learning",
      "authors": [
        {
          "name": "Melissa D. McCradden",
          "openalex_id": "A5063981686",
          "orcid": "https://orcid.org/0000-0002-6476-2165",
          "institutions": [
            "Hospital for Sick Children"
          ]
        },
        {
          "name": "Shalmali Joshi",
          "openalex_id": "A5035149567",
          "institutions": [
            "Vector Institute"
          ]
        },
        {
          "name": "Mjaye Mazwi",
          "openalex_id": "A5080855486",
          "orcid": "https://orcid.org/0000-0003-1345-5429",
          "institutions": [
            "Hospital for Sick Children"
          ]
        },
        {
          "name": "James A. Anderson",
          "openalex_id": "A5103410231",
          "institutions": [
            "Hospital for Sick Children"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-04-28",
      "abstract": "Artificial intelligence has exposed pernicious bias within health data that constitutes substantial ethical threat to the use of machine learning in medicine.1Char DS Shah NH Magnus D Implementing machine learning in health care\u2014addressing ethical challenges.N Engl J Med. 2018; 378: 981-983Crossref PubMed Scopus (445) Google Scholar, 2Obermeyer Z Powers B Vogeli C Mullainathan S Dissecting racial bias in an algorithm used to manage the health of populations.Science. 2019; 366: 447-453Crossref PubMed Scopus (1151) Google Scholar Solutions of algorithmic fairness have been developed to create neutral models: models designed to produce non-discriminatory predictions by constraining bias with respect to predicted outcomes for protected identities, such as race or gender.3Corbett-Davies S Goel S The measure and mismeasure of fairness: a critical review of fair machine learning.https://5harad.com/papers/fair-ml.pdfDate accessed: March 16, 2020Google Scholar These solutions can omit such variables from the model (widely regarded as ineffective and can increase discrimination), constrain it to ensure equal error rates across groups, derive outcomes that are independent of one's identity after controlling for the estimated risk of that outcome, or mathematically balance benefit and harm to all groups. The temptation to engineer ethics into algorithm design is immense and industry is increasingly pushing these solutions. In the health-care space, where the stakes could be higher, clinicians will integrate these models into their care, trusting the issue of bias has been sufficiently managed within the model. However, even if well recognised technical challenges are set aside,3Corbett-Davies S Goel S The measure and mismeasure of fairness: a critical review of fair machine learning.https://5harad.com/papers/fair-ml.pdfDate accessed: March 16, 2020Google Scholar, 4Marmot M Social determinants of health inequalities.Lancet. 2005; 365: 1099-1104Summary Full Text Full Text PDF PubMed Scopus (2858) Google Scholar framing fairness as a purely technical problem solvable by the inclusion of more data or accurate computations is ethically problematic. We highlight challenges to the ethical and empirical efficacy of solutions of algorithmic fairness that show risks of relying too heavily on the so called veneer of technical neutrality,5Benjamin R Assessing risk, automating racism.Science. 2019; 366: 421-422Crossref PubMed Scopus (97) Google Scholar which could exacerbate harms to vulnerable groups. Historically, algorithmic fairness has not accounted for complex causal relationships between biological, environmental, and social factors that give rise to differences in medical conditions across protected identities. Social determinants of health play an important role, particularly for risk models. Social and structural factors affect health across multiple intersecting identities,4Marmot M Social determinants of health inequalities.Lancet. 2005; 365: 1099-1104Summary Full Text Full Text PDF PubMed Scopus (2858) Google Scholar but the mechanism(s) by which social determinants affect health outcomes is not always well understood. Additional complications flow from the reality that difference does not always entail inequity. In some instances, it is appropriate to incorporate differences between identities because there is a reasonable presumption of causation. For example, biological differences between genders can affect the efficacy of pharmacological compounds; incorporating these differences into prescribing practices does not make those prescriptions unjust. However, incorporating non-causative factors into recommendations can propagate unequal treatment by reifying extant inequities and exacerbating their effects. We should not allow models to promote different standards of care according to protected identities that do not have a causative association with the outcome. Nevertheless, in many cases it is difficult to distinguish between acknowledging difference and propagating discrimination. Given the epistemic uncertainty surrounding the association between protected identities and health outcomes, the use of fairness solutions can create empirical challenges. Consider the case of heart attack symptoms among women.6Wenger NK Cardiovascular disease: the female heart is vulnerable. A call to action from the 10Q Report.Clin Cardiol. 2012; 35: 134-135Crossref PubMed Scopus (11) Google Scholar The under-representation of women (particularly women of colour) in research of heart health is now well recognised as problematic and directly affected uneven improvements in treatment of heart attacks between women and men. By tailoring health solutions to the majority (ie, referent) group, we inevitably fall short of helping all patients. Many algorithmic fairness solutions, in effect, replicate this problem by trying to fit the non-referent groups to that of the referent,7Friedler SA Scheidegger C Venkatasubramanian S On the (im)possibility of fairness.https://arxiv.org/pdf/1609.07236.pdfDate: Sept 23, 2019Date accessed: March 16, 2020Google Scholar, 8Barocas S Hardt M Narayanan A Fairness and machine learning: limitations and opportunities; 2018.Fairmlbook.orgDate: 2019Google Scholar ignoring heterogeneity and assuming that the latter represents a true underlying pattern. Another concern is disconnection between the patient's clinical trajectory and the fair prediction. Consider the implications at the point-of-care, a model, corrected for fairness, will predict that a patient will respond to a treatment as a patient in the referent class would. What happens when that patient does not have the predicted response? This difference between an idealised model and non-ideal, real-world behaviour affects metrics of model performance (eg, specificity, sensitivity) and clinical utility in practice. Moreover, the model has made an ineffective recommendation that could have obscured more relevant interventions to help that patient. If clinicians and patients believe that the mode has been rendered neutral, then any discrepancies between model prediction and the patient's true clinical state might be impossible to interpret. The result would be to camouflage persistent health inequalities. As such, fairness, operationalised by output metrics alone, is insufficient; real-world consequences should be carefully considered. Bias and ineffective solutions of algorithmic fairness threaten the ethical obligation to avoid or minimise harms to patients (non-maleficence; panel). Non-maleficence demands that any new clinical tool should be assessed for patient safety. For health-care machine learning, safety should include awareness of model limitations with respect to protected identities and social determinants of health. Considerations of justice requires that implemented models do not exacerbate pernicious bias. There is a movement toward developing guidelines of standardised reporting for machine learning models of health care9Collins GS Reitsma JB Altman DG Moons KGM TRIPOD GroupTransparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): The TRIPOD Statement.Ann Intern Med. 2015; 162: 55-63Crossref PubMed Scopus (1245) Google Scholar and their prospective appraisal through clinical trials.10The CONSORT-AI and SPIRIT-AI Steering GroupReporting guidelines for clinical trials evaluating artificial intelligence interventions are needed.Nat Med. 2019; 25: 1467-1468Crossref PubMed Scopus (69) Google Scholar Appraisal is particularly important in determining the real-world implications for vulnerable patients when machine learning models are integrated into clinical decision making. Clinical trials are essential to providing a sense of the model's performance for clinicians to make informed decisions at the point-of-care through awareness of identity-related model limitations.PanelRecommendations for ethical approaches to issues of bias in health models of machine learningRelying on neutral algorithms is problematicChallenges cast doubt on whether any solution can adequately facilitate ethical goals. Resisting the tendency to view machine learning solutions as objective is essential to remaining patient-centered and preventing unintended harms.Problem formulation can support improved modelsChanging the way the problem is conceptualised and operationalised can reduce the effect of pernicious bias on outputs.6Wenger NK Cardiovascular disease: the female heart is vulnerable. A call to action from the 10Q Report.Clin Cardiol. 2012; 35: 134-135Crossref PubMed Scopus (11) Google Scholar Clinicians have a key role in identifying actionable, clinical problems where the effects of bias are minimized, or causal knowledge exists to support algorithmic fairness solutions. Clinicians in non-medical sciences might have epistemic advantages to support such formulations.Transparency is required surrounding model development and statistical validationStandardisation of reporting for models of machine learning can promote transparency about training data and statistical validation which can help clinicians and health-care decision makers to determine the transferability of the model to their served population. Large discrepancies increase the risk of under-performance in under-represented patients. Group-specific performance metrics (eg, stratification by ethnicity, gender, or socioeconomic status) can inform considerations for patient safety in implementing a given model.Initiating transparency at point-of-careAs the field of explainable or interpretable machine learning evolves, we suggest that, when sensitive attributes are used in problem formulation and could affect predictions, they should be accompanied by visibility of the top predictive features. Where predictions are affected by sensitive variables, prediction and rationale should be documented for the ensuing clinical decision to promote fair and transparent medical decision making. Communication with patients about the rationale is essential to shared decision making.Transparency at the prediction levelRobust auditing processes are increasingly viewed as essential to proper oversight of machine learning tools. When bias is considered, auditing can promote reflexive understanding of how tools of machine learning can affect care management of vulnerable patients. Discrepancies in systematic prediction can become evident and used as evidence of the need for beneficial interventions and highlight large structural barriers that affect health inequalities.Ethical decision making suggests engaging diverse knowledge sourcesEthical analysis should consider real-world consequences for affected groups, weigh benefits and risks of various approaches, and engage stakeholders to come to the most supportable conclusion. Therefore, analysis needs to focus on the downstream effects on patients rather than adopting the presumption that fairness is accomplished solely in the metrics of the system. Relying on neutral algorithms is problematic Challenges cast doubt on whether any solution can adequately facilitate ethical goals. Resisting the tendency to view machine learning solutions as objective is essential to remaining patient-centered and preventing unintended harms. Problem formulation can support improved models Changing the way the problem is conceptualised and operationalised can reduce the effect of pernicious bias on outputs.6Wenger NK Cardiovascular disease: the female heart is vulnerable. A call to action from the 10Q Report.Clin Cardiol. 2012; 35: 134-135Crossref PubMed Scopus (11) Google Scholar Clinicians have a key role in identifying actionable, clinical problems where the effects of bias are minimized, or causal knowledge exists to support algorithmic fairness solutions. Clinicians in non-medical sciences might have epistemic advantages to support such formulations. Transparency is required surrounding model development and statistical validation Standardisation of reporting for models of machine learning can promote transparency about training data and statistical validation which can help clinicians and health-care decision makers to determine the transferability of the model to their served population. Large discrepancies increase the risk of under-performance in under-represented patients. Group-specific performance metrics (eg, stratification by ethnicity, gender, or socioeconomic status) can inform considerations for patient safety in implementing a given model. Initiating transparency at point-of-care As the field of explainable or interpretable machine learning evolves, we suggest that, when sensitive attributes are used in problem formulation and could affect predictions, they should be accompanied by visibility of the top predictive features. Where predictions are affected by sensitive variables, prediction and rationale should be documented for the ensuing clinical decision to promote fair and transparent medical decision making. Communication with patients about the rationale is essential to shared decision making. Transparency at the prediction level Robust auditing processes are increasingly viewed as essential to proper oversight of machine learning tools. When bias is considered, auditing can promote reflexive understanding of how tools of machine learning can affect care management of vulnerable patients. Discrepancies in systematic prediction can become evident and used as evidence of the need for beneficial interventions and highlight large structural barriers that affect health inequalities. Ethical decision making suggests engaging diverse knowledge sources Ethical analysis should consider real-world consequences for affected groups, weigh benefits and risks of various approaches, and engage stakeholders to come to the most supportable conclusion. Therefore, analysis needs to focus on the downstream effects on patients rather than adopting the presumption that fairness is accomplished solely in the metrics of the system. Some computations can promote justice through revealing unfairness and refining problem formulation. Obermeyer and colleagues2Obermeyer Z Powers B Vogeli C Mullainathan S Dissecting racial bias in an algorithm used to manage the health of populations.Science. 2019; 366: 447-453Crossref PubMed Scopus (1151) Google Scholar show how calibration can reveal unfairness in a seemingly neutral task through which choice of label can dictate how heavily bias is incorporated into predictions. It might be that no way exists to define a purely neutral problem; some clinical prediction tasks might be more susceptible to bias than others. Transparency at multiple points in the pipeline of machine learning including development, testing, and implementation stages can support interpretation of model outputs by relevant stakeholders (eg, researchers, clinicians, patients, and auditors). Combined with adequate documentation of outputs and ensuing decisions, these steps support a strong accountability framework for point-of-care machine learning tools with respect to safety and fairness to patients. Problem formulation with respect to bias will often be value-laden and ethically charged. Ethical decision making highlights the importance of converging knowledge sources to inform a given choice. Important stakeholders could include affected communities, cultural anthropologists, social scientists, and race and gender theorists. Computations alone clearly cannot solve the bias problem, but they could be offered a place within a broader approach to addressing fairness aims in healthcare. Algorithmic fairness could be necessary to fix statistical limitations reflective of perniciously biased data, and we encourage this work. The worry is that suggesting these as solutions risks unintended harms.5Benjamin R Assessing risk, automating racism.Science. 2019; 366: 421-422Crossref PubMed Scopus (97) Google Scholar Bias is not new; however, machine learning has potential to reveal bias, motivate change, and support ethical analysis while bringing this crucial conversation to a new audience. We are at a watershed moment in health care. Ethical considerations have rarely been so integral and essential to maximising success of a technology both empirically and clinically. The time is right to partake in thoughtful and collaborative engagement on the challenge of bias to bring about lasting change. We declare no competing interests.",
      "cited_by_count": 236,
      "type": "article",
      "source": {
        "name": "The Lancet Digital Health",
        "type": "journal",
        "issn": [
          "2589-7500"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "http://www.thelancet.com/article/S2589750020300650/pdf"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Healthcare cost, quality, practices",
        "COVID-19 and healthcare impacts"
      ],
      "referenced_works_count": 10,
      "url": "https://openalex.org/W3023069697"
    },
    {
      "openalex_id": "W4386094675",
      "doi": "10.48550/arxiv.2010.04053",
      "title": "Fairness in Machine Learning: A Survey",
      "authors": [
        {
          "name": "Simon Caton",
          "openalex_id": "A5021197359",
          "orcid": "https://orcid.org/0000-0001-9379-3879"
        },
        {
          "name": "Christian Haas",
          "openalex_id": "A5023136097",
          "orcid": "https://orcid.org/0000-0002-2690-5962"
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-10-04",
      "abstract": "As Machine Learning technologies become increasingly used in contexts that affect citizens, companies as well as researchers need to be confident that their application of these methods will not have unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches to mitigating (social) biases and increase fairness in the Machine Learning literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, unsupervised learning, and natural language processing is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as four dilemmas for fairness research.",
      "cited_by_count": 129,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://doi.org/10.48550/arxiv.2010.04053"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4386094675"
    },
    {
      "openalex_id": "W4392515337",
      "doi": "10.1016/j.jbi.2024.104622",
      "title": "A scoping review of fair machine learning techniques when using real-world data",
      "authors": [
        {
          "name": "Yu Huang",
          "openalex_id": "A5037823270",
          "orcid": "https://orcid.org/0000-0001-7373-4716",
          "institutions": [
            "University of Florida Health"
          ]
        },
        {
          "name": "Jingchuan Guo",
          "openalex_id": "A5059070322",
          "orcid": "https://orcid.org/0000-0001-9799-2592",
          "institutions": [
            "University of Florida"
          ]
        },
        {
          "name": "Wei\u2010Han Chen",
          "openalex_id": "A5090544928",
          "orcid": "https://orcid.org/0000-0001-8545-6127",
          "institutions": [
            "University of Florida"
          ]
        },
        {
          "name": "Hsin-Yueh Lin",
          "openalex_id": "A5101313283",
          "institutions": [
            "University of Florida"
          ]
        },
        {
          "name": "Huilin Tang",
          "openalex_id": "A5075479756",
          "orcid": "https://orcid.org/0000-0002-5814-6657",
          "institutions": [
            "University of Florida"
          ]
        },
        {
          "name": "Fei Wang",
          "openalex_id": "A5100455768",
          "orcid": "https://orcid.org/0000-0001-9459-9461",
          "institutions": [
            "Cornell University"
          ]
        },
        {
          "name": "Hua Xu",
          "openalex_id": "A5084072550",
          "orcid": "https://orcid.org/0000-0002-5274-4672",
          "institutions": [
            "Yale University"
          ]
        },
        {
          "name": "Jiang Bian",
          "openalex_id": "A5030951014",
          "orcid": "https://orcid.org/0000-0002-2238-5429",
          "institutions": [
            "University of Florida Health"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-03-01",
      "abstract": null,
      "cited_by_count": 30,
      "type": "review",
      "source": {
        "name": "Journal of Biomedical Informatics",
        "type": "journal",
        "issn": [
          "1532-0464",
          "1532-0480"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://doi.org/10.1016/j.jbi.2024.104622"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Healthcare cost, quality, practices",
        "Health Systems, Economic Evaluations, Quality of Life"
      ],
      "referenced_works_count": 75,
      "url": "https://openalex.org/W4392515337"
    },
    {
      "openalex_id": "W3024029620",
      "doi": "10.48550/arxiv.2005.07572",
      "title": "Participatory Problem Formulation for Fairer Machine Learning Through Community Based System Dynamics",
      "authors": [
        {
          "name": "Donald Martin",
          "openalex_id": "A5007466654",
          "orcid": "https://orcid.org/0000-0001-5913-2372",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Vinodkumar Prabhakaran",
          "openalex_id": "A5019297976",
          "orcid": "https://orcid.org/0000-0003-3329-2305",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Jill Kuhlberg",
          "openalex_id": "A5008299209",
          "institutions": [
            "Star Technology and Research (United States)"
          ]
        },
        {
          "name": "Andrew Smart",
          "openalex_id": "A5077833261",
          "orcid": "https://orcid.org/0000-0002-9816-7348",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "William Isaac",
          "openalex_id": "A5048272174",
          "orcid": "https://orcid.org/0000-0002-1297-5409",
          "institutions": [
            "DeepMind (United Kingdom)"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-05-15",
      "abstract": "Recent research on algorithmic fairness has highlighted that the problem formulation phase of ML system development can be a key source of bias that has significant downstream impacts on ML system fairness outcomes. However, very little attention has been paid to methods for improving the fairness efficacy of this critical phase of ML system development. Current practice neither accounts for the dynamic complexity of high-stakes domains nor incorporates the perspectives of vulnerable stakeholders. In this paper we introduce community based system dynamics (CBSD) as an approach to enable the participation of typically excluded stakeholders in the problem formulation phase of the ML system development process and facilitate the deep problem understanding required to mitigate bias during this crucial stage.",
      "cited_by_count": 28,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2005.07572"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Open Source Software Innovations",
        "Innovative Approaches in Technology and Social Development"
      ],
      "referenced_works_count": 25,
      "url": "https://openalex.org/W3024029620"
    },
    {
      "openalex_id": "W4316038168",
      "doi": "10.3390/bdcc7010015",
      "title": "Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods",
      "authors": [
        {
          "name": "Tiago Palma Pagano",
          "openalex_id": "A5051842648",
          "orcid": "https://orcid.org/0000-0003-2457-9064",
          "institutions": [
            "Servi\u00e7o Nacional de Aprendizagem Industrial"
          ]
        },
        {
          "name": "Rafael B. Loureiro",
          "openalex_id": "A5085026002",
          "orcid": "https://orcid.org/0000-0002-2404-0643",
          "institutions": [
            "Servi\u00e7o Nacional de Aprendizagem Industrial"
          ]
        },
        {
          "name": "Fernanda V. N. Lisboa",
          "openalex_id": "A5063443360",
          "orcid": "https://orcid.org/0000-0001-5203-2408",
          "institutions": [
            "Servi\u00e7o Nacional de Aprendizagem Industrial"
          ]
        },
        {
          "name": "Rodrigo M. Peixoto",
          "openalex_id": "A5000563103",
          "orcid": "https://orcid.org/0000-0001-6405-1593",
          "institutions": [
            "Servi\u00e7o Nacional de Aprendizagem Industrial"
          ]
        },
        {
          "name": "Guilherme A. de Sousa Guimar\u00e3es",
          "openalex_id": "A5039516765",
          "orcid": "https://orcid.org/0000-0002-7407-5908",
          "institutions": [
            "Servi\u00e7o Nacional de Aprendizagem Industrial"
          ]
        },
        {
          "name": "Gustavo O. R. Cruz",
          "openalex_id": "A5071847598",
          "orcid": "https://orcid.org/0000-0003-4613-9342",
          "institutions": [
            "Servi\u00e7o Nacional de Aprendizagem Industrial"
          ]
        },
        {
          "name": "Maira M. Araujo",
          "openalex_id": "A5036120514",
          "orcid": "https://orcid.org/0000-0002-5564-4781",
          "institutions": [
            "Servi\u00e7o Nacional de Aprendizagem Industrial"
          ]
        },
        {
          "name": "Lucas Lisboa dos Santos",
          "openalex_id": "A5061437347",
          "orcid": "https://orcid.org/0000-0002-5949-8515",
          "institutions": [
            "Servi\u00e7o Nacional de Aprendizagem Industrial"
          ]
        },
        {
          "name": "Marco A S Cruz",
          "openalex_id": "A5072917598",
          "orcid": "https://orcid.org/0000-0001-7196-1237"
        },
        {
          "name": "Ewerton L. S. Oliveira",
          "openalex_id": "A5060782998",
          "orcid": "https://orcid.org/0000-0002-3160-3571"
        },
        {
          "name": "Ingrid Winkler",
          "openalex_id": "A5020564129",
          "orcid": "https://orcid.org/0000-0001-6505-6636",
          "institutions": [
            "Servi\u00e7o Nacional de Aprendizagem Industrial"
          ]
        },
        {
          "name": "Erick Giovani Sperandio Nascimento",
          "openalex_id": "A5004611294",
          "orcid": "https://orcid.org/0000-0003-2219-0290",
          "institutions": [
            "Servi\u00e7o Nacional de Aprendizagem Industrial",
            "University of Surrey"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-13",
      "abstract": "One of the difficulties of artificial intelligence is to ensure that model decisions are fair and free of bias. In research, datasets, metrics, techniques, and tools are applied to detect and mitigate algorithmic unfairness and bias. This study examines the current knowledge on bias and unfairness in machine learning models. The systematic review followed the PRISMA guidelines and is registered on OSF plataform. The search was carried out between 2021 and early 2022 in the Scopus, IEEE Xplore, Web of Science, and Google Scholar knowledge bases and found 128 articles published between 2017 and 2022, of which 45 were chosen based on search string optimization and inclusion and exclusion criteria. We discovered that the majority of retrieved works focus on bias and unfairness identification and mitigation techniques, offering tools, statistical approaches, important metrics, and datasets typically used for bias experiments. In terms of the primary forms of bias, data, algorithm, and user interaction were addressed in connection to the preprocessing, in-processing, and postprocessing mitigation methods. The use of Equalized Odds, Opportunity Equality, and Demographic Parity as primary fairness metrics emphasizes the crucial role of sensitive attributes in mitigating bias. The 25 datasets chosen span a wide range of areas, including criminal justice image enhancement, finance, education, product pricing, and health, with the majority including sensitive attributes. In terms of tools, Aequitas is the most often referenced, yet many of the tools were not employed in empirical experiments. A limitation of current research is the lack of multiclass and multimetric studies, which are found in just a few works and constrain the investigation to binary-focused method. Furthermore, the results indicate that different fairness metrics do not present uniform results for a given use case, and that more research with varied model architectures is necessary to standardize which ones are more appropriate for a given context. We also observed that all research addressed the transparency of the algorithm, or its capacity to explain how decisions are taken.",
      "cited_by_count": 223,
      "type": "review",
      "source": {
        "name": "Big Data and Cognitive Computing",
        "type": "journal",
        "issn": [
          "2504-2289"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2504-2289/7/1/15/pdf?version=1674136071"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Privacy-Preserving Technologies in Data"
      ],
      "referenced_works_count": 82,
      "url": "https://openalex.org/W4316038168"
    },
    {
      "openalex_id": "W3131567681",
      "doi": "10.2139/ssrn.3792772",
      "title": "Bias Preservation in Machine Learning: The Legality of Fairness Metrics Under EU Non-Discrimination Law",
      "authors": [
        {
          "name": "Sandra Wachter",
          "openalex_id": "A5075172090",
          "orcid": "https://orcid.org/0000-0003-3800-0113",
          "institutions": [
            "University of Oxford",
            "Internet Society"
          ]
        },
        {
          "name": "Brent Mittelstadt",
          "openalex_id": "A5081516308",
          "orcid": "https://orcid.org/0000-0002-4709-6404",
          "institutions": [
            "Internet Society",
            "University of Oxford"
          ]
        },
        {
          "name": "Chris Russell",
          "openalex_id": "A5008943199",
          "orcid": "https://orcid.org/0000-0003-1665-1759",
          "institutions": [
            "Amazon (United States)"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-01-01",
      "abstract": null,
      "cited_by_count": 161,
      "type": "article",
      "source": {
        "name": "SSRN Electronic Journal",
        "type": "repository",
        "issn": [
          "1556-5068"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://papers.ssrn.com/sol3/Delivery.cfm?abstractid=3792772"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Digitalization, Law, and Regulation",
        "Law, AI, and Intellectual Property"
      ],
      "referenced_works_count": 5,
      "url": "https://openalex.org/W3131567681"
    },
    {
      "openalex_id": "W4281254423",
      "doi": "10.1007/s00146-022-01455-6",
      "title": "Beyond bias and discrimination: redefining the AI ethics principle of fairness in healthcare machine-learning algorithms",
      "authors": [
        {
          "name": "Benedetta Giovanola",
          "openalex_id": "A5037942178",
          "orcid": "https://orcid.org/0000-0002-8429-0871",
          "institutions": [
            "Tufts University",
            "University of Macerata"
          ]
        },
        {
          "name": "Simona Tiribelli",
          "openalex_id": "A5037516622",
          "orcid": "https://orcid.org/0000-0003-4101-5914",
          "institutions": [
            "University of Macerata"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-05-21",
      "abstract": null,
      "cited_by_count": 139,
      "type": "article",
      "source": {
        "name": "AI & Society",
        "type": "journal",
        "issn": [
          "0951-5666",
          "1435-5655"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s00146-022-01455-6.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Artificial Intelligence in Healthcare and Education",
        "Healthcare cost, quality, practices"
      ],
      "referenced_works_count": 98,
      "url": "https://openalex.org/W4281254423"
    },
    {
      "openalex_id": "W4288758404",
      "doi": "10.1145/3551390",
      "title": "In-Processing Modeling Techniques for Machine Learning Fairness: A Survey",
      "authors": [
        {
          "name": "Mingyang Wan",
          "openalex_id": "A5016314811",
          "orcid": "https://orcid.org/0000-0002-1536-3643",
          "institutions": [
            "Texas A&M University"
          ]
        },
        {
          "name": "Daochen Zha",
          "openalex_id": "A5058071176",
          "orcid": "https://orcid.org/0000-0002-6677-7504",
          "institutions": [
            "Rice University"
          ]
        },
        {
          "name": "Ninghao Liu",
          "openalex_id": "A5007489034",
          "orcid": "https://orcid.org/0000-0002-9170-2424",
          "institutions": [
            "University of Georgia"
          ]
        },
        {
          "name": "Na Zou",
          "openalex_id": "A5084497683",
          "orcid": "https://orcid.org/0000-0003-1984-795X"
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-07-30",
      "abstract": "Machine learning models are becoming pervasive in high-stakes applications. Despite their clear benefits in terms of performance, the models could show discrimination against minority groups and result in fairness issues in a decision-making process, leading to severe negative impacts on the individuals and the society. In recent years, various techniques have been developed to mitigate the unfairness for machine learning models. Among them, in-processing methods have drawn increasing attention from the community, where fairness is directly taken into consideration during model design to induce intrinsically fair models and fundamentally mitigate fairness issues in outputs and representations. In this survey, we review the current progress of in-processing fairness mitigation techniques. Based on where the fairness is achieved in the model, we categorize them into explicit and implicit methods, where the former directly incorporates fairness metrics in training objectives, and the latter focuses on refining latent representation learning. Finally, we conclude the survey with a discussion of the research challenges in this community to motivate future exploration.",
      "cited_by_count": 82,
      "type": "article",
      "source": {
        "name": "ACM Transactions on Knowledge Discovery from Data",
        "type": "journal",
        "issn": [
          "1556-4681",
          "1556-472X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "bronze",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3551390"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 61,
      "url": "https://openalex.org/W4288758404"
    },
    {
      "openalex_id": "W3014011525",
      "doi": "10.1145/3351095.3372831",
      "title": "An empirical study on the perceived fairness of realistic, imperfect machine learning models",
      "authors": [
        {
          "name": "Galen Harrison",
          "openalex_id": "A5029794493",
          "institutions": [
            "University of Chicago"
          ]
        },
        {
          "name": "Julia Hanson",
          "openalex_id": "A5073782498",
          "institutions": [
            "University of Chicago"
          ]
        },
        {
          "name": "Christine Jacinto",
          "openalex_id": "A5073590022",
          "institutions": [
            "University of Chicago"
          ]
        },
        {
          "name": "Julio Ram\u00edrez",
          "openalex_id": "A5108360190",
          "institutions": [
            "University of Chicago"
          ]
        },
        {
          "name": "Blase Ur",
          "openalex_id": "A5071246801",
          "orcid": "https://orcid.org/0000-0001-9365-3155",
          "institutions": [
            "University of Chicago"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-27",
      "abstract": "There are many competing definitions of what statistical properties make a machine learning model fair. Unfortunately, research has shown that some key properties are mutually exclusive. Realistic models are thus necessarily imperfect, choosing one side of a trade-off or the other. To gauge perceptions of the fairness of such realistic, imperfect models, we conducted a between-subjects experiment with 502 Mechanical Turk workers. Each participant compared two models for deciding whether to grant bail to criminal defendants. The first model equalized one potentially desirable model property, with the other property varying across racial groups. The second model did the opposite. We tested pairwise trade-offs between the following four properties: accuracy; false positive rate; outcomes; and the consideration of race. We also varied which racial group the model disadvantaged. We observed a preference among participants for equalizing the false positive rate between groups over equalizing accuracy. Nonetheless, no preferences were overwhelming, and both sides of each trade-off we tested were strongly preferred by a non-trivial fraction of participants. We observed nuanced distinctions between participants considering a model \"unbiased\" and considering it \"fair.\" Furthermore, even when a model within a trade-off pair was seen as fair and unbiased by a majority of participants, we did not observe consensus that a machine learning model was preferable to a human judge. Our results highlight challenges for building machine learning models that are perceived as fair and broadly acceptable in realistic situations.",
      "cited_by_count": 100,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3351095.3372831"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 28,
      "url": "https://openalex.org/W3014011525"
    },
    {
      "openalex_id": "W3080234712",
      "doi": "10.1145/3394486.3406461",
      "title": "Fairness in Machine Learning for Healthcare",
      "authors": [
        {
          "name": "Muhammad Aurangzeb Ahmad",
          "openalex_id": "A5060934705",
          "orcid": "https://orcid.org/0000-0001-7449-5956",
          "institutions": [
            "University of Washington Tacoma"
          ]
        },
        {
          "name": "Arpit Patel",
          "openalex_id": "A5108627316",
          "institutions": [
            "University of Washington"
          ]
        },
        {
          "name": "Carly Eckert",
          "openalex_id": "A5021241037",
          "institutions": [
            "University of Washington"
          ]
        },
        {
          "name": "Vikas Kumar",
          "openalex_id": "A5100694305",
          "orcid": "https://orcid.org/0000-0002-1184-6525",
          "institutions": [
            "Behavioral Tech Research, Inc."
          ]
        },
        {
          "name": "Ankur Teredesai",
          "openalex_id": "A5061359226",
          "orcid": "https://orcid.org/0000-0002-2112-5895",
          "institutions": [
            "University of Washington Tacoma"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-08-20",
      "abstract": "The issue of bias and fairness in healthcare has been around for centuries. With the integration of AI in healthcare the potential to discriminate and perpetuate unfair and biased practices in healthcare increases many folds The tutorial focuses on the challenges, requirements and opportunities in the area of fairness in healthcare AI and the various nuances associated with it. The problem healthcare as a multi-faceted systems level problem that necessitates careful of different notions of fairness in healthcare to corresponding concepts in machine learning is elucidated via different real world examples.",
      "cited_by_count": 65,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Healthcare cost, quality, practices",
        "Health Systems, Economic Evaluations, Quality of Life",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 6,
      "url": "https://openalex.org/W3080234712"
    },
    {
      "openalex_id": "W3166873126",
      "doi": "10.1145/3468264.3468536",
      "title": "Fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline",
      "authors": [
        {
          "name": "Sumon Biswas",
          "openalex_id": "",
          "orcid": "https://orcid.org/0000-0001-7074-1953",
          "institutions": [
            "Iowa State University"
          ]
        },
        {
          "name": "Hridesh Rajan",
          "openalex_id": "",
          "institutions": [
            "Iowa State University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-08-18",
      "abstract": "In recent years, many incidents have been reported where machine learning\\nmodels exhibited discrimination among people based on race, sex, age, etc.\\nResearch has been conducted to measure and mitigate unfairness in machine\\nlearning models. For a machine learning task, it is a common practice to build\\na pipeline that includes an ordered set of data preprocessing stages followed\\nby a classifier. However, most of the research on fairness has considered a\\nsingle classifier based prediction task. What are the fairness impacts of the\\npreprocessing stages in machine learning pipeline? Furthermore, studies showed\\nthat often the root cause of unfairness is ingrained in the data itself, rather\\nthan the model. But no research has been conducted to measure the unfairness\\ncaused by a specific transformation made in the data preprocessing stage. In\\nthis paper, we introduced the causal method of fairness to reason about the\\nfairness impact of data preprocessing stages in ML pipeline. We leveraged\\nexisting metrics to define the fairness measures of the stages. Then we\\nconducted a detailed fairness evaluation of the preprocessing stages in 37\\npipelines collected from three different sources. Our results show that certain\\ndata transformers are causing the model to exhibit unfairness. We identified a\\nnumber of fairness patterns in several categories of data transformers.\\nFinally, we showed how the local fairness of a preprocessing stage composes in\\nthe global fairness of the pipeline. We used the fairness composition to choose\\nappropriate downstream transformer that mitigates unfairness in the machine\\nlearning pipeline.\\n",
      "cited_by_count": 82,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3468264.3468536"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 37,
      "url": "https://openalex.org/W3166873126"
    },
    {
      "openalex_id": "W3047533667",
      "doi": "10.1007/s10287-022-00425-z",
      "title": "Accuracy and fairness trade-offs in machine learning: a stochastic multi-objective approach",
      "authors": [
        {
          "name": "Suyun Liu",
          "openalex_id": "A5063729830",
          "orcid": "https://orcid.org/0000-0003-2285-5184",
          "institutions": [
            "Lehigh University"
          ]
        },
        {
          "name": "L. N. Vicente",
          "openalex_id": "A5045737795",
          "orcid": "https://orcid.org/0000-0003-1097-6384",
          "institutions": [
            "Lehigh University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-04-21",
      "abstract": null,
      "cited_by_count": 67,
      "type": "article",
      "source": {
        "name": "Computational Management Science",
        "type": "journal",
        "issn": [
          "1619-697X",
          "1619-6988"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Machine Learning and Data Classification",
        "Adversarial Robustness in Machine Learning",
        "Stochastic Gradient Optimization Techniques"
      ],
      "referenced_works_count": 42,
      "url": "https://openalex.org/W3047533667"
    },
    {
      "openalex_id": "W4385416124",
      "doi": "10.1038/s42256-023-00697-3",
      "title": "Algorithmic fairness and bias mitigation for clinical machine learning with deep reinforcement learning",
      "authors": [
        {
          "name": "Jenny Yang",
          "openalex_id": "A5030438721",
          "orcid": "https://orcid.org/0000-0003-0352-8452",
          "institutions": [
            "University of Oxford"
          ]
        },
        {
          "name": "Andrew A. S. Soltan",
          "openalex_id": "A5023184985",
          "orcid": "https://orcid.org/0000-0003-2391-5361",
          "institutions": [
            "University of Oxford",
            "John Radcliffe Hospital"
          ]
        },
        {
          "name": "David W. Eyre",
          "openalex_id": "A5064800233",
          "orcid": "https://orcid.org/0000-0001-5095-6367",
          "institutions": [
            "University of Oxford",
            "Open Data Institute"
          ]
        },
        {
          "name": "David A. Clifton",
          "openalex_id": "A5040302008",
          "orcid": "https://orcid.org/0000-0002-9848-8555",
          "institutions": [
            "University of Oxford",
            "Suzhou Research Institute"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-07-31",
      "abstract": "Abstract As models based on machine learning continue to be developed for healthcare applications, greater effort is needed to ensure that these technologies do not reflect or exacerbate any unwanted or discriminatory biases that may be present in the data. Here we introduce a reinforcement learning framework capable of mitigating biases that may have been acquired during data collection. In particular, we evaluated our model for the task of rapidly predicting COVID-19 for patients presenting to hospital emergency departments and aimed to mitigate any site (hospital)-specific and ethnicity-based biases present in the data. Using a specialized reward function and training procedure, we show that our method achieves clinically effective screening performances, while significantly improving outcome fairness compared with current benchmarks and state-of-the-art machine learning methods. We performed external validation across three independent hospitals, and additionally tested our method on a patient intensive care unit discharge status task, demonstrating model generalizability.",
      "cited_by_count": 90,
      "type": "article",
      "source": {
        "name": "Nature Machine Intelligence",
        "type": "journal",
        "issn": [
          "2522-5839"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.nature.com/articles/s42256-023-00697-3.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Artificial Intelligence in Healthcare and Education",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 38,
      "url": "https://openalex.org/W4385416124"
    },
    {
      "openalex_id": "W3165211483",
      "doi": "10.1145/3468507.3468511",
      "title": "On the Applicability of Machine Learning Fairness Notions",
      "authors": [
        {
          "name": "Karima Makhlouf",
          "openalex_id": "A5000858958",
          "orcid": "https://orcid.org/0000-0001-6318-0713",
          "institutions": [
            "Universit\u00e9 du Qu\u00e9bec \u00e0 Montr\u00e9al"
          ]
        },
        {
          "name": "Sami Zhioua",
          "openalex_id": "A5001717771",
          "orcid": "https://orcid.org/0000-0003-2029-175X",
          "institutions": [
            "Higher Colleges of Technology"
          ]
        },
        {
          "name": "Catuscia Palamidessi",
          "openalex_id": "A5090842450",
          "orcid": "https://orcid.org/0000-0003-4597-7002",
          "institutions": [
            "Institut Polytechnique de Paris",
            "Institut national de recherche en informatique et en automatique",
            "\u00c9cole Polytechnique"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-05-26",
      "abstract": "Machine Learning (ML) based predictive systems are increasingly used to support decisions with a critical impact on individuals' lives such as college admission, job hiring, child custody, criminal risk assessment, etc. As a result, fairness emerged as an important requirement to guarantee that ML predictive systems do not discriminate against specific individuals or entire sub-populations, in particular, minorities. Given the inherent subjectivity of viewing the concept of fairness, several notions of fairness have been introduced in the literature. This paper is a survey of fairness notions that, unlike other surveys in the literature, addresses the question of \"which notion of fairness is most suited to a given real-world scenario and why?\". Our attempt to answer this question consists in (1) identifying the set of fairness-related characteristics of the real-world scenario at hand, (2) analyzing the behavior of each fairness notion, and then (3) fitting these two elements to recommend the most suitable fairness notion in every specific setup. The results are summarized in a decision diagram that can be used by practitioners and policy makers to navigate the relatively large catalogue of ML fairness notions.",
      "cited_by_count": 67,
      "type": "article",
      "source": {
        "name": "ACM SIGKDD Explorations Newsletter",
        "type": "journal",
        "issn": [
          "1931-0145",
          "1931-0153"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2006.16745"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 95,
      "url": "https://openalex.org/W3165211483"
    },
    {
      "openalex_id": "W3157518401",
      "doi": "10.1136/bmjhci-2020-100289",
      "title": "Equity in essence: a call for operationalising fairness in machine learning for healthcare",
      "authors": [
        {
          "name": "Judy Wawira Gichoya",
          "openalex_id": "A5076075666",
          "orcid": "https://orcid.org/0000-0002-1097-316X",
          "institutions": [
            "Fogarty International Center",
            "Emory University",
            "National Institutes of Health"
          ]
        },
        {
          "name": "Liam G. McCoy",
          "openalex_id": "A5082108649",
          "orcid": "https://orcid.org/0000-0002-4468-2256",
          "institutions": [
            "University of Toronto"
          ]
        },
        {
          "name": "Leo Anthony Celi",
          "openalex_id": "A5031401755",
          "orcid": "https://orcid.org/0000-0001-6712-6626",
          "institutions": [
            "Harvard\u2013MIT Division of Health Sciences and Technology",
            "Beth Israel Deaconess Medical Center",
            "Harvard University"
          ]
        },
        {
          "name": "Marzyeh Ghassemi",
          "openalex_id": "A5070063054",
          "orcid": "https://orcid.org/0000-0001-6349-7251",
          "institutions": [
            "University of Toronto",
            "Vector Institute"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-04-01",
      "abstract": "Machine learning for healthcare (MLHC) is at the juncture of leaping from the pages of journals and conference proceedings to clinical implementation at the bedside. Succeeding in this endeavour requires the synthesis of insights from both the machine learning and healthcare domains, in order to",
      "cited_by_count": 102,
      "type": "article",
      "source": {
        "name": "BMJ Health & Care Informatics",
        "type": "journal",
        "issn": [
          "2632-1009"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://informatics.bmj.com/content/bmjhci/28/1/e100289.full.pdf"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Healthcare cost, quality, practices",
        "Ethics in Clinical Research"
      ],
      "referenced_works_count": 24,
      "url": "https://openalex.org/W3157518401"
    },
    {
      "openalex_id": "W4388971801",
      "doi": "10.1007/s10664-023-10402-y",
      "title": "Fairness-aware machine learning engineering: how far are we?",
      "authors": [
        {
          "name": "Carmine Ferrara",
          "openalex_id": "A5108460232",
          "institutions": [
            "University of Salerno"
          ]
        },
        {
          "name": "Giulia Sellitto",
          "openalex_id": "A5052880185",
          "orcid": "https://orcid.org/0000-0002-5491-0873",
          "institutions": [
            "University of Salerno"
          ]
        },
        {
          "name": "Filomena Ferrucci",
          "openalex_id": "A5053084752",
          "orcid": "https://orcid.org/0000-0002-0975-8972",
          "institutions": [
            "University of Salerno"
          ]
        },
        {
          "name": "Fabio Palomba",
          "openalex_id": "A5033738898",
          "orcid": "https://orcid.org/0000-0001-9337-5116",
          "institutions": [
            "University of Salerno"
          ]
        },
        {
          "name": "Andrea De Lucia",
          "openalex_id": "A5079088548",
          "orcid": "https://orcid.org/0000-0002-4238-1425",
          "institutions": [
            "University of Salerno"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-11-24",
      "abstract": "Abstract Machine learning is part of the daily life of people and companies worldwide. Unfortunately, bias in machine learning algorithms risks unfairly influencing the decision-making process and reiterating possible discrimination. While the interest of the software engineering community in software fairness is rapidly increasing, there is still a lack of understanding of various aspects connected to fair machine learning engineering, i.e., the software engineering process involved in developing fairness-critical machine learning systems. Questions connected to the practitioners\u2019 awareness and maturity about fairness, the skills required to deal with the matter, and the best development phase(s) where fairness should be faced more are just some examples of the knowledge gaps currently open. In this paper, we provide insights into how fairness is perceived and managed in practice, to shed light on the instruments and approaches that practitioners might employ to properly handle fairness. We conducted a survey with 117 professionals who shared their knowledge and experience highlighting the relevance of fairness in practice, and the skills and tools required to handle it. The key results of our study show that fairness is still considered a second-class quality aspect in the development of artificial intelligence systems. The building of specific methods and development environments, other than automated validation tools, might help developers to treat fairness throughout the software lifecycle and revert this trend.",
      "cited_by_count": 44,
      "type": "article",
      "source": {
        "name": "Empirical Software Engineering",
        "type": "journal",
        "issn": [
          "1382-3256",
          "1573-7616"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s10664-023-10402-y.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 73,
      "url": "https://openalex.org/W4388971801"
    },
    {
      "openalex_id": "W3023119642",
      "doi": "10.1007/s10940-022-09545-w",
      "title": "In Pursuit of Interpretable, Fair and Accurate Machine Learning for Criminal Recidivism Prediction",
      "authors": [
        {
          "name": "Caroline Wang",
          "openalex_id": "A5102356031",
          "institutions": [
            "The University of Texas at Austin"
          ]
        },
        {
          "name": "Bin Han",
          "openalex_id": "A5102741786",
          "orcid": "https://orcid.org/0000-0002-5280-9456",
          "institutions": [
            "University of Washington"
          ]
        },
        {
          "name": "Bhrij Patel",
          "openalex_id": "A5004951324",
          "institutions": [
            "Duke University"
          ]
        },
        {
          "name": "Cynthia Rudin",
          "openalex_id": "A5040468715",
          "orcid": "https://orcid.org/0000-0003-4283-2780",
          "institutions": [
            "Statistical and Applied Mathematical Sciences Institute",
            "Duke University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-03-28",
      "abstract": null,
      "cited_by_count": 84,
      "type": "article",
      "source": {
        "name": "Journal of Quantitative Criminology",
        "type": "journal",
        "issn": [
          "0748-4518",
          "1573-7799"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Crime Patterns and Interventions",
        "Criminal Justice and Corrections Analysis",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 60,
      "url": "https://openalex.org/W3023119642"
    },
    {
      "openalex_id": "W3135955057",
      "doi": "10.1109/tpds.2021.3068195",
      "title": "VeriML: Enabling Integrity Assurances and Fair Payments for Machine Learning as a Service",
      "authors": [
        {
          "name": "Lingchen Zhao",
          "openalex_id": "A5030015523",
          "orcid": "https://orcid.org/0000-0002-1700-3836",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Qian Wang",
          "openalex_id": "A5100391116",
          "orcid": "https://orcid.org/0000-0002-8967-8525",
          "institutions": [
            "Wuhan University"
          ]
        },
        {
          "name": "Cong Wang",
          "openalex_id": "A5100390514",
          "orcid": "https://orcid.org/0000-0003-0547-315X",
          "institutions": [
            "City University of Hong Kong"
          ]
        },
        {
          "name": "Qi Li",
          "openalex_id": "A5100350165",
          "orcid": "https://orcid.org/0000-0001-8776-8730",
          "institutions": [
            "Tsinghua University"
          ]
        },
        {
          "name": "Chao Shen",
          "openalex_id": "A5101843177",
          "orcid": "https://orcid.org/0000-0003-2783-5529",
          "institutions": [
            "Computer Network Information Center",
            "Xi'an Jiaotong University"
          ]
        },
        {
          "name": "Bo Feng",
          "openalex_id": "A5110736334",
          "institutions": [
            "Northeastern University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-03-23",
      "abstract": "Machine Learning as a Service (MLaaS) allows clients with limited resources to outsource their expensive ML tasks to powerful servers. Despite the huge benefits, current MLaaS solutions still lack strong assurances on: 1) service correctness (i.e., whether the MLaaS works as expected); 2) trustworthy accounting (i.e., whether the bill for the MLaaS resource consumption is correctly accounted); 3) fair payment (i.e., whether a client gets the entire MLaaS result before making the payment). Without these assurances, unfaithful service providers can return improperly-executed ML task results or partially-trained ML models while asking for over-claimed rewards. Moreover, it is hard to argue for wide adoption of MLaaS to both the client and the service provider, especially in the open market without a trusted third party. In this article, we present VeriML, a novel and efficient framework to bring integrity assurances and fair payments to MLaaS. With VeriML, clients can be assured that ML tasks are correctly executed on an untrusted server, and the resource consumption claimed by the service provider equals to the actual workload. We strategically use succinct non-interactive arguments of knowledge (SNARK) on randomly-selected iterations during the ML training phase for efficiency with tunable probabilistic assurance. We also develop multiple ML-specific optimizations to the arithmetic circuit required by SNARK. Our system implements six common algorithms: linear regression, logistic regression, neural network, support vector machine, K-means and decision tree. The experimental results have validated the practical performance of VeriML.",
      "cited_by_count": 80,
      "type": "article",
      "source": {
        "name": "IEEE Transactions on Parallel and Distributed Systems",
        "type": "journal",
        "issn": [
          "1045-9219",
          "1558-2183",
          "2161-9883"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Privacy-Preserving Technologies in Data",
        "Cryptography and Data Security",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 63,
      "url": "https://openalex.org/W3135955057"
    },
    {
      "openalex_id": "W3206100932",
      "doi": "10.48550/arxiv.2012.14406",
      "title": "dalex: Responsible Machine Learning with Interactive Explainability and\\n Fairness in Python",
      "authors": [
        {
          "name": "Hubert Baniecki",
          "openalex_id": "A5059946484",
          "orcid": "https://orcid.org/0000-0001-6661-5364"
        },
        {
          "name": "Wojciech Kretowicz",
          "openalex_id": "A5036053672",
          "orcid": "https://orcid.org/0000-0002-7333-8112"
        },
        {
          "name": "Piotr Pi\u0105tyszek",
          "openalex_id": "A5048777619"
        },
        {
          "name": "Jakub Bo\u017cydar Wi\u015bniewski",
          "openalex_id": "A5034174468",
          "orcid": "https://orcid.org/0000-0003-4126-1867"
        },
        {
          "name": "Przemys\u0142aw Biecek",
          "openalex_id": "A5049061860",
          "orcid": "https://orcid.org/0000-0001-8423-1823"
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-12-28",
      "abstract": "The increasing amount of available data, computing power, and the constant\\npursuit for higher performance results in the growing complexity of predictive\\nmodels. Their black-box nature leads to opaqueness debt phenomenon inflicting\\nincreased risks of discrimination, lack of reproducibility, and deflated\\nperformance due to data drift. To manage these risks, good MLOps practices ask\\nfor better validation of model performance and fairness, higher explainability,\\nand continuous monitoring. The necessity of deeper model transparency appears\\nnot only from scientific and social domains, but also emerging laws and\\nregulations on artificial intelligence. To facilitate the development of\\nresponsible machine learning models, we showcase dalex, a Python package which\\nimplements the model-agnostic interface for interactive model exploration. It\\nadopts the design crafted through the development of various tools for\\nresponsible machine learning; thus, it aims at the unification of the existing\\nsolutions. This library's source code and documentation are available under\\nopen license at https://python.drwhy.ai/.\\n",
      "cited_by_count": 73,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2012.14406"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Machine Learning and Data Classification",
        "Computational Physics and Python Applications"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W3206100932"
    },
    {
      "openalex_id": "W3032152562",
      "doi": "10.1145/3368089.3409704",
      "title": "Do the machine learning models on a crowd sourced platform exhibit bias? an empirical study on model fairness",
      "authors": [
        {
          "name": "Sumon Biswas",
          "openalex_id": "A5090690054",
          "orcid": "https://orcid.org/0000-0001-7074-1953",
          "institutions": [
            "Iowa State University"
          ]
        },
        {
          "name": "Hridesh Rajan",
          "openalex_id": "A5059626072",
          "orcid": "https://orcid.org/0000-0002-9410-9562",
          "institutions": [
            "Iowa State University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-11-08",
      "abstract": "Machine learning models are increasingly being used in important\\ndecision-making software such as approving bank loans, recommending criminal\\nsentencing, hiring employees, and so on. It is important to ensure the fairness\\nof these models so that no discrimination is made based on protected attribute\\n(e.g., race, sex, age) while decision making. Algorithms have been developed to\\nmeasure unfairness and mitigate them to a certain extent. In this paper, we\\nhave focused on the empirical evaluation of fairness and mitigations on\\nreal-world machine learning models. We have created a benchmark of 40 top-rated\\nmodels from Kaggle used for 5 different tasks, and then using a comprehensive\\nset of fairness metrics, evaluated their fairness. Then, we have applied 7\\nmitigation techniques on these models and analyzed the fairness, mitigation\\nresults, and impacts on performance. We have found that some model optimization\\ntechniques result in inducing unfairness in the models. On the other hand,\\nalthough there are some fairness control mechanisms in machine learning\\nlibraries, they are not documented. The mitigation algorithm also exhibit\\ncommon patterns such as mitigation in the post-processing is often costly (in\\nterms of performance) and mitigation in the pre-processing stage is preferred\\nin most cases. We have also presented different trade-off choices of fairness\\nmitigation decisions. Our study suggests future research directions to reduce\\nthe gap between theoretical fairness aware algorithms and the software\\nengineering methods to leverage them in practice.\\n",
      "cited_by_count": 83,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3368089.3409704"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Privacy-Preserving Technologies in Data",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 49,
      "url": "https://openalex.org/W3032152562"
    },
    {
      "openalex_id": "W3036745657",
      "doi": "10.1002/int.22354",
      "title": "<i>How fair can we go in machine learning</i> ? Assessing the boundaries of accuracy and fairness",
      "authors": [
        {
          "name": "Ana Valdivia",
          "openalex_id": "",
          "institutions": [
            "King's College London"
          ]
        },
        {
          "name": "Javier S\u00e1nchez\u2010Monedero",
          "openalex_id": "",
          "institutions": [
            "Cardiff University"
          ]
        },
        {
          "name": "Jorge Casillas",
          "openalex_id": "",
          "institutions": [
            "Universidad de Granada"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-01-01",
      "abstract": "Fair machine learning works have been focusing on the development of\\nequitable algorithms that address discrimination of certain groups. Yet, many\\nof these fairness-aware approaches aim to obtain a unique solution to the\\nproblem, which leads to a poor understanding of the statistical limits of bias\\nmitigation interventions. We present the first methodology that allows to\\nexplore those limits within a multi-objective framework that seeks to optimize\\nany measure of accuracy and fairness and provides a Pareto front with the best\\nfeasible solutions. In this work, we focus our study on decision tree\\nclassifiers since they are widely accepted in machine learning, are easy to\\ninterpret and can deal with non-numerical information naturally. We conclude\\nexperimentally that our method can optimize decision tree models by being\\nfairer with a small cost of the classification error. We believe that our\\ncontribution will help stakeholders of sociotechnical systems to assess how far\\nthey can go being fair and accurate, thus serving in the support of enhanced\\ndecision making where machine learning is used.\\n",
      "cited_by_count": 43,
      "type": "article",
      "source": {
        "name": "International Journal of Intelligent Systems",
        "type": "journal",
        "issn": [
          "0884-8173",
          "1098-111X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2006.12399"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Qualitative Comparative Analysis Research"
      ],
      "referenced_works_count": 19,
      "url": "https://openalex.org/W3036745657"
    },
    {
      "openalex_id": "W3206637938",
      "doi": "10.1038/s42256-021-00396-x",
      "title": "Empirical observation of negligible fairness-accuracy trade-offs in\\n machine learning for public policy",
      "authors": [
        {
          "name": "Kit T. Rodolfa",
          "openalex_id": "A5026014943",
          "orcid": "https://orcid.org/0000-0002-0829-1282",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Hemank Lamba",
          "openalex_id": "A5028407723",
          "orcid": "https://orcid.org/0000-0002-9794-3587",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Rayid Ghani",
          "openalex_id": "A5081839926",
          "orcid": "https://orcid.org/0000-0003-0235-1843",
          "institutions": [
            "Carnegie Mellon University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-12-05",
      "abstract": "Growing use of machine learning in policy and social impact settings have\\nraised concerns for fairness implications, especially for racial minorities.\\nThese concerns have generated considerable interest among machine learning and\\nartificial intelligence researchers, who have developed new methods and\\nestablished theoretical bounds for improving fairness, focusing on the source\\ndata, regularization and model training, or post-hoc adjustments to model\\nscores. However, little work has studied the practical trade-offs between\\nfairness and accuracy in real-world settings to understand how these bounds and\\nmethods translate into policy choices and impact on society. Our empirical\\nstudy fills this gap by investigating the impact of mitigating disparities on\\naccuracy, focusing on the common context of using machine learning to inform\\nbenefit allocation in resource-constrained programs across education, mental\\nhealth, criminal justice, and housing safety. Here we describe applied work in\\nwhich we find fairness-accuracy trade-offs to be negligible in practice. In\\neach setting studied, explicitly focusing on achieving equity and using our\\nproposed post-hoc disparity mitigation methods, fairness was substantially\\nimproved without sacrificing accuracy. This observation was robust across\\npolicy contexts studied, scale of resources available for intervention, time,\\nand relative size of the protected groups. These empirical results challenge a\\ncommonly held assumption that reducing disparities either requires accepting an\\nappreciable drop in accuracy or the development of novel, complex methods,\\nmaking reducing disparities in these applications more practical.\\n",
      "cited_by_count": 79,
      "type": "article",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2012.02972"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Health Systems, Economic Evaluations, Quality of Life"
      ],
      "referenced_works_count": 37,
      "url": "https://openalex.org/W3206637938"
    },
    {
      "openalex_id": "W3181882572",
      "doi": "10.1145/3404835.3462814",
      "title": "Tutorial on Fairness of Machine Learning in Recommender Systems",
      "authors": [
        {
          "name": "Yunqi Li",
          "openalex_id": "A5101668890",
          "orcid": "https://orcid.org/0000-0002-9482-9922",
          "institutions": [
            "Rutgers, The State University of New Jersey"
          ]
        },
        {
          "name": "Yingqiang Ge",
          "openalex_id": "A5020526977",
          "orcid": "https://orcid.org/0000-0002-3736-2377",
          "institutions": [
            "Rutgers, The State University of New Jersey"
          ]
        },
        {
          "name": "Yongfeng Zhang",
          "openalex_id": "A5100329828",
          "orcid": "https://orcid.org/0000-0003-2633-8555",
          "institutions": [
            "Rutgers, The State University of New Jersey"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-07-11",
      "abstract": "Recently, there has been growing attention on fairness considerations in machine learning. As one of the most pervasive applications of machine learning, recommender systems are gaining increasing and critical impacts on human and society since a growing number of users use them for information seeking and decision making. Therefore, it is crucial to address the potential unfairness problems in recommendation, which may hurt users' or providers' satisfaction in recommender systems as well as the interests of the platforms. The tutorial focuses on the foundations and algorithms for fairness in recommendation. It also presents a brief introduction about fairness in basic machine learning tasks such as classification and ranking. The tutorial will introduce the taxonomies of current fairness definitions and evaluation metrics for fairness concerns. We will introduce previous works about fairness in recommendation and also put forward future fairness research directions. The tutorial aims at introducing and communicating fairness in recommendation methods to the community, as well as gathering researchers and practitioners interested in this research direction for discussions, idea communications, and research promotions.",
      "cited_by_count": 43,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Ethics and Social Impacts of AI",
        "Advanced Bandit Algorithms Research"
      ],
      "referenced_works_count": 77,
      "url": "https://openalex.org/W3181882572"
    },
    {
      "openalex_id": "W3136824354",
      "doi": "10.1002/int.22415",
      "title": "Missing the missing values: The ugly duckling of fairness in machine learning",
      "authors": [
        {
          "name": "Fernando Mart\u00ednez\u2010Plumed",
          "openalex_id": "A5051725651",
          "orcid": "https://orcid.org/0000-0003-2902-6477",
          "institutions": [
            "Universitat Polit\u00e8cnica de Val\u00e8ncia",
            "Joint Research Center",
            "Artificial Intelligence Research Institute"
          ]
        },
        {
          "name": "C\u00e8sar Ferri",
          "openalex_id": "A5050804466",
          "orcid": "https://orcid.org/0000-0002-8975-1120",
          "institutions": [
            "Artificial Intelligence Research Institute",
            "Universitat Polit\u00e8cnica de Val\u00e8ncia"
          ]
        },
        {
          "name": "David Nieves",
          "openalex_id": "A5103187858",
          "institutions": [
            "Universitat Polit\u00e8cnica de Val\u00e8ncia",
            "Artificial Intelligence Research Institute"
          ]
        },
        {
          "name": "Jos\u00e9 Hern\u00e1ndez\u2010Orallo",
          "openalex_id": "A5029864546",
          "orcid": "https://orcid.org/0000-0001-9746-7632",
          "institutions": [
            "Leverhulme Trust",
            "Universitat Polit\u00e8cnica de Val\u00e8ncia",
            "Artificial Intelligence Research Institute",
            "University of Cambridge"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-03-22",
      "abstract": "Nowadays, there is an increasing concern in machine learning about the causes underlying unfair decision making, that is, algorithmic decisions discriminating some groups over others, especially with groups that are defined over protected attributes, such as gender, race and nationality. Missing values are one frequent manifestation of all these latent causes: protected groups are more reluctant to give information that could be used against them, sensitive information for some groups can be erased by human operators, or data acquisition may simply be less complete and systematic for minority groups. However, most recent techniques, libraries and experimental results dealing with fairness in machine learning have simply ignored missing data. In this paper, we present the first comprehensive analysis of the relation between missing values and algorithmic fairness for machine learning: (1) we analyse the sources of missing data and bias, mapping the common causes, (2) we find that rows containing missing values are usually fairer than the rest, which should discourage the consideration of missing values as the uncomfortable ugly data that different techniques and libraries for handling algorithmic bias get rid of at the first occasion, (3) we study the trade\u2010off between performance and fairness when the rows with missing values are used (either because the technique deals with them directly or by imputation methods), and (4) we show that the sensitivity of six different machine\u2010learning techniques to missing values is usually low, which reinforces the view that the rows with missing data contribute more to fairness through the other, nonmissing, attributes. We end the paper with a series of recommended procedures about what to do with missing data when aiming for fair decision making.",
      "cited_by_count": 52,
      "type": "article",
      "source": {
        "name": "International Journal of Intelligent Systems",
        "type": "journal",
        "issn": [
          "0884-8173",
          "1098-111X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/int.22415"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 78,
      "url": "https://openalex.org/W3136824354"
    },
    {
      "openalex_id": "W3160735296",
      "doi": "10.1145/3411764.3445365",
      "title": "Effect of Information Presentation on Fairness Perceptions of Machine Learning Predictors",
      "authors": [
        {
          "name": "Niels van Berkel",
          "openalex_id": "A5003896144",
          "orcid": "https://orcid.org/0000-0001-5106-7692",
          "institutions": [
            "Aalborg University"
          ]
        },
        {
          "name": "Jorge Gon\u00e7alves",
          "openalex_id": "A5023644163",
          "orcid": "https://orcid.org/0000-0002-0117-0322",
          "institutions": [
            "University of Melbourne"
          ]
        },
        {
          "name": "Daniel Russo",
          "openalex_id": "A5051068084",
          "orcid": "https://orcid.org/0000-0001-7253-101X",
          "institutions": [
            "Aalborg University"
          ]
        },
        {
          "name": "Simo Hosio",
          "openalex_id": "A5067689431",
          "orcid": "https://orcid.org/0000-0002-9609-0965",
          "institutions": [
            "University of Oulu"
          ]
        },
        {
          "name": "Mikael B. Skov",
          "openalex_id": "A5020573877",
          "orcid": "https://orcid.org/0000-0003-4488-5683",
          "institutions": [
            "Aalborg University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-05-06",
      "abstract": "The uptake of artificial intelligence-based applications raises concerns about the fairness and transparency of AI behaviour. Consequently, the Computer Science community calls for the involvement of the general public in the design and evaluation of AI systems. Assessing the fairness of individual predictors is an essential step in the development of equitable algorithms. In this study, we evaluate the effect of two common visualisation techniques (text-based and scatterplot) and the display of the outcome information (i.e., ground-truth) on the perceived fairness of predictors. Our results from an online crowdsourcing study (N = 80) show that the chosen visualisation technique significantly alters people\u2019s fairness perception and that the presented scenario, as well as the participant\u2019s gender and past education, influence perceived fairness. Based on these results we draw recommendations for future work that seeks to involve non-experts in AI fairness evaluations.",
      "cited_by_count": 80,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "http://hdl.handle.net/11343/281580"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Innovation, Sustainability, Human-Machine Systems",
        "Behavioral Health and Interventions"
      ],
      "referenced_works_count": 64,
      "url": "https://openalex.org/W3160735296"
    },
    {
      "openalex_id": "W3205037653",
      "doi": "10.1007/s10551-021-04939-5",
      "title": "Do the Ends Justify the Means? Variation in the Distributive and Procedural Fairness of Machine Learning Algorithms",
      "authors": [
        {
          "name": "Lily Morse",
          "openalex_id": "A5003882025",
          "orcid": "https://orcid.org/0000-0003-3154-1638",
          "institutions": [
            "West Virginia University"
          ]
        },
        {
          "name": "Mike Horia Teodorescu",
          "openalex_id": "A5010554523",
          "orcid": "https://orcid.org/0000-0001-7330-832X",
          "institutions": [
            "Massachusetts Institute of Technology",
            "Boston College"
          ]
        },
        {
          "name": "Yazeed Awwad",
          "openalex_id": "A5048176035",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Gerald C. Kane",
          "openalex_id": "A5069198621",
          "orcid": "https://orcid.org/0000-0001-5239-1902",
          "institutions": [
            "Boston College"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-10-18",
      "abstract": null,
      "cited_by_count": 65,
      "type": "article",
      "source": {
        "name": "Journal of Business Ethics",
        "type": "journal",
        "issn": [
          "0167-4544",
          "1573-0697"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Psychology of Moral and Emotional Judgment",
        "Qualitative Comparative Analysis Research"
      ],
      "referenced_works_count": 91,
      "url": "https://openalex.org/W3205037653"
    },
    {
      "openalex_id": "W3202598875",
      "doi": "10.1002/widm.1452",
      "title": "A survey on datasets for fairness\u2010aware machine learning",
      "authors": [
        {
          "name": "Tai Le Quy",
          "openalex_id": "A5040114911",
          "orcid": "https://orcid.org/0000-0001-8512-5854",
          "institutions": [
            "L3S Research Center"
          ]
        },
        {
          "name": "Arjun Roy",
          "openalex_id": "A5078516991",
          "orcid": "https://orcid.org/0000-0002-4279-9442",
          "institutions": [
            "Freie Universit\u00e4t Berlin",
            "L3S Research Center"
          ]
        },
        {
          "name": "Vasileios Iosifidis",
          "openalex_id": "A5012928144",
          "orcid": "https://orcid.org/0000-0002-3005-4507",
          "institutions": [
            "L3S Research Center"
          ]
        },
        {
          "name": "Eirini Ntoutsi",
          "openalex_id": "A5089247128",
          "orcid": "https://orcid.org/0000-0001-5729-1003",
          "institutions": [
            "Freie Universit\u00e4t Berlin",
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Eirini Ntoutsi",
          "openalex_id": "",
          "orcid": "https://orcid.org/0000-0001-5729-1003",
          "institutions": [
            "Freie Universit\u00e4t Berlin"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-03-03",
      "abstract": "Abstract As decision\u2010making increasingly relies on machine learning (ML) and (big) data, the issue of fairness in data\u2010driven artificial intelligence systems is receiving increasing attention from both research and industry. A large variety of fairness\u2010aware ML solutions have been proposed which involve fairness\u2010related interventions in the data, learning algorithms, and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real\u2010world datasets used for fairness\u2010aware ML. We focus on tabular data as the most common data representation for fairness\u2010aware ML. We start our analysis by identifying relationships between the different attributes, particularly with respect to protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate interesting relationships using exploratory analysis. This article is categorized under: Commercial, Legal, and Ethical Issues &gt; Fairness in Data Mining Fundamental Concepts of Data and Knowledge &gt; Data Concepts Technologies &gt; Data Preprocessing",
      "cited_by_count": 40,
      "type": "preprint",
      "source": {
        "name": "Wiley Interdisciplinary Reviews Data Mining and Knowledge Discovery",
        "type": "journal",
        "issn": [
          "1942-4787",
          "1942-4795"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/widm.1452"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Privacy-Preserving Technologies in Data"
      ],
      "referenced_works_count": 141,
      "url": "https://openalex.org/W3202598875"
    },
    {
      "openalex_id": "W4391400132",
      "doi": "10.1007/s13042-023-02083-2",
      "title": "Fairness issues, current approaches, and challenges in machine learning models",
      "authors": [
        {
          "name": "Tonni Das Jui",
          "openalex_id": "A5081313660",
          "institutions": [
            "Baylor University"
          ]
        },
        {
          "name": "Pablo Rivas",
          "openalex_id": "A5045360354",
          "orcid": "https://orcid.org/0000-0002-8690-0987",
          "institutions": [
            "Baylor University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-01-31",
      "abstract": "Abstract With the increasing influence of machine learning algorithms in decision-making processes, concerns about fairness have gained significant attention. This area now offers significant literature that is complex and hard to penetrate for newcomers to the domain. Thus, a mapping study of articles exploring fairness issues is a valuable tool to provide a general introduction to this field. Our paper presents a systematic approach for exploring existing literature by aligning their discoveries with predetermined inquiries and a comprehensive overview of diverse bias dimensions, encompassing training data bias, model bias, conflicting fairness concepts, and the absence of prediction transparency, as observed across several influential articles. To establish connections between fairness issues and various issue mitigation approaches, we propose a taxonomy of machine learning fairness issues and map the diverse range of approaches scholars developed to address issues. We briefly explain the responsible critical factors behind these issues in a graphical view with a discussion and also highlight the limitations of each approach analyzed in the reviewed articles. Our study leads to a discussion regarding the potential future direction in ML and AI fairness.",
      "cited_by_count": 45,
      "type": "article",
      "source": {
        "name": "International Journal of Machine Learning and Cybernetics",
        "type": "journal",
        "issn": [
          "1868-8071",
          "1868-808X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s13042-023-02083-2.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 129,
      "url": "https://openalex.org/W4391400132"
    },
    {
      "openalex_id": "W4220659214",
      "doi": "10.1145/3510003.3510202",
      "title": "Fairness-aware configuration of machine learning libraries",
      "authors": [
        {
          "name": "Saeid Tizpaz-Niari",
          "openalex_id": "A5076300752",
          "orcid": "https://orcid.org/0000-0002-1375-3154",
          "institutions": [
            "The University of Texas at El Paso"
          ]
        },
        {
          "name": "Ashish Kumar",
          "openalex_id": "A5101937116",
          "orcid": "https://orcid.org/0000-0001-8773-2084",
          "institutions": [
            "Pennsylvania State University"
          ]
        },
        {
          "name": "Gang Tan",
          "openalex_id": "A5010830558",
          "orcid": "https://orcid.org/0000-0001-6109-6091",
          "institutions": [
            "Pennsylvania State University"
          ]
        },
        {
          "name": "Ashutosh Trivedi",
          "openalex_id": "A5081839764",
          "orcid": "https://orcid.org/0000-0002-7773-4489",
          "institutions": [
            "University of Colorado System",
            "University of Colorado Boulder"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-05-21",
      "abstract": "This paper investigates the parameter space of machine learning (ML) algorithms in aggravating or mitigating fairness bugs. Data-driven software is increasingly applied in social-critical applications where ensuring fairness is of paramount importance. The existing approaches focus on addressing fairness bugs by either modifying the input dataset or modifying the learning algorithms. On the other hand, the selection of hyperparameters, which provide finer controls of ML algorithms, may enable a less intrusive approach to influence the fairness. Can hyperparameters amplify or suppress discrimination present in the input dataset? How can we help programmers in detecting, understanding, and exploiting the role of hyperparameters to improve the fairness? We design three search-based software testing algorithms to uncover the precision-fairness frontier of the hyperparameter space. We complement these algorithms with statistical debugging to explain the role of these parameters in improving fairness. We implement the proposed approaches in the tool Parfait-ML (PARameter FAIrness Testing for ML Libraries) and show its effectiveness and utility over five mature ML algorithms as used in six social-critical applications. In these applications, our approach successfully identified hyperparameters that significantly improve (vis-a-vis the state-of-the-art techniques) the fairness without sacrificing precision. Surprisingly, for some algorithms (e.g., random forest), our approach showed that certain configuration of hyperparameters (e.g., restricting the search space of attributes) can amplify biases across applications. Upon further investigation, we found intuitive explanations of these phenomena, and the results corroborate similar observations from the literature.",
      "cited_by_count": 42,
      "type": "article",
      "source": {
        "name": "Proceedings of the 44th International Conference on Software Engineering",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2202.06196"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 21,
      "url": "https://openalex.org/W4220659214"
    },
    {
      "openalex_id": "W3031923829",
      "doi": "10.1145/3313831.3376447",
      "title": "Silva: Interactively Assessing Machine Learning Fairness Using Causality",
      "authors": [
        {
          "name": "Jing Nathan Yan",
          "openalex_id": "A5113817523",
          "orcid": "https://orcid.org/0009-0004-0157-5065",
          "institutions": [
            "Cornell University"
          ]
        },
        {
          "name": "Ziwei Gu",
          "openalex_id": "A5050920147",
          "orcid": "https://orcid.org/0000-0001-9044-2651",
          "institutions": [
            "Cornell University"
          ]
        },
        {
          "name": "Hubert Lin",
          "openalex_id": "A5102046369",
          "institutions": [
            "Cornell University"
          ]
        },
        {
          "name": "Jeffrey M. Rzeszotarski",
          "openalex_id": "A5024501566",
          "orcid": "https://orcid.org/0000-0002-4317-9501",
          "institutions": [
            "Cornell University"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-04-21",
      "abstract": "Machine learning models risk encoding unfairness on the part of their developers or data sources. However, assessing fairness is challenging as analysts might misidentify sources of bias, fail to notice them, or misapply metrics. In this paper we introduce Silva, a system for exploring potential sources of unfairness in datasets or machine learning models interactively. Silva directs user attention to relationships between attributes through a global causal view, provides interactive recommendations, presents intermediate results, and visualizes metrics. We describe the implementation of Silva, identify salient design and technical challenges, and provide an evaluation of the tool in comparison to an existing fairness optimization tool.",
      "cited_by_count": 49,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 61,
      "url": "https://openalex.org/W3031923829"
    },
    {
      "openalex_id": "W4223643457",
      "doi": "10.1111/insr.12492",
      "title": "Bias, Fairness and Accountability with Artificial Intelligence and Machine Learning Algorithms",
      "authors": [
        {
          "name": "Nengfeng Zhou",
          "openalex_id": "A5066199859",
          "institutions": [
            "Wells Fargo (United States)"
          ]
        },
        {
          "name": "Zach Zhang",
          "openalex_id": "A5066243297",
          "institutions": [
            "Wells Fargo (United States)"
          ]
        },
        {
          "name": "Vijayan N. Nair",
          "openalex_id": "A5078363534",
          "orcid": "https://orcid.org/0000-0002-6511-7432",
          "institutions": [
            "Wells Fargo (United States)"
          ]
        },
        {
          "name": "Harsh Singhal",
          "openalex_id": "A5103600478",
          "institutions": [
            "Wells Fargo (United States)"
          ]
        },
        {
          "name": "Jie Chen",
          "openalex_id": "A5100333020",
          "orcid": "https://orcid.org/0000-0003-4599-3600",
          "institutions": [
            "Wells Fargo (United States)"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-04-10",
      "abstract": "Summary The advent of artificial intelligence (AI) and machine learning algorithms has led to opportunities as well as challenges in their use. In this overview paper, we begin with a discussion of bias and fairness issues that arise with the use of AI techniques, with a focus on supervised machine learning algorithms. We then describe the types and sources of data bias and discuss the nature of algorithmic unfairness. In addition, we provide a review of fairness metrics in the literature, discuss their limitations, and describe de\u2010biasing (or mitigation) techniques in the model life cycle.",
      "cited_by_count": 47,
      "type": "article",
      "source": {
        "name": "International Statistical Review",
        "type": "journal",
        "issn": [
          "0306-7734",
          "1751-5823"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Artificial Intelligence in Healthcare and Education",
        "Neuroethics, Human Enhancement, Biomedical Innovations"
      ],
      "referenced_works_count": 32,
      "url": "https://openalex.org/W4223643457"
    },
    {
      "openalex_id": "W4280609065",
      "doi": "10.1145/3531146.3533113",
      "title": "Exploring How Machine Learning Practitioners (Try To) Use Fairness Toolkits",
      "authors": [
        {
          "name": "Wesley Hanwen Deng",
          "openalex_id": "A5067411025",
          "orcid": "https://orcid.org/0000-0003-3375-5285",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Manish Nagireddy",
          "openalex_id": "A5106728623",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Michelle Seng Ah Lee",
          "openalex_id": "A5035772953",
          "orcid": "https://orcid.org/0000-0001-7725-2503",
          "institutions": [
            "University of Cambridge"
          ]
        },
        {
          "name": "Jatinder Singh",
          "openalex_id": "A5082866143",
          "orcid": "https://orcid.org/0000-0002-5102-6564",
          "institutions": [
            "University of Cambridge"
          ]
        },
        {
          "name": "Zhiwei Steven Wu",
          "openalex_id": "A5001070941",
          "orcid": "https://orcid.org/0000-0002-8125-8227",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Kenneth Holstein",
          "openalex_id": "A5022664382",
          "orcid": "https://orcid.org/0000-0001-6730-922X",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Haiyi Zhu",
          "openalex_id": "A5051842323",
          "orcid": "https://orcid.org/0000-0001-7271-9100",
          "institutions": [
            "Carnegie Mellon University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-06-20",
      "abstract": "Recent years have seen the development of many open-source ML fairness\\ntoolkits aimed at helping ML practitioners assess and address unfairness in\\ntheir systems. However, there has been little research investigating how ML\\npractitioners actually use these toolkits in practice. In this paper, we\\nconducted the first in-depth empirical exploration of how industry\\npractitioners (try to) work with existing fairness toolkits. In particular, we\\nconducted think-aloud interviews to understand how participants learn about and\\nuse fairness toolkits, and explored the generality of our findings through an\\nanonymous online survey. We identified several opportunities for fairness\\ntoolkits to better address practitioner needs and scaffold them in using\\ntoolkits effectively and responsibly. Based on these findings, we highlight\\nimplications for the design of future open-source fairness toolkits that can\\nsupport practitioners in better contextualizing, communicating, and\\ncollaborating around ML fairness efforts.\\n",
      "cited_by_count": 77,
      "type": "article",
      "source": {
        "name": "2022 ACM Conference on Fairness, Accountability, and Transparency",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3531146.3533113"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Open Source Software Innovations",
        "Mobile Crowdsensing and Crowdsourcing"
      ],
      "referenced_works_count": 127,
      "url": "https://openalex.org/W4280609065"
    },
    {
      "openalex_id": "W3207941483",
      "doi": "10.1145/3462244.3479897",
      "title": "Bias and Fairness in Multimodal Machine Learning: A Case Study of Automated Video Interviews",
      "authors": [
        {
          "name": "Brandon M. Booth",
          "openalex_id": "A5036735405",
          "orcid": "https://orcid.org/0000-0002-5780-8882",
          "institutions": [
            "University of Colorado Boulder"
          ]
        },
        {
          "name": "Louis Hickman",
          "openalex_id": "A5071909501",
          "orcid": "https://orcid.org/0000-0002-2752-7705",
          "institutions": [
            "Purdue University West Lafayette"
          ]
        },
        {
          "name": "Shree Krishna Subburaj",
          "openalex_id": "A5016069458",
          "institutions": [
            "University of Colorado Boulder"
          ]
        },
        {
          "name": "Louis Tay",
          "openalex_id": "A5050636888",
          "orcid": "https://orcid.org/0000-0002-5522-4728",
          "institutions": [
            "Purdue University West Lafayette"
          ]
        },
        {
          "name": "Sang Eun Woo",
          "openalex_id": "A5065878227",
          "orcid": "https://orcid.org/0000-0002-3232-5913",
          "institutions": [
            "Purdue University West Lafayette"
          ]
        },
        {
          "name": "Sidney K. D\u2019Mello",
          "openalex_id": "A5008316506",
          "orcid": "https://orcid.org/0000-0003-0347-2807",
          "institutions": [
            "University of Colorado Boulder"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-10-15",
      "abstract": "We introduce the psychometric concepts of bias and fairness in a multimodal machine learning context assessing individuals' hireability from prerecorded video interviews. We collected interviews from 733 participants and hireability ratings from a panel of trained annotators in a simulated hiring study, and then trained interpretable machine learning models on verbal, paraverbal, and visual features extracted from the videos to investigate unimodal versus multimodal bias and fairness. Our results demonstrate that, in the absence of any bias mitigation strategy, combining multiple modalities only marginally improves prediction accuracy at the cost of increasing bias and reducing fairness compared to the least biased and most fair unimodal predictor set (verbal). We further show that gender-norming predictors only reduces gender predictability for paraverbal and visual modalities, while removing gender-biased features can achieve gender blindness, minimal bias, and fairness (for all modalities except for visual) at the cost of some prediction accuracy. Overall, the reduced-feature approach using predictors from all modalities achieved the best balance between accuracy, bias, and fairness, with the verbal modality alone performing almost as well. Our analysis highlights how optimizing model prediction accuracy in isolation and in a multimodal context may cause bias, disparate impact, and potential social harm, while a more holistic optimization approach based on accuracy, bias, and fairness can avoid these pitfalls.",
      "cited_by_count": 54,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3462244.3479897"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 45,
      "url": "https://openalex.org/W3207941483"
    },
    {
      "openalex_id": "W3036196701",
      "doi": "10.1007/978-3-030-50423-6_46",
      "title": "Machine Learning \u2013 The Results Are Not the only Thing that Matters! What About Security, Explainability and Fairness?",
      "authors": [
        {
          "name": "Micha\u0142 Chora\u015b",
          "openalex_id": "A5009283476",
          "orcid": "https://orcid.org/0000-0003-1405-9911",
          "institutions": [
            "Instytut Technik Telekomunikacyjnych i Informatycznych (Poland)",
            "AGH University of Krakow",
            "University of Hagen",
            "Bydgoszcz University of Science and Technology"
          ]
        },
        {
          "name": "Marek Pawlicki",
          "openalex_id": "A5088536972",
          "orcid": "https://orcid.org/0000-0001-5881-6406",
          "institutions": [
            "Bydgoszcz University of Science and Technology",
            "Instytut Technik Telekomunikacyjnych i Informatycznych (Poland)",
            "AGH University of Krakow"
          ]
        },
        {
          "name": "Damian Puchalski",
          "openalex_id": "A5029009779",
          "institutions": [
            "Instytut Technik Telekomunikacyjnych i Informatycznych (Poland)"
          ]
        },
        {
          "name": "Rafa\u0142 Kozik",
          "openalex_id": "A5042632349",
          "orcid": "https://orcid.org/0000-0001-7122-3306",
          "institutions": [
            "Instytut Technik Telekomunikacyjnych i Informatycznych (Poland)",
            "Bydgoszcz University of Science and Technology",
            "AGH University of Krakow"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-01",
      "abstract": null,
      "cited_by_count": 66,
      "type": "book-chapter",
      "source": {
        "name": "Lecture notes in computer science",
        "type": "book series",
        "issn": [
          "0302-9743",
          "1611-3349"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/7303684"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 35,
      "url": "https://openalex.org/W3036196701"
    },
    {
      "openalex_id": "W3214916413",
      "doi": "10.1016/j.petrol.2021.109885",
      "title": "Fair train-test split in machine learning: Mitigating spatial autocorrelation for improved prediction accuracy",
      "authors": [
        {
          "name": "Jose J. Salazar",
          "openalex_id": "A5070019199",
          "orcid": "https://orcid.org/0000-0003-0551-4521",
          "institutions": [
            "Escuela Superior Politecnica del Litoral",
            "The University of Texas at Austin",
            "Hillenbrand (United States)"
          ]
        },
        {
          "name": "L\u00e9an Garland",
          "openalex_id": "A5065489694",
          "institutions": [
            "Equinor (Norway)"
          ]
        },
        {
          "name": "J. Ochoa",
          "openalex_id": "A5101549369",
          "orcid": "https://orcid.org/0009-0008-6368-7942"
        },
        {
          "name": "Michael J. Pyrcz",
          "openalex_id": "A5061964004",
          "orcid": "https://orcid.org/0000-0002-5983-219X",
          "institutions": [
            "Hillenbrand (United States)",
            "The University of Texas at Austin"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-11-18",
      "abstract": null,
      "cited_by_count": 67,
      "type": "article",
      "source": {
        "name": "Journal of Petroleum Science and Engineering",
        "type": "journal",
        "issn": [
          "0920-4105",
          "1873-4715"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Spatial and Panel Data Analysis",
        "Soil Geostatistics and Mapping",
        "Statistical Methods and Inference"
      ],
      "referenced_works_count": 39,
      "url": "https://openalex.org/W3214916413"
    },
    {
      "openalex_id": "W4391873612",
      "doi": "10.60087/jklst.vol1.n1.p138",
      "title": "Ethical Considerations in AI Addressing Bias and Fairness in Machine Learning Models",
      "authors": [
        {
          "name": "Jeevan Sreerama",
          "openalex_id": "A5093939962",
          "institutions": [
            "EP Analytics (United States)"
          ]
        },
        {
          "name": "Gowrisankar Krishnamoorthy",
          "openalex_id": "A5110490789"
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-09-14",
      "abstract": "The proliferation of artificial intelligence (AI) and machine learning (ML) technologies has brought about unprecedented advancements in various domains. However, concerns surrounding bias and fairness in ML models have gained significant attention, raising ethical considerations that must be addressed. This paper explores the ethical implications of bias in AI systems and the importance of ensuring fairness in ML models. It examines the sources of bias in data collection, algorithm design, and decision-making processes, highlighting the potential consequences of biased AI systems on individuals and society. Furthermore, the paper discusses various approaches and strategies for mitigating bias and promoting fairness in ML models, including data preprocessing techniques, algorithmic transparency, and diverse representation in training datasets. Ethical guidelines and frameworks for developing responsible AI systems are also reviewed, emphasizing the need for interdisciplinary collaboration and stakeholder engagement to address bias and fairness comprehensively. Finally, future directions and challenges in advancing ethical considerations in AI are discussed, underscoring the ongoing efforts required to build trustworthy and equitable AI technologies.",
      "cited_by_count": 50,
      "type": "article",
      "source": {
        "name": "Journal of Knowledge Learning and Science Technology ISSN 2959-6386 (online)",
        "type": "journal",
        "issn": [
          "2959-6386"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://jklst.org/index.php/home/article/download/133/108"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 9,
      "url": "https://openalex.org/W4391873612"
    },
    {
      "openalex_id": "W3216646432",
      "doi": "10.1163/17455243-20213439",
      "title": "Fairness in Machine Learning: Against False Positive Rate Equality as a Measure of Fairness",
      "authors": [
        {
          "name": "Robert R. Long",
          "openalex_id": "A5009824942",
          "institutions": [
            "New York University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-11-22",
      "abstract": "Abstract As machine learning informs increasingly consequential decisions, different metrics have been proposed for measuring algorithmic bias or unfairness. Two popular \u201cfairness measures\u201d are calibration and equality of false positive rate. Each measure seems intuitively important, but notably, it is usually impossible to satisfy both measures. For this reason, a large literature in machine learning speaks of a \u201cfairness tradeoff\u201d between these two measures. This framing assumes that both measures are, in fact, capturing something important. To date, philosophers have seldom examined this crucial assumption, and examined to what extent each measure actually tracks a normatively important property. This makes this inevitable statistical conflict \u2013 between calibration and false positive rate equality \u2013 an important topic for ethics. In this paper, I give an ethical framework for thinking about these measures and argue that, contrary to initial appearances, false positive rate equality is in fact morally irrelevant and does not measure fairness.",
      "cited_by_count": 40,
      "type": "article",
      "source": {
        "name": "Journal of Moral Philosophy",
        "type": "journal",
        "issn": [
          "1740-4681",
          "1745-5243"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning",
        "Psychology of Moral and Emotional Judgment"
      ],
      "referenced_works_count": 39,
      "url": "https://openalex.org/W3216646432"
    },
    {
      "openalex_id": "W4293055795",
      "doi": "10.1613/jair.1.13197",
      "title": "Impact of Imputation Strategies on Fairness in Machine Learning",
      "authors": [
        {
          "name": "Simon Caton",
          "openalex_id": "A5021197359",
          "orcid": "https://orcid.org/0000-0001-9379-3879",
          "institutions": [
            "University College Dublin"
          ]
        },
        {
          "name": "Saiteja Malisetty",
          "openalex_id": "A5002573282",
          "orcid": "https://orcid.org/0000-0002-2159-8617",
          "institutions": [
            "University of Nebraska at Omaha"
          ]
        },
        {
          "name": "Christian Haas",
          "openalex_id": "A5023136097",
          "orcid": "https://orcid.org/0000-0002-2690-5962",
          "institutions": [
            "Vienna University of Economics and Business"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-06-27",
      "abstract": "Research on Fairness and Bias Mitigation in Machine Learning often uses a set of reference datasets for the design and evaluation of novel approaches or definitions. While these datasets are well structured and useful for the comparison of various approaches, they do not reflect that datasets commonly used in real-world applications can have missing values. When such missing values are encountered, the use of imputation strategies is commonplace. However, as imputation strategies potentially alter the distribution of data they can also affect the performance, and potentially the fairness, of the resulting predictions, a topic not yet well understood in the fairness literature. In this article, we investigate the impact of different imputation strategies on classical performance and fairness in classification settings. We find that the selected imputation strategy, along with other factors including the type of classification algorithm, can significantly affect performance and fairness outcomes. The results of our experiments indicate that the choice of imputation strategy is an important factor when considering fairness in Machine Learning. We also provide some insights and guidance for researchers to help navigate imputation approaches for fairness.",
      "cited_by_count": 32,
      "type": "article",
      "source": {
        "name": "Journal of Artificial Intelligence Research",
        "type": "journal",
        "issn": [
          "1076-9757",
          "1943-5037"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "https://www.jair.org/index.php/jair/article/download/13197/26819"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 77,
      "url": "https://openalex.org/W4293055795"
    },
    {
      "openalex_id": "W4293767800",
      "doi": "10.1007/s10676-022-09658-7",
      "title": "Enabling Fairness in Healthcare Through Machine Learning",
      "authors": [
        {
          "name": "Thomas Grote",
          "openalex_id": "A5067459884",
          "orcid": "https://orcid.org/0000-0002-9832-6046",
          "institutions": [
            "University of T\u00fcbingen",
            "Ethics and Public Policy Center"
          ]
        },
        {
          "name": "Geoff Keeling",
          "openalex_id": "A5020476329",
          "orcid": "https://orcid.org/0000-0003-3251-4981",
          "institutions": [
            "Stanford University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-08-31",
      "abstract": null,
      "cited_by_count": 33,
      "type": "article",
      "source": {
        "name": "Ethics and Information Technology",
        "type": "journal",
        "issn": [
          "1388-1957",
          "1572-8439"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s10676-022-09658-7.pdf"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Healthcare cost, quality, practices",
        "Ethics in Clinical Research"
      ],
      "referenced_works_count": 63,
      "url": "https://openalex.org/W4293767800"
    },
    {
      "openalex_id": "W3093329511",
      "doi": "10.48550/arxiv.2010.09553",
      "title": "Survey on Causal-based Machine Learning Fairness Notions",
      "authors": [
        {
          "name": "Karima Makhlouf",
          "openalex_id": "A5000858958",
          "orcid": "https://orcid.org/0000-0001-6318-0713",
          "institutions": [
            "Universit\u00e9 du Qu\u00e9bec \u00e0 Montr\u00e9al"
          ]
        },
        {
          "name": "Sami Zhioua",
          "openalex_id": "A5001717771",
          "orcid": "https://orcid.org/0000-0003-2029-175X",
          "institutions": [
            "Higher Colleges of Technology"
          ]
        },
        {
          "name": "Catuscia Palamidessi",
          "openalex_id": "A5090842450",
          "orcid": "https://orcid.org/0000-0003-4597-7002"
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-10-19",
      "abstract": "Addressing the problem of fairness is crucial to safely use machine learning algorithms to support decisions with a critical impact on people's lives such as job hiring, child maltreatment, disease diagnosis, loan granting, etc. Several notions of fairness have been defined and examined in the past decade, such as statistical parity and equalized odds. The most recent fairness notions, however, are causal-based and reflect the now widely accepted idea that using causality is necessary to appropriately address the problem of fairness. This paper examines an exhaustive list of causal-based fairness notions and study their applicability in real-world scenarios. As the majority of causal-based fairness notions are defined in terms of non-observable quantities (e.g., interventions and counterfactuals), their deployment in practice requires to compute or estimate those quantities using observational data. This paper offers a comprehensive report of the different approaches to infer causal quantities from observational data including identifiability (Pearl's SCM framework) and estimation (potential outcome framework). The main contributions of this survey paper are (1) a guideline to help selecting a suitable fairness notion given a specific real-world scenario, and (2) a ranking of the fairness notions according to Pearl's causation ladder indicating how difficult it is to deploy each notion in practice.",
      "cited_by_count": 40,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2010.09553"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Advanced Causal Inference Techniques",
        "Qualitative Comparative Analysis Research"
      ],
      "referenced_works_count": 38,
      "url": "https://openalex.org/W3093329511"
    },
    {
      "openalex_id": "W3004604609",
      "doi": "10.1145/3375627.3375808",
      "title": "Normative Principles for Evaluating Fairness in Machine Learning",
      "authors": [
        {
          "name": "Derek Leben",
          "openalex_id": "A5050831061",
          "orcid": "https://orcid.org/0000-0001-7304-4854",
          "institutions": [
            "University of Pittsburgh at Johnstown"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-02-05",
      "abstract": "There are many incompatible ways to measure fair outcomes for machine learning algorithms. The goal of this paper is to characterize rates of success and error across protected groups (race, gender, sexual orientation) as a distribution problem, and describe the possible solutions to this problem according to different normative principles from moral and political philosophy. These normative principles are based on various competing attributes within a distribution problem: intentions, compensation, desert, consent, and consequences. Each principle will be applied to a sample risk-assessment classifier to demonstrate the philosophical arguments underlying different sets of fairness metrics.",
      "cited_by_count": 35,
      "type": "article",
      "source": {
        "name": "Proceedings of the AAAI/ACM Conference on AI Ethics and Society",
        "type": "journal",
        "issn": [
          "3065-8365"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 21,
      "url": "https://openalex.org/W3004604609"
    },
    {
      "openalex_id": "W4308641598",
      "doi": "10.1145/3540250.3549093",
      "title": "MAAT: a novel ensemble approach to addressing fairness and performance bugs for machine learning software",
      "authors": [
        {
          "name": "Zhenpeng Chen",
          "openalex_id": "A5031457464",
          "orcid": "https://orcid.org/0000-0002-4765-1893",
          "institutions": [
            "University College London"
          ]
        },
        {
          "name": "Jie M. Zhang",
          "openalex_id": "A5088708850",
          "orcid": "https://orcid.org/0000-0003-0481-7264",
          "institutions": [
            "King's College London"
          ]
        },
        {
          "name": "Federica Sarro",
          "openalex_id": "A5012165852",
          "orcid": "https://orcid.org/0000-0002-9146-442X",
          "institutions": [
            "University College London"
          ]
        },
        {
          "name": "Mark Harman",
          "openalex_id": "A5000019783",
          "orcid": "https://orcid.org/0000-0002-5864-4488",
          "institutions": [
            "University College London"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-11-07",
      "abstract": "Machine Learning (ML) software can lead to unfair and unethical decisions, making software fairness bugs an increasingly significant concern for software engineers. However, addressing fairness bugs often comes at the cost of introducing more ML performance (e.g., accuracy) bugs. In this paper, we propose MAAT, a novel ensemble approach to improving fairness-performance trade-off for ML software. Conventional ensemble methods combine different models with identical learning objectives. MAAT, instead, combines models optimized for different objectives: fairness and ML performance. We conduct an extensive evaluation of MAAT with 5 state-of-the-art methods, 9 software decision tasks, and 15 fairness-performance measurements. The results show that MAAT significantly outperforms the state-of-the-art. In particular, MAAT beats the trade-off baseline constructed by a recent benchmarking tool in 92.2% of the overall cases evaluated, 12.2 percentage points more than the best technique currently available. Moreover, the superiority of MAAT over the state-of-the-art holds on all the tasks and measurements that we study. We have made publicly available the code and data of this work to allow for future replication and extension.",
      "cited_by_count": 55,
      "type": "article",
      "source": {
        "name": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 59,
      "url": "https://openalex.org/W4308641598"
    },
    {
      "openalex_id": "W3182172753",
      "doi": "10.3905/jfds.2021.1.075",
      "title": "Fairness Measures for Machine Learning in Finance",
      "authors": [
        {
          "name": "Sanjiv Ranjan Das",
          "openalex_id": "A5072909289",
          "orcid": "https://orcid.org/0000-0003-2168-0909",
          "institutions": [
            "Amazon (United States)",
            "Santa Clara University"
          ]
        },
        {
          "name": "Michele Donini",
          "openalex_id": "A5056775618",
          "orcid": "https://orcid.org/0000-0002-9769-3899",
          "institutions": [
            "Amazon (Germany)"
          ]
        },
        {
          "name": "Jason Gelman",
          "openalex_id": "A5022077270",
          "institutions": [
            "Amazon (United States)"
          ]
        },
        {
          "name": "Kevin Haas",
          "openalex_id": "A5064924118",
          "institutions": [
            "Amazon (United States)"
          ]
        },
        {
          "name": "Mila Hardt",
          "openalex_id": "A5059917487",
          "institutions": [
            "Amazon (United States)"
          ]
        },
        {
          "name": "Jared Katzman",
          "openalex_id": "A5012514758",
          "institutions": [
            "Microsoft (United States)"
          ]
        },
        {
          "name": "Krishnaram Kenthapadi",
          "openalex_id": "A5002843568",
          "orcid": "https://orcid.org/0000-0003-1237-087X",
          "institutions": [
            "Amazon (United States)"
          ]
        },
        {
          "name": "Pedro Larroy",
          "openalex_id": "A5015836963",
          "institutions": [
            "Amazon (United States)"
          ]
        },
        {
          "name": "P\u0131nar Yilmaz",
          "openalex_id": "A5101549691",
          "orcid": "https://orcid.org/0000-0001-9890-6510",
          "institutions": [
            "Amazon (United States)"
          ]
        },
        {
          "name": "Muhammad Bilal Zafar",
          "openalex_id": "A5102901191",
          "orcid": "https://orcid.org/0000-0001-8347-7813",
          "institutions": [
            "Amazon (Germany)"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-09-14",
      "abstract": "The authors present a machine learning pipeline for fairness-aware machine learning (FAML) in finance that encompasses metrics for fairness (and accuracy). Whereas accuracy metrics are well understood and the principal ones are used frequently, there is no consensus as to which of several available measures for fairness should be used in a generic manner in the financial services industry. The authors explore these measures and discuss which ones to focus on at various stages in the ML pipeline, pre-training and post-training, and they examine simple bias mitigation approaches. Using a standard dataset, they show that the sequencing in their FAML pipeline offers a cogent approach to arriving at a fair and accurate ML model. The authors discuss the intersection of bias metrics with legal considerations in the United States, and the entanglement of explainability and fairness is exemplified in the case study. They discuss possible approaches for training ML models while satisfying constraints imposed from various fairness metrics and the role of causality in assessing fairness. <b>Key Findings</b> \u25aa Sources of bias are presented and a range of metrics is considered for machine learning applications in finance, both pre-training and post-training of models. \u25aa A process of using the metrics to arrive at fair models is discussed. \u25aa Various considerations for the choice of specific metrics are also analyzed.",
      "cited_by_count": 28,
      "type": "article",
      "source": {
        "name": "The Journal of Financial Data Science",
        "type": "journal",
        "issn": [
          "2640-3943",
          "2640-3951"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Blockchain Technology Applications and Security"
      ],
      "referenced_works_count": 31,
      "url": "https://openalex.org/W3182172753"
    },
    {
      "openalex_id": "W4283164469",
      "doi": "10.1145/3531146.3534644",
      "title": "Data augmentation for fairness-aware machine learning",
      "authors": [
        {
          "name": "Ioannis Pastaltzidis",
          "openalex_id": "A5070653670",
          "orcid": "https://orcid.org/0009-0004-2859-467X",
          "institutions": [
            "Information Technologies Institute",
            "Centre for Research and Technology Hellas"
          ]
        },
        {
          "name": "Nikolaos Dimitriou",
          "openalex_id": "A5013666330",
          "orcid": "https://orcid.org/0000-0002-6650-7758",
          "institutions": [
            "Centre for Research and Technology Hellas",
            "Information Technologies Institute"
          ]
        },
        {
          "name": "Katherine Quezada Tav\u00e1rez",
          "openalex_id": "A5069332775",
          "orcid": "https://orcid.org/0000-0001-9695-8798",
          "institutions": [
            "KU Leuven"
          ]
        },
        {
          "name": "Stergios Aidinlis",
          "openalex_id": "A5001222592",
          "orcid": "https://orcid.org/0000-0001-8464-0416",
          "institutions": [
            "Trilateral Research & Consulting"
          ]
        },
        {
          "name": "Thomas Marquenie",
          "openalex_id": "A5078865029",
          "orcid": "https://orcid.org/0000-0003-2242-2223",
          "institutions": [
            "KU Leuven"
          ]
        },
        {
          "name": "Agata Gurzawska",
          "openalex_id": "A5068039982",
          "orcid": "https://orcid.org/0000-0001-8323-7211",
          "institutions": [
            "Trilateral Research & Consulting"
          ]
        },
        {
          "name": "Dimitrios Tzovaras",
          "openalex_id": "A5087452615",
          "orcid": "https://orcid.org/0000-0001-6915-6722",
          "institutions": [
            "Information Technologies Institute",
            "Centre for Research and Technology Hellas"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-06-20",
      "abstract": "Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.",
      "cited_by_count": 29,
      "type": "article",
      "source": {
        "name": "2022 ACM Conference on Fairness, Accountability, and Transparency",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3531146.3534644"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Ethics and Social Impacts of AI",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 35,
      "url": "https://openalex.org/W4283164469"
    },
    {
      "openalex_id": "W3083540916",
      "doi": "10.1007/978-3-030-58475-7_49",
      "title": "Towards Formal Fairness in Machine Learning",
      "authors": [
        {
          "name": "Alexey Ignatiev",
          "openalex_id": "A5051305025",
          "orcid": "https://orcid.org/0000-0002-4535-2902",
          "institutions": [
            "Monash University"
          ]
        },
        {
          "name": "Martin Cooper",
          "openalex_id": "A5004558530",
          "orcid": "https://orcid.org/0000-0003-4853-053X",
          "institutions": [
            "Institut de Recherche en Informatique de Toulouse",
            "Universit\u00e9 de Toulouse",
            "Universit\u00e9 Toulouse III - Paul Sabatier"
          ]
        },
        {
          "name": "Mohamed Siala",
          "openalex_id": "A5054913981",
          "orcid": "https://orcid.org/0000-0001-9503-4091",
          "institutions": [
            "Universit\u00e9 de Toulouse",
            "Centre National de la Recherche Scientifique",
            "Laboratoire d'Analyse et d'Architecture des Syst\u00e8mes",
            "Institut National des Sciences Appliqu\u00e9es de Toulouse"
          ]
        },
        {
          "name": "Emmanuel H\u00e9brard",
          "openalex_id": "A5048982899",
          "orcid": "https://orcid.org/0000-0003-3131-0709",
          "institutions": [
            "Laboratoire d'Analyse et d'Architecture des Syst\u00e8mes",
            "Universit\u00e9 de Toulouse",
            "Institut National des Sciences Appliqu\u00e9es de Toulouse",
            "Centre National de la Recherche Scientifique"
          ]
        },
        {
          "name": "Jo\u00e3o Marques\u2010Silva",
          "openalex_id": "A5002246957",
          "orcid": "https://orcid.org/0000-0002-6632-3086",
          "institutions": [
            "Universit\u00e9 de Toulouse"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-01",
      "abstract": null,
      "cited_by_count": 27,
      "type": "book-chapter",
      "source": {
        "name": "Lecture notes in computer science",
        "type": "book series",
        "issn": [
          "0302-9743",
          "1611-3349"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://research.monash.edu/en/publications/23c5b54c-be3f-4894-a044-3c2a556a8f05"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 67,
      "url": "https://openalex.org/W3083540916"
    },
    {
      "openalex_id": "W3171889065",
      "doi": "10.1016/j.ipm.2021.102642",
      "title": "Machine learning fairness notions: Bridging the gap with real-world applications",
      "authors": [
        {
          "name": "Karima Makhlouf",
          "openalex_id": "A5000858958",
          "orcid": "https://orcid.org/0000-0001-6318-0713",
          "institutions": [
            "Universit\u00e9 du Qu\u00e9bec \u00e0 Montr\u00e9al",
            "\u00c9cole Polytechnique"
          ]
        },
        {
          "name": "Sami Zhioua",
          "openalex_id": "A5001717771",
          "orcid": "https://orcid.org/0000-0003-2029-175X",
          "institutions": [
            "Higher Colleges of Technology",
            "\u00c9cole Polytechnique"
          ]
        },
        {
          "name": "Catuscia Palamidessi",
          "openalex_id": "A5090842450",
          "orcid": "https://orcid.org/0000-0003-4597-7002",
          "institutions": [
            "\u00c9cole Polytechnique",
            "Institut Polytechnique de Paris"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-06-01",
      "abstract": null,
      "cited_by_count": 53,
      "type": "article",
      "source": {
        "name": "Information Processing & Management",
        "type": "journal",
        "issn": [
          "0306-4573",
          "1873-5371"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "bronze",
        "oa_url": "https://www.sciencedirect.com/science/article/am/pii/S0306457321001321?via%3Dihub"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Artificial Intelligence in Healthcare and Education",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 137,
      "url": "https://openalex.org/W3171889065"
    },
    {
      "openalex_id": "W4284701600",
      "doi": "10.1145/3510003.3510091",
      "title": "Training data debugging for the fairness of machine learning software",
      "authors": [
        {
          "name": "Yanhui Li",
          "openalex_id": "A5100360608",
          "orcid": "https://orcid.org/0000-0003-2282-7175",
          "institutions": [
            "Nanjing University"
          ]
        },
        {
          "name": "Linghan Meng",
          "openalex_id": "A5019161243",
          "institutions": [
            "Nanjing University"
          ]
        },
        {
          "name": "Lin Chen",
          "openalex_id": "A5100443798",
          "orcid": "https://orcid.org/0000-0003-2352-2226",
          "institutions": [
            "Nanjing University"
          ]
        },
        {
          "name": "Li Yu",
          "openalex_id": "A5101874595",
          "orcid": "https://orcid.org/0000-0002-6208-3281",
          "institutions": [
            "Nanjing University"
          ]
        },
        {
          "name": "Di Wu",
          "openalex_id": "A5100599890",
          "orcid": "https://orcid.org/0000-0002-4753-8161"
        },
        {
          "name": "Yuming Zhou",
          "openalex_id": "A5031391841",
          "institutions": [
            "Nanjing University"
          ]
        },
        {
          "name": "Baowen Xu",
          "openalex_id": "A5100331400",
          "orcid": "https://orcid.org/0000-0001-7743-1296",
          "institutions": [
            "Nanjing University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-05-21",
      "abstract": "With the widespread application of machine learning (ML) software, especially in high-risk tasks, the concern about their unfairness has been raised towards both developers and users of ML software. The unfairness of ML software indicates the software behavior affected by the sensitive features (e.g., sex), which leads to biased and illegal decisions and has become a worthy problem for the whole software engineering community.",
      "cited_by_count": 45,
      "type": "article",
      "source": {
        "name": "Proceedings of the 44th International Conference on Software Engineering",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning",
        "Privacy-Preserving Technologies in Data"
      ],
      "referenced_works_count": 38,
      "url": "https://openalex.org/W4284701600"
    }
  ],
  "count": 60,
  "errors": []
}
