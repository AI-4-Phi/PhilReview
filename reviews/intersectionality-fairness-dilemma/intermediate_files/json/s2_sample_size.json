{
  "status": "success",
  "source": "semantic_scholar",
  "query": "sample size fairness machine learning",
  "results": [
    {
      "paperId": "4a4a94dcff78b188a23e7340567cbb6f155e73f3",
      "title": "Are fairness metric scores enough to assess discrimination biases in machine learning?",
      "authors": [
        {
          "name": "Fanny Jourdan",
          "authorId": "2210068687"
        },
        {
          "name": "L. Risser",
          "authorId": "144450552"
        },
        {
          "name": "J. Loubes",
          "authorId": "144736569"
        },
        {
          "name": "Nicholas M. Asher",
          "authorId": "1916126"
        }
      ],
      "year": 2023,
      "abstract": "This paper presents novel experiments shedding light on the shortcomings of current metrics for assessing biases of gender discrimination made by machine learning algorithms on textual data. We focus on the Bios dataset, and our learning task is to predict the occupation of individuals, based on their biography. Such prediction tasks are common in commercial Natural Language Processing (NLP) applications such as automatic job recommendations. We address an important limitation of theoretical discussions dealing with group-wise fairness metrics: they focus on large datasets, although the norm in many industrial NLP applications is to use small to reasonably large linguistic datasets for which the main practical constraint is to get a good prediction accuracy. We then question how reliable are different popular measures of bias when the size of the training set is simply sufficient to learn reasonably accurate predictions.Our experiments sample the Bios dataset and learn more than 200 models on different sample sizes. This allows us to statistically study our results and to confirm that common gender bias indices provide diverging and sometimes unreliable results when applied to relatively small training and test samples. This highlights the crucial importance of variance calculations for providing sound results in this field.",
      "citationCount": 6,
      "doi": "10.48550/arXiv.2306.05307",
      "arxivId": "2306.05307",
      "url": "https://www.semanticscholar.org/paper/4a4a94dcff78b188a23e7340567cbb6f155e73f3",
      "venue": "TRUSTNLP",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2306.05307"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7205569dc5eeb812d4ba93a8e1f37be1fc45eef7",
      "title": "Clinical prediction models using machine learning in oncology: challenges and recommendations",
      "authors": [
        {
          "name": "G. Collins",
          "authorId": "2246596255"
        },
        {
          "name": "Mae Chester-Jones",
          "authorId": "2301546652"
        },
        {
          "name": "Stephen Gerry",
          "authorId": "2238499551"
        },
        {
          "name": "Jie Ma",
          "authorId": "2213454893"
        },
        {
          "name": "Joao Matos",
          "authorId": "2367742484"
        },
        {
          "name": "Jyoti Sehjal",
          "authorId": "2384723168"
        },
        {
          "name": "Biruk Tsegaye",
          "authorId": "2340068157"
        },
        {
          "name": "P. Dhiman",
          "authorId": "2366098783"
        }
      ],
      "year": 2025,
      "abstract": "Abstract Clinical prediction models are widely developed in the field of oncology, providing individualised risk estimates to aid diagnosis and prognosis. Machine learning methods are increasingly being used to develop prediction models, yet many suffer from methodological flaws limiting clinical implementation. This review outlines key considerations for developing robust, equitable prediction models in cancer care. Critical steps include systematic review of existing models, protocol development, registration, end-user engagement, sample size calculations and ensuring data representativeness across target populations. Technical challenges encompass handling missing data, addressing fairness across demographic groups and managing complex data structures, including censored observations, competing risks or clustering effects. Comprehensive internal and external evaluation requires assessment of both statistical performance (discrimination and calibration) and clinical utility. Implementation barriers include limited stakeholder engagement, insufficient clinical utility evidence, a lack of consideration of workflow integration and the absence of post-deployment monitoring plans. Despite significant potential for personalising cancer care, most prediction models remain unimplemented due to these methodological and translational challenges. Addressing these considerations from study design through post implementation monitoring is essential for developing trustworthy tools that bridge the gap between model development and clinical practice in oncology.",
      "citationCount": 2,
      "doi": "10.1136/bmjonc-2025-000914",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7205569dc5eeb812d4ba93a8e1f37be1fc45eef7",
      "venue": "BMJ Oncology",
      "journal": {
        "name": "BMJ Oncology",
        "volume": "4"
      },
      "publicationTypes": [
        "Review",
        "JournalArticle"
      ]
    },
    {
      "paperId": "91827d8d46944027ef74fcd28d7f271b07e7b07f",
      "title": "Optimising test intervals for individuals with type 2 diabetes: A machine learning approach",
      "authors": [
        {
          "name": "S. M. Pedersen",
          "authorId": "1992806149"
        },
        {
          "name": "Nicolai Damslund",
          "authorId": "2345333279"
        },
        {
          "name": "T. Kj\u00e6r",
          "authorId": "4619761"
        },
        {
          "name": "Kim Rose Olsen",
          "authorId": "2279809110"
        }
      ],
      "year": 2025,
      "abstract": "Background Chronic disease monitoring programs often adopt a one-size-fits-all approach that does not consider variation in need, potentially leading to excessive or insufficient support for patients at different risk levels. Machine learning (ML) developments offer new opportunities for personalised medicine in clinical practice. Objective To demonstrate the potential of ML to guide resource allocation and tailored disease management, this study aims to predict the optimal testing interval for monitoring blood glucose (HbA1c) for patients with Type 2 Diabetes (T2D). We examine fairness across income and education levels and evaluate the risk of false-positives and false-negatives. Data Danish administrative registers are linked with national clinical databases. Our population consists of all T2D patients from 2015-2018, a sample of more than 57,000. Data contains patient-level clinical measures, healthcare utilisation, medicine, and socio-demographics. Methods We classify HbA1c test intervals into four categories (3, 6, 9, and 12 months) using three classification algorithms: logistic regression, random forest, and extreme gradient boosting (XGBoost). Feature importance is assessed with SHAP model explanations on the best-performing model, which was XGBoost. A training set comprising 80% of the data is used to predict optimal test intervals, with 20% reserved for testing. Cross-validation is employed to enhance the model\u2019s reliability and reduce overfitting. Model performance is evaluated using ROC-AUC, and optimal intervals are determined based on a \u201ctime-to-next-positive-test\u201d concept, with different durations associated with specific intervals. Results The model exhibits varying predictive accuracy, with AUC scores ranging from 0.53 to 0.89 across different test intervals. We find significant potential to free resources by prolonging the test interval for well-controlled patients. The fairness metric suggests models perform well in terms of equality. There is a sizeable risk of false negatives (predicting longer intervals than optimal), which requires attention. Conclusions We demonstrate the potential to use ML in personalised diabetes management by assisting physicians in categorising patients by testing frequencies. Clinical validation on diverse patient populations is needed to assess the model\u2019s performance in real-world settings.",
      "citationCount": 2,
      "doi": "10.1371/journal.pone.0317722",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/91827d8d46944027ef74fcd28d7f271b07e7b07f",
      "venue": "PLoS ONE",
      "journal": {
        "name": "PLOS ONE",
        "volume": "20"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "22dc44351e31ac45536b5fd5a3475a437af7e16c",
      "title": "Understanding Disparities in Post Hoc Machine Learning Explanation",
      "authors": [
        {
          "name": "Vishwali Mhasawade",
          "authorId": "51429443"
        },
        {
          "name": "Salman Rahman",
          "authorId": "2281734158"
        },
        {
          "name": "Zo\u00e9 Haskell-Craig",
          "authorId": "2281638176"
        },
        {
          "name": "R. Chunara",
          "authorId": "3144230"
        }
      ],
      "year": 2024,
      "abstract": "Previous work has highlighted that existing post-hoc explanation methods exhibit disparities in explanation fidelity (across \u201crace\u201d and \u201cgender\u201d as sensitive attributes), and while a large body of work focuses on mitigating these issues at the explanation metric level, the role of the data generating process and black box model in relation to explanation disparities remains largely unexplored. Accordingly, through both simulations as well as experiments on a real-world dataset, we specifically assess challenges to explanation disparities that originate from properties of the data: limited sample size, covariate shift, concept shift, omitted variable bias, and challenges based on model properties: inclusion of the sensitive attribute and appropriate functional form. Through controlled simulation analyses, our study demonstrates that increased covariate shift, concept shift, and omission of covariates increase explanation disparities, with the effect pronounced higher for neural network models that are better able to capture the underlying functional form in comparison to linear models. We also observe consistent findings regarding the effect of concept shift and omitted variable bias on explanation disparities in the Adult income dataset. Overall, results indicate that disparities in model explanations can also depend on data and model properties. Based on this systematic investigation, we provide recommendations for the design of explanation methods that mitigate undesirable disparities.",
      "citationCount": 7,
      "doi": "10.1145/3630106.3659043",
      "arxivId": "2401.14539",
      "url": "https://www.semanticscholar.org/paper/22dc44351e31ac45536b5fd5a3475a437af7e16c",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "379a81779aa887c4f5b70bfa20b289c8dd24edb1",
      "title": "Finite-Sample and Distribution-Free Fair Classification: Optimal Trade-off Between Excess Risk and Fairness, and the Cost of Group-Blindness",
      "authors": [
        {
          "name": "Xiaotian Hou",
          "authorId": "2292155108"
        },
        {
          "name": "Linjun Zhang",
          "authorId": "2249882382"
        }
      ],
      "year": 2024,
      "abstract": "Algorithmic fairness in machine learning has recently garnered significant attention. However, two pressing challenges remain: (1) The fairness guarantees of existing fair classification methods often rely on specific data distribution assumptions and large sample sizes, which can lead to fairness violations when the sample size is moderate-a common situation in practice. (2) Due to legal and societal considerations, using sensitive group attributes during decision-making (referred to as the group-blind setting) may not always be feasible. In this work, we quantify the impact of enforcing algorithmic fairness and group-blindness in binary classification under group fairness constraints. Specifically, we propose a unified framework for fair classification that provides distribution-free and finite-sample fairness guarantees with controlled excess risk. This framework is applicable to various group fairness notions in both group-aware and group-blind scenarios. Furthermore, we establish a minimax lower bound on the excess risk, showing the minimax optimality of our proposed algorithm up to logarithmic factors. Through extensive simulation studies and real data analysis, we further demonstrate the superior performance of our algorithm compared to existing methods, and provide empirical support for our theoretical findings.",
      "citationCount": 1,
      "doi": null,
      "arxivId": "2410.16477",
      "url": "https://www.semanticscholar.org/paper/379a81779aa887c4f5b70bfa20b289c8dd24edb1",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "75e6b3e84150f8dd4ad77f4362970cf58a95a22a",
      "title": "Investigating privacy-preserving machine learning for healthcare data sharing through federated learning",
      "authors": [
        {
          "name": "Shaik Khaleel Ahamed",
          "authorId": "2281883640"
        },
        {
          "name": "Neerav Nishant",
          "authorId": "2213621339"
        },
        {
          "name": "Ayyakkannu Selvaraj",
          "authorId": "2281873884"
        },
        {
          "name": "Nisarg Gandhewar",
          "authorId": "2281872559"
        },
        {
          "name": "Srithar A",
          "authorId": "2281880770"
        },
        {
          "name": "K.K.Baseer",
          "authorId": "2281878984"
        }
      ],
      "year": 2023,
      "abstract": "Privacy-Preserving Machine Learning (PPML) is a pivotal paradigm in healthcare research, offering innovative solutions to the challenges of data sharing and privacy preservation. In the context of Federated Learning, this paper investigates the implementation of PPML for healthcare data sharing, focusing on the dynamic nature of data collection, sample sizes, data modalities, patient demographics, and comorbidity indices. The results reveal substantial variations in sample sizes across substudies, underscoring the need to align data collection with research objectives and available resources. The distribution of measures demonstrates a balanced approach to healthcare data modalities, ensuring data fairness and equity. The interplay between average age and sample size highlights the significance of tailored privacy-preserving strategies. The comorbidity index distribution provides insights into the health status of the studied population and aids in personalized healthcare. Additionally, the fluctuation of sample sizes over substudies emphasizes the adaptability of privacy-preserving machine learning models in diverse healthcare research scenarios. Overall, this investigation contributes to the evolving landscape of healthcare data sharing by addressing the challenges of data heterogeneity, regulatory compliance, and collaborative model development. The findings empower researchers and healthcare professionals to strike a balance between data utility and privacy preservation, ultimately advancing the field of privacy-preserving machine learning in healthcare research.",
      "citationCount": 5,
      "doi": "10.58414/scientifictemper.2023.14.4.37",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/75e6b3e84150f8dd4ad77f4362970cf58a95a22a",
      "venue": "THE SCIENTIFIC TEMPER",
      "journal": {
        "name": "The Scientific Temper"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "459a74f64225fe9332e565193d5ecdf03efd84b8",
      "title": "Shedding light on underrepresentation and Sampling Bias in machine learning",
      "authors": [
        {
          "name": "Sami Zhioua",
          "authorId": "1722898"
        },
        {
          "name": "Ruta Binkyt.e",
          "authorId": "2175652653"
        }
      ],
      "year": 2023,
      "abstract": "Accurately measuring discrimination is crucial to faithfully assessing fairness of trained machine learning (ML) models. Any bias in measuring discrimination leads to either amplification or underestimation of the existing disparity. Several sources of bias exist and it is assumed that bias resulting from machine learning is born equally by different groups (e.g. females vs males, whites vs blacks, etc.). If, however, bias is born differently by different groups, it may exacerbate discrimination against specific sub-populations. Sampling bias, is inconsistently used in the literature to describe bias due to the sampling procedure. In this paper, we attempt to disambiguate this term by introducing clearly defined variants of sampling bias, namely, sample size bias (SSB) and underrepresentation bias (URB). We show also how discrimination can be decomposed into variance, bias, and noise. Finally, we challenge the commonly accepted mitigation approach that discrimination can be addressed by collecting more samples of the underrepresented group.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2306.05068",
      "arxivId": "2306.05068",
      "url": "https://www.semanticscholar.org/paper/459a74f64225fe9332e565193d5ecdf03efd84b8",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2306.05068"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "070dd75e0b47275d43a43693cdda2fdadc538598",
      "title": "A Central Limit Theorem for $L_p$ transportation cost with applications to Fairness Assessment in Machine Learning",
      "authors": [
        {
          "name": "E. Barrio",
          "authorId": "145001369"
        },
        {
          "name": "Paula Gordaliza",
          "authorId": "51130398"
        },
        {
          "name": "Jean-Michel Loubes",
          "authorId": "144736570"
        }
      ],
      "year": 2018,
      "abstract": "We provide a Central Limit Theorem for the Monge-Kantorovich distance between two empirical distributions with size $n$ and $m$, $W_p(P_n,Q_m)$ for $p>1$ for observations on the real line, using a minimal amount of assumptions. We provide an estimate of the asymptotic variance which enables to build a two sample test to assess the similarity between two distributions. This test is then used to provide a new criterion to assess the notion of fairness of a classification algorithm.",
      "citationCount": 2,
      "doi": null,
      "arxivId": "1807.06796",
      "url": "https://www.semanticscholar.org/paper/070dd75e0b47275d43a43693cdda2fdadc538598",
      "venue": "",
      "journal": {
        "name": "arXiv: Statistics Theory",
        "volume": ""
      },
      "publicationTypes": null
    },
    {
      "paperId": "cf90ea47e52f52164e457036a93d1a8e9912341f",
      "title": "Fairness-Aware PAC Learning from Corrupted Data",
      "authors": [
        {
          "name": "Nikola Konstantinov",
          "authorId": "145292827"
        },
        {
          "name": "Christoph H. Lampert",
          "authorId": "48523189"
        }
      ],
      "year": 2021,
      "abstract": "Addressing fairness concerns about machine learning models is a crucial step towards their long-term adoption in real-world automated systems. While many approaches have been developed for training fair models from data, little is known about the robustness of these methods to data corruption. In this work we consider fairness-aware learning under worst-case data manipulations. We show that an adversary can in some situations force any learner to return an overly biased classifier, regardless of the sample size and with or without degrading accuracy, and that the strength of the excess bias increases for learning problems with underrepresented protected groups in the data. We also prove that our hardness results are tight up to constant factors. To this end, we study two natural learning algorithms that optimize for both accuracy and fairness and show that these algorithms enjoy guarantees that are order-optimal in terms of the corruption ratio and the protected groups frequencies in the large data limit.",
      "citationCount": 22,
      "doi": null,
      "arxivId": "2102.06004",
      "url": "https://www.semanticscholar.org/paper/cf90ea47e52f52164e457036a93d1a8e9912341f",
      "venue": "Journal of machine learning research",
      "journal": {
        "name": "J. Mach. Learn. Res.",
        "pages": "160:1-160:60",
        "volume": "23"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "791c406b4155b8dc050ecc3af920fd0dae2535f6",
      "title": "On the Origins of Sampling Bias: Implications on Fairness Measurement and Mitigation",
      "authors": [
        {
          "name": "Sami Zhioua",
          "authorId": "2324599429"
        },
        {
          "name": "Ruta Binkyte",
          "authorId": "2199945445"
        },
        {
          "name": "Ayoub Ouni",
          "authorId": "1984767000"
        },
        {
          "name": "Farah Barika Ktata",
          "authorId": "2343566"
        }
      ],
      "year": 2025,
      "abstract": "Accurately measuring discrimination is crucial to faithfully assessing fairness of trained machine learning (ML) models. Any bias in measuring discrimination leads to either amplification or underestimation of the existing disparity. Several sources of bias exist and it is assumed that bias resulting from machine learning is born equally by different groups (e.g. females vs males, whites vs blacks, etc.). If, however, bias is born differently by different groups, it may exacerbate discrimination against specific sub-populations. Sampling bias, in particular, is inconsistently used in the literature to describe bias due to the sampling procedure. In this paper, we attempt to disambiguate this term by introducing clearly defined variants of sampling bias, namely, sample size bias (SSB) and underrepresentation bias (URB). Through an extensive set of experiments on benchmark datasets and using mainstream learning algorithms, we expose relevant observations in several model training scenarios. The observations are finally framed as actionable recommendations for practitioners.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2503.17956",
      "arxivId": "2503.17956",
      "url": "https://www.semanticscholar.org/paper/791c406b4155b8dc050ecc3af920fd0dae2535f6",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.17956"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7d2d1e6a3cefc2a337640c710f591a0d6a038908",
      "title": "PRECISION: Decentralized Constrained Min-Max Learning with Low Communication and Sample Complexities",
      "authors": [
        {
          "name": "Zhuqing Liu",
          "authorId": "2164219912"
        },
        {
          "name": "Xin Zhang",
          "authorId": "2149174583"
        },
        {
          "name": "Songtao Lu",
          "authorId": "1606015788"
        },
        {
          "name": "Jia Liu",
          "authorId": "2142651766"
        }
      ],
      "year": 2023,
      "abstract": "Recently, min-max optimization problems have received increasing attention due to their wide range of applications in machine learning (ML). However, most existing min-max solution techniques are either single-machine or distributed algorithms coordinated by a central server. In this paper, we focus on the decentralized min-max optimization for learning with domain constraints, where multiple agents collectively solve a nonconvex-strongly-concave min-max saddle point problem without coordination from any server. Decentralized min-max optimization problems with domain constraints underpins many important ML applications, including multi-agent ML fairness assurance, and policy evaluations in multi-agent reinforcement learning. We propose an algorithm called PRECISION (proximal gradient-tracking and stochastic recursive variance reduction) that enjoys a convergence rate of O(1/T), where T is the maximum number of iterations. To further reduce sample complexity, we propose PRECISION+ with an adaptive batch size technique. We show that the fast O(1/T) convergence of PRECISION and PRECISION+ to an \u03b5-stationary point imply O(\u03b5-2) communication complexity and [EQUATION] sample complexity, where m is the number of agents and n is the size of dataset at each agent. To our knowledge, this is the first work that achieves O(\u03b5-2) in both sample and communication complexities in decentralized min-max learning with domain constraints. Our experiments also corroborate the theoretical results.",
      "citationCount": 8,
      "doi": "10.1145/3565287.3610267",
      "arxivId": "2303.02532",
      "url": "https://www.semanticscholar.org/paper/7d2d1e6a3cefc2a337640c710f591a0d6a038908",
      "venue": "ACM Interational Symposium on Mobile Ad Hoc Networking and Computing",
      "journal": {
        "name": "Proceedings of the Twenty-fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
      ]
    },
    {
      "paperId": "5bce0f27907abf2a329c2517269eab304fc25d51",
      "title": "Bringing practical statistical science to AI and predictive model fairness testing",
      "authors": [
        {
          "name": "Victor S. Y. Lo",
          "authorId": "2315557851"
        },
        {
          "name": "S. Datta",
          "authorId": "2315563474"
        },
        {
          "name": "Youssouf Salami",
          "authorId": "2315558904"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 2,
      "doi": "10.1007/s43681-024-00518-2",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/5bce0f27907abf2a329c2517269eab304fc25d51",
      "venue": "AI and Ethics",
      "journal": {
        "name": "AI and Ethics",
        "pages": "2149 - 2164",
        "volume": "5"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b8a5d0ac9e0bb61646defcd009999731f6bf7964",
      "title": "Impacts of Individual Fairness on Group Fairness from the Perspective of Generalized Entropy",
      "authors": [
        {
          "name": "Young-Hwan Jin",
          "authorId": "104268376"
        },
        {
          "name": "Jio Gim",
          "authorId": "2136705993"
        },
        {
          "name": "Tae-Jin Lee",
          "authorId": "2156508141"
        },
        {
          "name": "Young-Joo Suh",
          "authorId": "2291254531"
        }
      ],
      "year": 2022,
      "abstract": "This paper investigates how the degree of group fairness changes when the degree of individual fairness is actively controlled. As a metric quantifying individual fairness, we consider generalized entropy (GE) recently introduced into machine learning community. To control the degree of individual fairness, we design a classification algorithm satisfying a given degree of individual fairness through an empirical risk minimization (ERM) with a fairness constraint specified in terms of GE. We show the PAC learnability of the fair ERM problem by proving that the true fairness degree does not deviate much from an empirical one with high probability for finite VC dimension if the sample size is big enough. Our experiments show that strengthening individual fairness degree does not always lead to enhancement of group fairness.",
      "citationCount": 1,
      "doi": null,
      "arxivId": "2202.11966",
      "url": "https://www.semanticscholar.org/paper/b8a5d0ac9e0bb61646defcd009999731f6bf7964",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "d3f12b6410486d004d9119f9745b159dfa461de3",
      "title": "The Role of AI and ML in Alternative Credit Scoring in Fintech Lending",
      "authors": [
        {
          "name": "Dr Preeti S Desai",
          "authorId": "2355018780"
        }
      ],
      "year": 2025,
      "abstract": "This research examines the influence of artificial intelligence (AI) and machine learning (ML) on alternative credit scoring in fintech lending, highlighting the effect of alternative data on financial inclusion and confidence in AI-based credit assessments.\u00a0 The study, using a sample size of 384 and evaluated via quantitative methodologies with SPSS and Structural Equation Modeling (SEM), emphasizes the need for ethical frameworks and transparent governance to guarantee fairness and accountability in AI-driven credit assessments. Conventional credit assessment techniques often exclude persons with sparse credit histories, whereas AI/ML-driven models use digital footprints, utility payments, and behavioral data to provide a more thorough credit review.\u00a0 The results demonstrate that alternative data substantially improves the perceived precision of AI/ML credit rating, resulting in heightened confidence in automated conclusions.\u00a0 Moreover, increased consumer understanding of AI/ML enhances trust in digital lending systems, hence promoting broader financial inclusion.",
      "citationCount": 1,
      "doi": "10.52783/jisem.v10i31s.4956",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/d3f12b6410486d004d9119f9745b159dfa461de3",
      "venue": "Journal of Information Systems Engineering & Management",
      "journal": {
        "name": "Journal of Information Systems Engineering and Management"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "0a12b6670bef76099e8ea30798a6a00b50cb164e",
      "title": "Measures of Disparity and their Efficient Estimation",
      "authors": [
        {
          "name": "Harvineet Singh",
          "authorId": "20400898"
        },
        {
          "name": "R. Chunara",
          "authorId": "3144230"
        }
      ],
      "year": 2023,
      "abstract": "Quantifying disparities, that is differences in outcomes among population groups, is an important task in public health, economics, and increasingly in machine learning. In this work, we study the question of how to collect data to measure disparities. The field of survey statistics provides extensive guidance on sample sizes necessary to accurately estimate quantities such as averages. However, there is limited guidance for estimating disparities. We consider a broad class of disparity metrics including those used in machine learning for measuring fairness of model outputs. For each metric, we derive the number of samples to be collected per group that increases the precision of disparity estimates given a fixed data collection budget. We also provide sample size calculations for hypothesis tests that check for significant disparities. Our methods can be used to determine sample sizes for fairness evaluations. We validate the methods on two nationwide surveys, used for understanding population-level attributes like employment and health, and a prediction model. Absent a priori information on the groups, we find that equally sampling the groups typically performs well.",
      "citationCount": 4,
      "doi": "10.1145/3600211.3604697",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/0a12b6670bef76099e8ea30798a6a00b50cb164e",
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book",
        "Review"
      ]
    },
    {
      "paperId": "5b282ec3412b5221e40226b349fe42601e0765e8",
      "title": "Multi-group Agnostic PAC Learnability",
      "authors": [
        {
          "name": "G. Rothblum",
          "authorId": "2551824"
        },
        {
          "name": "G. Yona",
          "authorId": "36825265"
        }
      ],
      "year": 2021,
      "abstract": "An agnostic PAC learning algorithm finds a predictor that is competitive with the best predictor in a benchmark hypothesis class, where competitiveness is measured with respect to a given loss function. However, its predictions might be quite sub-optimal for structured subgroups of individuals, such as protected demographic groups. Motivated by such fairness concerns, we study \u201cmulti-group agnostic PAC learnability\u201d: fixing a measure of loss, a benchmark classH and a (potentially) rich collection of subgroups G, the objective is to learn a single predictor such that the loss experienced by every group g \u2208 G is not much larger than the best possible loss for this group within H. Under natural conditions, we provide a characterization of the loss functions for which such a predictor is guaranteed to exist. For any such loss function we construct a learning algorithm whose sample complexity is logarithmic in the size of the collection G. Our results unify and extend previous positive and negative results from the multi-group fairness literature, which applied for specific loss functions.",
      "citationCount": 43,
      "doi": null,
      "arxivId": "2105.09989",
      "url": "https://www.semanticscholar.org/paper/5b282ec3412b5221e40226b349fe42601e0765e8",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "pages": "9107-9115"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "e6ab6e5fc10f1c9c4ceb96c8785e5cd8a0e86127",
      "title": "Machine learning strategies for small sample size in materials science",
      "authors": [
        {
          "name": "Qiuling Tao",
          "authorId": "2269804776"
        },
        {
          "name": "Jinxin Yu",
          "authorId": "2048328899"
        },
        {
          "name": "Xiangyu Mu",
          "authorId": "2202409447"
        },
        {
          "name": "Xue Jia",
          "authorId": "2375029079"
        },
        {
          "name": "Rongpei Shi",
          "authorId": "2266511139"
        },
        {
          "name": "Zhifu Yao",
          "authorId": "2117803122"
        },
        {
          "name": "Cuiping Wang",
          "authorId": "2266706083"
        },
        {
          "name": "Haijun Zhang",
          "authorId": "2339959926"
        },
        {
          "name": "Xingjun Liu",
          "authorId": "8015855"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 11,
      "doi": "10.1007/s40843-024-3204-5",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e6ab6e5fc10f1c9c4ceb96c8785e5cd8a0e86127",
      "venue": "Science China Materials",
      "journal": {
        "name": "Science China Materials",
        "pages": "387 - 405",
        "volume": "68"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "e9037b13d85e50437c1f1934463a4f8770357c56",
      "title": "Evaluation of a decided sample size in machine learning applications",
      "authors": [
        {
          "name": "Daniyal Rajput",
          "authorId": "70394305"
        },
        {
          "name": "Wei-Jen Wang",
          "authorId": "2165887424"
        },
        {
          "name": "Chun-Chuan Chen",
          "authorId": "1711985"
        }
      ],
      "year": 2023,
      "abstract": "Background An appropriate sample size is essential for obtaining a precise and reliable outcome of a study. In machine learning (ML), studies with inadequate samples suffer from overfitting of data and have a lower probability of producing true effects, while the increment in sample size increases the accuracy of prediction but may not cause a significant change after a certain sample size. Existing statistical approaches using standardized mean difference, effect size, and statistical power for determining sample size are potentially biased due to miscalculations or lack of experimental details. This study aims to design criteria for evaluating sample size in ML studies. We examined the average and grand effect sizes and the performance of five ML methods using simulated datasets and three real datasets to derive the criteria for sample size. We systematically increase the sample size, starting from 16, by randomly sampling and examine the impact of sample size on classifiers\u2019 performance and both effect sizes. Tenfold cross-validation was used to quantify the accuracy. Results The results demonstrate that the effect sizes and the classification accuracies increase while the variances in effect sizes shrink with the increment of samples when the datasets have a good discriminative power between two classes. By contrast, indeterminate datasets had poor effect sizes and classification accuracies, which did not improve by increasing sample size in both simulated and real datasets. A good dataset exhibited a significant difference in average and grand effect sizes. We derived two criteria based on the above findings to assess a decided sample size by combining the effect size and the ML accuracy. The sample size is considered suitable when it has appropriate effect sizes (\u2265\u20090.5) and ML accuracy (\u2265\u200980%). After an appropriate sample size, the increment in samples will not benefit as it will not significantly change the effect size and accuracy, thereby resulting in a good cost-benefit ratio. Conclusion We believe that these practical criteria can be used as a reference for both the authors and editors to evaluate whether the selected sample size is adequate for a study.",
      "citationCount": 283,
      "doi": "10.1186/s12859-023-05156-9",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e9037b13d85e50437c1f1934463a4f8770357c56",
      "venue": "BMC Bioinformatics",
      "journal": {
        "name": "BMC Bioinformatics",
        "volume": "24"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a83c95f28826f7a2a13cea82a9147858a867b04e",
      "title": "Sample size and predictive performance of machine learning methods with survival data: A simulation study",
      "authors": [
        {
          "name": "G. Infante",
          "authorId": "13127697"
        },
        {
          "name": "Rosalba Miceli",
          "authorId": "2254845585"
        },
        {
          "name": "F. Ambrogi",
          "authorId": "1767247"
        }
      ],
      "year": 2023,
      "abstract": "Prediction models are increasingly developed and used in diagnostic and prognostic studies, where the use of machine learning (ML) methods is becoming more and more popular over traditional regression techniques. For survival outcomes the Cox proportional hazards model is generally used and it has been proven to achieve good prediction performances with few strong covariates. The possibility to improve the model performance by including nonlinearities, covariate interactions and time\u2010varying effects while controlling for overfitting must be carefully considered during the model building phase. On the other hand, ML techniques are able to learn complexities from data at the cost of hyper\u2010parameter tuning and interpretability. One aspect of special interest is the sample size needed for developing a survival prediction model. While there is guidance when using traditional statistical models, the same does not apply when using ML techniques. This work develops a time\u2010to\u2010event simulation framework to evaluate performances of Cox regression compared, among others, to tuned random survival forest, gradient boosting, and neural networks at varying sample sizes. Simulations were based on replications of subjects from publicly available databases, where event times were simulated according to a Cox model with nonlinearities on continuous variables and time\u2010varying effects and on the SEER registry data.",
      "citationCount": 24,
      "doi": "10.1002/sim.9931",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a83c95f28826f7a2a13cea82a9147858a867b04e",
      "venue": "Statistics in Medicine",
      "journal": {
        "name": "Statistics in Medicine",
        "pages": "5657 - 5675",
        "volume": "42"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c9f81ff5b1632e6c6d8f56fbfcaf20b989fdfc21",
      "title": "Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Sample Size Estimation and Reducing Overfitting",
      "authors": [
        {
          "name": "Hamzeh Ghasemzadeh",
          "authorId": "144600888"
        },
        {
          "name": "Robert E. Hillman",
          "authorId": "2249491003"
        },
        {
          "name": "Daryush D. Mehta",
          "authorId": "3051832"
        }
      ],
      "year": 2023,
      "abstract": "PURPOSE\nMany studies using machine learning (ML) in speech, language, and hearing sciences rely upon cross-validations with single data splitting. This study's first purpose is to provide quantitative evidence that would incentivize researchers to instead use the more robust data splitting method of nested k-fold cross-validation. The second purpose is to present methods and MATLAB code to perform power analysis for ML-based analysis during the design of a study.\n\n\nMETHOD\nFirst, the significant impact of different cross-validations on ML outcomes was demonstrated using real-world clinical data. Then, Monte Carlo simulations were used to quantify the interactions among the employed cross-validation method, the discriminative power of features, the dimensionality of the feature space, the dimensionality of the model, and the sample size. Four different cross-validation methods (single holdout, 10-fold, train-validation-test, and nested 10-fold) were compared based on the statistical power and confidence of the resulting ML models. Distributions of the null and alternative hypotheses were used to determine the minimum required sample size for obtaining a statistically significant outcome (5% significance) with 80% power. Statistical confidence of the model was defined as the probability of correct features being selected for inclusion in the final model.\n\n\nRESULTS\nML models generated based on the single holdout method had very low statistical power and confidence, leading to overestimation of classification accuracy. Conversely, the nested 10-fold cross-validation method resulted in the highest statistical confidence and power while also providing an unbiased estimate of accuracy. The required sample size using the single holdout method could be 50% higher than what would be needed if nested k-fold cross-validation were used. Statistical confidence in the model based on nested k-fold cross-validation was as much as four times higher than the confidence obtained with the single holdout-based model. A computational model, MATLAB code, and lookup tables are provided to assist researchers with estimating the minimum sample size needed during study design.\n\n\nCONCLUSION\nThe adoption of nested k-fold cross-validation is critical for unbiased and robust ML studies in the speech, language, and hearing sciences.\n\n\nSUPPLEMENTAL MATERIAL\nhttps://doi.org/10.23641/asha.25237045.",
      "citationCount": 24,
      "doi": "10.1044/2023_JSLHR-23-00273",
      "arxivId": "2308.11197",
      "url": "https://www.semanticscholar.org/paper/c9f81ff5b1632e6c6d8f56fbfcaf20b989fdfc21",
      "venue": "Journal of Speech, Language and Hearing Research",
      "journal": {
        "name": "Journal of speech, language, and hearing research : JSLHR",
        "pages": "\n          1-29\n        "
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "fbf9812f29156024ec693b4633a21303eead309d",
      "title": "Machine learning algorithm validation with a limited sample size",
      "authors": [
        {
          "name": "A. Vabalas",
          "authorId": "8452323"
        },
        {
          "name": "E. Gowen",
          "authorId": "2283643"
        },
        {
          "name": "E. Poliakoff",
          "authorId": "2910758"
        },
        {
          "name": "A. Casson",
          "authorId": "1807736"
        }
      ],
      "year": 2019,
      "abstract": "Advances in neuroimaging, genomic, motion tracking, eye-tracking and many other technology-based data collection methods have led to a torrent of high dimensional datasets, which commonly have a small number of samples because of the intrinsic high cost of data collection involving human participants. High dimensional data with a small number of samples is of critical importance for identifying biomarkers and conducting feasibility and pilot work, however it can lead to biased machine learning (ML) performance estimates. Our review of studies which have applied ML to predict autistic from non-autistic individuals showed that small sample size is associated with higher reported classification accuracy. Thus, we have investigated whether this bias could be caused by the use of validation methods which do not sufficiently control overfitting. Our simulations show that K-fold Cross-Validation (CV) produces strongly biased performance estimates with small sample sizes, and the bias is still evident with sample size of 1000. Nested CV and train/test split approaches produce robust and unbiased performance estimates regardless of sample size. We also show that feature selection if performed on pooled training and testing data is contributing to bias considerably more than parameter tuning. In addition, the contribution to bias by data dimensionality, hyper-parameter space and number of CV folds was explored, and validation methods were compared with discriminable data. The results suggest how to design robust testing methodologies when working with small datasets and how to interpret the results of other studies based on what validation method was used.",
      "citationCount": 1218,
      "doi": "10.1371/journal.pone.0224365",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/fbf9812f29156024ec693b4633a21303eead309d",
      "venue": "PLoS ONE",
      "journal": {
        "name": "PLoS ONE",
        "volume": "14"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "503d16edd79db90f5b9b9f0d967b8bf9a84471f3",
      "title": "Machine learning outperformed logistic regression classification even with limit sample size: A model to predict pediatric HIV mortality and clinical progression to AIDS",
      "authors": [
        {
          "name": "Sara Dom\u00ednguez-Rodr\u00edguez",
          "authorId": "1398866389"
        },
        {
          "name": "Miquel Serna-Pascual",
          "authorId": "1918161076"
        },
        {
          "name": "Andrea Oletto",
          "authorId": "1410609571"
        },
        {
          "name": "S. Barnabas",
          "authorId": "49573969"
        },
        {
          "name": "Peter Zuidewind",
          "authorId": "79288326"
        },
        {
          "name": "Els Dobbels",
          "authorId": "8426681"
        },
        {
          "name": "S. Danaviah",
          "authorId": "3473401"
        },
        {
          "name": "O. Behuhuma",
          "authorId": "2047820827"
        },
        {
          "name": "M. G. Lain",
          "authorId": "1471924069"
        },
        {
          "name": "P. Vaz",
          "authorId": "143948583"
        },
        {
          "name": "Sheila Fern\u00e1ndez-Luis",
          "authorId": "1435275435"
        },
        {
          "name": "T. Nhampossa",
          "authorId": "3511993"
        },
        {
          "name": "E. L\u00f3pez-Varela",
          "authorId": "1398466006"
        },
        {
          "name": "K. Otwombe",
          "authorId": "3675611"
        },
        {
          "name": "A. Liberty",
          "authorId": "6778034"
        },
        {
          "name": "A. Violari",
          "authorId": "3813176"
        },
        {
          "name": "A. Maiga",
          "authorId": "6222017"
        },
        {
          "name": "Paolo Rossi",
          "authorId": "2250074010"
        },
        {
          "name": "C. Giaquinto",
          "authorId": "145790667"
        },
        {
          "name": "L. Kuhn",
          "authorId": "40702642"
        },
        {
          "name": "P. Rojo",
          "authorId": "104947646"
        },
        {
          "name": "A. Tagarro",
          "authorId": "16324659"
        }
      ],
      "year": 2022,
      "abstract": "Logistic regression (LR) is the most common prediction model in medicine. In recent years, supervised machine learning (ML) methods have gained popularity. However, there are many concerns about ML utility for small sample sizes. In this study, we aim to compare the performance of 7 algorithms in the prediction of 1-year mortality and clinical progression to AIDS in a small cohort of infants living with HIV from South Africa and Mozambique. The data set (n = 100) was randomly split into 70% training and 30% validation set. Seven algorithms (LR, Random Forest (RF), Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Na\u00efve Bayes (NB), Artificial Neural Network (ANN), and Elastic Net) were compared. The variables included as predictors were the same across the models including sociodemographic, virologic, immunologic, and maternal status features. For each of the models, a parameter tuning was performed to select the best-performing hyperparameters using 5 times repeated 10-fold cross-validation. A confusion-matrix was built to assess their accuracy, sensitivity, and specificity. RF ranked as the best algorithm in terms of accuracy (82,8%), sensitivity (78%), and AUC (0,73). Regarding specificity and sensitivity, RF showed better performance than the other algorithms in the external validation and the highest AUC. LR showed lower performance compared with RF, SVM, or KNN. The outcome of children living with perinatally acquired HIV can be predicted with considerable accuracy using ML algorithms. Better models would benefit less specialized staff in limited resources countries to improve prompt referral in case of high-risk clinical progression.",
      "citationCount": 25,
      "doi": "10.1371/journal.pone.0276116",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/503d16edd79db90f5b9b9f0d967b8bf9a84471f3",
      "venue": "PLoS ONE",
      "journal": {
        "name": "PLoS ONE",
        "volume": "17"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c81a34828fe1bae635f309b636fad1953bfaf9ef",
      "title": "Data vs. Model Machine Learning Fairness Testing: An Empirical Study",
      "authors": [
        {
          "name": "Arumoy Shome",
          "authorId": "1380220231"
        },
        {
          "name": "Lu\u00eds Cruz",
          "authorId": "2267243294"
        },
        {
          "name": "Arie van Deursen",
          "authorId": "10734708"
        }
      ],
      "year": 2024,
      "abstract": "Although several fairness definitions and bias mitigation techniques exist in the literature, all existing solutions evaluate fairness of Machine Learning (ML) systems after the training stage. In this paper, we take the first steps towards evaluating a more holistic approach by testing for fairness both before and after model training. We evaluate the effectiveness of the proposed approach and position it within the ML development lifecycle, using an empirical analysis of the relationship between model dependent and independent fairness metrics. The study uses 2 fairness metrics, 4 ML algorithms, 5 real-world datasets and 1600 fairness evaluation cycles. We find a linear relationship between data and model fairness metrics when the distribution and the size of the training data changes. Our results indicate that testing for fairness prior to training can be a \u201ccheap\u201d and effective means of catching a biased data collection process early; detecting data drifts in production systems and minimising execution of full training cycles thus reducing development time and costs.",
      "citationCount": 6,
      "doi": "10.1145/3643786.3648022",
      "arxivId": "2401.07697",
      "url": "https://www.semanticscholar.org/paper/c81a34828fe1bae635f309b636fad1953bfaf9ef",
      "venue": "Workshop on Deep Learning for Testing and Testing for Deep Learning",
      "journal": {
        "name": "2024 IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest)",
        "pages": "1-8"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "45b814004b99da8e8c2bd1afb5935e5503e7993e",
      "title": "Radiomics machine learning study with a small sample size: Single random training-test set split may lead to unreliable results",
      "authors": [
        {
          "name": "Chansik An",
          "authorId": "6934404"
        },
        {
          "name": "Y. Park",
          "authorId": "49819192"
        },
        {
          "name": "S. Ahn",
          "authorId": "50487794"
        },
        {
          "name": "Kyunghwa Han",
          "authorId": "152742026"
        },
        {
          "name": "Hwiyoung Kim",
          "authorId": "11661412"
        },
        {
          "name": "Seung-Koo Lee",
          "authorId": "47089801"
        }
      ],
      "year": 2021,
      "abstract": "This study aims to determine how randomly splitting a dataset into training and test sets affects the estimated performance of a machine learning model and its gap from the test performance under different conditions, using real-world brain tumor radiomics data. We conducted two classification tasks of different difficulty levels with magnetic resonance imaging (MRI) radiomics features: (1) \u201cSimple\u201d task, glioblastomas [n = 109] vs. brain metastasis [n = 58] and (2) \u201cdifficult\u201d task, low- [n = 163] vs. high-grade [n = 95] meningiomas. Additionally, two undersampled datasets were created by randomly sampling 50% from these datasets. We performed random training-test set splitting for each dataset repeatedly to create 1,000 different training-test set pairs. For each dataset pair, the least absolute shrinkage and selection operator model was trained and evaluated using various validation methods in the training set, and tested in the test set, using the area under the curve (AUC) as an evaluation metric. The AUCs in training and testing varied among different training-test set pairs, especially with the undersampled datasets and the difficult task. The mean (\u00b1standard deviation) AUC difference between training and testing was 0.039 (\u00b10.032) for the simple task without undersampling and 0.092 (\u00b10.071) for the difficult task with undersampling. In a training-test set pair with the difficult task without undersampling, for example, the AUC was high in training but much lower in testing (0.882 and 0.667, respectively); in another dataset pair with the same task, however, the AUC was low in training but much higher in testing (0.709 and 0.911, respectively). When the AUC discrepancy between training and test, or generalization gap, was large, none of the validation methods helped sufficiently reduce the generalization gap. Our results suggest that machine learning after a single random training-test set split may lead to unreliable results in radiomics studies especially with small sample sizes.",
      "citationCount": 70,
      "doi": "10.1371/journal.pone.0256152",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/45b814004b99da8e8c2bd1afb5935e5503e7993e",
      "venue": "PLoS ONE",
      "journal": {
        "name": "PLoS ONE",
        "volume": "16"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6881dfe1e5b3d1666b4baf357ed818d5d58d1a96",
      "title": "Sample Size Analysis for Machine Learning Clinical Validation Studies",
      "authors": [
        {
          "name": "Daniel Goldenholz",
          "authorId": "2321160280"
        }
      ],
      "year": 2021,
      "abstract": "OBJECTIVE: Before integrating new machine learning (ML) into clinical practice, algorithms must undergo validation. Validation studies require sample size estimates. Unlike hypothesis testing studies seeking a p-value, the goal of validating predictive models is obtaining estimates of model performance. Our aim was to provide a standardized, data distribution- and model-agnostic approach to sample size calculations for validation studies of predictive ML models. MATERIALS AND METHODS: Sample Size Analysis for Machine Learning (SSAML) was tested in three previously published models: brain age to predict mortality (Cox Proportional Hazard), COVID hospitalization risk prediction (ordinal regression), and seizure risk forecasting (deep learning). The SSAML steps are: 1) Specify performance metrics for model discrimination and calibration. For discrimination, we use area under the receiver operating curve (AUC) for classification and Harrell's C-statistic for survival models. For calibration, we employ calibration slope and calibration-in-the-large. 2) Specify the required precision and accuracy (<=0.5 normalized confidence interval width and +/-5% accuracy). 3) Specify the required coverage probability (95%). 4) For increasing sample sizes, calculate the expected precision and bias that is achievable. 5) Choose the minimum sample size that meets all requirements. RESULTS: Minimum sample sizes were obtained in each dataset using standardized criteria. DISCUSSION: SSAML provides a formal expectation of precision and accuracy at a desired confidence level. CONCLUSION: SSAML is open-source and agnostics to data type and ML model. It can be used for clinical validation studies of ML models.",
      "citationCount": 42,
      "doi": "10.3390/biomedicines11030685",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6881dfe1e5b3d1666b4baf357ed818d5d58d1a96",
      "venue": "medRxiv",
      "journal": {
        "name": "Biomedicines",
        "volume": "11"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 25,
  "errors": []
}
