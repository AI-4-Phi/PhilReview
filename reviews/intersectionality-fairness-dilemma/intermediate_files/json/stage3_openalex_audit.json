{
  "status": "success",
  "source": "openalex",
  "query": "fairness auditing machine learning",
  "results": [
    {
      "openalex_id": "W4224312515",
      "doi": "10.1145/3485447.3512244",
      "title": "Fairness Audit of Machine Learning Models with Confidential Computing",
      "authors": [
        {
          "name": "Saerom Park",
          "openalex_id": "A5022945584",
          "orcid": "https://orcid.org/0000-0002-2687-7105",
          "institutions": [
            "Sungshin Women's University"
          ]
        },
        {
          "name": "Seongmin Kim",
          "openalex_id": "A5100358165",
          "orcid": "https://orcid.org/0000-0002-8183-0641",
          "institutions": [
            "Sungshin Women's University"
          ]
        },
        {
          "name": "Yeon-sup Lim",
          "openalex_id": "A5101628313",
          "orcid": "https://orcid.org/0000-0001-7647-6185",
          "institutions": [
            "Sungshin Women's University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-04-25",
      "abstract": "Algorithmic discrimination is one of the significant concerns in applying machine learning models to a real-world system. Many researchers have focused on developing fair machine learning algorithms without discrimination based on legally protected attributes. However, the existing research has barely explored various security issues that can occur while evaluating model fairness and verifying fair models. In this study, we propose a fairness audit framework that assesses the fairness of ML algorithms while addressing potential security issues such as data privacy, model secrecy, and trustworthiness. To this end, our proposed framework utilizes confidential computing and builds a chain of trust through enclave attestation primitives combined with public scrutiny and state-of-the-art software-based security techniques, enabling fair ML models to be securely certified and clients to verify a certified one. Our micro-benchmarks on various ML models and real-world datasets show the feasibility of the fairness certification implemented with Intel SGX in practice. In addition, we analyze the impact of data poisoning, which is an additional threat during data collection for fairness auditing. Based on the analysis, we illustrate the theoretical curves of fairness gap and minimal group size and the empirical results of fairness certification on poisoned datasets.",
      "cited_by_count": 19,
      "type": "article",
      "source": {
        "name": "Proceedings of the ACM Web Conference 2022",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Privacy-Preserving Technologies in Data",
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 31,
      "url": "https://openalex.org/W4224312515"
    },
    {
      "openalex_id": "W4323341796",
      "doi": "10.14569/ijacsa.2023.0140294",
      "title": "Privacy-Preserving and Trustless Verifiable Fairness Audit of Machine Learning Models",
      "authors": [
        {
          "name": "Tang Gui",
          "openalex_id": "A5110373948",
          "institutions": [
            "Jinan University"
          ]
        },
        {
          "name": "Wuzheng Tan",
          "openalex_id": "A5041555592",
          "institutions": [
            "Jinan University"
          ]
        },
        {
          "name": "Mei Cai",
          "openalex_id": "A5111724618",
          "orcid": "https://orcid.org/0000-0001-9648-0878",
          "institutions": [
            "Jinan University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-01",
      "abstract": "In the big data era, machine learning has devel-oped prominently and is widely used in real-world systems. Yet, machine learning raises fairness concerns, which incurs discrimination against groups determined by sensitive attributes such as gender and race. Many researchers have focused on developing fairness audit technique of machine learning model that enable users to protect themselves from discrimination. Existing solutions, however, rely on additional external trust as-sumptions, either on third-party entities or external components, that significantly lower the security. In this study, we propose a trustless verifiable fairness audit framework that assesses the fairness of ML algorithms while addressing potential security issues such as data privacy, model secrecy, and trustworthiness. With succinctness and non-interactive of zero knowledge proof, our framework not only guarantees audit integrity, but also clearly enhance security, enabling fair ML models to be publicly auditable and any client to verify audit results without extra trust assumption. Our evaluation on various machine learning models and real-world datasets shows that our framework achieves practical performance.",
      "cited_by_count": 7,
      "type": "article",
      "source": {
        "name": "International Journal of Advanced Computer Science and Applications",
        "type": "journal",
        "issn": [
          "2156-5570",
          "2158-107X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "http://thesai.org/Downloads/Volume14No2/Paper_94-Privacy_Preserving_and_Trustless_Verifiable_Fairness_Audit.pdf"
      },
      "topics": [
        "Privacy-Preserving Technologies in Data",
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 84,
      "url": "https://openalex.org/W4323341796"
    },
    {
      "openalex_id": "W3008694996",
      "doi": "10.1177/2053951719897945",
      "title": "Algorithmic content moderation: Technical and political challenges in the automation of platform governance",
      "authors": [
        {
          "name": "Robert Gorwa",
          "openalex_id": "A5057098134",
          "orcid": "https://orcid.org/0000-0002-4891-5053",
          "institutions": [
            "University of Oxford"
          ]
        },
        {
          "name": "Reuben Binns",
          "openalex_id": "A5101687507",
          "orcid": "https://orcid.org/0000-0003-4718-2190",
          "institutions": [
            "University of Oxford"
          ]
        },
        {
          "name": "Christian Katzenbach",
          "openalex_id": "A5020105431",
          "orcid": "https://orcid.org/0000-0003-1897-2783",
          "institutions": [
            "Alexander von Humboldt Institute for Internet and Society"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-01-01",
      "abstract": "As government pressure on major technology companies builds, both firms and legislators are searching for technical solutions to difficult platform governance puzzles such as hate speech and misinformation. Automated hash-matching and predictive machine learning tools \u2013 what we define here as algorithmic moderation systems \u2013 are increasingly being deployed to conduct content moderation at scale by major platforms for user-generated content such as Facebook, YouTube and Twitter. This article provides an accessible technical primer on how algorithmic moderation works; examines some of the existing automated tools used by major platforms to handle copyright infringement, terrorism and toxic speech; and identifies key political and ethical issues for these systems as the reliance on them grows. Recent events suggest that algorithmic moderation has become necessary to manage growing public expectations for increased platform responsibility, safety and security on the global stage; however, as we demonstrate, these systems remain opaque, unaccountable and poorly understood. Despite the potential promise of algorithms or \u2018AI\u2019, we show that even \u2018well optimized\u2019 moderation systems could exacerbate, rather than relieve, many existing problems with content policy as enacted by platforms for three main reasons: automated moderation threatens to (a) further increase opacity, making a famously non-transparent set of practices even more difficult to understand or audit, (b) further complicate outstanding issues of fairness and justice in large-scale sociotechnical systems and (c) re-obscure the fundamentally political nature of speech decisions being executed at scale.",
      "cited_by_count": 815,
      "type": "article",
      "source": {
        "name": "Big Data & Society",
        "type": "journal",
        "issn": [
          "2053-9517"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://journals.sagepub.com/doi/pdf/10.1177/2053951719897945"
      },
      "topics": [
        "Hate Speech and Cyberbullying Detection",
        "Ethics and Social Impacts of AI",
        "Social Media and Politics"
      ],
      "referenced_works_count": 14,
      "url": "https://openalex.org/W3008694996"
    },
    {
      "openalex_id": "W4388451743",
      "doi": "10.1007/s10207-023-00774-z",
      "title": "Fairness as a Service (FaaS): verifiable and privacy-preserving fairness auditing of machine learning systems",
      "authors": [
        {
          "name": "Ehsan Toreini",
          "openalex_id": "A5029147151",
          "orcid": "https://orcid.org/0000-0002-5172-2957",
          "institutions": [
            "University of Surrey"
          ]
        },
        {
          "name": "Maryam Mehrnezhad",
          "openalex_id": "A5023973244",
          "orcid": "https://orcid.org/0000-0002-4223-6885",
          "institutions": [
            "Royal Holloway University of London"
          ]
        },
        {
          "name": "Aad van Moorsel",
          "openalex_id": "A5072969450",
          "orcid": "https://orcid.org/0000-0001-7233-6943",
          "institutions": [
            "University of Birmingham"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-11-07",
      "abstract": "Abstract Providing trust in machine learning (ML) systems and their fairness is a socio-technical challenge, and while the use of ML continues to rise, there is lack of adequate processes and governance practices to assure their fairness. In this paper, we propose FaaS, a novel privacy-preserving, end-to-end verifiable solution, that audits the algorithmic fairness of ML systems. FaaS offers several features, which are absent from previous designs. The FAAS protocol is model-agnostic and independent of specific fairness metrics and can be utilised as a service by multiple stakeholders. FAAS uses zero knowledge proofs to assure the well-formedness of the cryptograms and provenance in the steps of the protocol. We implement a proof of concept of the FaaS architecture and protocol using off-the-shelf hardware, software, and datasets and run experiments to demonstrate its practical feasibility and to analyse its performance and scalability. Our experiments confirm that our proposed protocol is scalable to large-scale auditing scenarios (e.g. over 1000 participants) and secure against various attack vectors.",
      "cited_by_count": 3,
      "type": "article",
      "source": {
        "name": "International Journal of Information Security",
        "type": "journal",
        "issn": [
          "1615-5262",
          "1615-5270"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s10207-023-00774-z.pdf"
      },
      "topics": [
        "Privacy-Preserving Technologies in Data",
        "Adversarial Robustness in Machine Learning",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 34,
      "url": "https://openalex.org/W4388451743"
    },
    {
      "openalex_id": "W4386363446",
      "doi": "10.1007/978-3-031-36938-4_10",
      "title": "Auditing Algorithmic Fairness in Machine Learning for Health with Severity-Based LOGAN",
      "authors": [
        {
          "name": "Anaelia Ovalle",
          "openalex_id": "A5064234845",
          "orcid": "https://orcid.org/0000-0002-0531-7520",
          "institutions": [
            "University of California, Los Angeles"
          ]
        },
        {
          "name": "Sunipa Dev",
          "openalex_id": "A5090145147",
          "orcid": "https://orcid.org/0000-0002-6647-9662",
          "institutions": [
            "University of California, Los Angeles"
          ]
        },
        {
          "name": "Jieyu Zhao",
          "openalex_id": "A5066282713",
          "orcid": "https://orcid.org/0000-0002-1013-557X",
          "institutions": [
            "University of California, Los Angeles"
          ]
        },
        {
          "name": "Majid Sarrafzadeh",
          "openalex_id": "A5078132977",
          "orcid": "https://orcid.org/0000-0001-8407-8689",
          "institutions": [
            "University of California, Los Angeles"
          ]
        },
        {
          "name": "Kai-Wei Chang",
          "openalex_id": "A5087096372",
          "orcid": "https://orcid.org/0000-0001-5365-0072",
          "institutions": [
            "University of California, Los Angeles"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-01",
      "abstract": null,
      "cited_by_count": 3,
      "type": "book-chapter",
      "source": {
        "name": "Studies in computational intelligence",
        "type": "book series",
        "issn": [
          "1860-949X",
          "1860-9503"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Chronic Disease Management Strategies",
        "Machine Learning in Healthcare",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 37,
      "url": "https://openalex.org/W4386363446"
    },
    {
      "openalex_id": "W3006437051",
      "doi": "10.1109/vast47406.2019.8986948",
      "title": "FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning",
      "authors": [
        {
          "name": "\u00c1ngel Alexander Cabrera",
          "openalex_id": "A5060162507",
          "orcid": "https://orcid.org/0000-0003-0348-3362",
          "institutions": [
            "Georgia Institute of Technology"
          ]
        },
        {
          "name": "Will Epperson",
          "openalex_id": "A5041694845",
          "orcid": "https://orcid.org/0000-0002-2745-4315",
          "institutions": [
            "Georgia Institute of Technology"
          ]
        },
        {
          "name": "Fred Hohman",
          "openalex_id": "A5034842879",
          "institutions": [
            "Georgia Institute of Technology"
          ]
        },
        {
          "name": "Minsuk Kahng",
          "openalex_id": "A5042350842",
          "orcid": "https://orcid.org/0000-0002-0291-6026",
          "institutions": [
            "Georgia Institute of Technology"
          ]
        },
        {
          "name": "Jamie Morgenstern",
          "openalex_id": "A5045592180",
          "orcid": "https://orcid.org/0000-0003-3753-8405",
          "institutions": [
            "Georgia Institute of Technology"
          ]
        },
        {
          "name": "Duen Horng Chau",
          "openalex_id": "A5020153026",
          "orcid": "https://orcid.org/0000-0001-9824-3323",
          "institutions": [
            "Georgia Institute of Technology"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-04-10",
      "abstract": "The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis' coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.",
      "cited_by_count": 175,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/1904.05419"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Embodied and Extended Cognition"
      ],
      "referenced_works_count": 40,
      "url": "https://openalex.org/W3006437051"
    },
    {
      "openalex_id": "W3023069697",
      "doi": "10.1016/s2589-7500(20)30065-0",
      "title": "Ethical limitations of algorithmic fairness solutions in health care machine learning",
      "authors": [
        {
          "name": "Melissa D. McCradden",
          "openalex_id": "A5063981686",
          "orcid": "https://orcid.org/0000-0002-6476-2165",
          "institutions": [
            "Hospital for Sick Children"
          ]
        },
        {
          "name": "Shalmali Joshi",
          "openalex_id": "A5035149567",
          "institutions": [
            "Vector Institute"
          ]
        },
        {
          "name": "Mjaye Mazwi",
          "openalex_id": "A5080855486",
          "orcid": "https://orcid.org/0000-0003-1345-5429",
          "institutions": [
            "Hospital for Sick Children"
          ]
        },
        {
          "name": "James A. Anderson",
          "openalex_id": "A5103410231",
          "institutions": [
            "Hospital for Sick Children"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-04-28",
      "abstract": "Artificial intelligence has exposed pernicious bias within health data that constitutes substantial ethical threat to the use of machine learning in medicine.1Char DS Shah NH Magnus D Implementing machine learning in health care\u2014addressing ethical challenges.N Engl J Med. 2018; 378: 981-983Crossref PubMed Scopus (445) Google Scholar, 2Obermeyer Z Powers B Vogeli C Mullainathan S Dissecting racial bias in an algorithm used to manage the health of populations.Science. 2019; 366: 447-453Crossref PubMed Scopus (1151) Google Scholar Solutions of algorithmic fairness have been developed to create neutral models: models designed to produce non-discriminatory predictions by constraining bias with respect to predicted outcomes for protected identities, such as race or gender.3Corbett-Davies S Goel S The measure and mismeasure of fairness: a critical review of fair machine learning.https://5harad.com/papers/fair-ml.pdfDate accessed: March 16, 2020Google Scholar These solutions can omit such variables from the model (widely regarded as ineffective and can increase discrimination), constrain it to ensure equal error rates across groups, derive outcomes that are independent of one's identity after controlling for the estimated risk of that outcome, or mathematically balance benefit and harm to all groups. The temptation to engineer ethics into algorithm design is immense and industry is increasingly pushing these solutions. In the health-care space, where the stakes could be higher, clinicians will integrate these models into their care, trusting the issue of bias has been sufficiently managed within the model. However, even if well recognised technical challenges are set aside,3Corbett-Davies S Goel S The measure and mismeasure of fairness: a critical review of fair machine learning.https://5harad.com/papers/fair-ml.pdfDate accessed: March 16, 2020Google Scholar, 4Marmot M Social determinants of health inequalities.Lancet. 2005; 365: 1099-1104Summary Full Text Full Text PDF PubMed Scopus (2858) Google Scholar framing fairness as a purely technical problem solvable by the inclusion of more data or accurate computations is ethically problematic. We highlight challenges to the ethical and empirical efficacy of solutions of algorithmic fairness that show risks of relying too heavily on the so called veneer of technical neutrality,5Benjamin R Assessing risk, automating racism.Science. 2019; 366: 421-422Crossref PubMed Scopus (97) Google Scholar which could exacerbate harms to vulnerable groups. Historically, algorithmic fairness has not accounted for complex causal relationships between biological, environmental, and social factors that give rise to differences in medical conditions across protected identities. Social determinants of health play an important role, particularly for risk models. Social and structural factors affect health across multiple intersecting identities,4Marmot M Social determinants of health inequalities.Lancet. 2005; 365: 1099-1104Summary Full Text Full Text PDF PubMed Scopus (2858) Google Scholar but the mechanism(s) by which social determinants affect health outcomes is not always well understood. Additional complications flow from the reality that difference does not always entail inequity. In some instances, it is appropriate to incorporate differences between identities because there is a reasonable presumption of causation. For example, biological differences between genders can affect the efficacy of pharmacological compounds; incorporating these differences into prescribing practices does not make those prescriptions unjust. However, incorporating non-causative factors into recommendations can propagate unequal treatment by reifying extant inequities and exacerbating their effects. We should not allow models to promote different standards of care according to protected identities that do not have a causative association with the outcome. Nevertheless, in many cases it is difficult to distinguish between acknowledging difference and propagating discrimination. Given the epistemic uncertainty surrounding the association between protected identities and health outcomes, the use of fairness solutions can create empirical challenges. Consider the case of heart attack symptoms among women.6Wenger NK Cardiovascular disease: the female heart is vulnerable. A call to action from the 10Q Report.Clin Cardiol. 2012; 35: 134-135Crossref PubMed Scopus (11) Google Scholar The under-representation of women (particularly women of colour) in research of heart health is now well recognised as problematic and directly affected uneven improvements in treatment of heart attacks between women and men. By tailoring health solutions to the majority (ie, referent) group, we inevitably fall short of helping all patients. Many algorithmic fairness solutions, in effect, replicate this problem by trying to fit the non-referent groups to that of the referent,7Friedler SA Scheidegger C Venkatasubramanian S On the (im)possibility of fairness.https://arxiv.org/pdf/1609.07236.pdfDate: Sept 23, 2019Date accessed: March 16, 2020Google Scholar, 8Barocas S Hardt M Narayanan A Fairness and machine learning: limitations and opportunities; 2018.Fairmlbook.orgDate: 2019Google Scholar ignoring heterogeneity and assuming that the latter represents a true underlying pattern. Another concern is disconnection between the patient's clinical trajectory and the fair prediction. Consider the implications at the point-of-care, a model, corrected for fairness, will predict that a patient will respond to a treatment as a patient in the referent class would. What happens when that patient does not have the predicted response? This difference between an idealised model and non-ideal, real-world behaviour affects metrics of model performance (eg, specificity, sensitivity) and clinical utility in practice. Moreover, the model has made an ineffective recommendation that could have obscured more relevant interventions to help that patient. If clinicians and patients believe that the mode has been rendered neutral, then any discrepancies between model prediction and the patient's true clinical state might be impossible to interpret. The result would be to camouflage persistent health inequalities. As such, fairness, operationalised by output metrics alone, is insufficient; real-world consequences should be carefully considered. Bias and ineffective solutions of algorithmic fairness threaten the ethical obligation to avoid or minimise harms to patients (non-maleficence; panel). Non-maleficence demands that any new clinical tool should be assessed for patient safety. For health-care machine learning, safety should include awareness of model limitations with respect to protected identities and social determinants of health. Considerations of justice requires that implemented models do not exacerbate pernicious bias. There is a movement toward developing guidelines of standardised reporting for machine learning models of health care9Collins GS Reitsma JB Altman DG Moons KGM TRIPOD GroupTransparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): The TRIPOD Statement.Ann Intern Med. 2015; 162: 55-63Crossref PubMed Scopus (1245) Google Scholar and their prospective appraisal through clinical trials.10The CONSORT-AI and SPIRIT-AI Steering GroupReporting guidelines for clinical trials evaluating artificial intelligence interventions are needed.Nat Med. 2019; 25: 1467-1468Crossref PubMed Scopus (69) Google Scholar Appraisal is particularly important in determining the real-world implications for vulnerable patients when machine learning models are integrated into clinical decision making. Clinical trials are essential to providing a sense of the model's performance for clinicians to make informed decisions at the point-of-care through awareness of identity-related model limitations.PanelRecommendations for ethical approaches to issues of bias in health models of machine learningRelying on neutral algorithms is problematicChallenges cast doubt on whether any solution can adequately facilitate ethical goals. Resisting the tendency to view machine learning solutions as objective is essential to remaining patient-centered and preventing unintended harms.Problem formulation can support improved modelsChanging the way the problem is conceptualised and operationalised can reduce the effect of pernicious bias on outputs.6Wenger NK Cardiovascular disease: the female heart is vulnerable. A call to action from the 10Q Report.Clin Cardiol. 2012; 35: 134-135Crossref PubMed Scopus (11) Google Scholar Clinicians have a key role in identifying actionable, clinical problems where the effects of bias are minimized, or causal knowledge exists to support algorithmic fairness solutions. Clinicians in non-medical sciences might have epistemic advantages to support such formulations.Transparency is required surrounding model development and statistical validationStandardisation of reporting for models of machine learning can promote transparency about training data and statistical validation which can help clinicians and health-care decision makers to determine the transferability of the model to their served population. Large discrepancies increase the risk of under-performance in under-represented patients. Group-specific performance metrics (eg, stratification by ethnicity, gender, or socioeconomic status) can inform considerations for patient safety in implementing a given model.Initiating transparency at point-of-careAs the field of explainable or interpretable machine learning evolves, we suggest that, when sensitive attributes are used in problem formulation and could affect predictions, they should be accompanied by visibility of the top predictive features. Where predictions are affected by sensitive variables, prediction and rationale should be documented for the ensuing clinical decision to promote fair and transparent medical decision making. Communication with patients about the rationale is essential to shared decision making.Transparency at the prediction levelRobust auditing processes are increasingly viewed as essential to proper oversight of machine learning tools. When bias is considered, auditing can promote reflexive understanding of how tools of machine learning can affect care management of vulnerable patients. Discrepancies in systematic prediction can become evident and used as evidence of the need for beneficial interventions and highlight large structural barriers that affect health inequalities.Ethical decision making suggests engaging diverse knowledge sourcesEthical analysis should consider real-world consequences for affected groups, weigh benefits and risks of various approaches, and engage stakeholders to come to the most supportable conclusion. Therefore, analysis needs to focus on the downstream effects on patients rather than adopting the presumption that fairness is accomplished solely in the metrics of the system. Relying on neutral algorithms is problematic Challenges cast doubt on whether any solution can adequately facilitate ethical goals. Resisting the tendency to view machine learning solutions as objective is essential to remaining patient-centered and preventing unintended harms. Problem formulation can support improved models Changing the way the problem is conceptualised and operationalised can reduce the effect of pernicious bias on outputs.6Wenger NK Cardiovascular disease: the female heart is vulnerable. A call to action from the 10Q Report.Clin Cardiol. 2012; 35: 134-135Crossref PubMed Scopus (11) Google Scholar Clinicians have a key role in identifying actionable, clinical problems where the effects of bias are minimized, or causal knowledge exists to support algorithmic fairness solutions. Clinicians in non-medical sciences might have epistemic advantages to support such formulations. Transparency is required surrounding model development and statistical validation Standardisation of reporting for models of machine learning can promote transparency about training data and statistical validation which can help clinicians and health-care decision makers to determine the transferability of the model to their served population. Large discrepancies increase the risk of under-performance in under-represented patients. Group-specific performance metrics (eg, stratification by ethnicity, gender, or socioeconomic status) can inform considerations for patient safety in implementing a given model. Initiating transparency at point-of-care As the field of explainable or interpretable machine learning evolves, we suggest that, when sensitive attributes are used in problem formulation and could affect predictions, they should be accompanied by visibility of the top predictive features. Where predictions are affected by sensitive variables, prediction and rationale should be documented for the ensuing clinical decision to promote fair and transparent medical decision making. Communication with patients about the rationale is essential to shared decision making. Transparency at the prediction level Robust auditing processes are increasingly viewed as essential to proper oversight of machine learning tools. When bias is considered, auditing can promote reflexive understanding of how tools of machine learning can affect care management of vulnerable patients. Discrepancies in systematic prediction can become evident and used as evidence of the need for beneficial interventions and highlight large structural barriers that affect health inequalities. Ethical decision making suggests engaging diverse knowledge sources Ethical analysis should consider real-world consequences for affected groups, weigh benefits and risks of various approaches, and engage stakeholders to come to the most supportable conclusion. Therefore, analysis needs to focus on the downstream effects on patients rather than adopting the presumption that fairness is accomplished solely in the metrics of the system. Some computations can promote justice through revealing unfairness and refining problem formulation. Obermeyer and colleagues2Obermeyer Z Powers B Vogeli C Mullainathan S Dissecting racial bias in an algorithm used to manage the health of populations.Science. 2019; 366: 447-453Crossref PubMed Scopus (1151) Google Scholar show how calibration can reveal unfairness in a seemingly neutral task through which choice of label can dictate how heavily bias is incorporated into predictions. It might be that no way exists to define a purely neutral problem; some clinical prediction tasks might be more susceptible to bias than others. Transparency at multiple points in the pipeline of machine learning including development, testing, and implementation stages can support interpretation of model outputs by relevant stakeholders (eg, researchers, clinicians, patients, and auditors). Combined with adequate documentation of outputs and ensuing decisions, these steps support a strong accountability framework for point-of-care machine learning tools with respect to safety and fairness to patients. Problem formulation with respect to bias will often be value-laden and ethically charged. Ethical decision making highlights the importance of converging knowledge sources to inform a given choice. Important stakeholders could include affected communities, cultural anthropologists, social scientists, and race and gender theorists. Computations alone clearly cannot solve the bias problem, but they could be offered a place within a broader approach to addressing fairness aims in healthcare. Algorithmic fairness could be necessary to fix statistical limitations reflective of perniciously biased data, and we encourage this work. The worry is that suggesting these as solutions risks unintended harms.5Benjamin R Assessing risk, automating racism.Science. 2019; 366: 421-422Crossref PubMed Scopus (97) Google Scholar Bias is not new; however, machine learning has potential to reveal bias, motivate change, and support ethical analysis while bringing this crucial conversation to a new audience. We are at a watershed moment in health care. Ethical considerations have rarely been so integral and essential to maximising success of a technology both empirically and clinically. The time is right to partake in thoughtful and collaborative engagement on the challenge of bias to bring about lasting change. We declare no competing interests.",
      "cited_by_count": 236,
      "type": "article",
      "source": {
        "name": "The Lancet Digital Health",
        "type": "journal",
        "issn": [
          "2589-7500"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "http://www.thelancet.com/article/S2589750020300650/pdf"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Healthcare cost, quality, practices",
        "COVID-19 and healthcare impacts"
      ],
      "referenced_works_count": 10,
      "url": "https://openalex.org/W3023069697"
    },
    {
      "openalex_id": "W2962925443",
      "doi": "10.1145/3306618.3314287",
      "title": "Multiaccuracy",
      "authors": [
        {
          "name": "Michael P. Kim",
          "openalex_id": "A5039980118",
          "orcid": "https://orcid.org/0000-0002-8269-854X",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Amirata Ghorbani",
          "openalex_id": "A5034659013",
          "orcid": "https://orcid.org/0000-0001-6465-2321",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "James Zou",
          "openalex_id": "A5005779176",
          "orcid": "https://orcid.org/0000-0001-8880-4764",
          "institutions": [
            "Stanford University"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-01-27",
      "abstract": "Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for \"black women\") even when the sensitive features (e.g. \"race\", \"gender\") are not given to the algorithm explicitly.",
      "cited_by_count": 179,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3306618.3314287"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Artificial Intelligence in Healthcare and Education",
        "Imbalanced Data Classification Techniques"
      ],
      "referenced_works_count": 37,
      "url": "https://openalex.org/W2962925443"
    },
    {
      "openalex_id": "W4320560900",
      "doi": "10.48550/arxiv.2211.08742",
      "title": "Auditing Algorithmic Fairness in Machine Learning for Health with Severity-Based LOGAN",
      "authors": [
        {
          "name": "Anaelia Ovalle",
          "openalex_id": "A5064234845",
          "orcid": "https://orcid.org/0000-0002-0531-7520"
        },
        {
          "name": "Sunipa Dev",
          "openalex_id": "A5090145147",
          "orcid": "https://orcid.org/0000-0002-6647-9662"
        },
        {
          "name": "Jieyu Zhao",
          "openalex_id": "A5066282713",
          "orcid": "https://orcid.org/0000-0002-1013-557X"
        },
        {
          "name": "Majid Sarrafzadeh",
          "openalex_id": "A5078132977",
          "orcid": "https://orcid.org/0000-0001-8407-8689"
        },
        {
          "name": "Kai-Wei Chang",
          "openalex_id": "A5087096372",
          "orcid": "https://orcid.org/0000-0001-5365-0072"
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-11-16",
      "abstract": "Auditing machine learning-based (ML) healthcare tools for bias is critical to preventing patient harm, especially in communities that disproportionately face health inequities. General frameworks are becoming increasingly available to measure ML fairness gaps between groups. However, ML for health (ML4H) auditing principles call for a contextual, patient-centered approach to model assessment. Therefore, ML auditing tools must be (1) better aligned with ML4H auditing principles and (2) able to illuminate and characterize communities vulnerable to the most harm. To address this gap, we propose supplementing ML4H auditing frameworks with SLOGAN (patient Severity-based LOcal Group biAs detectioN), an automatic tool for capturing local biases in a clinical prediction task. SLOGAN adapts an existing tool, LOGAN (LOcal Group biAs detectioN), by contextualizing group bias detection in patient illness severity and past medical history. We investigate and compare SLOGAN's bias detection capabilities to LOGAN and other clustering techniques across patient subgroups in the MIMIC-III dataset. On average, SLOGAN identifies larger fairness disparities in over 75% of patient groups than LOGAN while maintaining clustering quality. Furthermore, in a diabetes case study, health disparity literature corroborates the characterizations of the most biased clusters identified by SLOGAN. Our results contribute to the broader discussion of how machine learning biases may perpetuate existing healthcare disparities.",
      "cited_by_count": 1,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2211.08742"
      },
      "topics": [
        "Health disparities and outcomes",
        "Chronic Disease Management Strategies",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4320560900"
    },
    {
      "openalex_id": "W4383213657",
      "doi": "10.1145/3597512.3597522",
      "title": "MACAIF: Machine Learning Auditing for Clinical AI Fairness",
      "authors": [
        {
          "name": "Pepita Barnard",
          "openalex_id": "A5075253300",
          "orcid": "https://orcid.org/0000-0003-4518-1207",
          "institutions": [
            "University of Nottingham"
          ]
        },
        {
          "name": "John Robert Bautista",
          "openalex_id": "A5022028283",
          "orcid": "https://orcid.org/0000-0002-4892-9543",
          "institutions": [
            "The University of Texas at Austin"
          ]
        },
        {
          "name": "Joshua Krook",
          "openalex_id": "A5090155411",
          "orcid": "https://orcid.org/0000-0001-7215-9780",
          "institutions": [
            "University of Southampton"
          ]
        },
        {
          "name": "Anqi Liu",
          "openalex_id": "A5100757581",
          "orcid": "https://orcid.org/0000-0002-9432-4836",
          "institutions": [
            "Johns Hopkins University"
          ]
        },
        {
          "name": "H\u00e9ctor D. Men\u00e9ndez",
          "openalex_id": "A5037098621",
          "orcid": "https://orcid.org/0000-0002-6314-3725",
          "institutions": [
            "King's College London"
          ]
        },
        {
          "name": "Aurora Schmidt",
          "openalex_id": "A5037432599",
          "orcid": "https://orcid.org/0000-0001-9027-8484",
          "institutions": [
            "Johns Hopkins University"
          ]
        },
        {
          "name": "Tamim Sookoor",
          "openalex_id": "A5103203540",
          "institutions": [
            "Johns Hopkins University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-07-05",
      "abstract": "Artificial intelligence in the form of machine learning algorithms is driving the latest industrial revolution, leading to disruptive changes in the ways we communicate, interact, design, collect information, and express ourselves. While these changes offer new possibilities for our societies, they may also introduce biases that can lead to unfair decisions. This issue is particularly critical in the context of medical diagnosis, as bias can jeopardize patient treatment and health. To mitigate these biases, it is essential to such biases and involve all relevant stakeholders in the design of fair machine learning algorithms. In this context, the MACAIF project aims to develop user-centred interfaces that allow stakeholders, including doctors, to challenge the fairness of machine learning algorithms based on demographics, such as gender or race. Our project proposes a methodology to engage with stakeholders and incorporate their concerns during the design of a dashboard based on MLighter - an adversarial tool which is applied to identify fairness-related issues in machine learning models.",
      "cited_by_count": 1,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Healthcare cost, quality, practices",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 8,
      "url": "https://openalex.org/W4383213657"
    },
    {
      "openalex_id": "W2900572965",
      "doi": "10.48550/arxiv.1811.05577",
      "title": "Aequitas: A Bias and Fairness Audit Toolkit",
      "authors": [
        {
          "name": "Pedro Saleiro",
          "openalex_id": "A5039038824",
          "orcid": "https://orcid.org/0000-0003-2750-1692"
        },
        {
          "name": "Benedict Kuester",
          "openalex_id": "A5051351413"
        },
        {
          "name": "Abby Stevens",
          "openalex_id": "A5038346089",
          "orcid": "https://orcid.org/0000-0003-1976-1806"
        },
        {
          "name": "Ari Anisfeld",
          "openalex_id": "A5076679290"
        },
        {
          "name": "Loren Hinkson",
          "openalex_id": "A5038987490"
        },
        {
          "name": "Jesse London",
          "openalex_id": "A5082210493"
        },
        {
          "name": "Rayid Ghani",
          "openalex_id": "A5081839926",
          "orcid": "https://orcid.org/0000-0003-0235-1843"
        },
        {
          "name": "Ghani, Rayid",
          "openalex_id": ""
        }
      ],
      "publication_year": 2018,
      "publication_date": "2018-11-14",
      "abstract": "Recent work has raised concerns on the risk of unintended bias in AI systems being used nowadays that can affect individuals unfairly based on race, gender or religion, among other possible characteristics. While a lot of bias metrics and fairness definitions have been proposed in recent years, there is no consensus on which metric/definition should be used and there are very few available resources to operationalize them. Therefore, despite recent awareness, auditing for bias and fairness when developing and deploying AI systems is not yet a standard practice. We present Aequitas, an open source bias and fairness audit toolkit that is an intuitive and easy to use addition to the machine learning workflow, enabling users to seamlessly test models for several bias and fairness metrics in relation to multiple population sub-groups. Aequitas facilitates informed and equitable decisions around developing and deploying algorithmic decision making systems for both data scientists, machine learning researchers and policymakers.",
      "cited_by_count": 64,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/1811.05577"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 27,
      "url": "https://openalex.org/W2900572965"
    },
    {
      "openalex_id": "W3128475867",
      "doi": "10.1109/jiot.2020.3028101",
      "title": "Hybrid Blockchain-Based Resource Trading System for Federated Learning in Edge Computing",
      "authors": [
        {
          "name": "Sizheng Fan",
          "openalex_id": "A5050107793",
          "orcid": "https://orcid.org/0000-0002-3622-1302",
          "institutions": [
            "Shenzhen Academy of Robotics",
            "Chinese University of Hong Kong, Shenzhen"
          ]
        },
        {
          "name": "Hongbo Zhang",
          "openalex_id": "A5100461100",
          "orcid": "https://orcid.org/0000-0002-8788-339X",
          "institutions": [
            "Chinese University of Hong Kong, Shenzhen",
            "Shenzhen Academy of Robotics"
          ]
        },
        {
          "name": "Yuchen Zeng",
          "openalex_id": "A5065836136",
          "orcid": "https://orcid.org/0000-0002-2766-0055",
          "institutions": [
            "Chinese University of Hong Kong, Shenzhen",
            "Shenzhen Academy of Robotics"
          ]
        },
        {
          "name": "Wei Cai",
          "openalex_id": "A5053432286",
          "orcid": "https://orcid.org/0000-0002-4658-0034",
          "institutions": [
            "Chinese University of Hong Kong, Shenzhen",
            "Shenzhen Academy of Robotics"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-10-14",
      "abstract": "By training a machine learning algorithm across multiple decentralized edge nodes, federated learning (FL) ensures the privacy of the data generated by the massive Internet-of-Things (IoT) devices. To economically encourage the participation of heterogeneous edge nodes, a transparent and decentralized trading platform is needed to establish a fair market among distinct edge companies. In this article, we propose a hybrid blockchain-based resource trading system that combines the advantages of both public and consortium blockchains. We design and implement a smart contract to facilitate an automatic, autonomous, and auditable rational reverse auction mechanism among edge nodes. Moreover, we leverage the payment channel technique to enable credible, fast, low-cost, and high-frequency payment transactions between requesters and edge nodes. Simulation results show that the proposed reverse auction mechanism can achieve the properties, including budget feasibility, truthfulness, and computational efficiency.",
      "cited_by_count": 117,
      "type": "article",
      "source": {
        "name": "IEEE Internet of Things Journal",
        "type": "journal",
        "issn": [
          "2327-4662",
          "2372-2541"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Privacy-Preserving Technologies in Data",
        "Blockchain Technology Applications and Security",
        "Mobile Crowdsensing and Crowdsourcing"
      ],
      "referenced_works_count": 43,
      "url": "https://openalex.org/W3128475867"
    },
    {
      "openalex_id": "W4386952188",
      "doi": "10.1109/tce.2023.3318509",
      "title": "AP2FL: Auditable Privacy-Preserving Federated Learning Framework for Electronics in Healthcare",
      "authors": [
        {
          "name": "Abbas Yazdinejad",
          "openalex_id": "A5087870144",
          "orcid": "https://orcid.org/0000-0002-8669-9777",
          "institutions": [
            "University of Guelph"
          ]
        },
        {
          "name": "Ali Dehghantanha",
          "openalex_id": "A5038019914",
          "orcid": "https://orcid.org/0000-0002-9294-7554",
          "institutions": [
            "University of Guelph"
          ]
        },
        {
          "name": "Gautam Srivastava",
          "openalex_id": "A5041541232",
          "orcid": "https://orcid.org/0000-0001-9851-4103",
          "institutions": [
            "China Medical University",
            "Lebanese American University",
            "Brandon University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-09-22",
      "abstract": "The growing application of machine learning (ML) techniques in healthcare has led to increased interest in federated learning (FL), which enables the secure and private training of robust ML models. However, conventional FL methods often fall short of providing adequate privacy protection and face challenges in handling non-independent and identically distributed (Non-IID) training data. These shortcomings are of significant concern when employing FL in electronic devices in healthcare. To address these issues, we propose an Auditable Privacy-Preserving Federated Learning (AP2FL) model tailored for electronics in healthcare settings. By leveraging Trusted Execution Environments (TEEs), AP2FL ensures secure training and aggregation processes on both client and server sides, effectively mitigating data leakage risks. To manage Non-IID data within the proposed framework, we incorporate the Active Personalized Federated Learning (ActPerFL) model and Batch Normalization (BN) techniques to consolidate user updates and identify data similarities. Additionally, we introduce an auditing mechanism in AP2FL that reveals the contribution of each client to the FL process, facilitating the updating of the global model following diverse data types and distributions. In other words, it ensures the FL process's integrity, transparency, fairness, and robustness. Our results demonstrate that the proposed AP2FL model outperforms existing methods in accuracy and effectively eliminates privacy leakage.",
      "cited_by_count": 97,
      "type": "article",
      "source": {
        "name": "IEEE Transactions on Consumer Electronics",
        "type": "journal",
        "issn": [
          "0098-3063",
          "1558-4127"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Privacy-Preserving Technologies in Data",
        "Cryptography and Data Security",
        "Advanced Data and IoT Technologies"
      ],
      "referenced_works_count": 37,
      "url": "https://openalex.org/W4386952188"
    },
    {
      "openalex_id": "W4389497068",
      "doi": "10.1016/j.engappai.2023.107620",
      "title": "Explainable, interpretable, and trustworthy AI for an intelligent digital twin: A case study on remaining useful life",
      "authors": [
        {
          "name": "Kazuma Kobayashi",
          "openalex_id": "A5012088444",
          "orcid": "https://orcid.org/0000-0002-2565-1374",
          "institutions": [
            "University of Illinois Urbana-Champaign"
          ]
        },
        {
          "name": "Syed Bahauddin Alam",
          "openalex_id": "A5063457131",
          "orcid": "https://orcid.org/0000-0001-5506-6953",
          "institutions": [
            "National Center for Supercomputing Applications",
            "University of Illinois Urbana-Champaign"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-12-08",
      "abstract": null,
      "cited_by_count": 115,
      "type": "article",
      "source": {
        "name": "Engineering Applications of Artificial Intelligence",
        "type": "journal",
        "issn": [
          "0952-1976",
          "1873-6769"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Technology Assessment and Management",
        "Fault Detection and Control Systems",
        "Reliability and Maintenance Optimization"
      ],
      "referenced_works_count": 75,
      "url": "https://openalex.org/W4389497068"
    },
    {
      "openalex_id": "W4239744509",
      "doi": "10.31235/osf.io/fj6pg",
      "title": "Algorithmic content moderation: Technical and political challenges in the automation of platform governance",
      "authors": [
        {
          "name": "Robert Gorwa",
          "openalex_id": "A5057098134",
          "orcid": "https://orcid.org/0000-0002-4891-5053",
          "institutions": [
            "University of Oxford"
          ]
        },
        {
          "name": "Reuben Binns",
          "openalex_id": "A5101687507",
          "orcid": "https://orcid.org/0000-0003-4718-2190",
          "institutions": [
            "University of Oxford"
          ]
        },
        {
          "name": "Christian Katzenbach",
          "openalex_id": "A5020105431",
          "orcid": "https://orcid.org/0000-0003-1897-2783",
          "institutions": [
            "Alexander von Humboldt Institute for Internet and Society"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-03-11",
      "abstract": "As government pressure on major technology companies builds, both firms and legislators are searching for technical solutions to difficult platform governance puzzles such as hate speech and misinformation. Automated hash-matching and predictive machine learning tools \u2013 what we define here as algorithmic moderation systems \u2013 are increasingly being deployed to conduct content moderation at scale by major platforms for user-generated content such as Facebook, YouTube and Twitter. This article provides an accessible technical primer on how algorithmic moderation works; examines some of the existing automated tools used by major platforms to handle copyright infringement, terrorism and toxic speech; and identifies key political and ethical issues for these systems as the reliance on them grows. Recent events suggest that algorithmic moderation has become necessary to manage growing public expectations for increased platform responsibility, safety and security on the global stage; however, as we demonstrate, these systems remain opaque, unaccountable and poorly understood. Despite the potential promise of algorithms or \u2018AI\u2019, we show that even \u2018well optimized\u2019 moderation systems could exacerbate, rather than relieve, many existing problems with content policy as enacted by platforms for three main reasons: automated moderation threatens to (a) further increase opacity, making a famously non-transparent set of practices even more difficult to understand or audit, (b) further complicate outstand- ing issues of fairness and justice in large-scale sociotechnical systems and (c) re-obscure the fundamentally political nature of speech decisions being executed at scale.",
      "cited_by_count": 122,
      "type": "preprint",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://osf.io/fj6pg/download"
      },
      "topics": [
        "Hate Speech and Cyberbullying Detection"
      ],
      "referenced_works_count": 49,
      "url": "https://openalex.org/W4239744509"
    },
    {
      "openalex_id": "W4220970489",
      "doi": "10.1145/3527448",
      "title": "Explainable Deep Reinforcement Learning: State of the Art and Challenges",
      "authors": [
        {
          "name": "George A. Vouros",
          "openalex_id": "A5040575826",
          "orcid": "https://orcid.org/0000-0001-5451-622X",
          "institutions": [
            "University of Piraeus"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-03-31",
      "abstract": "Interpretability, explainability, and transparency are key issues to introducing artificial intelligence methods in many critical domains. This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability, and fairness, and has important consequences toward keeping the human in the loop in high levels of automation, especially in critical cases for decision making, where both (human and the machine) play important roles. Although the research community has given much attention to explainability of closed (or black) prediction boxes, there are tremendous needs for explainability of closed-box methods that support agents to act autonomously in the real world. Reinforcement learning methods, and especially their deep versions, are such closed-box methods. In this article, we aim to provide a review of state-of-the-art methods for explainable deep reinforcement learning methods, taking also into account the needs of human operators\u2014that is, of those who make the actual and critical decisions in solving real-world problems. We provide a formal specification of the deep reinforcement learning explainability problems, and we identify the necessary components of a general explainable reinforcement learning framework. Based on these, we provide a comprehensive review of state-of-the-art methods, categorizing them into classes according to the paradigm they follow, the interpretable models they use, and the surface representation of explanations provided. The article concludes by identifying open questions and important challenges.",
      "cited_by_count": 113,
      "type": "review",
      "source": {
        "name": "ACM Computing Surveys",
        "type": "journal",
        "issn": [
          "0360-0300",
          "1557-7341"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2301.09937"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Adversarial Robustness in Machine Learning",
        "Reinforcement Learning in Robotics"
      ],
      "referenced_works_count": 68,
      "url": "https://openalex.org/W4220970489"
    },
    {
      "openalex_id": "W4229442586",
      "doi": "10.1145/3531146.3533179",
      "title": "The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations",
      "authors": [
        {
          "name": "Aparna Balagopalan",
          "openalex_id": "A5010008842",
          "orcid": "https://orcid.org/0000-0003-1621-9536",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Haoran Zhang",
          "openalex_id": "A5100340498",
          "orcid": "https://orcid.org/0000-0003-1027-9976",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Kimia Hamidieh",
          "openalex_id": "A5009409152",
          "institutions": [
            "Vector Institute",
            "University of Toronto"
          ]
        },
        {
          "name": "Thomas Hartvigsen",
          "openalex_id": "A5075881948",
          "orcid": "https://orcid.org/0000-0002-5288-2792",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Frank Rudzicz",
          "openalex_id": "A5111170441",
          "institutions": [
            "Vector Institute",
            "University of Toronto"
          ]
        },
        {
          "name": "Marzyeh Ghassemi",
          "openalex_id": "A5070063054",
          "orcid": "https://orcid.org/0000-0001-6349-7251",
          "institutions": [
            "Massachusetts Institute of Technology",
            "Vector Institute"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-06-20",
      "abstract": "Machine learning models in safety-critical settings like healthcare are often\\nblackboxes: they contain a large number of parameters which are not transparent\\nto users. Post-hoc explainability methods where a simple, human-interpretable\\nmodel imitates the behavior of these blackbox models are often proposed to help\\nusers trust model predictions. In this work, we audit the quality of such\\nexplanations for different protected subgroups using real data from four\\nsettings in finance, healthcare, college admissions, and the US justice system.\\nAcross two different blackbox model architectures and four popular\\nexplainability methods, we find that the approximation quality of explanation\\nmodels, also known as the fidelity, differs significantly between subgroups. We\\nalso demonstrate that pairing explainability methods with recent advances in\\nrobust machine learning can improve explanation fairness in some settings.\\nHowever, we highlight the importance of communicating details of non-zero\\nfidelity gaps to users, since a single solution might not exist across all\\nsettings. Finally, we discuss the implications of unfair explanation models as\\na challenging and understudied problem facing the machine learning community.\\n",
      "cited_by_count": 59,
      "type": "article",
      "source": {
        "name": "2022 ACM Conference on Fairness, Accountability, and Transparency",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3531146.3533179"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Artificial Intelligence in Healthcare and Education",
        "Machine Learning in Healthcare"
      ],
      "referenced_works_count": 64,
      "url": "https://openalex.org/W4229442586"
    },
    {
      "openalex_id": "W3032920906",
      "doi": "10.1515/pjbr-2020-0030",
      "title": "AI and recruiting software: Ethical and legal implications",
      "authors": [
        {
          "name": "Carmen Fern\u00e1ndez-Mart\u00ednez",
          "openalex_id": "A5001163192",
          "institutions": [
            "Universidad Rey Juan Carlos"
          ]
        },
        {
          "name": "Alberto Fern\u00e1ndez",
          "openalex_id": "A5063206552",
          "orcid": "https://orcid.org/0000-0002-8962-6856",
          "institutions": [
            "Universidad Rey Juan Carlos"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-05-28",
      "abstract": "Abstract In this article, we examine the state-of-the-art and current applications of artificial intelligence (AI), specifically for human resources (HR). We study whether, due to the experimental state of the algorithms used and the nature of training and test samples, a further control and auditing in the research community is necessary to guarantee fair and accurate results. In particular, we identify the positive and negative consequences of the usage of video-interview analysis via AI in recruiting processes as well as the main machine learning techniques used and their degrees of efficiency. We focus on some controversial characteristics that could lead to ethical and legal consequences for candidates, companies and states regarding discrimination in the job market (e.g. gender and race). There is a lack of regulation and a need for external and neutral auditing for the type of analyses done in interviews. We present a multi-agent architecture that aims at total legal compliance and more effective HR processes management.",
      "cited_by_count": 61,
      "type": "article",
      "source": {
        "name": "Paladyn Journal of Behavioral Robotics",
        "type": "journal",
        "issn": [
          "2081-4836"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.degruyter.com/document/doi/10.1515/pjbr-2020-0030/pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Artificial Intelligence in Law",
        "Imbalanced Data Classification Techniques"
      ],
      "referenced_works_count": 45,
      "url": "https://openalex.org/W3032920906"
    },
    {
      "openalex_id": "W4376137450",
      "doi": "10.1007/s43681-023-00292-7",
      "title": "They shall be fair, transparent, and robust: auditing learning analytics systems",
      "authors": [
        {
          "name": "Katharina Simbeck",
          "openalex_id": "A5065288785",
          "orcid": "https://orcid.org/0000-0001-6792-461X",
          "institutions": [
            "HTW Berlin - University of Applied Sciences"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-05-11",
      "abstract": "Abstract In the near future, systems, that use Artificial Intelligence (AI) methods, such as machine learning, are required to be certified or audited for fairness if used in ethically sensitive fields such as education. One example of those upcoming regulatory initiatives is the European Artificial Intelligence Act. Interconnected with fairness are the notions of system transparency (i.e. how understandable is the system) and system robustness (i.e. will similar inputs lead to similar results). Ensuring fairness, transparency, and robustness requires looking at data, models, system processes, and the use of systems as the ethical implications arise at the intersection between those. The potential societal consequences are domain specific, it is, therefore, necessary to discuss specifically for Learning Analytics (LA) what fairness, transparency, and robustness mean and how they can be certified. Approaches to certifying and auditing fairness in LA include assessing datasets, machine learning models, and the end-to-end LA process for fairness, transparency, and robustness. Based on Slade and Prinsloo\u2019s six principals for ethical LA, relevant audit approaches will be deduced. Auditing AI applications in LA is a complex process that requires technical capabilities and needs to consider the perspectives of all stakeholders. This paper proposes a comprehensive framework for auditing AI applications in LA systems from the perspective of learners' autonomy, provides insights into different auditing methodologies, and emphasizes the importance of reflection and dialogue among providers, buyers, and users of these systems to ensure their ethical and responsible use.",
      "cited_by_count": 24,
      "type": "article",
      "source": {
        "name": "AI and Ethics",
        "type": "journal",
        "issn": [
          "2730-5953",
          "2730-5961"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s43681-023-00292-7.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 87,
      "url": "https://openalex.org/W4376137450"
    },
    {
      "openalex_id": "W3125437910",
      "doi": "10.48550/arxiv.1901.04730",
      "title": "Fair and Unbiased Algorithmic Decision Making: Current State and Future\\n Challenges",
      "authors": [
        {
          "name": "Song\u00fcl Tolan",
          "openalex_id": "A5017647508",
          "orcid": "https://orcid.org/0000-0002-2323-1878"
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-01-15",
      "abstract": "Machine learning algorithms are now frequently used in sensitive contexts\\nthat substantially affect the course of human lives, such as credit lending or\\ncriminal justice. This is driven by the idea that `objective' machines base\\ntheir decisions solely on facts and remain unaffected by human cognitive\\nbiases, discriminatory tendencies or emotions. Yet, there is overwhelming\\nevidence showing that algorithms can inherit or even perpetuate human biases in\\ntheir decision making when they are based on data that contains biased human\\ndecisions. This has led to a call for fairness-aware machine learning. However,\\nfairness is a complex concept which is also reflected in the attempts to\\nformalize fairness for algorithmic decision making. Statistical formalizations\\nof fairness lead to a long list of criteria that are each flawed (or harmful\\neven) in different contexts. Moreover, inherent tradeoffs in these criteria\\nmake it impossible to unify them in one general framework. Thus, fairness\\nconstraints in algorithms have to be specific to the domains to which the\\nalgorithms are applied. In the future, research in algorithmic decision making\\nsystems should be aware of data and developer biases and add a focus on\\ntransparency to facilitate regular fairness audits.\\n",
      "cited_by_count": 41,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/1901.04730"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W3125437910"
    },
    {
      "openalex_id": "W2944357355",
      "doi": "10.48550/arxiv.1905.03674",
      "title": "Proportionally Fair Clustering",
      "authors": [
        {
          "name": "Xingyu Chen",
          "openalex_id": "A5100387332",
          "orcid": "https://orcid.org/0000-0001-5711-7996"
        },
        {
          "name": "Brandon Fain",
          "openalex_id": "A5015652321",
          "institutions": [
            "Duke University"
          ]
        },
        {
          "name": "Liang Lyu",
          "openalex_id": "A5003400236",
          "institutions": [
            "Duke University"
          ]
        },
        {
          "name": "Kamesh Munagala",
          "openalex_id": "A5047351951",
          "orcid": "https://orcid.org/0000-0003-2636-9650",
          "institutions": [
            "Duke University"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-05-09",
      "abstract": "We extend the fair machine learning literature by considering the problem of proportional centroid clustering in a metric context. For clustering $n$ points with $k$ centers, we define fairness as proportionality to mean that any $n/k$ points are entitled to form their own cluster if there is another center that is closer in distance for all $n/k$ points. We seek clustering solutions to which there are no such justified complaints from any subsets of agents, without assuming any a priori notion of protected subsets. We present and analyze algorithms to efficiently compute, optimize, and audit proportional solutions. We conclude with an empirical examination of the tradeoff between proportional solutions and the $k$-means objective.",
      "cited_by_count": 43,
      "type": "article",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/1905.03674"
      },
      "topics": [
        "Game Theory and Voting Systems"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W2944357355"
    },
    {
      "openalex_id": "W4200630031",
      "doi": "10.48550/arxiv.2112.05700",
      "title": "A Framework for Fairness: A Systematic Review of Existing Fair AI Solutions",
      "authors": [
        {
          "name": "Brianna Richardson",
          "openalex_id": "A5110661438",
          "orcid": "https://orcid.org/0000-0003-1326-0453"
        },
        {
          "name": "Juan E. Gilbert",
          "openalex_id": "A5063670127",
          "orcid": "https://orcid.org/0000-0002-6801-2206"
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-12-10",
      "abstract": "In a world of daily emerging scientific inquisition and discovery, the prolific launch of machine learning across industries comes to little surprise for those familiar with the potential of ML. Neither so should the congruent expansion of ethics-focused research that emerged as a response to issues of bias and unfairness that stemmed from those very same applications. Fairness research, which focuses on techniques to combat algorithmic bias, is now more supported than ever before. A large portion of fairness research has gone to producing tools that machine learning practitioners can use to audit for bias while designing their algorithms. Nonetheless, there is a lack of application of these fairness solutions in practice. This systematic review provides an in-depth summary of the algorithmic bias issues that have been defined and the fairness solution space that has been proposed. Moreover, this review provides an in-depth breakdown of the caveats to the solution space that have arisen since their release and a taxonomy of needs that have been proposed by machine learning practitioners, fairness researchers, and institutional stakeholders. These needs have been organized and addressed to the parties most influential to their implementation, which includes fairness researchers, organizations that produce ML algorithms, and the machine learning practitioners themselves. These findings can be used in the future to bridge the gap between practitioners and fairness experts and inform the creation of usable fair ML toolkits.",
      "cited_by_count": 30,
      "type": "review",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2112.05700"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4200630031"
    },
    {
      "openalex_id": "W4283161771",
      "doi": "10.1145/3531146.3533132",
      "title": "Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Feminicide Counterdata Collection",
      "authors": [
        {
          "name": "Harini Suresh",
          "openalex_id": "A5005452839",
          "orcid": "https://orcid.org/0000-0002-9769-4947",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Rajiv Movva",
          "openalex_id": "A5079551137",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Amelia Lee Do\u011fan",
          "openalex_id": "A5023130537",
          "orcid": "https://orcid.org/0009-0006-6540-231X",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Rahul Bhargava",
          "openalex_id": "A5020526243",
          "orcid": "https://orcid.org/0000-0003-3904-4302",
          "institutions": [
            "Northeastern University"
          ]
        },
        {
          "name": "Isadora Ara\u00fajo Crux\u00ean",
          "openalex_id": "A5057346428",
          "institutions": [
            "Queen Mary University of London",
            "London School of Business and Management"
          ]
        },
        {
          "name": "Angeles Martinez Cuba",
          "openalex_id": "A5038118944",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Guilia Taurino",
          "openalex_id": "A5026155125",
          "institutions": [
            "Northeastern University"
          ]
        },
        {
          "name": "Wonyoung So",
          "openalex_id": "A5089177147",
          "orcid": "https://orcid.org/0000-0002-4867-3429",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Catherine D\u2019Ignazio",
          "openalex_id": "A5050163961",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-06-20",
      "abstract": "Data ethics and fairness have emerged as important areas of research in recent years. However, much work in this area focuses on retroactively auditing and \u201cmitigating bias\u201d in existing, potentially flawed systems, without interrogating the deeper structural inequalities underlying them. There are not yet examples of how to apply feminist and participatory methodologies from the start, to conceptualize and design machine learning-based tools that center and aim to challenge power inequalities. Our work targets this more prospective goal. Guided by the framework of data feminism, we co-design datasets and machine learning models to support the efforts of activists who collect and monitor data about feminicide\u2014gender-based killings of women and girls. We describe how intersectional feminist goals and participatory processes shaped each stage of our approach, from problem conceptualization to data collection to model evaluation. We highlight several methodological contributions, including 1) an iterative data collection and annotation process that targets model weaknesses and interrogates framing concepts (such as who is included/excluded in \u201cfeminicide\u201d), 2) models that explicitly focus on intersectional identities rather than statistical majorities, and 3) a multi-step evaluation process\u2014with quantitative, qualitative and participatory steps\u2014focused on context-specific relevance. We also distill insights and tensions that arise from bridging intersectional feminist goals with ML. These include reflections on how ML may challenge power, embrace pluralism, rethink binaries and consider context, as well as the inherent limitations of any technology-based solution to address durable structural inequalities.",
      "cited_by_count": 51,
      "type": "article",
      "source": {
        "name": "2022 ACM Conference on Fairness, Accountability, and Transparency",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3531146.3533132"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Hate Speech and Cyberbullying Detection",
        "Innovative Human-Technology Interaction"
      ],
      "referenced_works_count": 45,
      "url": "https://openalex.org/W4283161771"
    },
    {
      "openalex_id": "W3008907338",
      "doi": "10.48550/arxiv.2002.09343",
      "title": "Robust Optimization for Fairness with Noisy Protected Groups",
      "authors": [
        {
          "name": "Serena Wang",
          "openalex_id": "A5047154233",
          "orcid": "https://orcid.org/0000-0002-5532-071X"
        },
        {
          "name": "Wenshuo Guo",
          "openalex_id": "A5083541779",
          "orcid": "https://orcid.org/0000-0003-2154-5467"
        },
        {
          "name": "Harikrishna Narasimhan",
          "openalex_id": "A5077482746",
          "orcid": "https://orcid.org/0009-0007-0476-6635"
        },
        {
          "name": "Andrew Cotter",
          "openalex_id": "A5113779130",
          "orcid": "https://orcid.org/0000-0002-1933-863X"
        },
        {
          "name": "Maya R. Gupta",
          "openalex_id": "A5111921762"
        },
        {
          "name": "Michael I. Jordan",
          "openalex_id": "A5049812527",
          "orcid": "https://orcid.org/0000-0001-8935-817X"
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-02-21",
      "abstract": "Many existing fairness criteria for machine learning involve equalizing some metric across protected groups such as race or gender. However, practitioners trying to audit or enforce such group-based criteria can easily face the problem of noisy or biased protected group information. First, we study the consequences of naively relying on noisy protected group labels: we provide an upper bound on the fairness violations on the true groups G when the fairness criteria are satisfied on noisy groups $\\hat{G}$. Second, we introduce two new approaches using robust optimization that, unlike the naive approach of only relying on $\\hat{G}$, are guaranteed to satisfy fairness criteria on the true protected groups G while minimizing a training objective. We provide theoretical guarantees that one such approach converges to an optimal feasible solution. Using two case studies, we show empirically that the robust approaches achieve better true group fairness guarantees than the naive approach.",
      "cited_by_count": 41,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2002.09343"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Privacy-Preserving Technologies in Data",
        "Advanced Bandit Algorithms Research"
      ],
      "referenced_works_count": 52,
      "url": "https://openalex.org/W3008907338"
    },
    {
      "openalex_id": "W4283168170",
      "doi": "10.1145/3531146.3533204",
      "title": "Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models",
      "authors": [
        {
          "name": "Emily Black",
          "openalex_id": "A5006802134",
          "orcid": "https://orcid.org/0000-0002-4301-3739",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Hadi Elzayn",
          "openalex_id": "A5002313823",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Alexandra Chouldechova",
          "openalex_id": "A5057438082",
          "orcid": "https://orcid.org/0000-0002-2337-9610",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Jacob Goldin",
          "openalex_id": "A5024608138",
          "orcid": "https://orcid.org/0000-0001-5518-0027",
          "institutions": [
            "Stanford University"
          ]
        },
        {
          "name": "Daniel E. Ho",
          "openalex_id": "A5058408154",
          "orcid": "https://orcid.org/0000-0002-2195-5469",
          "institutions": [
            "Stanford University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-06-20",
      "abstract": "This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity\u2014appropriately accounting for relevant differences across individuals\u2014which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods\u2014as opposed to simpler models\u2014shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.",
      "cited_by_count": 19,
      "type": "article",
      "source": {
        "name": "2022 ACM Conference on Fairness, Accountability, and Transparency",
        "type": "conference",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3531146.3533204"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Regulation and Compliance Studies",
        "Digital Economy and Work Transformation"
      ],
      "referenced_works_count": 20,
      "url": "https://openalex.org/W4283168170"
    },
    {
      "openalex_id": "W4282915382",
      "doi": "10.2196/34366",
      "title": "Fairness in Mobile Phone\u2013Based Mental Health Assessment Algorithms: Exploratory Study",
      "authors": [
        {
          "name": "Jinkyung Park",
          "openalex_id": "A5101734911",
          "orcid": "https://orcid.org/0000-0002-0804-832X",
          "institutions": [
            "Rutgers, The State University of New Jersey"
          ]
        },
        {
          "name": "Ramanathan Arunachalam",
          "openalex_id": "A5047957264",
          "orcid": "https://orcid.org/0000-0001-8934-0981",
          "institutions": [
            "Rutgers, The State University of New Jersey"
          ]
        },
        {
          "name": "Vincent Silenzio",
          "openalex_id": "A5082235131",
          "orcid": "https://orcid.org/0000-0003-1408-7955",
          "institutions": [
            "Rutgers, The State University of New Jersey"
          ]
        },
        {
          "name": "Vivek K. Singh",
          "openalex_id": "A5011229206",
          "orcid": "https://orcid.org/0000-0002-8194-2336",
          "institutions": [
            "Massachusetts Institute of Technology",
            "Rutgers, The State University of New Jersey",
            "The Huntington Library, Art Museum, and Botanical Gardens"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-06-14",
      "abstract": "Background Approximately 1 in 5 American adults experience mental illness every year. Thus, mobile phone\u2013based mental health prediction apps that use phone data and artificial intelligence techniques for mental health assessment have become increasingly important and are being rapidly developed. At the same time, multiple artificial intelligence\u2013related technologies (eg, face recognition and search results) have recently been reported to be biased regarding age, gender, and race. This study moves this discussion to a new domain: phone-based mental health assessment algorithms. It is important to ensure that such algorithms do not contribute to gender disparities through biased predictions across gender groups. Objective This research aimed to analyze the susceptibility of multiple commonly used machine learning approaches for gender bias in mobile mental health assessment and explore the use of an algorithmic disparate impact remover (DIR) approach to reduce bias levels while maintaining high accuracy. Methods First, we performed preprocessing and model training using the data set (N=55) obtained from a previous study. Accuracy levels and differences in accuracy across genders were computed using 5 different machine learning models. We selected the random forest model, which yielded the highest accuracy, for a more detailed audit and computed multiple metrics that are commonly used for fairness in the machine learning literature. Finally, we applied the DIR approach to reduce bias in the mental health assessment algorithm. Results The highest observed accuracy for the mental health assessment was 78.57%. Although this accuracy level raises optimism, the audit based on gender revealed that the performance of the algorithm was statistically significantly different between the male and female groups (eg, difference in accuracy across genders was 15.85%; P&lt;.001). Similar trends were obtained for other fairness metrics. This disparity in performance was found to reduce significantly after the application of the DIR approach by adapting the data used for modeling (eg, the difference in accuracy across genders was 1.66%, and the reduction is statistically significant with P&lt;.001). Conclusions This study grounds the need for algorithmic auditing in phone-based mental health assessment algorithms and the use of gender as a protected attribute to study fairness in such settings. Such audits and remedial steps are the building blocks for the widespread adoption of fair and accurate mental health assessment algorithms in the future.",
      "cited_by_count": 23,
      "type": "article",
      "source": {
        "name": "JMIR Formative Research",
        "type": "journal",
        "issn": [
          "2561-326X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://formative.jmir.org/2022/6/e34366/PDF"
      },
      "topics": [
        "Digital Mental Health Interventions",
        "Mental Health Research Topics",
        "COVID-19 and Mental Health"
      ],
      "referenced_works_count": 48,
      "url": "https://openalex.org/W4282915382"
    },
    {
      "openalex_id": "W4402057292",
      "doi": "10.1038/s42256-024-00878-8",
      "title": "A large-scale audit of dataset licensing and attribution in AI",
      "authors": [
        {
          "name": "Shayne Longpre",
          "openalex_id": "A5001884064",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Robert Mahari",
          "openalex_id": "A5028144841",
          "orcid": "https://orcid.org/0000-0003-2372-2746",
          "institutions": [
            "Massachusetts Institute of Technology",
            "Harvard University Press"
          ]
        },
        {
          "name": "Anthony Chen",
          "openalex_id": "A5087171054",
          "orcid": "https://orcid.org/0000-0003-4363-5041",
          "institutions": [
            "University of California, Irvine"
          ]
        },
        {
          "name": "Naana Obeng-Marnu",
          "openalex_id": "A5092064801",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Damien Sileo",
          "openalex_id": "A5068059558",
          "orcid": "https://orcid.org/0000-0002-3274-291X",
          "institutions": [
            "Universit\u00e9 de Lille"
          ]
        },
        {
          "name": "William Brannon",
          "openalex_id": "A5079113224",
          "orcid": "https://orcid.org/0000-0002-1435-8535",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Niklas Muennighoff",
          "openalex_id": "A5000043237",
          "institutions": [
            "Contextual Change (United States)"
          ]
        },
        {
          "name": "Nathan Khazam",
          "openalex_id": "A5093134538",
          "institutions": [
            "University of Colorado Boulder"
          ]
        },
        {
          "name": "Jad Kabbara",
          "openalex_id": "A5026429870",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Kartik Perisetla",
          "openalex_id": "A5003528238"
        },
        {
          "name": "Xinyi Wu",
          "openalex_id": "A5101882209",
          "orcid": "https://orcid.org/0000-0001-9475-3510",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Enrico Shippole",
          "openalex_id": "A5092741733"
        },
        {
          "name": "Kurt Bollacker",
          "openalex_id": "A5013580260",
          "institutions": [
            "Creative Commons"
          ]
        },
        {
          "name": "Tongshuang Wu",
          "openalex_id": "A5004225142",
          "orcid": "https://orcid.org/0000-0003-1630-0588",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "L. F. Villa",
          "openalex_id": "A5108457098"
        },
        {
          "name": "Sandy Pentland",
          "openalex_id": "A5107954910",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Sara Hooker",
          "openalex_id": "A5078850040",
          "orcid": "https://orcid.org/0000-0002-0190-6459"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-08-30",
      "abstract": null,
      "cited_by_count": 40,
      "type": "article",
      "source": {
        "name": "Nature Machine Intelligence",
        "type": "journal",
        "issn": [
          "2522-5839"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.nature.com/articles/s42256-024-00878-8.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 59,
      "url": "https://openalex.org/W4402057292"
    },
    {
      "openalex_id": "W3094334402",
      "doi": "10.1109/jiot.2020.3032706",
      "title": "Learning Markets: An AI Collaboration Framework Based on Blockchain and Smart Contracts",
      "authors": [
        {
          "name": "Liwei Ouyang",
          "openalex_id": "A5091630356",
          "orcid": "https://orcid.org/0000-0001-9533-6727",
          "institutions": [
            "Institute of Automation",
            "University of Chinese Academy of Sciences",
            "Chinese Academy of Sciences",
            "Beijing Academy of Artificial Intelligence"
          ]
        },
        {
          "name": "Yong Yuan",
          "openalex_id": "A5110476655",
          "orcid": "https://orcid.org/0000-0001-8310-2712",
          "institutions": [
            "Renmin University of China",
            "Institute of Automation",
            "Chinese Academy of Sciences"
          ]
        },
        {
          "name": "Fei\u2010Yue Wang",
          "openalex_id": "A5113600509",
          "orcid": "https://orcid.org/0000-0001-9185-3989",
          "institutions": [
            "Qingdao Academy of Intelligent Industries",
            "Macau University of Science and Technology",
            "Institute of Automation",
            "Chinese Academy of Sciences"
          ]
        }
      ],
      "publication_year": 2020,
      "publication_date": "2020-10-21",
      "abstract": "Artificial intelligence (AI) has been witnessed to provide valuable solutions to all walks of life. However, data island and computing resources limitations in the centralized AI architectures have increased their technical barriers, and thus distributed AI collaboration in data, models, and resources has attracted intensive research interests. Since the existing trust-based collaboration models are no longer applicable for the large-scale distributed collaboration among trustless machines in open and dynamic environments, this article proposes a novel decentralized AI collaboration framework, i.e., learning markets (LM), in which blockchain provides a trustless environment for collaboration and transaction, while smart contracts serve as software-defined agents to encapsulate and process scalable collaboration relationships and market mechanisms. LM can not only help those participants without mutual trust realize collaborative mining with dynamic and quantitative rewards but also build an AI market with natural auditability and traceability for trading trusted and verified models. We implement and comprehensively analyze LM based on the Ethereum interplenary file system platform (IPFS), and the results prove that it has advantages in collaboration fairness, transparency, security, decentralization and universality. Based on our collaboration framework, distributed AI contributors are expected to cooperate and complete those learning tasks that cannot be done previously due to lack of complete data, sufficient computing resources and state-of-the-art models.",
      "cited_by_count": 59,
      "type": "article",
      "source": {
        "name": "IEEE Internet of Things Journal",
        "type": "journal",
        "issn": [
          "2327-4662",
          "2372-2541"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Blockchain Technology Applications and Security",
        "Privacy-Preserving Technologies in Data",
        "Cryptography and Data Security"
      ],
      "referenced_works_count": 39,
      "url": "https://openalex.org/W3094334402"
    },
    {
      "openalex_id": "W4297094586",
      "doi": "10.1109/tvcg.2022.3209484",
      "title": "D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias",
      "authors": [
        {
          "name": "Bhavya Ghai",
          "openalex_id": "A5075846286",
          "orcid": "https://orcid.org/0000-0003-3932-1525",
          "institutions": [
            "Stony Brook University"
          ]
        },
        {
          "name": "Klaus Mueller",
          "openalex_id": "A5070670810",
          "orcid": "https://orcid.org/0000-0002-0996-8590",
          "institutions": [
            "Stony Brook University"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-01-01",
      "abstract": "With the rise of AI, algorithms have become better at learning underlying patterns from the training data including ingrained social biases based on gender, race, etc. Deployment of such algorithms to domains such as hiring, healthcare, law enforcement, etc. has raised serious concerns about fairness, accountability, trust and interpretability in machine learning algorithms. To alleviate this problem, we propose D-BIAS, a visual interactive tool that embodies human-in-the-loop AI approach for auditing and mitigating social biases from tabular datasets. It uses a graphical causal model to represent causal relationships among different features in the dataset and as a medium to inject domain knowledge. A user can detect the presence of bias against a group, say females, or a subgroup, say black females, by identifying unfair causal relationships in the causal network and using an array of fairness metrics. Thereafter, the user can mitigate bias by refining the causal model and acting on the unfair causal edges. For each interaction, say weakening/deleting a biased causal edge, the system uses a novel method to simulate a new (debiased) dataset based on the current causal model while ensuring a minimal change from the original dataset. Users can visually assess the impact of their interactions on different fairness metrics, utility metrics, data distortion, and the underlying data distribution. Once satisfied, they can download the debiased dataset and use it for any downstream application for fairer predictions. We evaluate D-BIAS by conducting experiments on 3 datasets and also a formal user study. We found that D-BIAS helps reduce bias significantly compared to the baseline debiasing approach across different fairness metrics while incurring little data distortion and a small loss in utility. Moreover, our human-in-the-loop based approach significantly outperforms an automated approach on trust, interpretability and accountability.",
      "cited_by_count": 38,
      "type": "article",
      "source": {
        "name": "IEEE Transactions on Visualization and Computer Graphics",
        "type": "journal",
        "issn": [
          "1077-2626",
          "1941-0506",
          "2160-9306"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)",
        "Privacy-Preserving Technologies in Data"
      ],
      "referenced_works_count": 58,
      "url": "https://openalex.org/W4297094586"
    },
    {
      "openalex_id": "W4384071683",
      "doi": "10.1038/s41586-023-06291-2",
      "title": "Large language models encode clinical knowledge",
      "authors": [
        {
          "name": "Karan Singhal",
          "openalex_id": "A5027454515",
          "orcid": "https://orcid.org/0000-0001-9002-7490",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Shekoofeh Azizi",
          "openalex_id": "A5047463591",
          "orcid": "https://orcid.org/0000-0002-7447-6031",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Tao Tu",
          "openalex_id": "A5059213795",
          "orcid": "https://orcid.org/0000-0003-3420-7889",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "S. Sara Mahdavi",
          "openalex_id": "A5063201022",
          "orcid": "https://orcid.org/0000-0001-6823-598X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Jason Lee",
          "openalex_id": "A5100657725",
          "orcid": "https://orcid.org/0000-0003-4042-795X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Hyung Won Chung",
          "openalex_id": "A5051828575",
          "orcid": "https://orcid.org/0000-0002-1280-9953",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Nathan Scales",
          "openalex_id": "A5030765685",
          "orcid": "https://orcid.org/0000-0002-9535-7138",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Ajay Kumar Tanwani",
          "openalex_id": "A5088063475",
          "orcid": "https://orcid.org/0000-0002-6365-8315",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Heather Cole-Lewis",
          "openalex_id": "A5069557194",
          "orcid": "https://orcid.org/0000-0002-7275-1810",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Stephen Pfohl",
          "openalex_id": "A5021812637",
          "orcid": "https://orcid.org/0000-0003-0551-9664",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Perry W. Payne",
          "openalex_id": "A5014637990",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Martin Seneviratne",
          "openalex_id": "A5058677067",
          "orcid": "https://orcid.org/0000-0003-0435-3738",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Paul Gamble",
          "openalex_id": "A5090718376",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Christopher Kelly",
          "openalex_id": "A5026540467",
          "orcid": "https://orcid.org/0000-0002-1246-844X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Abubakr Babiker",
          "openalex_id": "A5066029226",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Nathanael Sch\u00e4rli",
          "openalex_id": "A5007588003",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Aakanksha Chowdhery",
          "openalex_id": "A5055969617",
          "orcid": "https://orcid.org/0000-0002-0628-5225",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "P. Mansfield",
          "openalex_id": "A5086361722",
          "orcid": "https://orcid.org/0000-0003-4969-0543",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Dina Demner\u2010Fushman",
          "openalex_id": "A5046764593",
          "institutions": [
            "United States National Library of Medicine"
          ]
        },
        {
          "name": "Blaise Ag\u00fcera y Arcas",
          "openalex_id": "A5044698998",
          "orcid": "https://orcid.org/0000-0003-2256-9823",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Dale R. Webster",
          "openalex_id": "A5060000122",
          "orcid": "https://orcid.org/0000-0002-3023-8824",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Greg S. Corrado",
          "openalex_id": "A5068955381",
          "orcid": "https://orcid.org/0000-0001-8817-0992",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Yossi Matias",
          "openalex_id": "A5065128060",
          "orcid": "https://orcid.org/0000-0003-3960-6002",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Katherine Chou",
          "openalex_id": "A5070366042",
          "orcid": "https://orcid.org/0000-0002-0318-7857",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Juraj Gottweis",
          "openalex_id": "A5057932939",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Nenad Toma\u0161ev",
          "openalex_id": "A5057195145",
          "orcid": "https://orcid.org/0000-0003-1624-0220",
          "institutions": [
            "DeepMind (United Kingdom)"
          ]
        },
        {
          "name": "Yun Liu",
          "openalex_id": "A5078784976",
          "orcid": "https://orcid.org/0000-0003-4079-8275",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Alvin Rajkomar",
          "openalex_id": "A5022388476",
          "orcid": "https://orcid.org/0000-0001-5750-5016",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Jo\u00eblle Barral",
          "openalex_id": "A5043862316",
          "orcid": "https://orcid.org/0009-0009-0432-5148",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Christopher Semturs",
          "openalex_id": "A5010171106",
          "orcid": "https://orcid.org/0000-0001-6108-2773",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Alan Karthikesalingam",
          "openalex_id": "A5003509342",
          "orcid": "https://orcid.org/0000-0001-5074-898X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Vivek Natarajan",
          "openalex_id": "A5103234563",
          "orcid": "https://orcid.org/0000-0001-7849-2074",
          "institutions": [
            "Google (United States)"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-07-12",
      "abstract": null,
      "cited_by_count": 2374,
      "type": "article",
      "source": {
        "name": "Nature",
        "type": "journal",
        "issn": [
          "0028-0836",
          "1476-4687"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.nature.com/articles/s41586-023-06291-2.pdf"
      },
      "topics": [
        "Topic Modeling",
        "Artificial Intelligence in Healthcare and Education",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 91,
      "url": "https://openalex.org/W4384071683"
    },
    {
      "openalex_id": "W4401521928",
      "doi": "10.1038/s42256-024-00874-y",
      "title": "On responsible machine learning datasets emphasizing fairness, privacy and regulatory norms with examples in biometrics and healthcare",
      "authors": [
        {
          "name": "Surbhi Mittal",
          "openalex_id": "A5030169316",
          "orcid": "https://orcid.org/0000-0001-8910-4161",
          "institutions": [
            "Indian Institute of Technology Jodhpur"
          ]
        },
        {
          "name": "Kartik Thakral",
          "openalex_id": "A5069444808",
          "orcid": "https://orcid.org/0000-0002-2528-9950",
          "institutions": [
            "Indian Institute of Technology Jodhpur"
          ]
        },
        {
          "name": "Richa Singh",
          "openalex_id": "A5103866220",
          "institutions": [
            "Indian Institute of Technology Jodhpur"
          ]
        },
        {
          "name": "Mayank Vatsa",
          "openalex_id": "A5050521702",
          "orcid": "https://orcid.org/0000-0001-5952-2274",
          "institutions": [
            "Indian Institute of Technology Jodhpur"
          ]
        },
        {
          "name": "Tamar Glaser",
          "openalex_id": "A5030284058",
          "orcid": "https://orcid.org/0009-0007-9322-2444",
          "institutions": [
            "Menlo School"
          ]
        },
        {
          "name": "Cristian Canton Ferrer",
          "openalex_id": "A5079611849",
          "orcid": "https://orcid.org/0000-0002-3189-5498",
          "institutions": [
            "Menlo School"
          ]
        },
        {
          "name": "Tal Hassner",
          "openalex_id": "A5075765401",
          "orcid": "https://orcid.org/0000-0003-2275-1406",
          "institutions": [
            "Alameda Hospital"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-08-12",
      "abstract": "Abstract Artificial Intelligence (AI) has seamlessly integrated into numerous scientific domains, catalysing unparalleled enhancements across a broad spectrum of tasks; however, its integrity and trustworthiness have emerged as notable concerns. The scientific community has focused on the development of trustworthy AI algorithms; however, machine learning and deep learning algorithms, popular in the AI community today, intrinsically rely on the quality of their training data. These algorithms are designed to detect patterns within the data, thereby learning the intended behavioural objectives. Any inadequacy in the data has the potential to translate directly into algorithms. In this study we discuss the importance of responsible machine learning datasets through the lens of fairness, privacy and regulatory compliance, and present a large audit of computer vision datasets. Despite the ubiquity of fairness and privacy challenges across diverse data domains, current regulatory frameworks primarily address human-centric data concerns. We therefore focus our discussion on biometric and healthcare datasets, although the principles we outline are broadly applicable across various domains. The audit is conducted through evaluation of the proposed responsible rubric. After surveying over 100 datasets, our detailed analysis of 60 distinct datasets highlights a universal susceptibility to fairness, privacy and regulatory compliance issues. This finding emphasizes the urgent need for revising dataset creation methodologies within the scientific community, especially in light of global advancements in data protection legislation. We assert that our study is critically relevant in the contemporary AI context, offering insights and recommendations that are both timely and essential for the ongoing evolution of AI technologies.",
      "cited_by_count": 22,
      "type": "article",
      "source": {
        "name": "Nature Machine Intelligence",
        "type": "journal",
        "issn": [
          "2522-5839"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://doi.org/10.1038/s42256-024-00874-y"
      },
      "topics": [
        "Privacy-Preserving Technologies in Data",
        "Ethics and Social Impacts of AI",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 101,
      "url": "https://openalex.org/W4401521928"
    },
    {
      "openalex_id": "W3130399513",
      "doi": "10.17762/pae.v58i2.2265",
      "title": "A Study on Application of Artificial Intelligence and Machine Learning in Indian Taxation System",
      "authors": [
        {
          "name": "Ankit Rathi Et al.",
          "openalex_id": "A5018055928",
          "institutions": [
            "Manipal University Jaipur"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-02-01",
      "abstract": "In a developing economy like India taxation is a main source of public finance. Indian taxation system always suffered from problems such as tax evasion, inefficient administration etc. Administration of taxation always needs such a system which will be less in error and prompt in decision making. Indian taxation system is suffering from lack of manpower to perform tedious tasks such as data entry, scrutiny of return, tax audit etc.&#x0D; To manage the changing tax landscape alongside use of analytics recently Indian government announced the use of Artificial Intelligence/Machine Learning in tax assessment system. Artificial Intelligence or known as AI is a relatively new phenomenon in tax. Recently the government of India announced to use faceless tax assessment system empowered by AI/ML.&#x0D; In the Present paper we attempt to find out the role of AI/ML in Indian taxation system and on the basis of factors such as tax knowledge, tax education, legal sanction, complexity of tax system, relationship with tax authority, perceived fairness of the tax system, ethics and attitudes towards tax compliance, awareness of offences and penalties, tax education, possibility of being audited etc. we want to know about the perception of taxpayers towards adoption of Artificial Intelligence based tax system.&#x0D;",
      "cited_by_count": 26,
      "type": "article",
      "source": {
        "name": "Psychology and Education Journal",
        "type": "journal",
        "issn": [
          "0033-3077"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "http://psychologyandeducation.net/pae/index.php/pae/article/download/2265/1974"
      },
      "topics": [
        "Impact of AI and Big Data on Business and Society"
      ],
      "referenced_works_count": 19,
      "url": "https://openalex.org/W3130399513"
    },
    {
      "openalex_id": "W4399362975",
      "doi": "10.1145/3630106.3658912",
      "title": "Operationalizing the Search for Less Discriminatory Alternatives in Fair Lending",
      "authors": [
        {
          "name": "Talia B. Gillis",
          "openalex_id": "A5079596685",
          "orcid": "https://orcid.org/0000-0001-8084-5158",
          "institutions": [
            "Columbia University"
          ]
        },
        {
          "name": "Vitaly Meursault",
          "openalex_id": "A5017520735",
          "orcid": "https://orcid.org/0000-0001-8673-6294",
          "institutions": [
            "Federal Reserve Bank of Philadelphia"
          ]
        },
        {
          "name": "Berk Ustun",
          "openalex_id": "A5040537492",
          "orcid": "https://orcid.org/0000-0001-5188-3155"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-06-03",
      "abstract": "The Less Discriminatory Alternative is a key provision of the disparate impact doctrine in the United States. In fair lending, this provision mandates that lenders must adopt models that reduce discrimination when they do not compromise their business interests. In this paper, we develop practical methods to audit for less discriminatory alternatives. Our approach is designed to verify the existence of less discriminatory machine learning models \u2013 by returning an alternative model that can reduce discrimination without compromising performance (discovery) or by certifying that an alternative model does not exist (refutation). We develop a method to fit the least discriminatory linear classification model in a specific lending task \u2013 by minimizing an exact measure of disparity (e.g., the maximum gap in group FNR) and enforcing hard performance constraints for business necessity (e.g., on FNR and FPR). We apply our method to study the prevalence of less discriminatory alternatives on real-world datasets from consumer finance applications. Our results highlight how models may inadvertently lead to unnecessary discrimination across common deployment regimes, and demonstrate how our approach can support lenders, regulators, and plaintiffs by reliably detecting less discriminatory alternatives in such instances.",
      "cited_by_count": 29,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3630106.3658912"
      },
      "topics": [
        "Law, Economics, and Judicial Systems",
        "Legal and Constitutional Studies",
        "Regulation and Compliance Studies"
      ],
      "referenced_works_count": 28,
      "url": "https://openalex.org/W4399362975"
    },
    {
      "openalex_id": "W4290840946",
      "doi": "10.1016/j.patter.2022.100568",
      "title": "Measuring disparate outcomes of content recommendation algorithms with distributional inequality metrics",
      "authors": [
        {
          "name": "T. Lazovich",
          "openalex_id": "A5108021320",
          "orcid": "https://orcid.org/0000-0001-8183-2295",
          "institutions": [
            "Twitter (United States)"
          ]
        },
        {
          "name": "Luca Belli",
          "openalex_id": "A5101681190",
          "orcid": "https://orcid.org/0000-0002-2749-0586",
          "institutions": [
            "Twitter (United States)"
          ]
        },
        {
          "name": "Aaron Gonzales",
          "openalex_id": "A5051330739",
          "orcid": "https://orcid.org/0000-0002-2981-3203",
          "institutions": [
            "Twitter (United States)"
          ]
        },
        {
          "name": "Amanda Bower",
          "openalex_id": "A5036699573",
          "orcid": "https://orcid.org/0000-0002-4497-3088",
          "institutions": [
            "Twitter (United States)"
          ]
        },
        {
          "name": "Uthaipon Tantipongpipat",
          "openalex_id": "A5042603378",
          "orcid": "https://orcid.org/0000-0002-5573-5165",
          "institutions": [
            "Twitter (United States)"
          ]
        },
        {
          "name": "Kristian Lum",
          "openalex_id": "A5034482692",
          "orcid": "https://orcid.org/0000-0003-2637-5612",
          "institutions": [
            "Twitter (United States)"
          ]
        },
        {
          "name": "Ferenc Husz\u00e1r",
          "openalex_id": "A5052054395",
          "orcid": "https://orcid.org/0000-0002-4988-1430",
          "institutions": [
            "University of Cambridge"
          ]
        },
        {
          "name": "Rumman Chowdhury",
          "openalex_id": "A5000969043",
          "institutions": [
            "Twitter (United States)"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-08-01",
      "abstract": "The harmful impacts of algorithmic decision systems have recently come into focus, with many examples of machine learning (ML) models amplifying societal biases. In this paper, we propose adapting income inequality metrics from economics to complement existing model-level fairness metrics, which focus on intergroup differences of model performance. In particular, we evaluate their ability to measure disparities between exposures that individuals receive in a production recommendation system, the Twitter algorithmic timeline. We define desirable criteria for metrics to be used in an operational setting by ML practitioners. We characterize engagements with content on Twitter using these metrics and use the results to evaluate the metrics with respect to our criteria. We also show that we can use these metrics to identify content suggestion algorithms that contribute more strongly to skewed outcomes between users. Overall, we conclude that these metrics can be a useful tool for auditing algorithms in production settings.",
      "cited_by_count": 34,
      "type": "article",
      "source": {
        "name": "Patterns",
        "type": "journal",
        "issn": [
          "2666-3899"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "http://www.cell.com/article/S2666389922001799/pdf"
      },
      "topics": [
        "Mobile Crowdsensing and Crowdsourcing",
        "COVID-19 epidemiological studies",
        "Misinformation and Its Impacts"
      ],
      "referenced_works_count": 43,
      "url": "https://openalex.org/W4290840946"
    },
    {
      "openalex_id": "W2989096391",
      "doi": "10.48550/arxiv.1911.01468",
      "title": "Auditing and Achieving Intersectional Fairness in Classification Problems",
      "authors": [
        {
          "name": "Giulio Morina",
          "openalex_id": "A5006885179"
        },
        {
          "name": "Viktoriia Oliinyk",
          "openalex_id": "A5101832051"
        },
        {
          "name": "Julian Waton",
          "openalex_id": "A5019238456"
        },
        {
          "name": "Ines Maru\u0161i\u0107",
          "openalex_id": "A5014376569"
        },
        {
          "name": "Konstantinos Georgatzis",
          "openalex_id": "A5086870685",
          "orcid": "https://orcid.org/0000-0001-8142-8495"
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-11-04",
      "abstract": "Machine learning algorithms are extensively used to make increasingly more consequential decisions about people, so achieving optimal predictive performance can no longer be the only focus. A particularly important consideration is fairness with respect to race, gender, or any other sensitive attribute. This paper studies intersectional fairness, where intersections of multiple sensitive attributes are considered. Prior research has mainly focused on fairness with respect to a single sensitive attribute, with intersectional fairness being comparatively less studied despite its critical importance for the safety of modern machine learning systems. We present a comprehensive framework for auditing and achieving intersectional fairness in classification problems: we define a suite of metrics to assess intersectional fairness in the data or model outputs by extending known single-attribute fairness metrics, and propose methods for robustly estimating them even when some intersectional subgroups are underrepresented. Furthermore, we develop post-processing techniques to mitigate any detected intersectional bias in a classification model. Our techniques do not rely on any assumptions regarding the underlying model and preserve predictive performance at a guaranteed level of fairness. Finally, we give guidance on a practical implementation, showing how the proposed methods perform on a real-world dataset.",
      "cited_by_count": 17,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/1911.01468"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 32,
      "url": "https://openalex.org/W2989096391"
    },
    {
      "openalex_id": "W3191409995",
      "doi": "10.1145/3461702.3462469",
      "title": "Examining Religion Bias in AI Text Generators",
      "authors": [
        {
          "name": "Deepa Muralidhar",
          "openalex_id": "A5047411215",
          "orcid": "https://orcid.org/0000-0002-1000-9135",
          "institutions": [
            "Georgia State University"
          ]
        }
      ],
      "publication_year": 2021,
      "publication_date": "2021-07-21",
      "abstract": "One of the biggest reasons artificial intelligence (AI) gets a backlash is because of inherent biases in AI software. Deep learning algorithms use data fed into the systems to find patterns to draw conclusions used to make application decisions. Patterns in data fed into machine learning algorithms have revealed that the AI software decisions have biases embedded within them. Algorithmic audits can certify that the software is making responsible decisions. These audits verify the standards centered around the various AI principles such as explainability, accountability, human-centered values, such as, fairness and transparency, to increase the trust in the algorithm and the software systems that implement AI algorithms.",
      "cited_by_count": 16,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Religion and Sociopolitical Dynamics in Nigeria"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W3191409995"
    },
    {
      "openalex_id": "W4402321740",
      "doi": "10.51594/csitrj.v3i3.1559",
      "title": "Ethical AI: Addressing bias in machine learning models and software applications",
      "authors": [
        {
          "name": "Oyekunle Claudius Oyeniran",
          "openalex_id": "A5094137529"
        },
        {
          "name": "Adebunmi Okechukwu Adewusi",
          "openalex_id": "A5093804232"
        },
        {
          "name": "Adams Gbolahan Adeleke",
          "openalex_id": "A5109784243"
        },
        {
          "name": "Lucy Anthony Akwawa",
          "openalex_id": "A5106884307",
          "institutions": [
            "Eastern Michigan University"
          ]
        },
        {
          "name": "Chidimma Francisca Azubuko",
          "openalex_id": "A5106878911"
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-12-30",
      "abstract": "As artificial intelligence (AI) increasingly integrates into various aspects of society, addressing bias in machine learning models and software applications has become crucial. Bias in AI systems can originate from various sources, including unrepresentative datasets, algorithmic assumptions, and human factors. These biases can perpetuate discrimination and inequity, leading to significant social and ethical consequences. This paper explores the nature of bias in AI, emphasizing the need for ethical AI practices to ensure fairness and accountability. We first define and categorize the different types of bias\u2014data bias, algorithmic bias, and human-induced bias\u2014highlighting real-world examples and their impacts. The discussion then shifts to methods for mitigating bias, including strategies for improving data quality, developing fairness-aware algorithms, and implementing robust auditing processes. We also review existing ethical guidelines and frameworks, such as those proposed by IEEE and the European Union, which provide a foundation for ethical AI development. Challenges in identifying and addressing bias are examined, such as the trade-offs between fairness and model accuracy, and the complexities of legal and regulatory requirements. Future directions are considered, including emerging trends in ethical AI, the importance of interdisciplinary collaboration, and innovations in bias detection and mitigation. In conclusion, ongoing vigilance and commitment to ethical practices are essential for developing AI systems that are equitable and just. This paper calls for continuous improvement and proactive measures from developers, researchers, and policymakers to create AI technologies that serve all individuals fairly and without bias. Keywords: Ethical AI, Bias, Machine Learning, Models, Software Applications.",
      "cited_by_count": 24,
      "type": "article",
      "source": {
        "name": "Computer Science & IT Research Journal",
        "type": "journal",
        "issn": [
          "2709-0043",
          "2709-0051"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "https://doi.org/10.51594/csitrj.v3i3.1559"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4402321740"
    },
    {
      "openalex_id": "W2969625533",
      "doi": "10.1016/j.ijinfomgt.2019.08.002",
      "title": "Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy",
      "authors": [
        {
          "name": "Yogesh K. Dwivedi",
          "openalex_id": "A5048622877",
          "orcid": "https://orcid.org/0000-0002-5547-9990",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Laurie Hughes",
          "openalex_id": "A5072512285",
          "orcid": "https://orcid.org/0000-0002-0956-0608",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Elvira Ismagilova",
          "openalex_id": "A5033472055",
          "orcid": "https://orcid.org/0000-0001-9634-194X",
          "institutions": [
            "University of Bradford"
          ]
        },
        {
          "name": "Gert Aarts",
          "openalex_id": "A5027512483",
          "orcid": "https://orcid.org/0000-0002-6038-3782",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Crispin Coombs",
          "openalex_id": "A5018731131",
          "orcid": "https://orcid.org/0000-0002-4203-9291",
          "institutions": [
            "Loughborough University"
          ]
        },
        {
          "name": "Tom Crick",
          "openalex_id": "A5015452463",
          "orcid": "https://orcid.org/0000-0001-5196-9389",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Yanqing Duan",
          "openalex_id": "A5032994640",
          "orcid": "https://orcid.org/0000-0003-4205-7403",
          "institutions": [
            "University of Bedfordshire"
          ]
        },
        {
          "name": "Rohita Dwivedi",
          "openalex_id": "A5070017829",
          "orcid": "https://orcid.org/0000-0003-3801-3635",
          "institutions": [
            "Prin. L. N. Welingkar Institute of Management Development and Research"
          ]
        },
        {
          "name": "John S. Edwards",
          "openalex_id": "A5007695310",
          "orcid": "https://orcid.org/0000-0003-3979-017X",
          "institutions": [
            "Aston University"
          ]
        },
        {
          "name": "Aled Eirug",
          "openalex_id": "A5000411910",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Vassilis Galanos",
          "openalex_id": "A5015685463",
          "orcid": "https://orcid.org/0000-0002-8363-4855",
          "institutions": [
            "University of Edinburgh"
          ]
        },
        {
          "name": "P. Vigneswara Ilavarasan",
          "openalex_id": "A5004125306",
          "orcid": "https://orcid.org/0000-0002-9431-3520",
          "institutions": [
            "Indian Institute of Technology Delhi"
          ]
        },
        {
          "name": "Marijn Janssen",
          "openalex_id": "A5062073470",
          "orcid": "https://orcid.org/0000-0001-6211-8790",
          "institutions": [
            "Delft University of Technology"
          ]
        },
        {
          "name": "Paul Jones",
          "openalex_id": "A5080152304",
          "orcid": "https://orcid.org/0000-0003-0417-9143",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Arpan Kumar Kar",
          "openalex_id": "A5061235109",
          "orcid": "https://orcid.org/0000-0003-4186-4887",
          "institutions": [
            "Indian Institute of Technology Delhi"
          ]
        },
        {
          "name": "Hatice Kizgin",
          "openalex_id": "A5037522059",
          "orcid": "https://orcid.org/0000-0003-0841-8973",
          "institutions": [
            "University of Bradford"
          ]
        },
        {
          "name": "Bianca Kronemann",
          "openalex_id": "A5037724665",
          "orcid": "https://orcid.org/0009-0002-5146-537X",
          "institutions": [
            "Swansea University"
          ]
        },
        {
          "name": "Banita Lal",
          "openalex_id": "A5103045218",
          "orcid": "https://orcid.org/0000-0003-1340-1746",
          "institutions": [
            "University of Bedfordshire"
          ]
        },
        {
          "name": "Biagio Lucini",
          "openalex_id": "A5003777616",
          "orcid": "https://orcid.org/0000-0001-8974-8266",
          "institutions": [
            "Swansea University",
            "Foundry (United Kingdom)"
          ]
        },
        {
          "name": "Rony Medaglia",
          "openalex_id": "A5003475331",
          "orcid": "https://orcid.org/0000-0001-7292-5895",
          "institutions": [
            "Copenhagen Business School"
          ]
        },
        {
          "name": "Kenneth Le Meunier\u2010FitzHugh",
          "openalex_id": "A5059234800",
          "institutions": [
            "University of East Anglia"
          ]
        },
        {
          "name": "Leslie Caroline Le Meunier-FitzHugh",
          "openalex_id": "A5076965699"
        },
        {
          "name": "Santosh K. Misra",
          "openalex_id": "A5018760782",
          "orcid": "https://orcid.org/0000-0002-3313-4895",
          "institutions": [
            "University of East Anglia",
            "Government of Tamil Nadu"
          ]
        },
        {
          "name": "Emmanuel Mogaji",
          "openalex_id": "A5082000265",
          "orcid": "https://orcid.org/0000-0003-0544-4842",
          "institutions": [
            "University of Greenwich"
          ]
        },
        {
          "name": "Sujeet Kumar Sharma",
          "openalex_id": "A5037836435",
          "orcid": "https://orcid.org/0000-0003-3614-9053"
        },
        {
          "name": "Jang Bahadur Singh",
          "openalex_id": "A5040405430",
          "orcid": "https://orcid.org/0000-0001-7017-1989",
          "institutions": [
            "Indian Institute of Management Tiruchirappalli"
          ]
        },
        {
          "name": "Vishnupriya Raghavan",
          "openalex_id": "A5050603102"
        },
        {
          "name": "Ramakrishnan Raman",
          "openalex_id": "A5072261829",
          "orcid": "https://orcid.org/0000-0003-3642-6989",
          "institutions": [
            "Symbiosis International University"
          ]
        },
        {
          "name": "Nripendra P. Rana",
          "openalex_id": "A5034248834",
          "orcid": "https://orcid.org/0000-0003-1105-8729",
          "institutions": [
            "University of Bradford"
          ]
        },
        {
          "name": "Spyridon Samothrakis",
          "openalex_id": "A5033216620",
          "orcid": "https://orcid.org/0000-0003-1902-9690",
          "institutions": [
            "University of Essex"
          ]
        },
        {
          "name": "Jak Spencer",
          "openalex_id": "A5016998336",
          "institutions": [
            "Training Programs in Epidemiology and Public Health Interventions Network"
          ]
        },
        {
          "name": "Kuttimani Tamilmani",
          "openalex_id": "A5008267619",
          "orcid": "https://orcid.org/0000-0002-9615-1465",
          "institutions": [
            "University of Bradford"
          ]
        },
        {
          "name": "Annie Tubadji",
          "openalex_id": "A5070089201",
          "orcid": "https://orcid.org/0000-0002-6134-3520",
          "institutions": [
            "University of the West of England"
          ]
        },
        {
          "name": "Paul Walton",
          "openalex_id": "A5109157215",
          "institutions": [
            "Capgemini (United Kingdom)",
            "Swansea University"
          ]
        },
        {
          "name": "Michael D. Williams",
          "openalex_id": "A5002569790",
          "orcid": "https://orcid.org/0000-0002-3047-0332",
          "institutions": [
            "Swansea University"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-08-27",
      "abstract": null,
      "cited_by_count": 3558,
      "type": "article",
      "source": {
        "name": "International Journal of Information Management",
        "type": "journal",
        "issn": [
          "0268-4012",
          "1873-4707"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.sciencedirect.com/science/article/pii/S026840121930917X"
      },
      "topics": [
        "Big Data and Business Intelligence",
        "Economic and Technological Systems Analysis",
        "Digital Transformation in Industry"
      ],
      "referenced_works_count": 338,
      "url": "https://openalex.org/W2969625533"
    },
    {
      "openalex_id": "W4296205861",
      "doi": "10.1017/s0008197322000460",
      "title": "A LEGAL FRAMEWORK FOR ARTIFICIAL INTELLIGENCE FAIRNESS REPORTING",
      "authors": [
        {
          "name": "Jia Qing Yap",
          "openalex_id": "A5055574504",
          "institutions": [
            "National University of Singapore"
          ]
        },
        {
          "name": "Ernest Lim",
          "openalex_id": "A5000867871",
          "orcid": "https://orcid.org/0000-0003-4573-0544",
          "institutions": [
            "National University of Singapore"
          ]
        }
      ],
      "publication_year": 2022,
      "publication_date": "2022-09-16",
      "abstract": "Abstract Clear understanding of artificial intelligence (AI) usage risks and how they are being addressed is needed, which requires proper and adequate corporate disclosure. We advance a legal framework for AI Fairness Reporting to which companies can and should adhere on a comply-or-explain basis. We analyse the sources of unfairness arising from different aspects of AI models and the disparities in the performance of machine learning systems. We evaluate how the machine learning literature has sought to address the problem of unfairness through the use of different fairness metrics. We then put forward a nuanced and viable framework for AI Fairness Reporting comprising: (1) disclosure of all machine learning models usage; (2) disclosure of fairness metrics used and the ensuing trade-offs; (3) disclosure of de-biasing methods used; and (d) release of datasets for public inspection or for third-party audit. We then apply this reporting framework to two case studies.",
      "cited_by_count": 10,
      "type": "article",
      "source": {
        "name": "The Cambridge Law Journal",
        "type": "journal",
        "issn": [
          "0008-1973",
          "1469-2139"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/C2D73FBE9BB74E5D41DDA6BDCA208424/S0008197322000460a.pdf/div-class-title-a-legal-framework-for-artificial-intelligence-fairness-reporting-div.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 32,
      "url": "https://openalex.org/W4296205861"
    },
    {
      "openalex_id": "W2910705748",
      "doi": "10.1073/pnas.1900654116",
      "title": "Definitions, methods, and applications in interpretable machine learning",
      "authors": [
        {
          "name": "William J. Murdoch",
          "openalex_id": "A5057838960",
          "orcid": "https://orcid.org/0009-0009-7610-5187",
          "institutions": [
            "University of California, Berkeley"
          ]
        },
        {
          "name": "Chandan Singh",
          "openalex_id": "A5017514239",
          "orcid": "https://orcid.org/0000-0003-0318-2340",
          "institutions": [
            "University of California, Berkeley"
          ]
        },
        {
          "name": "Karl Kumbier",
          "openalex_id": "A5013534934",
          "orcid": "https://orcid.org/0000-0001-6521-1173",
          "institutions": [
            "University of California, Berkeley"
          ]
        },
        {
          "name": "Reza Abbasi-Asl",
          "openalex_id": "A5029714016",
          "orcid": "https://orcid.org/0000-0001-7824-4628",
          "institutions": [
            "University of California, Berkeley",
            "University of California, San Francisco",
            "Allen Institute",
            "Allen Institute for Brain Science"
          ]
        },
        {
          "name": "Bin Yu",
          "openalex_id": "A5100646137",
          "orcid": "https://orcid.org/0000-0002-8888-4060",
          "institutions": [
            "University of California, Berkeley"
          ]
        }
      ],
      "publication_year": 2019,
      "publication_date": "2019-10-16",
      "abstract": "Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work.",
      "cited_by_count": 1900,
      "type": "article",
      "source": {
        "name": "Proceedings of the National Academy of Sciences",
        "type": "journal",
        "issn": [
          "0027-8424",
          "1091-6490"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "bronze",
        "oa_url": "https://www.pnas.org/content/pnas/116/44/22071.full.pdf"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Adversarial Robustness in Machine Learning",
        "Machine Learning and Data Classification"
      ],
      "referenced_works_count": 137,
      "url": "https://openalex.org/W2910705748"
    }
  ],
  "count": 40,
  "errors": []
}
