{
  "status": "success",
  "source": "semantic_scholar",
  "query": "causal models intersectionality",
  "results": [
    {
      "paperId": "b400756067cef54c09feb4a27c8d7a12d5d9042f",
      "title": "What is causal about causal models and representations?",
      "authors": [
        {
          "name": "Frederik Hytting Jorgensen",
          "authorId": "2219325941"
        },
        {
          "name": "Luigi Gresele",
          "authorId": "2288278834"
        },
        {
          "name": "Sebastian Weichwald",
          "authorId": "2257016714"
        }
      ],
      "year": 2025,
      "abstract": "Causal Bayesian networks are 'causal' models since they make predictions about interventional distributions. To connect such causal model predictions to real-world outcomes, we must determine which actions in the world correspond to which interventions in the model. For example, to interpret an action as an intervention on a treatment variable, the action will presumably have to a) change the distribution of treatment in a way that corresponds to the intervention, and b) not change other aspects, such as how the outcome depends on the treatment; while the marginal distributions of some variables may change as an effect. We introduce a formal framework to make such requirements for different interpretations of actions as interventions precise. We prove that the seemingly natural interpretation of actions as interventions is circular: Under this interpretation, every causal Bayesian network that correctly models the observational distribution is trivially also interventionally valid, and no action yields empirical data that could possibly falsify such a model. We prove an impossibility result: No interpretation exists that is non-circular and simultaneously satisfies a set of natural desiderata. Instead, we examine non-circular interpretations that may violate some desiderata and show how this may in turn enable the falsification of causal models. By rigorously examining how a causal Bayesian network could be a 'causal' model of the world instead of merely a mathematical object, our formal framework contributes to the conceptual foundations of causal representation learning, causal discovery, and causal abstraction, while also highlighting some limitations of existing approaches.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2501.19335",
      "arxivId": "2501.19335",
      "url": "https://www.semanticscholar.org/paper/b400756067cef54c09feb4a27c8d7a12d5d9042f",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2501.19335"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "386901912643988c7940f98604fbe41a7d65fdde",
      "title": "Causal models and causal relativism",
      "authors": [
        {
          "name": "Jennifer McDonald",
          "authorId": "2303875437"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.1007/s11229-024-04893-5",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/386901912643988c7940f98604fbe41a7d65fdde",
      "venue": "Synthese",
      "journal": {
        "name": "Synthese",
        "volume": "205"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5dcca871dcade0a4e29148bb9a1f282520b32200",
      "title": "Canonical Representations of Markovian Structural Causal Models: A Framework for Counterfactual Reasoning",
      "authors": [
        {
          "name": "Lucas de Lara",
          "authorId": "2381257832"
        }
      ],
      "year": 2025,
      "abstract": "Counterfactual reasoning aims at answering contrary-to-fact questions like ``Would have Alice recovered had she taken aspirin?''and corresponds to the most fine-grained layer of causation. Critically, while many counterfactual statements cannot be falsified-even by randomized experiments-they underpin fundamental concepts like individual-wise fairness. Therefore, providing models to formalize and implement counterfactual beliefs remains a fundamental scientific problem. In the Markovian setting of Pearl's causal framework, we propose an alternative approach to structural causal models to represent counterfactuals compatible with a given causal graphical model. More precisely, we introduce counterfactual models, also called canonical representations of structural causal models. They enable analysts to choose a counterfactual assumption via random-process probability distributions with preassigned marginals and characterize the counterfactual equivalence class of structural causal models. Using these representations, we present a normalization procedure to disentangle the (arbitrary and unfalsifiable) counterfactual choice from the (typically testable) interventional constraints. In contrast to structural causal models, this allows to implement many counterfactual assumptions while preserving interventional knowledge, and does not require any estimation step at the individual-counterfactual layer: only to make a choice. Finally, we illustrate the specific role of counterfactuals in causality and the benefits of our approach on theoretical and numerical examples.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2507.16370",
      "arxivId": "2507.16370",
      "url": "https://www.semanticscholar.org/paper/5dcca871dcade0a4e29148bb9a1f282520b32200",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.16370"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4765e91984dc57a9cf61779fdb7b06d7fe530176",
      "title": "Actual Causation and Nondeterministic Causal Models",
      "authors": [
        {
          "name": "Sander Beckers",
          "authorId": "2302799035"
        }
      ],
      "year": 2025,
      "abstract": "In (Beckers, 2025) I introduced nondeterministic causal models as a generalization of Pearl's standard deterministic causal models. I here take advantage of the increased expressivity offered by these models to offer a novel definition of actual causation (that also applies to deterministic models). Instead of motivating the definition by way of (often subjective) intuitions about examples, I proceed by developing it based entirely on the unique function that it can fulfil in communicating and learning a causal model. First I generalize the more basic notion of counterfactual dependence, second I show how this notion has a vital role to play in the logic of causal discovery, third I introduce the notion of a structural simplification of a causal model, and lastly I bring both notions together in my definition of actual causation. Although novel, the resulting definition arrives at verdicts that are almost identical to those of my previous definition (Beckers, 2021, 2022).",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2503.07849",
      "arxivId": "2503.07849",
      "url": "https://www.semanticscholar.org/paper/4765e91984dc57a9cf61779fdb7b06d7fe530176",
      "venue": "CLEaR",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.07849"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "203de58d0d30c119bac6a2feca01ad4c191ddff6",
      "title": "On Stalnakerian and Lewisian causal models",
      "authors": [
        {
          "name": "Jingzhi Fang",
          "authorId": "2265976343"
        },
        {
          "name": "Jiji Zhang",
          "authorId": "2265932385"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/s11229-025-05092-6",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/203de58d0d30c119bac6a2feca01ad4c191ddff6",
      "venue": "Synthese",
      "journal": {
        "name": "Synthese",
        "volume": "206"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "85731e3a6c688c3f16906ffbb4c551f19adb825d",
      "title": "Topos Causal Models",
      "authors": [
        {
          "name": "Sridhar Mahadevan",
          "authorId": "2288261162"
        }
      ],
      "year": 2025,
      "abstract": "We propose topos causal models (TCMs), a novel class of causal models that exploit the key properties of a topos category: they are (co)complete, meaning all (co)limits exist, they admit a subobject classifier, and allow exponential objects. The main goal of this paper is to show that these properties are central to many applications in causal inference. For example, subobject classifiers allow a categorical formulation of causal intervention, which creates sub-models. Limits and colimits allow causal diagrams of arbitrary complexity to be ``solved\", using a novel interpretation of causal approximation. Exponential objects enable reasoning about equivalence classes of operations on causal models, such as covered edge reversal and causal homotopy. Analogous to structural causal models (SCMs), TCMs are defined by a collection of functions, each defining a ``local autonomous\"causal mechanism that assemble to induce a unique global function from exogenous to endogenous variables. Since the category of TCMs is (co)complete, which we prove in this paper, every causal diagram has a ``solution\"in the form of a (co)limit: this implies that any arbitrary causal model can be ``approximated\"by some global function with respect to the morphisms going into or out of the diagram. Natural transformations are crucial in measuring the quality of approximation. In addition, we show that causal interventions are modeled by subobject classifiers: any sub-model is defined by a monic arrow into its parent model. Exponential objects permit reasoning about entire classes of causal equivalences and interventions. Finally, as TCMs form a topos, they admit an internal logic defined as a Mitchell-Benabou language with an associated Kripke-Joyal semantics. We show how to reason about causal models in TCMs using this internal logic.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.08295",
      "arxivId": "2508.08295",
      "url": "https://www.semanticscholar.org/paper/85731e3a6c688c3f16906ffbb4c551f19adb825d",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.08295"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "93c8b01a3934d7c3b7cbba376b248b7710ca035b",
      "title": "Causality Without Causal Models",
      "authors": [
        {
          "name": "Joseph Y. Halpern",
          "authorId": "2265550766"
        },
        {
          "name": "Rafael Pass",
          "authorId": "2325948296"
        }
      ],
      "year": 2025,
      "abstract": "Perhaps the most prominent current definition of (actual) causality is due to Halpern and Pearl. It is defined using causal models (also known as structural equations models). We abstract the definition, extracting its key features, so that it can be applied to any other model where counterfactuals are defined. By abstracting the definition, we gain a number of benefits. Not only can we apply the definition in a wider range of models, including ones that allow, for example, backtracking, but we can apply the definition to determine if A is a cause of B even if A and B are formulas involving disjunctions, negations, beliefs, and nested counterfactuals (none of which can be handled by the Halpern-Pearl definition). Moreover, we can extend the ideas to getting an abstract definition of explanation that can be applied beyond causal models. Finally, we gain a deeper understanding of features of the definition even in causal models.",
      "citationCount": 0,
      "doi": "10.4204/EPTCS.437.16",
      "arxivId": "2511.21260",
      "url": "https://www.semanticscholar.org/paper/93c8b01a3934d7c3b7cbba376b248b7710ca035b",
      "venue": "Electronic Proceedings in Theoretical Computer Science",
      "journal": {
        "name": "Electronic Proceedings in Theoretical Computer Science"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "85024e1d0a6e5e7e3257f762ddf22096882e3c40",
      "title": "Towards Error-Centric Intelligence II: Energy-Structured Causal Models",
      "authors": [
        {
          "name": "Marcus A. Thomas",
          "authorId": "2386579315"
        }
      ],
      "year": 2025,
      "abstract": "Contemporary machine learning optimizes for predictive accuracy, yet systems that achieve state of the art performance remain causally opaque: their internal representations provide no principled handle for intervention. We can retrain such models, but we cannot surgically edit specific mechanisms while holding others fixed, because learned latent variables lack causal semantics. We argue for a conceptual reorientation: intelligence is the ability to build and refine explanations, falsifiable claims about manipulable structure that specify what changes and what remains invariant under intervention. Explanations subsume prediction but demand more: causal commitments that can be independently tested and corrected at the level of mechanisms. We introduce computational explanations, mappings from observations to intervention ready causal accounts. We instantiate these explanations with Energy Structured Causal Models (ESCMs), in which mechanisms are expressed as constraints (energy functions or vector fields) rather than explicit input output maps, and interventions act by local surgery on those constraints. This shift makes internal structure manipulable at the level where explanations live: which relations must hold, which can change, and what follows when they do. We provide concrete instantiations of the structural-causal principles LAP and ICM in the ESCM context, and also argue that empirical risk minimization systematically produces fractured, entangled representations, a failure we analyze as gauge ambiguity in encoder energy pairs. Finally, we show that under mild conditions, ESCMs recover standard SCM semantics. Building on Part I's principles (LAP, ICM, CAP) and its definition of intelligence as explanation-building under criticism, this paper offers a formal language for causal reasoning in systems that aspire to understand, not merely to predict.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.22050",
      "arxivId": "2510.22050",
      "url": "https://www.semanticscholar.org/paper/85024e1d0a6e5e7e3257f762ddf22096882e3c40",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.22050"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e0ab2deb660da2074a0e95625b801c0454f2dbc1",
      "title": "Essential Structure for Causal Models",
      "authors": [
        {
          "name": "Jennifer McDonald",
          "authorId": "2303875437"
        }
      ],
      "year": 2025,
      "abstract": "ABSTRACT This paper introduces and defends a new principle for when a structural equation model is apt for analyzing actual causation. Any such analysis in terms of these models has two components: a recipe for reading claims of actual causation off an apt model, and an articulation of what makes a model apt. The primary focus in the literature has been on the first component. But the problem of structural isomorphs has made the second especially pressing (Hall 2007; Hitchcock 2007a). Those with realist sympathies have reason to resist the standard response to this problem, which introduces a normative parameter into the metaphysics (Hall 2007; Halpern and Hitchcock 2010, 2015; Halpern 2016a; Menzies 2017; Gallow 2021). However, the only alternative solution in the literature leaves central questions unanswered (Blanchard and Schaffer 2017). I propose an independently motivated aptness requirement, Evident Mediation, that provides the missing details and resolves the structural isomorph problem without need for a normative parameter.",
      "citationCount": 5,
      "doi": "10.1080/00048402.2024.2410848",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e0ab2deb660da2074a0e95625b801c0454f2dbc1",
      "venue": "Australasian Journal of Philosophy",
      "journal": {
        "name": "Australasian Journal of Philosophy",
        "pages": "293 - 315",
        "volume": "103"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4bedfd4ca926bdc868f7d758021059b9d2eae84a",
      "title": "Large Language Models as Nondeterministic Causal Models",
      "authors": [
        {
          "name": "Sander Beckers",
          "authorId": "2302799035"
        }
      ],
      "year": 2025,
      "abstract": "Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first time, a method for generating counterfactuals of probabilistic Large Language Models. Such counterfactuals tell us what would - or might - have been the output of an LLM if some factual prompt ${\\bf x}$ had been ${\\bf x}^*$ instead. The ability to generate such counterfactuals is an important necessary step towards explaining, evaluating, and comparing, the behavior of LLMs. I argue, however, that the existing method rests on an ambiguous interpretation of LLMs: it does not interpret LLMs literally, for the method involves the assumption that one can change the implementation of an LLM's sampling process without changing the LLM itself, nor does it interpret LLMs as intended, for the method involves explicitly representing a nondeterministic LLM as a deterministic causal model. I here present a much simpler method for generating counterfactuals that is based on an LLM's intended interpretation by representing it as a nondeterministic causal model instead. The advantage of my simpler method is that it is directly applicable to any black-box LLM without modification, as it is agnostic to any implementation details. The advantage of the existing method, on the other hand, is that it directly implements the generation of a specific type of counterfactuals that is useful for certain purposes, but not for others. I clarify how both methods relate by offering a theoretical foundation for reasoning about counterfactuals in LLMs based on their intended semantics, thereby laying the groundwork for novel application-specific methods for generating counterfactuals.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2509.22297",
      "arxivId": "2509.22297",
      "url": "https://www.semanticscholar.org/paper/4bedfd4ca926bdc868f7d758021059b9d2eae84a",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.22297"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4f40b1dc9dcf92f6c2f3ffd4711626e1e608531e",
      "title": "Nondeterministic Causal Models",
      "authors": [
        {
          "name": "Sander Beckers",
          "authorId": "2302799035"
        }
      ],
      "year": 2024,
      "abstract": "I generalize acyclic deterministic structural causal models to the nondeterministic case and argue that this offers an improved semantics for counterfactuals. The standard, deterministic, semantics developed by Halpern (and based on the initial proposal of Galles&Pearl) assumes that for each assignment of values to parent variables there is a unique assignment to their child variable, and it assumes that the actual world (an assignment of values to all variables of a model) specifies a unique counterfactual world for each intervention. Both assumptions are unrealistic, and therefore I drop both of them in my proposal. I do so by allowing multi-valued functions in the structural equations. In addition, I adjust the semantics so that the solutions to the equations that obtained in the actual world are preserved in any counterfactual world. I provide a sound and complete axiomatization of the resulting logic and compare it to the standard one by Halpern and to more recent proposals that are closer to mine. Finally, I extend these models to the probabilistic case and show that they open up the way to identifying counterfactuals even in Causal Bayesian Networks.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2405.14001",
      "arxivId": "2405.14001",
      "url": "https://www.semanticscholar.org/paper/4f40b1dc9dcf92f6c2f3ffd4711626e1e608531e",
      "venue": "CLEaR",
      "journal": {
        "pages": "1532-1554"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "cc610e09861c0df9be9f992fffc34a76fd70339a",
      "title": "Bread prices and sea levels: why probabilistic causal models need to be monotonic",
      "authors": [
        {
          "name": "Vera Hoffmann-Kolss",
          "authorId": "2298185299"
        }
      ],
      "year": 2024,
      "abstract": "A key challenge for probabilistic causal models is to distinguish non-causal probabilistic dependencies from true causal relations. To accomplish this task, causal models are usually required to satisfy several constraints. Two prominent constraints are the causal Markov condition and the faithfulness condition. However, other constraints are also needed. One of these additional constraints is the causal sufficiency condition, which states that models must not omit any direct common causes of the variables they contain. In this paper, I argue that the causal sufficiency condition is problematic: (1) it is incompatible with the requirement that the variables in a model must not stand in non-causal necessary dependence relations, such as mathematical or conceptual relations, or relations described in terms of supervenience or grounding, (2) it presupposes more causal knowledge as primitive than is actually needed to create adequate causal models, and (3) if models are only required to be causally sufficient, they cannot deal with cases where variables are probabilistically related by accident, such as Sober\u2019s example of the relationship between bread prices in England and the sea level in Venice. I show that these problems can be avoided if causal models are required to be monotonic in the following sense: the causal relations occurring in a model M would not disappear if further variables were added to M. I give a definition of this monotonicity condition and conclude that causal models should be required to be monotonic rather than causally sufficient.",
      "citationCount": 1,
      "doi": "10.1007/s11098-024-02112-y",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/cc610e09861c0df9be9f992fffc34a76fd70339a",
      "venue": "Philosophical Studies",
      "journal": {
        "name": "Philosophical Studies",
        "pages": "2113 - 2128",
        "volume": "181"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "928107f2315355c60b8b861774103f6515466026",
      "title": "Causal Models and Metaphysics \u2013 Part 1: Using Causal Models",
      "authors": [
        {
          "name": "Jennifer McDonald",
          "authorId": "2303875437"
        }
      ],
      "year": 2024,
      "abstract": "This paper provides a general introduction to the use of causal models in the metaphysics of causation, specifically structural equation models and directed acyclic graphs. It reviews the formal framework, lays out a method of interpretation capable of representing different underlying metaphysical relations, and describes the use of these models in analyzing causation.",
      "citationCount": 0,
      "doi": "10.1111/phc3.12975",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/928107f2315355c60b8b861774103f6515466026",
      "venue": "Philosophy Compass",
      "journal": {
        "name": "Philosophy Compass"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "db5748bf8946f062787cfec3482d20a27bd53c21",
      "title": "Identifying Intended Effects with Causal Models",
      "authors": [
        {
          "name": "Dario Compagno",
          "authorId": "2284220923"
        }
      ],
      "year": 2024,
      "abstract": "The aim of this paper is to extend the framework of causal inference, in particular as it has been developed by Judea Pearl, in order to model actions and identify their intended effects, in the direction opened by Elisabeth Anscombe. We show how intentions can be inferred from a causal model and its implied correlations observable in data. The paper defines confounding effects as the reasons why teleological inference may fail and introduces interference as a way to control for them. The ''fundamental problem'' of teleological inference is presented, explaining why causal analysis needs an extension in order to take intentions into account.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2402.09472",
      "url": "https://www.semanticscholar.org/paper/db5748bf8946f062787cfec3482d20a27bd53c21",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "6fcc99b24143c7983a3cef5a3ad65cd3430ee9c8",
      "title": "Bridging the AI/ML gap with explainable symbolic causal models using information theory",
      "authors": [
        {
          "name": "Stuart W. Card",
          "authorId": "2305420154"
        }
      ],
      "year": 2024,
      "abstract": "We report favorable preliminary findings of work in progress bridging the Artificial Intelligence (AI) gap between bottom-up data-driven Machine Learning (ML) and top-down conceptually driven symbolic reasoning. Our overall goal is automatic generation, maintenance and utilization of explainable, parsimonious, plausibly causal, probably approximately correct, hybrid symbolic/numeric models of the world, the self and other agents, for prediction, what-if (counter-factual) analysis and control. Our old Evolutionary Learning with Information Theoretic Evaluation of Ensembles (ELITE2) techniques quantify strengths of arbitrary multivariate nonlinear statistical dependencies, prior to discovering forms by which observed variables may drive others. We extend these to apply Granger causality, in terms of conditional Mutual Information (MI), to distinguish causal relationships and find their directions. As MI can reflect one observable driving a second directly or via a mediator, two being driven by a common cause, etc., to untangle the causal graph we will apply Pearl causality with its back- and front-door adjustments and criteria. Initial efforts verified that our information theoretic indices detect causality in noise corrupted data despite complex relationships among hidden variables with chaotic dynamics disturbed by process noise, The next step is to apply these information theoretic filters in Genetic Programming (GP) to reduce the population of discovered statistical dependencies to plausibly causal relationships, represented symbolically for use by a reasoning engine in a cognitive architecture. Success could bring broader generalization, using not just learned patterns but learned general principles, enabling AI/ML based systems to autonomously navigate complex unknown environments and handle \u201cblack swans\u201d.",
      "citationCount": 1,
      "doi": "10.1117/12.3014447",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6fcc99b24143c7983a3cef5a3ad65cd3430ee9c8",
      "venue": "Defense + Commercial Sensing",
      "journal": {
        "pages": "1305802 - 1305802-4",
        "volume": "13058"
      },
      "publicationTypes": null
    },
    {
      "paperId": "7829541bb4269770d6fcb550e6c7ae05fd23823f",
      "title": "Quantifying the quality of configurational causal models",
      "authors": [
        {
          "name": "Michael Baumgartner",
          "authorId": "2310902748"
        },
        {
          "name": "Christoph Falk",
          "authorId": "2130299422"
        }
      ],
      "year": 2024,
      "abstract": "Abstract There is a growing number of studies benchmarking the performance of configurational comparative methods (CCMs) of causal data analysis. A core benchmark criterion used in these studies is a dichotomous (i.e., non-quantitative) correctness criterion, which measures whether all causal claims entailed by a model are true of the data-generating causal structure or not. To date, Arel-Bundock [The double bind of Qualitative Comparative Analysis] is the only one who has proposed a measure quantifying correctness. That measure, however, as this study argues, is problematic because it tends to overcount errors in models. Moreover, we show that all available correctness measures are unsuited to assess relations of indirect causation. We therefore introduce a new correctness measure that adequately quantifies errors and does justice to indirect causation. We also offer a new completeness measure quantifying the informativeness of CCM models. Together, these new measures broaden and sharpen the resources for CCM benchmarking.",
      "citationCount": 1,
      "doi": "10.1515/jci-2023-0032",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7829541bb4269770d6fcb550e6c7ae05fd23823f",
      "venue": "Journal of Causal Inference",
      "journal": {
        "name": "Journal of Causal Inference",
        "volume": "12"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "87ad83f736a69f51f3f29017abdb8101bd456b19",
      "title": "Building compressed causal models of the world",
      "authors": [
        {
          "name": "David Kinney",
          "authorId": "2295629791"
        },
        {
          "name": "Tania Lombrozo",
          "authorId": "2298296032"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 5,
      "doi": "10.1016/j.cogpsych.2024.101682",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/87ad83f736a69f51f3f29017abdb8101bd456b19",
      "venue": "Cognitive Psychology",
      "journal": {
        "name": "Cognitive Psychology",
        "volume": "155"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a8d7d7b7efdbe55c906f15d4723b3a0ebb250352",
      "title": "Causal Models and Metaphysics\u2014Part 2: Interpreting Causal Models",
      "authors": [
        {
          "name": "Jennifer McDonald",
          "authorId": "2303875437"
        }
      ],
      "year": 2024,
      "abstract": "This paper addresses the question of what constitutes an apt interpreted model for the purpose of analyzing causation. I first collect universally adopted aptness principles into a basic account, flagging open questions and choice points along the way. I then explore various additional aptness principles that have been proposed in the literature but have not been widely adopted, the motivations behind their proposals, and the concerns with each that stand in the way of universal adoption. I conclude that the remaining work of articulating aptness for a SEM analysis of causation is tied up with issues to do with modality, ontology, and mereology. Continuing this work is therefore likely to shed light on the relationship between these areas and causation more generally.",
      "citationCount": 3,
      "doi": "10.1111/phc3.13007",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a8d7d7b7efdbe55c906f15d4723b3a0ebb250352",
      "venue": "Philosophy Compass",
      "journal": {
        "name": "Philosophy Compass"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
      "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
      "authors": [
        {
          "name": "Emre K\u0131c\u0131man",
          "authorId": "3528206"
        },
        {
          "name": "R. Ness",
          "authorId": "36670968"
        },
        {
          "name": "Amit Sharma",
          "authorId": "2143678801"
        },
        {
          "name": "Chenhao Tan",
          "authorId": "2111727675"
        }
      ],
      "year": 2023,
      "abstract": "The causal capabilities of large language models (LLMs) are a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We conduct a\"behavorial\"study of LLMs to benchmark their capability in generating causal arguments. Across a wide range of tasks, we find that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best-performing existing methods. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain) and event causality (86% accuracy in determining necessary and sufficient causes in vignettes). We perform robustness checks across tasks and show that the capabilities cannot be explained by dataset memorization alone, especially since LLMs generalize to novel datasets that were created after the training cutoff date. That said, LLMs exhibit unpredictable failure modes, and we discuss the kinds of errors that may be improved and what are the fundamental limits of LLM-based answers. Overall, by operating on the text metadata, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. As a result, LLMs may be used by human domain experts to save effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. Given that LLMs ignore the actual data, our results also point to a fruitful research direction of developing algorithms that combine LLMs with existing causal techniques. Code and datasets are available at https://github.com/py-why/pywhy-llm.",
      "citationCount": 381,
      "doi": "10.48550/arXiv.2305.00050",
      "arxivId": "2305.00050",
      "url": "https://www.semanticscholar.org/paper/10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
      "venue": "Trans. Mach. Learn. Res.",
      "journal": {
        "name": "Trans. Mach. Learn. Res.",
        "volume": "2024"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "abc608d221146ee49f8fdfa29ae5787be80cf18c",
      "title": "Causal models, creativity, and diversity",
      "authors": [
        {
          "name": "D. Baciu",
          "authorId": "46216661"
        }
      ],
      "year": 2023,
      "abstract": "Causal models find application in almost all areas of science, and they often support the development of theories that are straightforward and testable. Yet scientists also observe things that surprise them. Fascinated by such observations, they learn to admire the playful aspects of life, as well as its creativity and diversity. Under these circumstances, a compelling question arises: Can causal models explain life\u2019s creativity and diversity? Some life scientists say yes. However, other humanities scholars cast doubt, positing that they reached the end of theory. Here, I build on common empirical observations as well as long-accumulated modeling experience, and I develop a unified framework for causal modeling. The framework gives special attention to life\u2019s creativity and diversity, and it applies to all sciences including physics, biology, the sciences of the city, and the humanities.",
      "citationCount": 7,
      "doi": "10.1057/s41599-023-01540-1",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/abc608d221146ee49f8fdfa29ae5787be80cf18c",
      "venue": "Humanities and Social Sciences Communications",
      "journal": {
        "name": "Humanities and Social Sciences Communications",
        "pages": "1-15",
        "volume": "10"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "3a600863b7d389a52bf24d0b0982dcbbfcc2bbb8",
      "title": "Neural Causal Models for Counterfactual Identification and Estimation",
      "authors": [
        {
          "name": "K. Xia",
          "authorId": "2125225990"
        },
        {
          "name": "Yushu Pan",
          "authorId": "116753268"
        },
        {
          "name": "E. Bareinboim",
          "authorId": "2778721"
        }
      ],
      "year": 2022,
      "abstract": "Evaluating hypothetical statements about how the world would be had a different course of action been taken is arguably one key capability expected from modern AI systems. Counterfactual reasoning underpins discussions in fairness, the determination of blame and responsibility, credit assignment, and regret. In this paper, we study the evaluation of counterfactual statements through neural models. Specifically, we tackle two causal problems required to make such evaluations, i.e., counterfactual identification and estimation from an arbitrary combination of observational and experimental data. First, we show that neural causal models (NCMs) are expressive enough and encode the structural constraints necessary for performing counterfactual reasoning. Second, we develop an algorithm for simultaneously identifying and estimating counterfactual distributions. We show that this algorithm is sound and complete for deciding counterfactual identification in general settings. Third, considering the practical implications of these results, we introduce a new strategy for modeling NCMs using generative adversarial networks. Simulations corroborate with the proposed methodology.",
      "citationCount": 54,
      "doi": "10.48550/arXiv.2210.00035",
      "arxivId": "2210.00035",
      "url": "https://www.semanticscholar.org/paper/3a600863b7d389a52bf24d0b0982dcbbfcc2bbb8",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2210.00035"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "bb7318c3f2989cf2437145c2db3221d68e023881",
      "title": "A Characterization of Lewisian Causal Models",
      "authors": [
        {
          "name": "Jingzhi Fang",
          "authorId": "2265976343"
        },
        {
          "name": "Jiji Zhang",
          "authorId": "2265932385"
        }
      ],
      "year": 2023,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.1007/978-3-031-45558-2_8",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/bb7318c3f2989cf2437145c2db3221d68e023881",
      "venue": "Logic, Rationality, and Interaction",
      "journal": {
        "pages": "94-108"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "49cdf8cdf245bdd9b34b0b762927b6e11558fa0f",
      "title": "What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure",
      "authors": [
        {
          "name": "C\u00e9line Budding",
          "authorId": "2354835196"
        }
      ],
      "year": 2025,
      "abstract": "Abstract It is sometimes assumed that large language models (LLMs) know language, or for example that they know that Paris is the capital of France. But what\u2014if anything\u2014do LLMs actually know? In this paper, I argue that LLMs can acquire tacit knowledge as defined by Martin Davies (1990). Whereas Davies himself denies that neural networks can acquire tacit knowledge, I demonstrate that certain architectural features of LLMs satisfy the constraints of semantic description, syntactic structure, and causal systematicity. Thus, tacit knowledge may serve as a conceptual framework for describing, explaining, and intervening on LLMs and their behavior.",
      "citationCount": 3,
      "doi": "10.1017/psa.2025.19",
      "arxivId": "2504.12187",
      "url": "https://www.semanticscholar.org/paper/49cdf8cdf245bdd9b34b0b762927b6e11558fa0f",
      "venue": "Philosophia Scienti\u00e6",
      "journal": {
        "name": "Philosophy of Science",
        "pages": "785 - 806",
        "volume": "92"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "eb75ab959a4bd5073494e1991d5328d23b8d233e",
      "title": "Causal CSSE: integrating counterfactuals and causality in the explanation of machine learning models",
      "authors": [
        {
          "name": "Omar F. P. Krauss",
          "authorId": "2255649430"
        },
        {
          "name": "M. S. Balbino",
          "authorId": "2255654209"
        },
        {
          "name": "Cristiane N. Nobre",
          "authorId": "2255642373"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.1007/s10994-025-06856-4",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/eb75ab959a4bd5073494e1991d5328d23b8d233e",
      "venue": "Machine-mediated learning",
      "journal": {
        "name": "Machine Learning",
        "volume": "114"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "107f713d0a15ece2fc80c3672bb8d5dfdee3eda2",
      "title": ") Cephalopod molluscs, causal models, and curious minds. Animal Sentience 26(13)",
      "authors": [
        {
          "name": "W. Andrew",
          "authorId": "114694853"
        },
        {
          "name": "Andrew W. Corcoran",
          "authorId": "35336477"
        }
      ],
      "year": 2023,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/107f713d0a15ece2fc80c3672bb8d5dfdee3eda2",
      "venue": "",
      "journal": null,
      "publicationTypes": [
        "Review"
      ]
    },
    {
      "paperId": "914faf9494fbfde1fa96584e9578356a344818ed",
      "title": "Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?",
      "authors": [
        {
          "name": "Louis Vervoort",
          "authorId": "2367271256"
        },
        {
          "name": "Vitaly Nikolaev",
          "authorId": "48942032"
        }
      ],
      "year": 2025,
      "abstract": "We propose a test for abstract causal reasoning in AI, based on scholarship in the philosophy of causation, in particular on the neuron diagrams popularized by D. Lewis. We illustrate the test on advanced Large Language Models (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already capable of correctly identifying causes in cases that are hotly debated in the literature. In order to assess the results of these LLMs and future dedicated AI, we propose a definition of cause in neuron diagrams with a wider validity than published hitherto, which challenges the widespread view that such a definition is elusive. We submit that these results are an illustration of how future philosophical research might evolve: as an interplay between human and artificial expertise.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2506.14239",
      "arxivId": "2506.14239",
      "url": "https://www.semanticscholar.org/paper/914faf9494fbfde1fa96584e9578356a344818ed",
      "venue": "Journal for General Philosophy of Science, Zeitschrift f\u00fcr allgemeine Wissenschaftstheorie",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.14239"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7e38ec1f5f34a1cf1d34603029e1fbaed501461d",
      "title": "Exploring the Dilemma of Causal Incoherence: A Study on the Approaches and Limitations of Large Language Models in Natural Language Inference",
      "authors": [
        {
          "name": "Jon F. Apaolaza",
          "authorId": "2357732753"
        },
        {
          "name": "Bego\u00f1a Altuna",
          "authorId": "2357732244"
        },
        {
          "name": "A. Soroa",
          "authorId": "2260104163"
        },
        {
          "name": "I\u00f1igo Lopez-Gazpio",
          "authorId": "2300292957"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7e38ec1f5f34a1cf1d34603029e1fbaed501461d",
      "venue": "Proces. del Leng. Natural",
      "journal": {
        "name": "Proces. del Leng. Natural",
        "pages": "207-219",
        "volume": "74"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "57c69154955b79b25d5634bd3f29bb6524274d10",
      "title": "From Causal Parrots to Causal Prophets? Towards Sound Causal Reasoning with Large Language Models",
      "authors": [
        {
          "name": "Rahul Babu Shrestha",
          "authorId": "2365801484"
        },
        {
          "name": "Simon Malberg",
          "authorId": "2248470127"
        },
        {
          "name": "Georg Groh",
          "authorId": "2322506420"
        }
      ],
      "year": 2025,
      "abstract": ",",
      "citationCount": 2,
      "doi": "10.18653/v1/2025.nlp4dh-1.29",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/57c69154955b79b25d5634bd3f29bb6524274d10",
      "venue": "Proceedings of the 5th International Conference on Natural Language Processing for Digital Humanities",
      "journal": {
        "name": "Proceedings of the 5th International Conference on Natural Language Processing for Digital Humanities"
      },
      "publicationTypes": [
        "Conference"
      ]
    },
    {
      "paperId": "34cef4009b5e3b23e464a201eaa833d5a44c8959",
      "title": "Causal-driven Large Language Models with Faithful Reasoning for Knowledge Question Answering",
      "authors": [
        {
          "name": "Jiawei Wang",
          "authorId": "2130162840"
        },
        {
          "name": "Da Cao",
          "authorId": "2147412314"
        },
        {
          "name": "Shaofei Lu",
          "authorId": "2205062495"
        },
        {
          "name": "Zhanchang Ma",
          "authorId": "2261954625"
        },
        {
          "name": "Junbin Xiao",
          "authorId": "66358686"
        },
        {
          "name": "Tat-seng Chua",
          "authorId": "2258950626"
        }
      ],
      "year": 2024,
      "abstract": "In Large Language Models (LLMs), text generation that involves knowledge representation is often fraught with the risk of \"hallucinations'', where models confidently produce erroneous or fabricated content. These inaccuracies often stem from intrinsic biases in the pre-training stage or from the incorporation of human preference biases during the fine-tuning process. To mitigate these issues, we take inspiration from Goldman's causal theory of knowledge, which asserts that knowledge is not merely about having a true belief but also involves a causal connection between the belief and the truth of the proposition. We instantiate this theory within the context of Knowledge Question Answering (KQA) by constructing a causal graph that delineates the pathways between the candidate knowledge and belief. Through the application of the do-calculus rules from structural causal models, we devise an unbiased estimation framework based on this causal graph, thereby establishing a methodology for knowledge modeling grounded in causal inference. The resulting CORE framework (short for \"Causal knOwledge REasoning'') is comprised of four essential components: question answering, causal reasoning, belief scoring, and refinement. Together, they synergistically improve the KQA system by fostering faithful reasoning and introspection. Extensive experiments are conducted on ScienceQA and HotpotQA datasets, which demonstrate the effectiveness and rationality of the CORE framework.",
      "citationCount": 7,
      "doi": "10.1145/3664647.3681263",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/34cef4009b5e3b23e464a201eaa833d5a44c8959",
      "venue": "ACM Multimedia",
      "journal": {
        "name": "Proceedings of the 32nd ACM International Conference on Multimedia"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle"
      ]
    },
    {
      "paperId": "d5b5da9b75de98a1a891b8b615c920ec487c43a3",
      "title": "Explaining Causal Models with Argumentation: the Case of Bi-variate Reinforcement",
      "authors": [
        {
          "name": "Antonio Rago",
          "authorId": "1911563"
        },
        {
          "name": "P. Baroni",
          "authorId": "1710592"
        },
        {
          "name": "Francesca Toni",
          "authorId": "49973505"
        }
      ],
      "year": 2022,
      "abstract": "Causal models are playing an increasingly important role in machine learning, particularly in the realm of explainable AI. We introduce a conceptualisation for generating argumentation frameworks (AFs) from causal models for the purpose of forging explanations for the models\u2019 outputs. The conceptualisation is based on reinterpreting desirable properties of semantics of AFs as explanation moulds, which are means for characterising the relations in the causal model argumentatively. We demonstrate our methodology by reinterpreting the property of bi-variate reinforcement as an explanation mould to forge bipolar AFs as explanations for the outputs of causal models. We perform a theoretical evaluation of these\nargumentative explanations, examining whether they satisfy a range of desirable explanatory and argumentative properties.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2205.11589",
      "arxivId": "2205.11589",
      "url": "https://www.semanticscholar.org/paper/d5b5da9b75de98a1a891b8b615c920ec487c43a3",
      "venue": "International Conference on Principles of Knowledge Representation and Reasoning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2205.11589"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    }
  ],
  "count": 30,
  "errors": []
}
