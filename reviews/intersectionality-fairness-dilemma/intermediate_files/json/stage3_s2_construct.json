{
  "status": "success",
  "source": "semantic_scholar",
  "query": "construct validity fairness metrics",
  "results": [
    {
      "paperId": "39aa25abb5ea0037cb09993da4c1aa081d6d06f8",
      "title": "Toward Valid Measurement Of (Un)fairness For Generative AI: A Proposal For Systematization Through The Lens Of Fair Equality of Chances",
      "authors": [
        {
          "name": "K. Truong",
          "authorId": "2372638663"
        },
        {
          "name": "Annette Zimmermann",
          "authorId": "2373046081"
        },
        {
          "name": "Hoda Heidari",
          "authorId": "2256993437"
        }
      ],
      "year": 2025,
      "abstract": "Disparities in the societal harms and impacts of Generative AI (GenAI) systems highlight the critical need for effective unfairness measurement approaches. While numerous benchmarks exist, designing valid measurements requires proper systematization of the unfairness construct. However, this process is often neglected, resulting in metrics that may mischaracterize unfairness by overlooking contextual nuances, thereby compromising the validity of the resulting measurements. Building on established (un)fairness measurement frameworks for predictive AI, this paper focuses on assessing and improving the validity of the measurement task. By extending existing conceptual work in political philosophy, we propose a novel framework for evaluating GenAI unfairness measurement through the lens of the Fair Equality of Chances framework. Our framework decomposes unfairness into three core constituents: the harm or benefit resulting from the system outcomes, morally arbitrary factors that should not lead to inequality in the distribution of harm or benefit, and the morally decisive factors, which distinguish subsets that can justifiably receive different treatments. By examining fairness through this structured lens, we integrate diverse notions of (un)fairness while accounting for the contextual dynamics that shape GenAI outcomes. We analyze factors contributing to each component and the appropriate processes to systematize and measure each in turn. This work establishes a foundation for developing more valid (un)fairness measurements for GenAI systems.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2507.04641",
      "arxivId": "2507.04641",
      "url": "https://www.semanticscholar.org/paper/39aa25abb5ea0037cb09993da4c1aa081d6d06f8",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.04641"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "e2deacbfb81161edc5ae3a77d8d91fd90fc26ac8",
      "title": "Metrics, indicators and analytics to support government excellence programme : the case of Dubai Government Website Excellence Model (WEM)",
      "authors": [
        {
          "name": "Hazza Khalfan M K Hadday Alnuaimi",
          "authorId": "1644445399"
        }
      ],
      "year": 2019,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e2deacbfb81161edc5ae3a77d8d91fd90fc26ac8",
      "venue": "",
      "journal": {
        "name": "",
        "volume": ""
      },
      "publicationTypes": null
    },
    {
      "paperId": "8a4f20a313ce6276fc5f7a166191e52b4f41668d",
      "title": "On Hedden's proof that machine learning fairness metrics are flawed",
      "authors": [
        {
          "name": "Anders S\u00f8gaard",
          "authorId": "2283860651"
        },
        {
          "name": "K. Kappel",
          "authorId": "1726454"
        },
        {
          "name": "Thor Gr\u00fcnbaum",
          "authorId": "2486828"
        }
      ],
      "year": 2024,
      "abstract": "ABSTRACT Brian Hedden, in a recent article in Philosophy and Public Affairs [Hedden 2021. \u201cOn Statistical Criteria of Algorithmic Fairness.\u201d Philosophy and Public Affairs 49 (2): 209\u2013231. https://doi.org/10.1111/papa.v49.2.], presented a thought experiment designed to probe the validity of the fairness metrics used in machine learning (ML). The thought experiment has caused a great stir, also within machine learning [Vigan\u00f3 et al. \u201cPeople are Not Coins: Morally Distinct Types of Predictions Necessitate Different Fairness Constraints.\u201d In 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT '22, 2293\u20132301, New York, NY: Association for Computing Machinery.]. Brian Hedden describes a particular prediction problem p \u2013 involving 40 people divided into two rooms flipping biased coins \u2013 and a binary classification model m for predicting the outcome of these 40 coin flips. Brian Hedden argues that in the thought experiment, m is \u2018perfectly fair\u2019, but at the same time, he shows that almost all existing fairness metrics would score m as unfair. He concludes that almost all existing fairness metrics are flawed. If he is right, this seriously undermines most recent work on fair ML. We present three counter-arguments to Brian Hedden's thought experiment, of which the first is the most important: (a) the prediction problem p is irrelevant for ML because p is not (evaluated as) a learning problem, (b) the model m is not actually fair and (c) the prediction problem p is irrelevant for fairness metrics, because group assignment in p is random.",
      "citationCount": 2,
      "doi": "10.1080/0020174X.2024.2315169",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/8a4f20a313ce6276fc5f7a166191e52b4f41668d",
      "venue": "Inquiry",
      "journal": {
        "name": "Inquiry",
        "pages": "1198 - 1217",
        "volume": "68"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2d23402001da59509b1bc8ee629491e33f5cbbbf",
      "title": "Measuring what Matters: Construct Validity in Large Language Model Benchmarks",
      "authors": [
        {
          "name": "Andrew M. Bean",
          "authorId": "2242554313"
        },
        {
          "name": "R. Kearns",
          "authorId": "2348544925"
        },
        {
          "name": "Angelika Romanou",
          "authorId": "1910588458"
        },
        {
          "name": "Franziska Sofia Hafner",
          "authorId": "2205250896"
        },
        {
          "name": "Harry Mayne",
          "authorId": "2290011658"
        },
        {
          "name": "Jan Batzner",
          "authorId": "2366021434"
        },
        {
          "name": "Negar Foroutan",
          "authorId": "9737058"
        },
        {
          "name": "Chris Schmitz",
          "authorId": "2366011151"
        },
        {
          "name": "Karolina Korgul",
          "authorId": "2257035147"
        },
        {
          "name": "Hunar Batra",
          "authorId": "2290487774"
        },
        {
          "name": "Oishi Deb",
          "authorId": "2273066972"
        },
        {
          "name": "Emma Beharry",
          "authorId": "2391520984"
        },
        {
          "name": "Cornelius Emde",
          "authorId": "153438331"
        },
        {
          "name": "Thomas Foster",
          "authorId": "2345922826"
        },
        {
          "name": "Anna Gausen",
          "authorId": "2275054359"
        },
        {
          "name": "Mar\u00eda Grandury",
          "authorId": "2176184513"
        },
        {
          "name": "Simeng Han",
          "authorId": "3226782"
        },
        {
          "name": "Valentin Hofmann",
          "authorId": "2325952857"
        },
        {
          "name": "Lujain Ibrahim",
          "authorId": "2296990193"
        },
        {
          "name": "Hazel Kim",
          "authorId": "2401590388"
        },
        {
          "name": "Hannah Rose Kirk",
          "authorId": "90729626"
        },
        {
          "name": "Fangru Lin",
          "authorId": "2279899894"
        },
        {
          "name": "Gabrielle Kaili-May Liu",
          "authorId": "2383414185"
        },
        {
          "name": "Lennart Luettgau",
          "authorId": "2372808848"
        },
        {
          "name": "Jabez Magomere",
          "authorId": "2305622172"
        },
        {
          "name": "Jonathan Rystr\u00f8m",
          "authorId": "2346109360"
        },
        {
          "name": "Anna Sotnikova",
          "authorId": "2289611422"
        },
        {
          "name": "Yushi Yang",
          "authorId": "2316173013"
        },
        {
          "name": "Yilun Zhao",
          "authorId": "46316984"
        },
        {
          "name": "Adel Bibi",
          "authorId": "2257303816"
        },
        {
          "name": "A. Bosselut",
          "authorId": "2284866282"
        },
        {
          "name": "Ronald Clark",
          "authorId": "2333506799"
        },
        {
          "name": "Arman Cohan",
          "authorId": "2266838179"
        },
        {
          "name": "Jakob Foerster",
          "authorId": "2345921728"
        },
        {
          "name": "Yarin Gal",
          "authorId": "2315116895"
        },
        {
          "name": "Scott A. Hale",
          "authorId": "1741886127"
        },
        {
          "name": "Inioluwa Deborah Raji",
          "authorId": "81316798"
        },
        {
          "name": "Chris Summerfield",
          "authorId": "2343744579"
        },
        {
          "name": "Philip H. S. Torr",
          "authorId": "2282534002"
        },
        {
          "name": "C. Ududec",
          "authorId": "11285234"
        },
        {
          "name": "Luc Rocher",
          "authorId": "21687784"
        },
        {
          "name": "Adam Mahdi",
          "authorId": "2380689923"
        }
      ],
      "year": 2025,
      "abstract": "Evaluating large language models (LLMs) is crucial for both assessing their capabilities and identifying safety or robustness issues prior to deployment. Reliably measuring abstract and complex phenomena such as'safety'and'robustness'requires strong construct validity, that is, having measures that represent what matters to the phenomenon. With a team of 29 expert reviewers, we conduct a systematic review of 445 LLM benchmarks from leading conferences in natural language processing and machine learning. Across the reviewed articles, we find patterns related to the measured phenomena, tasks, and scoring metrics which undermine the validity of the resulting claims. To address these shortcomings, we provide eight key recommendations and detailed actionable guidance to researchers and practitioners in developing LLM benchmarks.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2511.04703",
      "arxivId": "2511.04703",
      "url": "https://www.semanticscholar.org/paper/2d23402001da59509b1bc8ee629491e33f5cbbbf",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2511.04703"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "326a2f3705dc7e761e1753aadf95259d7e0e0596",
      "title": "Construct Validity in Software Engineering Research and Software Metrics",
      "authors": [
        {
          "name": "P. Ralph",
          "authorId": "144785030"
        },
        {
          "name": "E. Tempero",
          "authorId": "1795768"
        }
      ],
      "year": 2018,
      "abstract": "Construct validity is essentially the degree to which our scales, metrics and instruments actually measure the properties they are supposed to measure. Although construct validity is widely considered an important quality criterion for most empirical research, many software engineering studies simply assume that proposed measures are valid and make no attempt to assess construct validity. Researchers may ignore construct validity because evaluating it is intrinsically difficult, or due to lack of specific guidance for addressing it. In any case, some research inevitably produces erroneous conclusions, because due to invalid measures. This article therefore attempts to address these problems by explaining the theoretical basis of construct validity, presenting a framework for understanding it, and developing specific guidelines for assessing it. The paper draws on a detailed example involving 15 software metrics, which ostensibly measure the size, coupling and cohesion of Java classes.",
      "citationCount": 74,
      "doi": "10.1145/3210459.3210461",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/326a2f3705dc7e761e1753aadf95259d7e0e0596",
      "venue": "International Conference on Evaluation & Assessment in Software Engineering",
      "journal": {
        "name": "Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "bd2d5f2e445c3b7a2dd4f62a8c7131c78264dd70",
      "title": "Eye-Based Construct Measurement and Its Validity",
      "authors": [
        {
          "name": "Feiyan Jia",
          "authorId": "2119621482"
        },
        {
          "name": "C. Sia",
          "authorId": "144588482"
        },
        {
          "name": "Yue Liu",
          "authorId": "2331575067"
        },
        {
          "name": "Jingwei Li",
          "authorId": "2144505832"
        }
      ],
      "year": 2025,
      "abstract": "Objective eye movement data have the potential to measure users' states instantly and in real time, providing a basis for timely intervention and personalized adaptation. Despite the broad applicability of eye-based construct measurement, research on its development and validation remains limited. Following the preferred reporting items for systematic reviews and meta-analyses guidelines, this review analyzes 127 studies that investigate the use of eye-tracking metrics to measure abstract constructs and the corresponding validity evidence. The findings reveal that eye-tracking metrics can measure a wide variety of constructs. Drawing on validity evidence commonly employed in psychometric-based construct measurement, this study synthesizes and summarizes validity evidence applicable to eye-based construct measurement. To illustrate the application of eye-tracking metrics and their supporting evidence, this article uses the example of detecting a vehicle driver's cognitive load, offering guidance for future studies and practical applications.",
      "citationCount": 0,
      "doi": "10.4018/jdm.394245",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/bd2d5f2e445c3b7a2dd4f62a8c7131c78264dd70",
      "venue": "Journal of Database Management",
      "journal": {
        "name": "Journal of Database Management"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "360179f6b4201cf0780f29c9c1a7b7cb4992216f",
      "title": "Face, content, and construct validity of a novel VR/AR surgical simulator of a minimally invasive spine operation",
      "authors": [
        {
          "name": "Sami Alkadri",
          "authorId": "2124234058"
        },
        {
          "name": "Rolando F. Del Maestro",
          "authorId": "2301464229"
        },
        {
          "name": "M. Driscoll",
          "authorId": "2264567023"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.1007/s11517-024-03053-8",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/360179f6b4201cf0780f29c9c1a7b7cb4992216f",
      "venue": "Medical and Biological Engineering and Computing",
      "journal": {
        "name": "Medical & Biological Engineering & Computing",
        "pages": "1887 - 1897",
        "volume": "62"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d895fd4fe246ae4f38813a5ecfbd6ec6d09bd306",
      "title": "Increasing the Construct Validity of Computational Phenotypes of Mental Illness Through Active Inference and Brain Imaging",
      "authors": [
        {
          "name": "Roberto Limongi",
          "authorId": "2336405263"
        },
        {
          "name": "Alexandra B. Skelton",
          "authorId": "2336401291"
        },
        {
          "name": "Lydia H. Tzianas",
          "authorId": "2336399580"
        },
        {
          "name": "Angelica M. Silva",
          "authorId": "2336537935"
        }
      ],
      "year": 2024,
      "abstract": "After more than 30 years since its inception, the utility of brain imaging for understanding and diagnosing mental illnesses is in doubt, receiving well-grounded criticisms from clinical practitioners. Symptom-based correlational approaches have struggled to provide psychiatry with reliable brain-imaging metrics. However, the emergence of computational psychiatry has paved a new path not only for understanding the psychopathology of mental illness but also to provide practical tools for clinical practice in terms of computational metrics, specifically computational phenotypes. However, these phenotypes still lack sufficient test\u2013retest reliability. In this review, we describe recent works revealing that mind and brain-related computational phenotypes show structural (not random) variation over time, longitudinal changes. Furthermore, we show that these findings suggest that understanding the causes of these changes will improve the construct validity of the phenotypes with an ensuing increase in test\u2013retest reliability. We propose that the active inference framework offers a general-purpose approach for causally understanding these longitudinal changes by incorporating brain imaging as observations within partially observable Markov decision processes.",
      "citationCount": 0,
      "doi": "10.3390/brainsci14121278",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/d895fd4fe246ae4f38813a5ecfbd6ec6d09bd306",
      "venue": "Brain Science",
      "journal": {
        "name": "Brain Sciences",
        "volume": "14"
      },
      "publicationTypes": [
        "Review",
        "JournalArticle"
      ]
    },
    {
      "paperId": "0c85c9aa0a4146a3d089db293b39d2b6c9fd28a7",
      "title": "An Assessment of Construct Validity and Nomological Validity of the Measurement Model for ChatGPT UX Evaluation",
      "authors": [
        {
          "name": "Da Un Jeong",
          "authorId": "2338589394"
        },
        {
          "name": "Ken Nah",
          "authorId": "2338587888"
        }
      ],
      "year": 2024,
      "abstract": "The present study aims to evaluate the construct validity and nomological validity of the measurement model for ChatGPT UX evaluation. A careful literature review leaded to the identification of a total of thirty-six UX evaluation metrics. N=16 respondents participated in Delphi and nine ChatGPT UX evaluation metrics were selected, including performance expectancy, effort expectancy, usability, capability, communication, co-creation, effectiveness, innovativeness, and interaction quality. A survey study was conducted and N=212 respondents rated their ChatGPT UX via fifty-six measurement items. An exploratory factor analysis was performed on the collected survey data and the result indicates that two UX metrics of usability and effectiveness were excluded. A confirmatory factor analysis was conducted to assess the construct validity of the measurement model, and the results reveal that the validity level of the measurement model proposed in the present work is fairly satisfactory. Lastly, the results from the test of nomological validity of the measurement model indicate that, the seven UX evaluation metrics collectively constitute a single second-order construct representing ChatGPT UX.",
      "citationCount": 0,
      "doi": "10.46248/kidrs.2024.4.108",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/0c85c9aa0a4146a3d089db293b39d2b6c9fd28a7",
      "venue": "Korea Institute of Design Research Society",
      "journal": {
        "name": "Korea Institute of Design Research Society"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "ea0d6e13ff6c7fbd7ed8d0cf7fbc6f35e5ec40c8",
      "title": "Fairness measurement system embedding teacher professional accreditation metrics in foreign language EdTech adoption",
      "authors": [
        {
          "name": "Baixu Chen",
          "authorId": "2402127021"
        }
      ],
      "year": 2025,
      "abstract": "This study designs and evaluates a fairness measurement system that embeds teacher accreditation metrics into the evaluation logic of a foreign language Educational Technology platform. The system defines a composite Fairness Metric Index integrating pedagogical quality, assessment literacy, and professional reflection, and is deployed with 264 English as a Foreign Language teachers across 18 institutions, covering 38,412 lessons and 126,507 feedback events. Compared with the platforms original performance index, the integrated model increases the correlation with human accreditation ratings from 0.46 to 0.71 and raises the conditional R squared of mixed models from 0.37 to 0.62. Group-wise parity loss between institutional and experience groups falls by roughly fifty percent, and agreement with an independent accreditation panel improves from kappa 0.51 to 0.74. These results indicate that embedding accreditation constructs into algorithmic scoring can simultaneously improve alignment with professional standards and reduce systematic unfairness in teacher evaluation.",
      "citationCount": 0,
      "doi": "10.54254/3049-7248/2025.31036",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/ea0d6e13ff6c7fbd7ed8d0cf7fbc6f35e5ec40c8",
      "venue": "Journal of Education and Educational Policy Studies",
      "journal": {
        "name": "Journal of Education and Educational Policy Studies"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5d14808ffffb9add548be4c1e74ed0022d4c2734",
      "title": "CAN ARTIFICIAL INTELLIGENCE (AI) JUDGE REFLECTION? VALIDITY, RELIABILITY, AND FAIRNESS OF GENERATIVE AI-ASSISTED ASSESSMENT IN POSTGRADUATE HEALTH PROFESSIONS EDUCATION",
      "authors": [
        {
          "name": "B. Jamil",
          "authorId": "66992170"
        },
        {
          "name": "Nowshad Asim",
          "authorId": "14503924"
        }
      ],
      "year": 2025,
      "abstract": "ABSTRACT\nOBJECTIVES\nThis study aimed to evaluate the role of GenAI in assessing reflective writing among Master of Health Professions Education (MHPE) students by comparing GenAI scores with those of human raters, examining subgroup fairness, exploring stakeholder perceptions, and proposing governance recommendations.\nMETHODOLOGY\nA sequential mixed-methods study was conducted in an MHPE programme at Khyber Medical University, Pakistan. In Phase I, 120 Gibbs-structured reflections from 40 students were scored by three trained faculty raters and a GPT-4-level GenAI model using an eight-dimensional rubric. Inter-rater reliability, AI-human agreement, and subgroup differences by gender, discipline, and career stage were examined. In Phase II, semi-structured interviews were conducted with 10 MHPE students and the three faculty raters. Data were analysed using reflexive thematic analysis and integrated with quantitative results.\nRESULTSHuman scoring demonstrated strong reliability (ICC = .82). GenAI showed high alignment with human ratings for surface-level dimensions such as clarity and language mechanics (r = .81-.84), but only modest agreement for higher-order reflective constructs including feelings, analysis, and conclusion (r = .49-.59). Exploratory subgroup analyses revealed no statistically significant differences in AI-human discrepancies. However, qualitative accounts highlighted concerns about linguistic and cultural fairness. Participants valued AI for efficient, organised feedback but consistently emphasised its inability to interpret emotional nuance, contextual meaning, or developmental trajectories. Faculty stressed the irreplaceability of human judgment and the need for transparent governance and fairness monitoring.\nCONCLUSION\nGenAI can effectively support the assessment of structural and linguistic aspects of reflective writing but remains limited in evaluating deeper reflective constructs central to postgraduate learning. Ethical and educationally sound integration requires hybrid human-AI approaches in which AI provides formative support while human evaluators retain primary responsibility for interpretive judgment, fairness oversight, and professional mentorship. GenAI should supplement, not replace, human assessment of reflective writing.",
      "citationCount": 0,
      "doi": "10.37762/jgmds.13-1.835",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/5d14808ffffb9add548be4c1e74ed0022d4c2734",
      "venue": "Journal of Gandhara Medical and Dental Science",
      "journal": {
        "name": "Journal of Gandhara Medical and Dental Science"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b6430c57c0b57473a5b6e15e0c217a13d2556f27",
      "title": "Model-agnostic bias mitigation methods with regressor distribution control for Wasserstein-based fairness metrics",
      "authors": [
        {
          "name": "A. Miroshnikov",
          "authorId": "1413127992"
        },
        {
          "name": "Konstandinos Kotsiopoulos",
          "authorId": "90197834"
        },
        {
          "name": "Ryan Franks",
          "authorId": "2141584320"
        },
        {
          "name": "Arjun Ravi Kannan",
          "authorId": "2007715324"
        }
      ],
      "year": 2021,
      "abstract": "This article is a companion paper to our earlier work Miroshnikov et al. (2021) on fairness interpretability, which introduces bias explanations. In the current work, we propose a bias mitigation methodology based upon the construction of post-processed models with fairer regressor distributions for Wasserstein-based fairness metrics. By identifying the list of predictors contributing the most to the bias, we reduce the dimensionality of the problem by mitigating the bias originating from those predictors. The post-processing methodology involves reshaping the predictor distributions by balancing the positive and negative bias explanations and allows for the regressor bias to decrease. We design an algorithm that uses Bayesian optimization to construct the bias-performance efficient frontier over the family of post-processed models, from which an optimal model is selected. Our novel methodology performs optimization in low-dimensional spaces and avoids expensive model retraining.",
      "citationCount": 5,
      "doi": null,
      "arxivId": "2111.11259",
      "url": "https://www.semanticscholar.org/paper/b6430c57c0b57473a5b6e15e0c217a13d2556f27",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2111.11259"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f6194a242b2184b8896481d4cd6f1e81db4f7286",
      "title": "Help or Hinder? Evaluating the Impact of Fairness Metrics and Algorithms in Visualizations for Consensus Ranking",
      "authors": [
        {
          "name": "Hilson Shrestha",
          "authorId": "2176781440"
        },
        {
          "name": "Kathleen Cachel",
          "authorId": "2176779395"
        },
        {
          "name": "Mallak Alkhathlan",
          "authorId": "2125482219"
        },
        {
          "name": "Elke A. Rundensteiner",
          "authorId": "66044002"
        },
        {
          "name": "Lane Harrison",
          "authorId": "2061185877"
        }
      ],
      "year": 2023,
      "abstract": "For applications where multiple stakeholders provide recommendations, a fair consensus ranking must not only ensure that the preferences of rankers are well represented, but must also mitigate disadvantages among socio-demographic groups in the final result. However, there is little empirical guidance on the value or challenges of visualizing and integrating fairness metrics and algorithms into human-in-the-loop systems to aid decision-makers. In this work, we design a study to analyze the effectiveness of integrating such fairness metrics-based visualization and algorithms. We explore this through a task-based crowdsourced experiment comparing an interactive visualization system for constructing consensus rankings, ConsensusFuse, with a similar system that includes visual encodings of fairness metrics and fair-rank generation algorithms, FairFuse. We analyze the measure of fairness, agreement of rankers\u2019 decisions, and user interactions in constructing the fair consensus ranking across these two systems. In our study with 200 participants, results suggest that providing these fairness-oriented support features nudges users to align their decision with the fairness metrics while minimizing the tedious process of manually having to amend the consensus ranking. We discuss the implications of these results for the design of next-generation fairness oriented-systems and along with emerging directions for future research.",
      "citationCount": 5,
      "doi": "10.1145/3593013.3594108",
      "arxivId": "2308.06233",
      "url": "https://www.semanticscholar.org/paper/f6194a242b2184b8896481d4cd6f1e81db4f7286",
      "venue": "Conference on Fairness, Accountability and Transparency",
      "journal": {
        "name": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "00b42fd3bc8a2d27ac38141bd7a1ce1cca4a4fe2",
      "title": "AB061. SOH21AS058. Evaluating the use of a proximal femoral nail module using a virtual reality orthopaedic simulator: a face and construct validity study",
      "authors": [
        {
          "name": "A. Feeley",
          "authorId": "2051520431"
        },
        {
          "name": "I. Feeley",
          "authorId": "50077084"
        },
        {
          "name": "K. Merghani",
          "authorId": "1782533894"
        },
        {
          "name": "E. Sheehan",
          "authorId": "144416009"
        }
      ],
      "year": 2021,
      "abstract": "Background: Changing surgical practice due to the European Working Time Directive, shorter training times, and novel technological advancements compel current surgical curricula to find innovative methods of training to ensure adequate development and retention of surgical skills in our trainees. Virtual reality simulation as a learning tool to transcend current training methods have been of recent interest. This study aimed to identify the face and construct validity of the Precision OS trauma module proximal femoral nail procedure. Methods: A comparative interventional study was carried out in a regional orthopaedics trauma unit hospital. Volunteers were stratified into novice, intermediate and expert groups based on self-reported levels of experience. Each participant carried out a simulated proximal femoral nail on an immersive virtual platform following instruction on its use, with objective metrics such as time and X-rays, and novel metrics calculated by the simulation module recorded. Face validity was also assessed. Results: The proximal femoral nail module demonstrated construct validity. Kruskal Wallis test demonstrated a statistically significant difference across all groups novel performance (P=0.018). Intermediate surgeons performed significantly better than novices (P=0.022), with shorter procedural times (P=0.018) Three of the intermediate group achieved the proficiency level set by the expert group, with no significant difference noted between these two groups (P=0.06). Conclusions: The proximal femoral nail module demonstrated good face, and construct validity. Further research is needed to evaluate virtual reality system uses in trauma cases, the potential for acquisition of non-technical skills and the transfer of these skills to the operating room.",
      "citationCount": 0,
      "doi": "10.21037/MAP-21-AB061",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/00b42fd3bc8a2d27ac38141bd7a1ce1cca4a4fe2",
      "venue": "",
      "journal": {
        "name": "",
        "volume": "5"
      },
      "publicationTypes": null
    },
    {
      "paperId": "e9b4c320d49b29cd1b19a773fa007830783e8838",
      "title": "User Experience of Alexa when controlling music: comparison of face and construct validity of four questionnaires",
      "authors": [
        {
          "name": "Birgit Popp",
          "authorId": "2165578062"
        },
        {
          "name": "Michael Breiter",
          "authorId": "1811419554"
        },
        {
          "name": "Miriam Kurz",
          "authorId": "147512572"
        },
        {
          "name": "Johanna Schiwy",
          "authorId": "1811472404"
        }
      ],
      "year": 2020,
      "abstract": "We evaluate the user experience (UX) of Amazon's Alexa when users play and control music. For measuring UX we use established UX metrics (SASSI, SUISQ-R, SUS, AttrakDiff). We investigated face validity by asking users to rate how well they think a questionnaire measures what it is supposed to measure and we assessed construct validity by correlating UX scores of questionnaires with each other. We find a mismatch between face and construct validity of the evaluated questionnaires. Specifically, users feel that SASSI represents their experience better than other questionnaires, however this is not supported by correlations between questionnaires, which suggest that all investigated questionnaires measure UX to a similar extent. Importantly, the fact that face validity and construct validity diverge is not surprising as this has been observed before. Our work adds to existing literature by providing face and construct validity scores of UX questionnaires for interactions with the common speech assistant Alexa.",
      "citationCount": 15,
      "doi": "10.1145/3405755.3406122",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/e9b4c320d49b29cd1b19a773fa007830783e8838",
      "venue": "CIU",
      "journal": {
        "name": "Proceedings of the 2nd Conference on Conversational User Interfaces"
      },
      "publicationTypes": [
        "Book",
        "JournalArticle"
      ]
    },
    {
      "paperId": "ba6ba91a4bf329e6e6eaf7e7459d328cac0cdd2a",
      "title": "Fairness Metrics and Maximum Completeness for the Prediction of Discrimination",
      "authors": [
        {
          "name": "A. Simonetta",
          "authorId": "153889305"
        },
        {
          "name": "Tsuyoshi Nakajima",
          "authorId": "2115176587"
        },
        {
          "name": "M. Paoletti",
          "authorId": "46675486"
        },
        {
          "name": "A. Venticinque",
          "authorId": "3013876"
        }
      ],
      "year": 2022,
      "abstract": null,
      "citationCount": 1,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/ba6ba91a4bf329e6e6eaf7e7459d328cac0cdd2a",
      "venue": "IWESQ/APSEDEI@APSEC",
      "journal": {
        "pages": "13-20"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "91ab18a90e2d074527a5fa686c818f26a76c694b",
      "title": "Construct validity of eoSim \u2013 a low-cost and portable laparoscopic simulator",
      "authors": [
        {
          "name": "Saira Mauland Mansoor",
          "authorId": "50646447"
        },
        {
          "name": "C. V\u00e5penstad",
          "authorId": "2087614"
        },
        {
          "name": "R. M\u00e5rvik",
          "authorId": "2040102"
        },
        {
          "name": "T. Glomsaker",
          "authorId": "4150688"
        },
        {
          "name": "M. Bliks\u00f8en",
          "authorId": "1397803543"
        }
      ],
      "year": 2019,
      "abstract": "Abstract Purpose: To examine the construct validity of the low-cost, portable laparoscopic simulator eoSim using motion analysis. Material and methods: Novice and experienced surgeons (\u2264 100 and >100 laparoscopic procedures performed, respectively) completed four tasks on the eoSim using the SurgTrac software: intracorporeal suture and tie, tube ligation, peg capping and precision cutting. The following metrics were recorded: Time to complete task, distance traveled, handedness (left- versus right hand use), time off-screen, distance between instrument tips, speed, acceleration and motion smoothness. Results: Compared to novices (n\u2009=\u200922), experienced surgeons (n\u2009=\u200914) completed tasks in less time (p\u2009\u2264\u2009.025), except when performing peg capping (p\u2009=\u2009.052). On all tasks, they also scored lower on the distance metric (p\u2009\u2264\u2009.001). Differences in handedness (left hand compared between groups, right hand compared between groups) were found to be significant for three tasks (p\u2009\u2264\u2009.025). In general, the experienced group made greater use of their left hand than the novice group. Conclusion: The eoSim can differentiate between experienced and novice surgeons on the tasks intracorporeal suture and tie, tube ligation and precision cutting, thus providing a convenient method for surgical departments to implement testing of their surgeons\u2019 basic laparoscopic skills.",
      "citationCount": 13,
      "doi": "10.1080/13645706.2019.1638411",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/91ab18a90e2d074527a5fa686c818f26a76c694b",
      "venue": "MITAT. Minimally invasive therapy & allied technologies",
      "journal": {
        "name": "Minimally Invasive Therapy & Allied Technologies",
        "pages": "261 - 268",
        "volume": "29"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "6ff1c2b68b43bd3b4a051e15763490260ea74836",
      "title": "Use of a Low-Cost Portable 3D Virtual Reality Simulator for Psychomotor Skill Training in Minimally Invasive Surgery: Task Metrics and Score Validity",
      "authors": [
        {
          "name": "Fernando Alvarez-Lopez",
          "authorId": "1422757137"
        },
        {
          "name": "M. Maina",
          "authorId": "13526925"
        },
        {
          "name": "Fernando Arango",
          "authorId": "143681290"
        },
        {
          "name": "F. Saig\u00ed-Rubi\u00f3",
          "authorId": "102159135"
        }
      ],
      "year": 2020,
      "abstract": "Background The high cost and low availability of virtual reality simulators in surgical specialty training programs in low- and middle-income countries make it necessary to develop and obtain sources of validity for new models of low-cost portable simulators that enable ubiquitous learning of psychomotor skills in minimally invasive surgery. Objective The aim of this study was to obtain validity evidence for relationships to other variables, internal structure, and consequences of testing for the task scores of a new low-cost portable simulator mediated by gestures for learning basic psychomotor skills in minimally invasive surgery. This new simulator is called SIMISGEST-VR (Simulator of Minimally Invasive Surgery mediated by Gestures - Virtual Reality). Methods In this prospective observational validity study, the authors looked for multiple sources of evidence (known group construct validity, prior videogaming experience, internal structure, test-retest reliability, and consequences of testing) for the proposed SIMISGEST-VR tasks. Undergraduate students (n=100, reference group), surgical residents (n=20), and experts in minimally invasive surgery (n=28) took part in the study. After answering a demographic questionnaire and watching a video of the tasks to be performed, they individually repeated each task 10 times with each hand. The simulator provided concurrent, immediate, and terminal feedback and obtained the task metrics (time and score). From the reference group, 29 undergraduate students were randomly selected to perform the tasks 6 months later in order to determine test-retest reliability. Results Evidence from multiple sources, including strong intrarater reliability and internal consistency, considerable evidence for the hypothesized consequences of testing, and partial confirmation for relations to other variables, supports the validity of the scores and the metrics used to train and teach basic psychomotor skills for minimally invasive surgery via a new low-cost portable simulator that utilizes interaction technology mediated by gestures. Conclusions The results obtained provided multiple sources of evidence to validate SIMISGEST-VR tasks aimed at training novices with no prior experience and enabling them to learn basic psychomotor skills for minimally invasive surgery.",
      "citationCount": 15,
      "doi": "10.2196/19723",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6ff1c2b68b43bd3b4a051e15763490260ea74836",
      "venue": "JMIR Serious Games",
      "journal": {
        "name": "JMIR Serious Games",
        "volume": "8"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f4e766da620d7d80f25152e12b0cf03e76bcdcc2",
      "title": "On the Choice of Fairness: Finding Representative Fairness Metrics for a Given Context",
      "authors": [
        {
          "name": "H. Anahideh",
          "authorId": "2126533826"
        },
        {
          "name": "Nazanin Nezami",
          "authorId": "2029527236"
        },
        {
          "name": "Abolfazl Asudeh",
          "authorId": "1717283"
        }
      ],
      "year": 2021,
      "abstract": null,
      "citationCount": 6,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/f4e766da620d7d80f25152e12b0cf03e76bcdcc2",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2109.05697"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a9ce1a76c3b3839294bbdf9eaaaf684b21138728",
      "title": "Reducing Bias in Classification using Fairness Stacking Meta-Learning",
      "authors": [
        {
          "name": "Omar Shakir",
          "authorId": "2392301143"
        },
        {
          "name": "Maan Y Anad Alsaleem",
          "authorId": "2392306969"
        },
        {
          "name": "Dindar M Ahmed",
          "authorId": "2392303817"
        }
      ],
      "year": 2025,
      "abstract": "The predictive validity of machine learning models depends on the training data. In some cases, training data contains historical, social, or demographic inequalities, which leads algorithms to reproduce unfair results. This paper proposes a fairness-constrained stacking meta-learning approach for reducing bias in classification by aggregating a set of classifiers through a constrained ensemble learning scheme. A set of base classifiers, including Decision Tree, Naive Bayes, Support Vector Machine (SVM), and LightGBM, are trained and evaluated on the Adult Census Income dataset using both predictive and fairness metrics. The final meta-model is constructed as an aggregation of only the fair-performing models, while models failing to meet the fairness threshold are excluded. Learned weights are then optimized to maximize the F1-score while maintaining fairness constraints. Experimental results demonstrate that the proposed method achieves predictive performance (Accuracy = 0.91, F1-score = 0.82) while substantially reducing disparity between demographic groups (EOD = 0.03 for sex and 0.04 for race). These findings indicate that fairness-aware stacking ensembles can provide a solution for mitigating algorithmic bias through an aggregation framework that balances accuracy and fairness.",
      "citationCount": 0,
      "doi": "10.25195/ijci.v51i2.629",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a9ce1a76c3b3839294bbdf9eaaaf684b21138728",
      "venue": "Iraqi Journal for Computers and Informatics",
      "journal": {
        "name": "Iraqi Journal for Computers and Informatics"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ca6789df4ce150fec1898820a7e2ecc38e2e01da",
      "title": "LLMs on Trial: Evaluating Judicial Fairness for Large Language Models",
      "authors": [
        {
          "name": "Yiran Hu",
          "authorId": "2212045428"
        },
        {
          "name": "Zongyue Xue",
          "authorId": "2253400746"
        },
        {
          "name": "Haitao Li",
          "authorId": "2108590438"
        },
        {
          "name": "Siyuan Zheng",
          "authorId": "2374022118"
        },
        {
          "name": "Qingjing Chen",
          "authorId": "2347043166"
        },
        {
          "name": "Shaochun Wang",
          "authorId": "2373694579"
        },
        {
          "name": "Xihan Zhang",
          "authorId": "2373532621"
        },
        {
          "name": "Ning Zheng",
          "authorId": "2307916600"
        },
        {
          "name": "Yun Liu",
          "authorId": "2238545223"
        },
        {
          "name": "Qingyao Ai",
          "authorId": "2256982003"
        },
        {
          "name": "Yiqun Liu",
          "authorId": "2260835922"
        },
        {
          "name": "Charles L.A. Clarke",
          "authorId": "2351797159"
        },
        {
          "name": "Weixing Shen",
          "authorId": "2211946754"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) are increasingly used in high-stakes fields where their decisions impact rights and equity. However, LLMs'judicial fairness and implications for social justice remain underexplored. When LLMs act as judges, the ability to fairly resolve judicial issues is a prerequisite to ensure their trustworthiness. Based on theories of judicial fairness, we construct a comprehensive framework to measure LLM fairness, leading to a selection of 65 labels and 161 corresponding values. Applying this framework to the judicial system, we compile an extensive dataset, JudiFair, comprising 177,100 unique case facts. To achieve robust statistical inference, we develop three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and introduce a method to assess the overall fairness of multiple LLMs across various labels. Through experiments with 16 LLMs, we uncover pervasive inconsistency, bias, and imbalanced inaccuracy across models, underscoring severe LLM judicial unfairness. Particularly, LLMs display notably more pronounced biases on demographic labels, with slightly less bias on substance labels compared to procedure ones. Interestingly, increased inconsistency correlates with reduced biases, but more accurate predictions exacerbate biases. While we find that adjusting the temperature parameter can influence LLM fairness, model size, release date, and country of origin do not exhibit significant effects on judicial fairness. Accordingly, we introduce a publicly available toolkit containing all datasets and code, designed to support future research in evaluating and improving LLM fairness.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2507.10852",
      "arxivId": "2507.10852",
      "url": "https://www.semanticscholar.org/paper/ca6789df4ce150fec1898820a7e2ecc38e2e01da",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.10852"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ba6e1afe8a5fcaedb18a15cc148b3ea27e9f0bfa",
      "title": "An External Fairness Evaluation of LinkedIn Talent Search",
      "authors": [
        {
          "name": "Tina Behzad",
          "authorId": "2392722586"
        },
        {
          "name": "Siddartha Devic",
          "authorId": "117403158"
        },
        {
          "name": "Vatsal Sharan",
          "authorId": "2798845"
        },
        {
          "name": "Aleksandra Korolova",
          "authorId": "2316636550"
        },
        {
          "name": "David Kempe",
          "authorId": "2284065558"
        }
      ],
      "year": 2025,
      "abstract": "We conduct an independent, third-party audit for bias of LinkedIn's Talent Search ranking system, focusing on potential ranking bias across two attributes: gender and race. To do so, we first construct a dataset of rankings produced by the system, collecting extensive Talent Search results across a diverse set of occupational queries. We then develop a robust labeling pipeline that infers the two demographic attributes of interest for the returned users. To evaluate potential biases in the collected dataset of real-world rankings, we utilize two exposure disparity metrics: deviation from group proportions and MinSkew. Our analysis reveals an under-representation of minority groups in early ranks across many queries. We further examine potential causes of this disparity, and discuss why they may be difficult or, in some cases, impossible to fully eliminate among the early ranks of queries. Beyond static metrics, we also investigate the concept of subgroup fairness over time, highlighting temporal disparities in exposure and retention, which are often more difficult to audit for in practice. In employer recruiting platforms such as LinkedIn Talent Search, the persistence of a particular candidate over multiple days in the ranking can directly impact the probability that the given candidate is selected for opportunities. Our analysis reveals demographic disparities in this temporal stability, with some groups experiencing greater volatility in their ranked positions than others. We contextualize all our findings alongside LinkedIn's published self-audits of its Talent Search system and reflect on the methodological constraints of a black-box external evaluation, including limited observability and noisy demographic inference.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2511.10752",
      "url": "https://www.semanticscholar.org/paper/ba6e1afe8a5fcaedb18a15cc148b3ea27e9f0bfa",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "a1269725d5357579f0ad9d8e7276d9a1084a2d5a",
      "title": "mFARM: Towards Multi-Faceted Fairness Assessment based on HARMs in Clinical Decision Support",
      "authors": [
        {
          "name": "Shreyash Adappanavar",
          "authorId": "2341718598"
        },
        {
          "name": "Krithi Shailya",
          "authorId": "2354244896"
        },
        {
          "name": "Gokul S Krishnan",
          "authorId": "2271252313"
        },
        {
          "name": "S. Natarajan",
          "authorId": "143927103"
        },
        {
          "name": "Balaraman Ravindran",
          "authorId": "2268206514"
        }
      ],
      "year": 2025,
      "abstract": "The deployment of Large Language Models (LLMs) in high-stakes medical settings poses a critical AI alignment challenge, as models can inherit and amplify societal biases, leading to significant disparities. Existing fairness evaluation methods fall short in these contexts as they typically use simplistic metrics that overlook the multi-dimensional nature of medical harms. This also promotes models that are fair only because they are clinically inert, defaulting to safe but potentially inaccurate outputs. To address this gap, our contributions are mainly two-fold: first, we construct two large-scale, controlled benchmarks (ED-Triage and Opioid Analgesic Recommendation) from MIMIC-IV, comprising over 50,000 prompts with twelve race x gender variants and three context tiers. Second, we propose a multi-metric framework - Multi-faceted Fairness Assessment based on hARMs ($mFARM$) to audit fairness for three distinct dimensions of disparity (Allocational, Stability, and Latent) and aggregate them into an $mFARM$ score. We also present an aggregated Fairness-Accuracy Balance (FAB) score to benchmark and observe trade-offs between fairness and prediction accuracy. We empirically evaluate four open-source LLMs (Mistral-7B, BioMistral-7B, Qwen-2.5-7B, Bio-LLaMA3-8B) and their finetuned versions under quantization and context variations. Our findings showcase that the proposed $mFARM$ metrics capture subtle biases more effectively under various settings. We find that most models maintain robust performance in terms of $mFARM$ score across varying levels of quantization but deteriorate significantly when the context is reduced. Our benchmarks and evaluation code are publicly released to enhance research in aligned AI for healthcare.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2509.02007",
      "arxivId": "2509.02007",
      "url": "https://www.semanticscholar.org/paper/a1269725d5357579f0ad9d8e7276d9a1084a2d5a",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.02007"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8ac0e67546751169cf4141495969efa3272d9c48",
      "title": "Assessing causal claims about complex engineered systems with quantitative data: internal, external, and construct validity",
      "authors": [
        {
          "name": "David A. Broniatowski",
          "authorId": "3119101"
        },
        {
          "name": "Conrad S. Tucker",
          "authorId": "115814663"
        }
      ],
      "year": 2017,
      "abstract": "Engineers seek to design systems that will produce an intended change in the state of the world. How are we to know if a system will behave as intended? This article addresses ways that this question can be answered. Specifically, we focus on three types of research validity: (1) internal validity, or whether an observed association between two variables can be attributed to a causal link between them; (2) external validity, or whether a causal link generalizes across contexts; and (3) construct validity, or whether a specific set of metrics corresponds to what they are intended to measure. In each case, we discuss techniques that may be used to establish the corresponding type of validity: namely, quasi\u2010experimental design, replication, and establishment of convergent\u2010discriminant validity and reliability. These techniques typically require access to data, which has historically been limited for research on complex engineered systems. This is likely to change in the era of \u201cbig data.\u201d Thus, we discuss the continued utility of these validity concepts in the face of advances in machine learning and big data as they pertain to complex engineered sociotechnical systems. Next, we discuss relationships between these validity concepts and other prominent approaches to evaluating research in the field. Finally, we propose a set of criteria by which one may evaluate research utilizing quantitative observation to test causal theory in the field of complex engineered systems.",
      "citationCount": 23,
      "doi": "10.1002/sys.21414",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/8ac0e67546751169cf4141495969efa3272d9c48",
      "venue": "Systems Engineering",
      "journal": {
        "name": "Systems Engineering",
        "pages": "483 - 496",
        "volume": "20"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "328ab19d15ec061c8e630295bc0fa569c8307644",
      "title": "SOFT PEDAGOGICAL MEASUREMENTS OF RELIABILITY AND VALIDITY OF ASSESSMENT OF STUDENTS\u2019 PRACTICE-ORIENTED SKILLS BASED ON FUZZY COMPUTATIONAL MODELS",
      "authors": [
        {
          "name": "V. Minazova",
          "authorId": "118075363"
        },
        {
          "name": "Vera Anatolyevna Shepilova",
          "authorId": "2402325136"
        },
        {
          "name": "A. A. Albakova",
          "authorId": "2240438809"
        }
      ],
      "year": 2025,
      "abstract": "This article examines the theoretical and methodological aspects of the reliability and validity of pedagogical measurements used to assess students\u2019 practice-oriented skills within a competency-based approach. The need for a comprehensive assessment of the quality of measurement procedures aimed at assessing professional performance in the educational environment is substantiated. The key concepts of \u201creliability\u201d and \u201cvalidity\u201d in the context of pedagogical assessment are explored, classifications of their types are presented, and factors influencing the stability and reliability of results are analyzed. Particular attention is paid to issues of internal consistency of tasks, inter-rater reliability, content and construct validity, as well as the risks associated with subjective assessment and distortion of measured constructs. A conclusion is drawn regarding the need for rigorous psychometric substantiation and standardization of assessment tools to ensure objectivity and fairness in the assessment of students\u2019 practice-oriented competencies.",
      "citationCount": 0,
      "doi": "10.36871/2618-9976.2025.10-2.012",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/328ab19d15ec061c8e630295bc0fa569c8307644",
      "venue": "SOFT MEASUREMENTS AND COMPUTING",
      "journal": {
        "name": "SOFT MEASUREMENTS AND COMPUTING"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d84970ed545894c278337104ee62c223f9fe8632",
      "title": "Beyond Static Scoring: Enhancing Assessment Validity via AI-Generated Interactive Verification",
      "authors": [
        {
          "name": "Tom Lee",
          "authorId": "2398849337"
        },
        {
          "name": "Sihoon Lee",
          "authorId": "2398806825"
        },
        {
          "name": "Seonghun Kim",
          "authorId": "2398898298"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) challenge the validity of traditional open-ended assessments by blurring the lines of authorship. While recent research has focused on the accuracy of automated scoring (AES), these static approaches fail to capture process evidence or verify genuine student understanding. This paper introduces a novel Human-AI Collaboration framework that enhances assessment integrity by combining rubric-based automated scoring with AI-generated, targeted follow-up questions. In a pilot study with university instructors (N=9), we demonstrate that while Stage 1 (Auto-Scoring) ensures procedural fairness and consistency, Stage 2 (Interactive Verification) is essential for construct validity, effectively diagnosing superficial reasoning or unverified AI use. We report on the systems design, instructor perceptions of fairness versus validity, and the necessity of adaptive difficulty in follow-up questioning. The findings offer a scalable pathway for authentic assessment that moves beyond policing AI to integrating it as a synergistic partner in the evaluation process.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2512.12592",
      "url": "https://www.semanticscholar.org/paper/d84970ed545894c278337104ee62c223f9fe8632",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "0e06ecad8e704035c47b99ed9c6e3eeee6f5ba0e",
      "title": "Validity Verification of the New TOEFL Writing Task Based on Classical Test Theory",
      "authors": [
        {
          "name": "Yinyu Zhang",
          "authorId": "2379700734"
        }
      ],
      "year": 2025,
      "abstract": "The TOEFL iBT has introduced the Academic Discussion Task (ADT) to assess test-takers'ability to engage in academic discourse, reflecting the growing emphasis on interactive communication skills in higher education. However, research on the ADT's validity and fairness particularly for culturally and linguistically diverse groups, such as Chinese students, remains limited. This study addresses this gap by employing Classical Test Theory (CTT) to evaluate the psychometric properties of the ADT among Chinese university students. This study finds a robust correlation between the ADT and the CET-6 writing and translation subscores. In addition, there is a high level of expert agreement regarding the construct validity evidence and the appropriateness of the scoring rubric. Furthermore, the results indicate that gender differences in validity indices are minimal. Taken together, these results suggest that the ADT is a valid measure for Chinese test-takers without gender discrimination. However, it is recommended that the cultural sensitivity of the scoring rubric be further refined and that the CET-6 subscores for writing be retained for predictive purposes, in order to better accommodate the needs of diverse test-taker populations. By addressing these issues, this study contributes to the broader discourse on fairness and validity in high-stakes language assessments.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2509.05347",
      "arxivId": "2509.05347",
      "url": "https://www.semanticscholar.org/paper/0e06ecad8e704035c47b99ed9c6e3eeee6f5ba0e",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2509.05347"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c053ba0f99420516b872e5e33ef41a9df6e56326",
      "title": "Skills assessment for laparoscopic surgery based on computer analysis metrics: ScopePro Trainer",
      "authors": [
        {
          "name": "D. Lorias-Espinoza",
          "authorId": "150078310"
        },
        {
          "name": "Sandra Karina D\u00edaz Cardona",
          "authorId": "2375705301"
        },
        {
          "name": "Leobardo El\u00ed S\u00e1nchez Velasco",
          "authorId": "2375703586"
        },
        {
          "name": "R. Ordorica-Flores",
          "authorId": "1403004120"
        },
        {
          "name": "A. Minor-Mart\u00ednez",
          "authorId": "150022871"
        },
        {
          "name": "Salvador Montoya-Alvarez",
          "authorId": "1881105502"
        },
        {
          "name": "Fernando P\u00e9rez-Escamirosa",
          "authorId": "1397181340"
        }
      ],
      "year": 2025,
      "abstract": "The aim of this study is to present the construct validity of the ScopePro Trainer system and determine its effectiveness as a training and objective assessment tool for surgeons\u2019 psychomotor laparoscopic skills. Four surgeons, eight PG2 to PG4 residents, and twenty medical students participated in this study. All participants performed three FLS-based tasks using conventional laparoscopic instruments and the ScopePro Trainer system. Using the linearity principle of the spiral, a passive marker was added to each instrument, shaped like a spiral. Six metrics related to time, linear motion, and angular motion were used to assess the laparoscopic performance of the participants in all tasks. Statistical analysis was performed using the analysis of variance (ANOVA) test, followed by a Student\u2019s t test with Tukey\u2019s correction for multiple testing. The performance of ScopePro Trainer is illustrated by the graphical representation of the instruments' paths and the calculations of metrics derived from their position and rotation. Outcomes presented significant differences in the participant's skills during the execution of three surgical training tasks. The ScopePro Trainer system has been presented and successfully validated. Outcomes showed that ScopePro was able to differentiate between participants with varying levels of laparoscopic expertise. The ScopePro Trainer offers an alternative to traditional tracking systems for analyzing the rotational motion of surgical instruments.",
      "citationCount": 0,
      "doi": "10.1007/s00464-025-12017-4",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/c053ba0f99420516b872e5e33ef41a9df6e56326",
      "venue": "Surgical Endoscopy",
      "journal": {
        "name": "Surgical Endoscopy",
        "pages": "6449 - 6458",
        "volume": "39"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7af7770751fe917ae342c8396993c5208cad950f",
      "title": "Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity",
      "authors": [
        {
          "name": "Yuhan Zhang",
          "authorId": "2352612382"
        },
        {
          "name": "Long Zhuo",
          "authorId": "2317070316"
        },
        {
          "name": "Ziyang Chu",
          "authorId": "2333354862"
        },
        {
          "name": "Tong Wu",
          "authorId": "2269858155"
        },
        {
          "name": "Zhibing Li",
          "authorId": "2271261968"
        },
        {
          "name": "Liang Pan",
          "authorId": "2272233402"
        },
        {
          "name": "Dahua Lin",
          "authorId": "2258618427"
        },
        {
          "name": "Ziwei Liu",
          "authorId": "2145252993"
        }
      ],
      "year": 2025,
      "abstract": "Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at https://zyh482.github.io/Hi3DEval/.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2508.05609",
      "arxivId": "2508.05609",
      "url": "https://www.semanticscholar.org/paper/7af7770751fe917ae342c8396993c5208cad950f",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.05609"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f81d59aa6c616905eb31513359c3cbb2f9365d12",
      "title": "What makes a good metric? Evaluating automatic metrics for text-to-image consistency",
      "authors": [
        {
          "name": "Candace Ross",
          "authorId": "2256372432"
        },
        {
          "name": "Melissa Hall",
          "authorId": "2293590162"
        },
        {
          "name": "Adriana Romero-Soriano",
          "authorId": "1456285042"
        },
        {
          "name": "Adina Williams",
          "authorId": "2293907712"
        }
      ],
      "year": 2024,
      "abstract": "Language models are increasingly being incorporated as components in larger AI systems for various purposes, from prompt optimization to automatic evaluation. In this work, we analyze the construct validity of four recent, commonly used methods for measuring text-to-image consistency - CLIPScore, TIFA, VPEval, and DSG - which rely on language models and/or VQA models as components. We define construct validity for text-image consistency metrics as a set of desiderata that text-image consistency metrics should have, and find that no tested metric satisfies all of them. We find that metrics lack sufficient sensitivity to language and visual properties. Next, we find that TIFA, VPEval and DSG contribute novel information above and beyond CLIPScore, but also that they correlate highly with each other. We also ablate different aspects of the text-image consistency metrics and find that not all model components are strictly necessary, also a symptom of insufficient sensitivity to visual information. Finally, we show that all three VQA-based metrics likely rely on familiar text shortcuts (such as yes-bias in QA) that call their aptitude as quantitative evaluations of model performance into question.",
      "citationCount": 8,
      "doi": "10.48550/arXiv.2412.13989",
      "arxivId": "2412.13989",
      "url": "https://www.semanticscholar.org/paper/f81d59aa6c616905eb31513359c3cbb2f9365d12",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2412.13989"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "578d184b3538926ea49099827c872f20d317f09d",
      "title": "Measuring (meta)cognitive processes in multimedia learning: Matching eye tracking metrics and think-aloud protocols in case of seductive details",
      "authors": [
        {
          "name": "Lisa Stark",
          "authorId": "2059052711"
        },
        {
          "name": "Andreas Korbach",
          "authorId": "2457950"
        },
        {
          "name": "Roland Br\u00fcnken",
          "authorId": "2791184"
        },
        {
          "name": "Babette Park",
          "authorId": "1888329"
        }
      ],
      "year": 2024,
      "abstract": "In recent years, eye tracking has become a prominent method for learning research as it is assumed to indicate (meta)cognitive processes. However, there is little empirical evidence for hypothesized relations between eye tracking indicators and specific (meta)cognitive processes so that construct validity of used metrics can be questioned.The main goal was to provide validity hints in order to create an empirical basis for interpreting specific eye tracking indicators in terms of respective (meta)cognitive processes of multimedia learning.N\u2009=\u200960 students learned with multimedia instructional material. Referring to a process model of multimedia learning, correlations between theoretically deduced eye tracking indicators with verbalized (meta)cognitive processes of multimedia learning captured by think\u2010aloud protocols were examined. In addition, the sensitivity of both process measures of (meta)cognitive processes was regarded considering the well\u2010investigated seductive details effect of an established multimedia instruction in a two\u2010group design. Finally, serial mediations were calculated in order to investigate whether both process measures complement one another in a joint explanation of the seductive details effect.Eye tracking indicators and verbalized (meta)cognitive processes did only partly correspond as it was shown by correlation and serial mediation analyses. However, both measures were sensitive to indicate the seductive details effect. Thus, even though the study provided insights in how validation could be possible, further systematic research will be needed for validating eye tracking indicators of specific (meta)cognitive processes in multimedia learning.",
      "citationCount": 4,
      "doi": "10.1111/jcal.13051",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/578d184b3538926ea49099827c872f20d317f09d",
      "venue": "Journal of Computer Assisted Learning",
      "journal": {
        "name": "J. Comput. Assist. Learn.",
        "pages": "2985-3004",
        "volume": "40"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2d462de3957978d5f064011d871686ed5e572053",
      "title": "Navigating Fairness in AI-based Prediction Models: Theoretical Constructs and Practical Applications",
      "authors": [
        {
          "name": "S. L. van der Meijden",
          "authorId": "2311777863"
        },
        {
          "name": "Y. Wang",
          "authorId": "2351710588"
        },
        {
          "name": "M. S. Arbous",
          "authorId": "2276997963"
        },
        {
          "name": "B. F. Geerts",
          "authorId": "2351707080"
        },
        {
          "name": "E. Steyerberg",
          "authorId": "2316528390"
        },
        {
          "name": "T. Hernandez-Boussard",
          "authorId": "2250653096"
        }
      ],
      "year": 2025,
      "abstract": "Artificial Intelligence (AI)-based prediction models, including risk scoring systems and decision support systems, are increasingly adopted in healthcare. Addressing AI fairness is essential to fighting health disparities and achieving equitable performance and patient outcomes. Numerous and conflicting definitions of fairness complicate this effort. This paper aims to structure the transition of AI fairness from theory to practical application with appropriate fairness metrics. For 27 definitions of fairness identified in the recent literature, we assess the relation with the model's intended use, type of decision influenced and ethical principles of distributive justice. We advocate that due to limitations in some notions of fairness, clinical utility, performance-based metrics (area under the receiver operating characteristic curve), calibration, and statistical parity are the most relevant group-based metrics for medical applications. Through two use cases, we demonstrate that different metrics may be applicable depending on the intended use and ethical framework. Our approach provides a foundation for AI developers and assessors by assessing model fairness and the impact of bias mitigation strategies, hence promoting more equitable AI-based implementations.",
      "citationCount": 4,
      "doi": "10.1101/2025.03.24.25324500",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/2d462de3957978d5f064011d871686ed5e572053",
      "venue": "medRxiv",
      "journal": {
        "name": "medRxiv"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2d917ad9c874cccb99338f6c31a41c9dd920a8ef",
      "title": "Fair Representation Learning for Continuous Sensitive Attributes Using Expectation of Integral Probability Metrics",
      "authors": [
        {
          "name": "Insung Kong",
          "authorId": "2153469078"
        },
        {
          "name": "Kunwoong Kim",
          "authorId": "2115763935"
        },
        {
          "name": "Yongdai Kim",
          "authorId": "2292403403"
        }
      ],
      "year": 2025,
      "abstract": "AI fairness, also known as algorithmic fairness, aims to ensure that algorithms operate without bias or discrimination towards any individual or group. Among various AI algorithms, the Fair Representation Learning (FRL) approach has gained significant interest in recent years. However, existing FRL algorithms have a limitation: they are primarily designed for categorical sensitive attributes and thus cannot be applied to continuous sensitive attributes, such as age or income. In this paper, we propose an FRL algorithm for continuous sensitive attributes. First, we introduce a measure called the Expectation of Integral Probability Metrics (EIPM) to assess the fairness level of representation space for continuous sensitive attributes. We demonstrate that if the distribution of the representation has a low EIPM value, then any prediction head constructed on the top of the representation become fair, regardless of the selection of the prediction head. Furthermore, EIPM possesses a distinguished advantage in that it can be accurately estimated using our proposed estimator with finite samples. Based on these properties, we propose a new FRL algorithm called Fair Representation using EIPM with MMD (FREM). Experimental evidences show that FREM outperforms other baseline methods.",
      "citationCount": 6,
      "doi": "10.1109/TPAMI.2025.3538915",
      "arxivId": "2505.06435",
      "url": "https://www.semanticscholar.org/paper/2d917ad9c874cccb99338f6c31a41c9dd920a8ef",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "journal": {
        "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "pages": "3784-3795",
        "volume": "47"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "3e65f572322e192fe36ae52a8a7f025b0685dfc6",
      "title": "Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
      "authors": [
        {
          "name": "Su Lin Blodgett",
          "authorId": "3422038"
        },
        {
          "name": "Gilsinia Lopez",
          "authorId": "2057983441"
        },
        {
          "name": "Alexandra Olteanu",
          "authorId": "2064011617"
        },
        {
          "name": "Robert Sim",
          "authorId": "1562202621"
        },
        {
          "name": "Hanna M. Wallach",
          "authorId": "1831395"
        }
      ],
      "year": 2021,
      "abstract": "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system\u2019s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens\u2014originating from the social sciences\u2014to inventory a range of pitfalls that threaten these benchmarks\u2019 validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.",
      "citationCount": 346,
      "doi": "10.18653/v1/2021.acl-long.81",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/3e65f572322e192fe36ae52a8a7f025b0685dfc6",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "1004-1015"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "6882ca76d65014f2e26b31e7cc1b846de9b1da52",
      "title": "FAIRNESS AND BIAS MITIGATION IN AI-BASED CREDIT SCORING USING ALTERNATIVE DATA: A FRAMEWORK FOR ETHICAL FINANCIAL INCLUSION",
      "authors": [
        {
          "name": "Derrick Atuobi Oware",
          "authorId": "2374670567"
        },
        {
          "name": "Samuel Amfo Junior",
          "authorId": "2374673272"
        }
      ],
      "year": 2025,
      "abstract": "There are a lot of opportunities to speed up financial inclusion through the use of artificial intelligence (AI) and alternative data in credit scoring, especially for underprivileged groups that have been shut out of formal financial services. Systemic exclusion is reinforced by the inability of traditional credit scoring methods to account for the financial conduct of those without prior credit histories. The paper describes how more comprehensive, data-driven credit assessment models can be constructed by leveraging alternative data sources, such as web footprints, utility bill payments, and mobile phone usage patterns. However, there are also technical and ethical issues with using AI in this way, like algorithmic bias, opacity, and data privacy issues. This paper responds by offering a strategy for achieving equity and mitigating bias in AI-based credit assessment models using alternative data. The framework consists of four integral parts: responsible data gathering, bias-aware model development, fairness metrics, and regulatory conformity processes. Relying on case studies and model simulations, the paper explores how these parts cooperate to generate credit decisions and addresses how to enhance transparency, accountability, and trust. Specific attention is drawn to the necessity for inclusive data governance, stakeholder engagement, and the application of explainable AI techniques. By addressing both the promise and risks of this emerging field, the proposed framework contributes to ongoing work to synchronize technological innovation with ethical principles and promote equitable access to credit.\nKeywords: Financial Inclusion, Alternative Data, Credit Scoring, Algorithmic Fairness, Bias Mitigation",
      "citationCount": 0,
      "doi": "10.36713/epra23347",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6882ca76d65014f2e26b31e7cc1b846de9b1da52",
      "venue": "EPRA international journal of multidisciplinary research",
      "journal": {
        "name": "EPRA International Journal of Multidisciplinary Research (IJMR)"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1d21287d39f6980794fcaca85726768506bc63de",
      "title": "Data bias, algorithmic discrimination and the fairness issues of individual credit accessibility",
      "authors": [
        {
          "name": "Shenggang Yang",
          "authorId": "2369602491"
        },
        {
          "name": "Wenbo Ma",
          "authorId": "2370994971"
        },
        {
          "name": "Sangzhi Zhu",
          "authorId": "9065577"
        },
        {
          "name": "Ao Shu",
          "authorId": "2368018472"
        },
        {
          "name": "Yue Zhou",
          "authorId": "2248087893"
        }
      ],
      "year": 2025,
      "abstract": "PurposeThis study examines the impact of data bias and algorithmic discrimination on individual credit accessibility in China\u2019s financial system. It aims to align financial inclusion and equity goals with statistical fairness conditions by constructing fairness metrics from multiple dimensions. The paper evaluates the fairness of commonly used credit evaluation models and proposes a novel approach to eliminate data bias in historical datasets.Design/methodology/approachWe model credit evaluation using Logistic Regression, Random Forest, and XGBoost algorithms, focusing on education level and work experience as sensitive attributes. To mitigate data bias in historical datasets, we employ the Metropolis-Hastings (M-H) algorithm for data preprocessing.Findings(1) Machine learning models like Random Forest and XGBoost outperform traditional methods in addressing unfairness arising from multiple sensitive attributes. (2) Sensitive attributes, while excluded from credit scoring models, may indirectly influence outcomes through other indicators. Limiting the gap in credit accessibility between the general population and protected groups is essential for fairness of opportunity. (3) Data bias significantly affects credit ratings, increasing the false positive rate for certain demographic subgroups and reducing their credit accessibility.Practical implicationsThe study provides a micro-level examination of individual credit accessibility and fairness in China. It analyzes the fairness of credit evaluation models used by Chinese financial institutions across different population groups and proposes an M-H algorithm\u2013based method to eliminate data bias in historical datasets.Originality/valueThis paper enhances research on fairness in individual credit accessibility in China by introducing three fairness metrics for evaluating credit evaluation models. It offers a micro-level perspective for scholars studying related issues.",
      "citationCount": 0,
      "doi": "10.1108/jal-11-2024-0322",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/1d21287d39f6980794fcaca85726768506bc63de",
      "venue": "Journal of Accounting Literature",
      "journal": {
        "name": "Journal of Accounting Literature"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "59f12d02da6549464aadb185ed615df195529f32",
      "title": "Online Fairness Auditing through Iterative Refinement",
      "authors": [
        {
          "name": "Pranav Maneriker",
          "authorId": "8394636"
        },
        {
          "name": "Codi Burley",
          "authorId": "65727106"
        },
        {
          "name": "Srinivas Parthasarathy",
          "authorId": "2739353"
        }
      ],
      "year": 2023,
      "abstract": "A sizable proportion of deployed machine learning models make their decisions in a black-box manner. Such decision-making procedures are susceptible to intrinsic biases, which has led to a call for accountability in deployed decision systems. In this work, we investigate mechanisms that help audit claimed mathematical guarantees of the fairness of such systems. We construct AVOIR, a system that reduces the number of observations required for the runtime monitoring of probabilistic assertions over fairness metrics specified on decision functions associated with black-box AI models. AVOIR provides an adaptive process that automates the inference of probabilistic guarantees associated with estimating a wide range of fairness metrics. In addition, AVOIR enables the exploration of fairness violations aligned with governance and regulatory requirements. We conduct case studies with fairness metrics on three different datasets and demonstrate how AVOIR can help detect and localize fairness violations and ameliorate the issues with faulty fairness metric design.",
      "citationCount": 16,
      "doi": "10.1145/3580305.3599454",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/59f12d02da6549464aadb185ed615df195529f32",
      "venue": "Knowledge Discovery and Data Mining",
      "journal": {
        "name": "Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book",
        "Conference"
      ]
    },
    {
      "paperId": "754290d634d20833c4f597c798e5670c35bbe838",
      "title": "Sustainable Development of Digital Financial Infrastructure: Evaluating Data Quality and Information Fairness in Cryptocurrency Exchanges",
      "authors": [
        {
          "name": "Jun-Kuan Tsao",
          "authorId": "2396232982"
        },
        {
          "name": "Chiang-Yu Cheng",
          "authorId": "2395988767"
        }
      ],
      "year": 2025,
      "abstract": "The quality of digital financial infrastructure directly impacts the fairness of information access for market participants, which is fundamental to the sustainable development of the entire financial ecosystem. As cryptocurrency markets operate globally 24/7 without interruption, ensuring equitable access to high-quality data is essential for promoting responsible investment and reducing market inequality. This study establishes a standardized data quality assessment framework for three major cryptocurrency exchanges to explore how data quality differences affect information fairness. Through systematic data collection conducted every 60 seconds for eight consecutive hours, 463 valid Bitcoin price data samples were successfully obtained, covering major Asian and European trading sessions. The research constructs a comprehensive Data Quality Index integrating three core dimensions of consistency, freshness, and availability metrics. The consistency component measures price synchronization between platforms by calculating relative price differences with a coefficient of 50,000 to capture micro-level variations. The freshness indicator evaluates information timeliness by measuring delays between API request time and exchange data timestamps using a stringent 0.2-second threshold. The availability metric assesses service reliability through success rate calculations. The final composite index employs weighted averaging with 40% for consistency, 40% for freshness, and 20% for availability. Results reveal that Binance ranks first with 92.50 points, followed by Bitget at 88.61 points and MEXC at 85.36 points, with the 7.14-point gap directly impacting information fairness. This study provides quantitative assessment tools for building sustainable digital financial ecosystems, supporting UN Sustainable Development Goals 9.1 and 10.5.",
      "citationCount": 0,
      "doi": "10.63665/gjis.v1.30",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/754290d634d20833c4f597c798e5670c35bbe838",
      "venue": "Glovento Journal of Integrated Studies",
      "journal": {
        "name": "Glovento Journal of Integrated Studies"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "62c0fcc2a69dcb48ac98db578e5395b5acc26860",
      "title": "Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models",
      "authors": [
        {
          "name": "Battemuulen Naranbat",
          "authorId": "2385467930"
        },
        {
          "name": "Seyed Sahand Mohamadi Ziabari",
          "authorId": "70387563"
        },
        {
          "name": "Yousuf Nasser Al Husaini",
          "authorId": "2344025533"
        },
        {
          "name": "A. M. M. Alsahag",
          "authorId": "66260820"
        }
      ],
      "year": 2025,
      "abstract": "Ensuring fairness in natural language processing for moral sentiment classification is challenging, particularly under cross-domain shifts where transformer models are increasingly deployed. Using the Moral Foundations Twitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work evaluates BERT and DistilBERT in a multi-label setting with in-domain and cross-domain protocols. Aggregate performance can mask disparities: we observe pronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by 14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness violations hidden by overall scores; notably, the authority label exhibits Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of 0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency (MFC) metric, which quantifies the cross-domain stability of moral foundation detection. MFC shows strong empirical validity, achieving a perfect negative correlation with Demographic Parity Difference (rho = -1.000, p<0.001) while remaining independent of standard performance metrics. Across labels, loyalty demonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC = 0.78). These findings establish MFC as a complementary, diagnosis-oriented metric for fairness-aware evaluation of moral reasoning models, enabling more reliable deployment across heterogeneous linguistic contexts. .",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.11222",
      "arxivId": "2510.11222",
      "url": "https://www.semanticscholar.org/paper/62c0fcc2a69dcb48ac98db578e5395b5acc26860",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.11222"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "fee5e455b4d871e40d46006bf0b578c009936f70",
      "title": "Reliable and Reproducible Demographic Inference for Fairness in Face Analysis",
      "authors": [
        {
          "name": "Alexandre Fournier-Montgieux",
          "authorId": "2356843628"
        },
        {
          "name": "H. Borgne",
          "authorId": "2138418"
        },
        {
          "name": "Adrian Popescu",
          "authorId": "2308038093"
        },
        {
          "name": "Bertrand Luvison",
          "authorId": "2333601375"
        }
      ],
      "year": 2025,
      "abstract": "Fairness evaluation in face analysis systems (FAS) typically depends on automatic demographic attribute inference (DAI), which itself relies on predefined demographic segmentation. However, the validity of fairness auditing hinges on the reliability of the DAI process. We begin by providing a theoretical motivation for this dependency, showing that improved DAI reliability leads to less biased and lower-variance estimates of FAS fairness. To address this, we propose a fully reproducible DAI pipeline that replaces conventional end-to-end training with a modular transfer learning approach. Our design integrates pretrained face recognition encoders with non-linear classification heads. We audit this pipeline across three dimensions: accuracy, fairness, and a newly introduced notion of robustness, defined via intra-identity consistency. The proposed robustness metric is applicable to any demographic segmentation scheme. We benchmark the pipeline on gender and ethnicity inference across multiple datasets and training setups. Our results show that the proposed method outperforms strong baselines, particularly on ethnicity, which is the more challenging attribute. To promote transparency and reproducibility, we will publicly release the training dataset metadata, full codebase, pretrained models, and evaluation toolkit. This work contributes a reliable foundation for demographic inference in fairness auditing.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2510.20482",
      "arxivId": "2510.20482",
      "url": "https://www.semanticscholar.org/paper/fee5e455b4d871e40d46006bf0b578c009936f70",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.20482"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 40,
  "errors": []
}
