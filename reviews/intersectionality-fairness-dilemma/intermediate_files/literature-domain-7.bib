@comment{
====================================================================
DOMAIN: Critical Perspectives on Fairness Formalization
SEARCH_DATE: 2026-01-10
PAPERS_FOUND: 12 total (High: 8, Medium: 4, Low: 0)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex, CrossRef
====================================================================

DOMAIN_OVERVIEW:
This domain encompasses fundamental critiques of fairness formalization
projects in machine learning. These critiques argue that mathematical
fairness metrics, while useful tools, cannot fully capture substantive
justice and may obscure structural inequalities. Key themes include: (1)
the impossibility of simultaneously satisfying multiple fairness criteria,
(2) fairness gerrymandering vulnerabilities where classifiers appear fair
on predefined groups but violate fairness on intersectional subgroups, (3)
structural critiques arguing that formal fairness metrics abstract away
from social context and systemic oppression, and (4) feminist and critical
race theory perspectives that highlight how formalization can legitimize
rather than challenge existing power structures.

The impossibility results (Kleinberg et al., Chouldechova) demonstrate
mathematical constraints on fairness formalization. The fairness
gerrymandering problem (Kearns et al.) shows how group fairness metrics
can miss intersectional disparities. Critical scholars (Green, Wachter et
al., Kasirzadeh, Hampton) argue for moving beyond "formal" to
"substantive" fairness that addresses structural injustice. Recent work
emphasizes participatory approaches and the limits of automation.

RELEVANCE_TO_PROJECT:
These critiques provide essential context for understanding whether the
intersectionality dilemma identified in the research proposal represents a
fundamental limitation of fairness formalization or a technical challenge
that can be overcome. If critics are correct that fairness cannot be fully
operationalized, the dilemma may be inherent rather than resolvable.

NOTABLE_GAPS:
Most critiques focus on criminal justice and employment domains. Less
attention to healthcare or education contexts. Limited work on specific
alternatives to formal metrics beyond general calls for participatory
approaches or "substantive" fairness.

SYNTHESIS_GUIDANCE:
Contrast impossibility results (technical constraints) with structural
critiques (conceptual limitations). Consider whether fairness
gerrymandering and the intersectionality dilemma are related phenomena.
Examine whether critics offer genuine alternatives or only diagnostic
critiques.

KEY_POSITIONS:
- Impossibility theorists: 8 papers - Mathematical constraints limit
  fairness formalization
- Structural critics: 6 papers - Formal metrics obscure systemic injustice
- Feminist critics: 4 papers - Formalization can reinforce oppression
====================================================================
}

@inproceedings{green2022escaping,
  author = {Green, Ben},
  title = {Escaping the Impossibility of Fairness: From Formal to Substantive Algorithmic Fairness},
  year = {2022},
  doi = {10.1007/s13347-022-00584-6},
  journal = {Philosophy \& Technology},
  volume = {35},
  number = {4},
  pages = {90},
  note = {
  CORE ARGUMENT: The "impossibility of fairness" (incompatibility between
  mathematical fairness definitions) stems from restricting analysis to
  isolated decision-making procedures—"formal algorithmic fairness." Green
  proposes "substantive algorithmic fairness," which takes a more expansive
  scope analyzing algorithms within their social context, enabling escape
  from impossibility results and providing a rigorous guide for alleviating
  injustice with algorithms.

  RELEVANCE: Directly addresses whether impossibility results represent
  fundamental limits. Green argues the impossibility is an artifact of
  formalist methodology, not inherent to fairness itself. His framework
  suggests the intersectionality dilemma might be resolvable by moving
  beyond formal metrics to substantive equality. However, it remains unclear
  whether "substantive fairness" can be operationalized for auditing.

  POSITION: Argues that formal fairness metrics are insufficient but not
  inherently wrong; calls for expanding scope rather than abandoning
  formalization entirely.
  },
  keywords = {impossibility-fairness, substantive-fairness, critique-formalization, High}
}

@article{wachter2021why,
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  title = {Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI},
  year = {2021},
  doi = {10.1016/j.clsr.2021.105567},
  journal = {Computer Law \& Security Review},
  volume = {41},
  pages = {105567},
  note = {
  CORE ARGUMENT: Fairness metrics cannot be automated to comply with EU
  non-discrimination law because they fail to capture the contextual,
  substantive nature of discrimination. Legal fairness requires considering
  intent, context, and structural factors that resist mathematical
  formalization. Current fairness metrics measure correlation, not causation
  or discrimination as legally understood.

  RELEVANCE: Provides legal-philosophical grounding for why fairness
  formalization has inherent limits. Suggests the intersectionality dilemma
  may be one instance of a broader problem: legal and moral conceptions of
  fairness exceed what formal metrics can capture. Argues that fairness
  auditing should complement, not replace, human judgment.

  POSITION: Strong critique of automation; argues fairness fundamentally
  requires human judgment and cannot be reduced to statistical criteria.
  },
  keywords = {automation-limits, legal-fairness, critique-formalization, High}
}

@inproceedings{green2018myth,
  author = {Green, Ben and Hu, Lily},
  title = {The Myth in the Methodology: Towards a Recontextualization of Fairness in Machine Learning},
  year = {2018},
  booktitle = {Proceedings of the Machine Learning: The Debates Workshop at the 35th International Conference on Machine Learning},
  note = {
  CORE ARGUMENT: The methodology of fairness in machine learning embeds a
  "myth" that fairness can be achieved through decontextualized mathematical
  metrics applied to isolated decision points. This abstracts away from the
  social structures that create and maintain inequality. Fairness research
  needs recontextualization to address structural injustice.

  RELEVANCE: Early statement of the critique that formal fairness metrics
  obscure structural context. Anticipates the impossibility-as-methodology
  argument in Green (2022). Suggests the intersectionality dilemma may
  reflect deeper problems with decontextualized fairness assessment, as
  intersectional identities are inherently context-dependent.

  POSITION: Foundational critique arguing fairness metrics rest on flawed
  methodological assumptions about decontextualization.
  },
  keywords = {critique-formalization, structural-injustice, methodology, High}
}

@article{kearns2018preventing,
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  title = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
  year = {2018},
  doi = {10.48550/arxiv.1711.05144},
  journal = {Proceedings of the 35th International Conference on Machine Learning},
  volume = {80},
  pages = {2564--2572},
  note = {
  CORE ARGUMENT: Statistical fairness constraints are susceptible to
  "fairness gerrymandering," where a classifier appears fair on predefined
  groups but badly violates fairness on structured subgroups defined over
  protected attributes. Proposes demanding fairness across exponentially many
  subgroups, but proves this is computationally equivalent to weak agnostic
  learning (hard in worst case). Provides algorithms that converge to best
  fair classifier given oracles for agnostic learning.

  RELEVANCE: Directly demonstrates a technical impossibility result related
  to intersectionality. Shows that auditing fairness over intersectional
  subgroups is computationally hard, even with simple subgroup definitions.
  This suggests the intersectionality dilemma may have computational as well
  as conceptual dimensions. The gerrymandering problem mirrors concerns about
  single-axis fairness missing intersectional harms.

  POSITION: Technical contribution showing fundamental computational limits;
  not a normative critique but a feasibility result.
  },
  keywords = {fairness-gerrymandering, impossibility-results, subgroup-fairness, High}
}

@inproceedings{kasirzadeh2022algorithmic,
  author = {Kasirzadeh, Atoosa},
  title = {Algorithmic Fairness and Structural Injustice: Insights from Feminist Political Philosophy},
  year = {2022},
  doi = {10.1145/3514094.3534188},
  booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
  pages = {399--408},
  note = {
  CORE ARGUMENT: Algorithmic fairness does not accommodate structural
  injustices in its current scope. Drawing on Iris Marion Young's work on
  structural injustice, Kasirzadeh argues that fairness metrics rooted in
  distributive justice ideals cannot address oppressive social structures.
  Proposes "responsible algorithmic fairness" paradigm that expands scope to
  include structural dimensions.

  RELEVANCE: Feminist philosophical critique linking fairness formalization
  limits to broader debates about justice paradigms. Argues the problem is
  not just technical constraints but conceptual: distributive justice
  frameworks (which underpin most fairness metrics) miss structural
  oppression. Suggests intersectionality dilemma may be irresolvable within
  distributive fairness paradigm.

  POSITION: Structural injustice critique from feminist philosophy; calls for
  paradigm shift beyond distributive justice.
  },
  keywords = {structural-injustice, feminist-critique, political-philosophy, High}
}

@inproceedings{hampton2021black,
  author = {Hampton, Lelia Marie},
  title = {Black Feminist Musings on Algorithmic Oppression},
  year = {2021},
  doi = {10.1145/3442188.3445929},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages = {122--124},
  note = {
  CORE ARGUMENT: Uses Black feminist theory to ground and extend the concept
  of algorithmic oppression, critiquing the language and assumptions of the
  fairness, accountability, and transparency (FAccT) community. Highlights
  the "double bind" of technology for marginalized communities and calls for
  abolition and empowerment rather than fairness optimization.

  RELEVANCE: Critical race feminist perspective arguing that fairness
  discourse itself may be inadequate for addressing algorithmic oppression.
  Suggests the intersectionality dilemma may not be resolvable through better
  fairness metrics because the fairness framework itself inadequately captures
  Black feminist concerns. Points toward abolition rather than reform.

  POSITION: Radical critique from Black feminism; questions whether fairness
  framework itself is appropriate for addressing algorithmic harm.
  },
  keywords = {Black-feminism, algorithmic-oppression, critique-fairness, High}
}

@article{valdivia2022elephant,
  author = {Valdivia, Ana and Serrajòrdia, Júlia Corbera and Swianiewicz, Aneta},
  title = {There is an Elephant in the Room: Towards a Critique on the Use of Fairness in Biometrics},
  year = {2022},
  doi = {10.1007/s43681-022-00249-2},
  journal = {AI and Ethics},
  volume = {3},
  pages = {1407--1422},
  note = {
  CORE ARGUMENT: Algorithmic fairness has been adopted by biometrics without
  critical analysis of political consequences. Proves biometrics will always
  be biased, yet argues fairness cannot distribute justice in scenarios whose
  intended purpose is to discriminate. By focusing on demographic bias rather
  than how systems reproduce historical and political injustices, fairness
  has "overshadowed the elephant in the room."

  RELEVANCE: Demonstrates a domain where fairness metrics are conceptually
  inappropriate—not because of technical limits but because the underlying
  purpose is problematic. Suggests there may be contexts where improving
  fairness metrics is misguided. Relevant to whether intersectional fairness
  auditing is an appropriate goal or whether some systems should not be
  deployed regardless of fairness properties.

  POSITION: Domain-specific critique arguing fairness improvements can
  legitimize fundamentally unjust systems.
  },
  keywords = {biometrics, critique-fairness, political-critique, High}
}

@article{buyl2024inherent,
  author = {Buyl, Maarten and De Bie, Tijl},
  title = {Inherent Limitations of AI Fairness},
  year = {2024},
  doi = {10.1145/3624700},
  journal = {Communications of the ACM},
  volume = {67},
  number = {1},
  pages = {48--55},
  note = {
  CORE ARGUMENT: AI fairness has inherent limitations that must be
  acknowledged. Fairness is not a panacea and requires critical thought and
  outside help to achieve positive societal impact. Technical fairness
  interventions alone are insufficient without addressing broader social
  context and power structures.

  RELEVANCE: Recent synthesis of inherent limitations arguments. Provides
  accessible overview of why fairness formalization has limits, useful for
  contextualizing the intersectionality dilemma within broader limitations
  landscape. Emphasizes need for sociotechnical rather than purely technical
  approaches.

  POSITION: Moderate critique acknowledging value of fairness metrics while
  highlighting inherent limitations.
  },
  keywords = {limitations-fairness, sociotechnical, synthesis, Medium}
}

@article{fleisher2021whats,
  author = {Fleisher, Will},
  title = {What's Fair about Individual Fairness?},
  year = {2021},
  doi = {10.2139/ssrn.3819799},
  journal = {SSRN Electronic Journal},
  note = {
  CORE ARGUMENT: Individual fairness (treating similar individuals similarly)
  faces four in-principle problems: (1) similar treatment is insufficient for
  fairness (counterexamples exist), (2) learning similarity metrics risks
  encoding implicit bias, (3) individual fairness requires prior moral
  judgments limiting its usefulness as fairness definition, and (4)
  incommensurability of moral values makes similarity metrics impossible for
  many tasks.

  RELEVANCE: Demonstrates conceptual problems with individual fairness, which
  is often proposed as alternative to group fairness. If both group and
  individual fairness face fundamental limitations, this strengthens the case
  that the intersectionality dilemma reflects deeper problems with fairness
  formalization. The incommensurability argument is particularly relevant to
  intersectionality.

  POSITION: Conceptual critique of individual fairness showing it faces
  in-principle problems beyond technical implementation.
  },
  keywords = {individual-fairness, conceptual-critique, incommensurability, Medium}
}

@article{ziosi2024genealogical,
  author = {Ziosi, Marta and Watson, David and Floridi, Luciano},
  title = {A Genealogical Approach to Algorithmic Bias},
  year = {2024},
  doi = {10.2139/ssrn.4734082},
  journal = {SSRN Electronic Journal},
  note = {
  CORE ARGUMENT: The FAccT literature tends to focus on bias as requiring ex
  post solutions (fairness metrics) rather than addressing underlying social
  and technical conditions that produce it. Proposes genealogy as
  constructive epistemic critique to explain algorithmic bias in terms of its
  enabling conditions rather than outcomes alone.

  RELEVANCE: Methodological critique arguing fairness metrics address
  symptoms not causes. Suggests the intersectionality dilemma may persist
  because auditing approaches focus on measuring outcomes rather than
  understanding generative processes. Points toward alternative epistemology
  for bias research.

  POSITION: Methodological critique proposing genealogical alternative to
  metric-based fairness approaches.
  },
  keywords = {genealogical-method, bias-critique, methodology, Medium}
}

@article{green2020false,
  author = {Green, Ben},
  title = {The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness},
  year = {2020},
  doi = {10.1145/3351095.3372869},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages = {594--606},
  note = {
  CORE ARGUMENT: Risk assessments fail to provide objectivity and create
  numerous sites of discretion. They provide no guarantee of reducing
  incarceration and risk legitimizing structural racism. Reinterprets
  impossibility results as evidence of deeper tension between notions of
  equality (formal vs. substantive), not just mathematical metrics. Argues
  "fair" algorithms can reinforce discrimination.

  RELEVANCE: Earlier development of Green's substantive fairness argument,
  specifically in criminal justice context. Shows how impossibility results
  can be interpreted as conceptual not just mathematical tensions. Relevant
  to understanding whether intersectionality dilemma reflects formal vs.
  substantive equality tension.

  POSITION: Epistemic critique of risk assessment fairness arguing for
  structural rather than procedural reform approach.
  },
  keywords = {risk-assessment, criminal-justice, epistemic-reform, Medium}
}

@article{lopez2024susceptibility,
  author = {Lopez, Paola},
  title = {More than the Sum of its Parts: Susceptibility to Algorithmic Disadvantage as a Conceptual Framework},
  year = {2024},
  doi = {10.1145/3630106.3658944},
  booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages = {1713--1726},
  note = {
  CORE ARGUMENT: Drawing from Elizabeth Anderson's relational equality,
  introduces "susceptibility to algorithmic disadvantage" framework with
  vertical dimension (state-individual relation) and horizontal dimension
  (intersectional inequalities). Argues these dimensions co-constitute and
  reinforce each other—more than sum of parts—paralleling intersectional
  feminist insight that interlocking oppressions exceed single-axis sum.

  RELEVANCE: Explicitly connects intersectionality theory to algorithmic
  fairness critique. The "more than sum of parts" argument directly parallels
  the intersectionality dilemma: just as interlocking oppressions exceed
  single-axis analysis, intersectional algorithmic harms may exceed what
  fairness metrics can capture. Provides conceptual framework for
  understanding why fairness on single axes may not aggregate to fairness
  across intersections.

  POSITION: Relational equality framework highlighting how intersectional
  harms emerge from interaction of vertical and horizontal dimensions.
  },
  keywords = {intersectionality, relational-equality, conceptual-framework, High}
}
