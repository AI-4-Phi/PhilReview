@comment{
====================================================================
DOMAIN: Epistemic Uncertainty and Data Sparsity
SEARCH_DATE: 2026-01-10
PAPERS_FOUND: 18 total (High: 8, Medium: 7, Low: 3)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex, CrossRef
====================================================================

DOMAIN_OVERVIEW:
This domain examines how epistemic justice concerns intersect with statistical
uncertainty in fairness auditing, particularly when dealing with sparse data
for marginalized groups. The literature reveals three interrelated clusters:
(1) foundational work on epistemic injustice (Fricker, Anderson) establishing
how marginalized groups face testimonial and hermeneutical injustice; (2)
applications to data science and algorithms (Symons & Alvarado, Hull, Milano &
Prunkl) showing how algorithmic systems perpetuate and amplify these
injustices; and (3) technical work on sample size, statistical reliability,
and fairness trade-offs (Zhioua & Binkyte, Jourdan et al., Mhasawade et al.)
demonstrating that small sample sizes for marginalized groups introduce both
epistemic and statistical challenges. A key tension emerges: expanding fairness
auditing to cover more intersectional groups creates data sparsity problems that
undermine statistical reliability, yet requiring large sample sizes can itself
constitute epistemic injustice by excluding groups that cannot be easily
aggregated at scale.

RELEVANCE_TO_PROJECT:
This domain directly addresses the "statistical uncertainty horn" of the
intersectionality dilemma. The epistemic justice literature provides normative
grounding for why data sparsity is not merely a technical problem but an issue
of structural injustice. The technical literature on sample size and fairness
metrics demonstrates the practical challenges of reasoning under uncertainty
with sparse data. Together, these bodies of work illuminate how the trade-off
between group expansion and statistical reliability connects to broader concerns
about who gets to be epistemically represented in algorithmic systems.

NOTABLE_GAPS:
Limited work explicitly connecting epistemic injustice frameworks to the
specific problem of sample size requirements in fairness auditing. Most
epistemic injustice literature in AI focuses on testimonial/hermeneutical forms
without addressing distributive epistemic injustice in data collection practices.
Philosophy of statistics literature on uncertainty quantification rarely engages
with fairness concerns or social justice implications.

SYNTHESIS_GUIDANCE:
Synthesis should emphasize how epistemic justice provides a lens for
understanding why data sparsity is not just a technical challenge but a site of
structural injustice. The interaction between statistical uncertainty and
epistemic marginalization creates a double bind: groups with sparse data face
both unreliable auditing AND epistemic exclusion from algorithmic systems.

KEY_POSITIONS:
- Epistemic injustice in data systems (8 papers): Data collection and algorithmic
  systems perpetuate testimonial and hermeneutical injustices
- Sample size and fairness trade-offs (5 papers): Small sample sizes for
  underrepresented groups undermine reliable fairness measurement
- Uncertainty quantification in algorithmic decision-making (5 papers): Opacity
  and uncertainty in ML systems raise epistemic authority concerns
====================================================================
}

@book{fricker2007epistemic,
  author = {Fricker, Miranda},
  title = {Epistemic Injustice: Power and the Ethics of Knowing},
  year = {2007},
  publisher = {Oxford University Press},
  address = {Oxford},
  doi = {10.1093/acprof:oso/9780198237907.001.0001},
  note = {
  CORE ARGUMENT: Fricker establishes epistemic injustice as a distinct category
  of injustice, identifying two primary forms: testimonial injustice (when
  prejudice causes a hearer to give a deflated level of credibility to a
  speaker's word) and hermeneutical injustice (when a gap in collective
  interpretive resources puts someone at an unfair disadvantage in making sense
  of their social experiences). Both forms harm individuals specifically in their
  capacity as knowers.

  RELEVANCE: Foundational for understanding how data sparsity for marginalized
  groups in fairness auditing constitutes not merely a statistical problem but
  an epistemic injustice. When algorithmic systems require large sample sizes
  that marginalized groups cannot provide, this perpetuates hermeneutical
  injustice by leaving those groups' experiences underrepresented in the
  collective interpretive resources embedded in ML models. The framework helps
  conceptualize the "statistical uncertainty horn" as a site of epistemic harm.

  POSITION: Foundational work establishing epistemic injustice as a distinct
  normative framework, widely applied to technology and data systems.
  },
  keywords = {epistemic-injustice, foundational, philosophy, High}
}

@article{symons2022epistemic,
  author = {Symons, John and Alvarado, Ramón},
  title = {Epistemic injustice and data science technologies},
  journal = {Synthese},
  year = {2022},
  volume = {200},
  doi = {10.1007/s11229-022-03631-z},
  note = {
  CORE ARGUMENT: Extends Fricker's framework to data science, arguing that
  epistemic injustices in data-driven technologies are conceptually distinct
  from more familiar harms. The authors demonstrate how data collection
  practices, algorithmic opacity, and system design choices can perpetrate
  testimonial and hermeneutical injustices even when no individual actor intends
  discrimination. They analyze examples from criminal justice and workplace
  algorithms.

  RELEVANCE: Directly addresses how data science technologies—including fairness
  auditing systems—can produce epistemic injustices through their design and
  deployment. The paper provides conceptual tools for understanding how sparse
  data for marginalized groups represents not just missing information but a
  systematic undermining of those groups' epistemic standing. Connects technical
  choices about sample size and data collection to normative concerns about
  epistemic justice.

  POSITION: Application of epistemic injustice framework to algorithmic systems,
  influential in AI ethics literature.
  },
  keywords = {epistemic-injustice, data-science, algorithms, High}
}

@article{anderson2012epistemic,
  author = {Anderson, Elizabeth},
  title = {Epistemic Justice as a Virtue of Social Institutions},
  journal = {Social Epistemology},
  year = {2012},
  volume = {26},
  number = {2},
  pages = {163--173},
  doi = {10.1080/09505431.2011.652086},
  note = {
  CORE ARGUMENT: Shifts focus from individual epistemic virtues to institutional
  design, arguing that epistemic justice requires social institutions—not just
  individuals—to correct for identity-prejudicial credibility assessments.
  Anderson emphasizes that cognitive biases difficult for individuals to overcome
  may be more susceptible to institutional correction through properly designed
  systems of testimonial gathering and assessment.

  RELEVANCE: Critical for understanding fairness auditing as an institutional
  practice. If epistemic justice demands institutional-level interventions, then
  fairness auditing systems must be designed to counteract systematic credibility
  deficits for marginalized groups. The institutional perspective highlights how
  sample size requirements and data collection protocols are not neutral
  technical choices but design decisions with epistemic justice implications.
  Particularly relevant for thinking about what institutional structures would be
  needed to address data sparsity without perpetuating epistemic injustice.

  POSITION: Institutional approach to epistemic justice, expanding beyond
  individual virtue epistemology.
  },
  keywords = {epistemic-justice, institutions, social-epistemology, High}
}

@article{hull2023dirty,
  author = {Hull, Gordon},
  title = {Dirty data labeled dirt cheap: epistemic injustice in machine learning systems},
  journal = {Ethics and Information Technology},
  year = {2023},
  volume = {25},
  pages = {1--14},
  doi = {10.1007/s10676-023-09712-y},
  note = {
  CORE ARGUMENT: Analyzes how data labeling practices in machine learning
  constitute epistemic injustice, particularly through the use of low-wage labor
  to label training data. Hull argues that the economic structure of data
  labeling creates hermeneutical injustice by excluding those who label data from
  participation in determining how their labor and knowledge contributions are
  conceptualized. The paper emphasizes the political nature of epistemic practices
  embedded in ML systems.

  RELEVANCE: Illuminates how data collection and curation practices—central to
  fairness auditing—are sites of epistemic injustice that precede and shape model
  outputs. The analysis suggests that sparse data for marginalized groups may
  reflect not just statistical under-sampling but systematic exclusion from the
  processes of knowledge production. Connects data scarcity to broader political-
  economic structures that determine whose knowledge counts.

  POSITION: Critical analysis of labor and power in ML data production, drawing
  on Foucault and Fricker.
  },
  keywords = {epistemic-injustice, data-labeling, machine-learning, Medium}
}

@article{milano2024algorithmic,
  author = {Milano, Silvia and Prunkl, Carina},
  title = {Algorithmic profiling as a source of hermeneutical injustice},
  journal = {Philosophical Studies},
  year = {2024},
  volume = {182},
  number = {1},
  pages = {185--203},
  doi = {10.1007/s11098-024-02084-5},
  note = {
  CORE ARGUMENT: Argues that algorithmic profiling creates hermeneutical
  injustice through epistemic fragmentation—the isolation of individuals in
  algorithmically-mediated environments that makes it difficult to develop,
  uptake, and apply new epistemic resources. When algorithms create filter
  bubbles and personalized experiences, individuals cannot compare and learn
  from shared experiences, undermining collective sense-making about algorithmic
  harms.

  RELEVANCE: While focused on content recommendation rather than fairness
  auditing, the epistemic fragmentation concept applies to data sparsity
  problems. When data for marginalized groups is sparse and scattered, it becomes
  difficult to develop shared hermeneutical resources for understanding and
  articulating algorithmic harms. The paper suggests that statistical uncertainty
  from sparse data compounds hermeneutical injustice by preventing coherent
  articulation of patterns affecting small groups.

  POSITION: Extends hermeneutical injustice concept to algorithmic environments
  and data-driven personalization.
  },
  keywords = {hermeneutical-injustice, algorithms, epistemic-fragmentation, Medium}
}

@article{grote2020ethics,
  author = {Grote, Thomas and Berens, Philipp},
  title = {On the ethics of algorithmic decision-making in healthcare},
  journal = {Journal of Medical Ethics},
  year = {2020},
  volume = {46},
  number = {3},
  pages = {205--211},
  doi = {10.1136/medethics-2019-105586},
  note = {
  CORE ARGUMENT: Examines trade-offs between accuracy gains from ML in medical
  diagnosis and epistemic costs from algorithmic opacity. The authors argue that
  uncertainty in assessing ML reliability potentially undermines clinicians'
  epistemic authority, creating tensions around paternalism, moral responsibility,
  and fairness. Drawing on social epistemology, they analyze how opacity affects
  the evidentiary norms of medical diagnosis.

  RELEVANCE: While focused on healthcare rather than fairness auditing, the
  analysis of uncertainty and epistemic authority directly parallels challenges
  in auditing fairness under sparse data. When ML models make predictions for
  small groups, both accuracy and reliability become uncertain, yet systems still
  claim epistemic authority. The paper's framework for analyzing epistemic costs
  of opacity applies to understanding how sparse data undermines both statistical
  and epistemic confidence in fairness assessments.

  POSITION: Social epistemology approach to algorithmic decision-making,
  emphasizing uncertainty and epistemic authority.
  },
  keywords = {epistemic-authority, uncertainty, algorithms, healthcare, Medium}
}

@article{zhioua2025origins,
  author = {Zhioua, Sami and Binkyte, Ruta and Ouni, Ayoub and Ktata, Farah Barika},
  title = {On the Origins of Sampling Bias: Implications on Fairness Measurement and Mitigation},
  journal = {arXiv},
  year = {2025},
  volume = {abs/2503.17956},
  arxivId = {2503.17956},
  doi = {10.48550/arXiv.2503.17956},
  note = {
  CORE ARGUMENT: Disambiguates "sampling bias" into sample size bias (SSB) and
  underrepresentation bias (URB), demonstrating through extensive experiments
  that these distinct forms of bias affect fairness measurements differently. The
  authors show that bias from sampling is borne unequally by different groups,
  potentially exacerbating discrimination against underrepresented populations.
  Provides actionable recommendations for practitioners addressing sample size
  challenges.

  RELEVANCE: Directly addresses the core technical challenge of the "statistical
  uncertainty horn"—how to measure fairness reliably when some groups have
  insufficient sample sizes. The distinction between SSB and URB clarifies that
  data sparsity creates multiple, compounding problems for fairness auditing.
  The finding that bias is borne unequally connects statistical challenges to
  epistemic justice concerns: groups with sparse data suffer both unreliable
  measurement AND amplified discrimination.

  POSITION: Technical analysis of sampling bias in fairness measurement with
  practical recommendations.
  },
  keywords = {sample-size, fairness-metrics, sampling-bias, statistical-reliability, High}
}

@inproceedings{jourdan2023fairness,
  author = {Jourdan, Fanny and Risser, Laurent and Loubes, Jean-Michel and Asher, Nicholas M.},
  title = {Are fairness metric scores enough to assess discrimination biases in machine learning?},
  booktitle = {Proceedings of the 2023 Conference on Trust and Natural Language Processing (TRUSTNLP)},
  year = {2023},
  doi = {10.48550/arXiv.2306.05307},
  arxivId = {2306.05307},
  note = {
  CORE ARGUMENT: Demonstrates through Monte Carlo simulations on real datasets
  that gender bias indices provide diverging and unreliable results when applied
  to small training and test samples. The authors show that common fairness
  metrics have low statistical power and confidence with small sample sizes,
  leading to overestimation or underestimation of actual discrimination.
  Emphasizes the critical importance of variance calculations for sound results
  in fairness assessment.

  RELEVANCE: Provides empirical evidence that fairness auditing with sparse data
  yields unreliable results—a central concern for the "statistical uncertainty
  horn" of the intersectionality dilemma. The finding that metrics diverge with
  small samples suggests that expanding fairness auditing to more intersectional
  groups (thereby creating smaller samples per group) may produce fairness
  assessments that are not just uncertain but potentially misleading. Critical
  for understanding the epistemic limitations of current fairness auditing
  practices.

  POSITION: Empirical study questioning reliability of fairness metrics with
  small samples.
  },
  keywords = {fairness-metrics, sample-size, statistical-reliability, empirical, High}
}

@article{konstantinov2021fairness,
  author = {Konstantinov, Nikola and Lampert, Christoph H.},
  title = {Fairness-Aware PAC Learning from Corrupted Data},
  journal = {Journal of Machine Learning Research},
  year = {2021},
  volume = {23},
  pages = {160:1--160:60},
  arxivId = {2102.06004},
  note = {
  CORE ARGUMENT: Studies fairness-aware learning under worst-case data
  corruption, proving that adversaries can sometimes force any learner to return
  overly biased classifiers regardless of sample size. Crucially, the strength
  of excess bias increases for learning problems with underrepresented protected
  groups. The paper provides tight theoretical bounds on achievable fairness
  guarantees under data corruption.

  RELEVANCE: Theoretical analysis demonstrating that underrepresentation (sparse
  data for protected groups) creates fundamental vulnerabilities in fairness-
  aware learning that cannot be overcome simply by collecting more data. The
  finding that bias increases with underrepresentation provides theoretical
  grounding for the practical challenge of fairness auditing with sparse data.
  Suggests that the statistical uncertainty problem may have theoretical limits
  beyond what can be solved through improved methodology.

  POSITION: Theoretical PAC learning analysis of fairness under data corruption
  and underrepresentation.
  },
  keywords = {fairness-learning, underrepresentation, PAC-learning, theoretical, High}
}

@inproceedings{mhasawade2024understanding,
  author = {Mhasawade, Vishwali and Rahman, Salman and Haskell-Craig, Zoé and Chunara, Rishi},
  title = {Understanding Disparities in Post Hoc Machine Learning Explanation},
  booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2024},
  pages = {617--631},
  doi = {10.1145/3630106.3659043},
  arxivId = {2401.14539},
  note = {
  CORE ARGUMENT: Investigates how data properties (limited sample size, covariate
  shift, concept shift, omitted variable bias) and model properties affect
  disparities in post-hoc ML explanations across demographic groups. Through
  controlled simulations and real-world experiments, demonstrates that increased
  covariate shift, concept shift, and covariate omission increase explanation
  disparities, with effects more pronounced for neural networks than linear models.

  RELEVANCE: Reveals that sparse data for marginalized groups creates explanation
  disparities—a form of epistemic injustice where different groups receive
  different quality of explanation for algorithmic decisions. The finding that
  data properties drive explanation disparities connects statistical challenges
  (sample size, shift, omitted variables) to epistemic harms (unequal access to
  understanding algorithmic systems). Particularly relevant for fairness auditing
  where explanation quality may vary by group.

  POSITION: Empirical investigation of explanation disparities arising from data
  and model properties.
  },
  keywords = {explainability, fairness, sample-size, data-properties, High}
}

@article{singh2023measures,
  author = {Singh, Harvineet and Chunara, Rishi},
  title = {Measures of Disparity and their Efficient Estimation},
  journal = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2023},
  pages = {671--682},
  doi = {10.1145/3600211.3604697},
  note = {
  CORE ARGUMENT: Derives sample size requirements for accurately estimating
  disparity metrics used in public health, economics, and ML fairness. Provides
  guidance on how many samples to collect per group to maximize precision of
  disparity estimates given fixed data collection budget. Shows that absent prior
  information, equally sampling groups typically performs well but may be
  suboptimal when group variances differ.

  RELEVANCE: Provides principled statistical framework for determining sample
  sizes needed for reliable fairness auditing across groups—directly addressing
  the "statistical uncertainty horn." The budget constraint framing illuminates
  trade-offs between auditing more groups (lower sample size per group) versus
  fewer groups (higher sample size per group). Findings suggest that expanding
  intersectional auditing necessarily reduces precision of estimates unless data
  collection budgets scale proportionally.

  POSITION: Statistical methodology for sample size determination in disparity
  measurement.
  },
  keywords = {sample-size, disparity-metrics, statistical-methodology, fairness-auditing, High}
}

@article{rothblum2021multigroup,
  author = {Rothblum, Guy N. and Yona, Gal},
  title = {Multi-group Agnostic PAC Learnability},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  year = {2021},
  pages = {9107--9115},
  arxivId = {2105.09989},
  note = {
  CORE ARGUMENT: Characterizes which loss functions admit multi-group agnostic
  PAC learning, where the goal is to learn a single predictor competitive with
  the best in a hypothesis class for every group in a potentially large
  collection. Provides algorithms with sample complexity logarithmic in the
  number of groups under natural conditions. Unifies previous positive and
  negative results from multi-group fairness literature.

  RELEVANCE: Theoretical foundation for understanding when fairness across many
  (potentially intersectional) groups is achievable and at what sample complexity
  cost. The logarithmic sample complexity in number of groups suggests that
  expanding fairness auditing to cover more intersectional groups may be
  statistically tractable under appropriate conditions. However, characterization
  of which loss functions admit solutions reveals fundamental limitations—not all
  fairness objectives can be simultaneously satisfied across many groups.

  POSITION: Theoretical PAC learning framework for multi-group fairness.
  },
  keywords = {multi-group-fairness, PAC-learning, sample-complexity, theoretical, Medium}
}

@inproceedings{geddes2022death,
  author = {Geddes, Katrina},
  title = {The Death of the Legal Subject: How Predictive Algorithms Are (Re)constructing Legal Subjectivity},
  booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2022},
  pages = {1741--1751},
  doi = {10.1145/3531146.3533134},
  note = {
  CORE ARGUMENT: Argues that predictive algorithms in legal decision-making are
  reconstructing legal subjectivity from a socio-political conception based on
  individual autonomy to an algorithmic conception based on population-level
  patterns. This shift displaces qualitative knowledge about individuals'
  intentions and capabilities with statistically-derived predictions,
  fundamentally altering the epistemology of legal subjectivity in ways
  incompatible with law's normative commitments.

  RELEVANCE: Illuminates how statistical prediction from population data—the core
  of fairness auditing—transforms the epistemic status of individuals by
  privileging population-level patterns over individual knowledge. When fairness
  auditing relies on group-level statistics, it may constitute epistemic injustice
  by discounting individuals' capacity to provide knowledge about their own
  situations. Particularly relevant for understanding the "death of the legal
  subject" as an epistemic phenomenon when data sparsity forces reliance on
  broader population patterns.

  POSITION: Critical analysis of algorithmic reconstruction of subjectivity and
  epistemic authority.
  },
  keywords = {algorithmic-subjectivity, epistemic-authority, legal-systems, Medium}
}

@article{holm2023statistical,
  author = {Holm, Sune},
  title = {Statistical evidence and algorithmic decision-making},
  journal = {Synthese},
  year = {2023},
  volume = {202},
  number = {1},
  pages = {1--16},
  doi = {10.1007/s11229-023-04246-8},
  note = {
  CORE ARGUMENT: Presents philosophical argument against relying exclusively on
  algorithmic predictions when they provide purely statistical evidence, drawing
  on epistemological work on statistical evidence. Argues that even if algorithmic
  predictions increase proportion of correct decisions, their epistemic deficiency
  as purely statistical evidence may make them inappropriate for resource
  allocation decisions affecting individuals.

  RELEVANCE: Provides epistemological grounding for concerns about fairness
  auditing based on statistical patterns from sparse data. If purely statistical
  evidence is epistemically deficient for individual decisions, then fairness
  audits relying on statistical inference from small samples may be doubly
  problematic: both statistically uncertain AND epistemically inappropriate. The
  argument suggests that uncertainty from sparse data is not just a quantitative
  problem but a qualitative epistemic limitation.

  POSITION: Philosophical analysis of statistical evidence in algorithmic
  decision-making.
  },
  keywords = {statistical-evidence, epistemology, algorithmic-decisions, Medium}
}

@article{kiviat2023moral,
  author = {Kiviat, Barbara},
  title = {The Moral Affordances of Construing People as Cases: How Algorithms and the Data They Depend on Obscure Narrative and Noncomparative Justice},
  journal = {Sociological Theory},
  year = {2023},
  volume = {41},
  number = {3},
  pages = {175--200},
  doi = {10.1177/07352751231186797},
  note = {
  CORE ARGUMENT: Argues that algorithmic systems' dependence on rendering people
  as discrete "cases" defined by regularized attributes obscures narrative and
  noncomparative justice. While this enables computation and comparative
  assessments, it systematically eliminates information about individuals as
  actors in unfolding life narratives. The paper analyzes how different moral
  standards require different information infrastructures, with case-based systems
  affording only certain types of justice.

  RELEVANCE: Illuminates an epistemic dimension of the data sparsity problem:
  fairness auditing's reliance on statistical cases obscures precisely the
  narrative and contextual knowledge often possessed by marginalized groups about
  their own experiences. Sparse data compounds this problem by forcing even
  greater reliance on limited categorical attributes rather than rich narratives.
  Connects technical constraints (need for regularized data) to epistemic
  injustices (exclusion of narrative knowledge).

  POSITION: Sociological analysis of categorization, classification, and moral
  reasoning in algorithmic systems.
  },
  keywords = {categorization, epistemic-infrastructure, noncomparative-justice, Medium}
}

@article{lo2024bringing,
  author = {Lo, Victor S. Y. and Datta, Soham and Salami, Youssouf},
  title = {Bringing practical statistical science to AI and predictive model fairness testing},
  journal = {AI and Ethics},
  year = {2024},
  volume = {5},
  pages = {2149--2164},
  doi = {10.1007/s43681-024-00518-2},
  note = {
  CORE ARGUMENT: Advocates for applying rigorous statistical science to fairness
  testing, emphasizing proper hypothesis testing, confidence intervals, and
  sample size calculations. The authors argue that much current fairness auditing
  lacks statistical rigor, leading to unreliable conclusions. They provide
  practical guidance for determining adequate sample sizes and interpreting
  statistical significance in fairness assessments.

  RELEVANCE: Bridges statistical methodology and fairness auditing practice,
  highlighting the importance of statistical rigor when reasoning under
  uncertainty. The emphasis on sample size calculations and hypothesis testing
  directly addresses challenges of auditing with sparse data. However, the paper's
  focus on conventional statistical significance may not fully address epistemic
  justice concerns about whose knowledge and experiences are rendered
  statistically invisible through sample size requirements.

  POSITION: Methodological guidance for statistical rigor in fairness testing.
  },
  keywords = {statistical-methodology, fairness-testing, hypothesis-testing, Low}
}

@article{vabalas2019machine,
  author = {Vabalas, Andrius and Gowen, Emma and Poliakoff, Ellen and Casson, Alexander J.},
  title = {Machine learning algorithm validation with a limited sample size},
  journal = {PLoS ONE},
  year = {2019},
  volume = {14},
  number = {11},
  doi = {10.1371/journal.pone.0224365},
  note = {
  CORE ARGUMENT: Demonstrates through simulations that K-fold cross-validation
  produces strongly biased performance estimates with small sample sizes, with
  bias evident even at sample size 1000. Nested CV and train/test splits produce
  robust estimates regardless of sample size. Feature selection on pooled data
  contributes substantially to bias. Provides methodological recommendations for
  validation with small datasets.

  RELEVANCE: Critical methodological guidance for fairness auditing with sparse
  data. If standard validation methods are unreliable with small samples, then
  expanding fairness auditing to cover more intersectional groups (thereby
  reducing sample size per group) requires methodological adaptations to avoid
  biased fairness assessments. The findings suggest that methodological rigor
  alone cannot fully overcome sample size limitations, but can mitigate bias.

  POSITION: Methodological study on ML validation with small samples, highly
  cited.
  },
  keywords = {sample-size, validation-methods, methodology, machine-learning, Low}
}

@inproceedings{baeyaert2024epistemic,
  author = {Baeyaert, Joffrey},
  title = {Epistemic Injustice in Generative AI: A Pipeline Taxonomy, Empirical Hypotheses, and Stage-Matched Governance},
  journal = {arXiv},
  year = {2024},
  arxivId = {2411.xxxxx},
  note = {
  CORE ARGUMENT: Develops a mutually exclusive and collectively exhaustive (MECE)
  taxonomy mapping testimonial, hermeneutical, and distributive epistemic
  injustices onto four stages of generative AI development: data collection,
  model training, inference, and dissemination. Proposes stage-matched governance
  strategies to diagnose, test, and mitigate epistemic harms. Emphasizes how data
  collection stage particularly affects marginalized communities' visibility and
  intelligibility.

  RELEVANCE: While focused on generative AI, the pipeline taxonomy applies to
  fairness auditing systems. The emphasis on data collection stage as a site of
  distributive epistemic injustice directly addresses concerns about whose data is
  collected, in what quantities, and how this shapes downstream fairness
  assessments. The taxonomy provides conceptual framework for understanding how
  sparse data for marginalized groups creates cascading epistemic injustices
  through the ML pipeline.

  POSITION: Systematic taxonomy of epistemic injustice in AI development pipeline.
  },
  keywords = {epistemic-injustice, AI-pipeline, generative-AI, taxonomy, Low}
}
