## Section 3: Measurement Validity—Do Fairness Metrics Measure What They Claim?

Even when intersectional groups are successfully specified, fairness auditing confronts a deeper problem: the metrics used to evaluate systems may fail to measure what they purport to measure. Drawing on measurement theory from the philosophy of science and psychometrics, recent work reveals systematic construct validity failures in fairness metrics. These failures manifest as target specification bias, metric incompatibility, and misalignment between algorithmic measures and human fairness judgments. This section demonstrates that fairness—as a contested normative concept—resists stable operationalization, compounding the dilemma established in earlier sections. Measurement challenges persist even after group specification is resolved.

### Subsection 3.1: Construct Validity and Target Specification Bias

Measurement theory distinguishes between unobservable theoretical constructs (like "intelligence," "toxicity," or "fairness") and their operationalizations through measurable proxies. Valid measurement requires that operationalizations actually capture the construct of interest, a property called construct validity (Jacobs et al. 2020). Recent work applying this framework to fairness metrics reveals troubling validity gaps.

Tal (2023) introduces "target specification bias" to describe a pervasive source of algorithmic bias in healthcare fairness: the operationalization of the target variable fails to match decision-makers' actual concerns. Drawing on metrology—the science of measurement—Tal argues this bias arises because stakeholders are typically interested in predicting outcomes of counterfactual scenarios (what would happen under fair treatment?) rather than actual scenarios (what did happen given existing disparities?). Crucially, this mismatch "persists independently of data limitations and health disparities" (Tal 2023, p. 2). It is a conceptual failure, not a technical one that better data or algorithms could resolve. When fairness auditing operationalizes concepts like "qualified applicant" or "creditworthy borrower" using historical data shaped by discrimination, the resulting metrics measure past discriminatory patterns rather than counterfactual fairness.

This conceptual gap between construct and operationalization pervades fairness benchmarking. Blodgett et al. (2021) apply a "measurement modeling lens" from quantitative social sciences to examine four NLP fairness benchmarks for stereotyping. Their systematic analysis reveals that benchmarks "frequently lack clear articulations of what is being measured" (Blodgett et al. 2021, p. 1004). The frameworks conflate distinct concepts—for instance, treating stereotyping, bias, and discrimination as interchangeable when they represent different theoretical constructs. They also contain unstated assumptions about how stereotyping should be conceptualized and operationalized, creating ambiguities that threaten validity. Without clear construct definitions, evaluators cannot assess whether deviations from expected metric values represent actual fairness problems or measurement artifacts.

Jacobs et al. (2020) formalize this analysis, showing how unobservable theoretical constructs like "creditworthiness" are transformed into measurable quantities and how this process introduces fairness-related harms. They demonstrate that mismatches between constructs and operationalizations create systematic errors: measures intended to capture one property (e.g., risk) instead capture confounded properties (e.g., risk plus historical discrimination). The measurement modeling framework they propose—adapted from educational testing—provides tools for detecting such mismatches through reliability and validity assessments. Yet even with sophisticated measurement methods, the fundamental challenge remains: fairness is precisely the kind of contested normative construct that resists determinate operationalization.

The most comprehensive recent examination of construct validity in AI evaluation comes from Bean et al. (2025), who systematically reviewed 445 large language model benchmarks with 29 expert reviewers. They find pervasive patterns undermining construct validity across benchmarks for complex phenomena like "safety" and "robustness": vague construct definitions, poor alignment between what benchmarks claim to measure and what they actually test, and lack of validation that benchmarks capture properties that matter for real-world performance. Their core insight applies directly to fairness metrics: "measuring complex abstract phenomena requires strong construct validity—having measures that represent what matters" (Bean et al. 2025, p. 1). Without this validity, even mathematically rigorous metrics may not measure fairness at all.

### Subsection 3.2: Empirical Evidence of Metric Unreliability

Theoretical arguments about construct validity find support in empirical studies revealing that fairness metrics are incompatible, unstable, and misaligned with human judgments. These findings suggest that fairness is not a unitary construct admitting single valid operationalizations.

Delobelle et al. (2022) provide systematic evidence that bias metrics for pre-trained language models are "biased rulers"—measurement instruments whose outputs depend heavily on arbitrary methodological choices rather than properties of what's being measured. Comparing multiple intrinsic bias metrics across models, they find that metric values are highly sensitive to template choices, random seed selection, and word embedding methods. Different metrics yield contradictory conclusions about the same model: one metric shows high bias where another shows low bias. Crucially, they find "no tangible evidence of intrinsic bias metrics relating to extrinsic bias in downstream tasks" (Delobelle et al. 2022, p. 1693), suggesting that what intrinsic metrics measure may not track actual fairness in applications. This represents a fundamental construct validity failure: if the operationalization doesn't predict outcomes that matter, it's not measuring the right thing.

Cao et al. (2022) extend this finding through an extensive correlation study across 19 contextualized language models, examining whether intrinsic and extrinsic fairness metrics align. Even after correcting for metric misalignments, evaluation dataset noise, and experimental configurations, they find that the two types of metrics "do not necessarily correlate" (Cao et al. 2022, p. 1). This disconnect persists across different bias notions and model architectures, suggesting a systematic problem: intrinsic metrics may not measure what matters for downstream fairness. The implication is profound—efforts to "debias" models based on intrinsic metrics may not reduce actual unfairness in applications.

Beyond metric incompatibility, fairness metrics fail to align with human fairness judgments. Constantin et al. (2022) developed a mixed-initiative system (FairAlign) where laypeople assess fairness through interactive visualizations, then compared their judgments with automated metrics. They find significant gaps between algorithmic measures and human perceptions of fairness, with cultural and perceptual factors creating divergence. If fairness metrics don't capture what people perceive as fair or unfair, they lack face validity—a basic form of construct validity indicating whether measures appear to capture the intended construct. Similarly, Saha et al. (2019) demonstrate that even when metrics are mathematically well-defined, non-experts systematically misinterpret what they measure, creating comprehension gaps that undermine informed decision-making.

The measurement validity challenge extends beyond metric design to fundamental questions about what fairness metrics can and should measure. Long (2020) provides philosophical analysis showing that false positive rate (FPR) equality—a widely used fairness criterion—is "morally irrelevant" despite being mathematically well-defined and intuitively appealing. Long argues that FPR equality seems fair only through confusion: it appears to equalize burdens across groups, but the specific burden it equalizes (incorrect positive predictions) has no inherent moral significance. The inevitable mathematical tradeoff between calibration and FPR equality reveals that one of these must not be measuring real fairness, since genuine fairness properties should not conflict. Long's analysis exemplifies the philosophical work required to assess construct validity: mathematical rigor doesn't guarantee normative validity.

### Subsection 3.3: Distributive Justice Foundations and Metric Diversity

The proliferation of incompatible fairness metrics partly reflects fairness's conceptual complexity and contested normative foundations. Different metrics operationalize fundamentally different conceptions of what fairness requires, often embedding implicit philosophical commitments about distributive justice.

Baumann et al. (2022) provide a unifying framework linking group fairness metrics to theories of distributive justice. They demonstrate that standard fairness metrics differ systematically in how they conceptualize benefits and harms and what moral claims to benefits they assume. Demographic parity treats equal probability of favorable outcomes as the relevant benefit; equalized odds focuses on equal error rates; and calibration emphasizes equal predictive value of scores. These differences are not merely technical variations but reflect contested normative commitments: "different fairness metrics differ in how they measure benefit/harm and what moral claims to benefits they assume" (Baumann et al. 2022, p. 1). Choosing between metrics requires taking stances on disputed questions in distributive justice—questions that political philosophers have debated for centuries without consensus.

This normative pluralism creates irreducible operationalization challenges. Truong et al. (2025) argue that construct validity problems in fairness measurement stem from "insufficient systematization" of the unfairness construct. They propose a framework based on Fair Equality of Chances from political philosophy, decomposing unfairness into harms/benefits, morally arbitrary factors (like race or gender), and morally decisive factors (like qualifications). This systematization aims to guide valid operationalization by clarifying what fairness metrics should measure. Yet their proposal illustrates the challenge: Fair Equality of Chances itself represents one contested philosophical framework among many. Rawlsian, utilitarian, sufficientarian, and relational egalitarian theories would systematize fairness differently, yielding different operationalizations.

Buijsman (2023) confronts this pluralism directly, arguing that the mathematical impossibility of simultaneously optimizing all fairness metrics creates need for substantive normative theory to navigate metric choices. Drawing on Rawls's theory of justice as fairness, Buijsman proposes prioritizing metrics that have the biggest impact on the most vulnerable groups. This represents a principled approach to operationalization: "metric incompatibility requires normative theory to guide choices" (Buijsman 2023, p. 1323). Yet implementing this approach requires resolving contested questions about which groups are most vulnerable (connecting to the group specification problem from earlier sections) and what forms of disadvantage matter most—precisely the questions on which normative theories diverge.

The measurement challenge extends to practical implementation. Smith et al. (2023) interviewed fairness practitioners working on recommender systems and found they face overwhelming complexity in operationalizing fairness: "The proliferation of fairness metrics creates a complex decision space where practitioners lack guidance on picking metrics appropriate to their context" (Smith et al. 2023, p. 1). Even when metrics are well-defined mathematically, determining which measure what matters in specific contexts requires contextual judgments that resist formalization. Similarly, Sahlgren (2024) identifies an "action-guidance gap" in AI ethics where fairness principles fail to guide practice due to "ambiguous operationalization" of concepts like fairness, equality, and justice (Sahlgren 2024, p. 1019). Construct validity issues directly undermine fairness implementation.

These findings suggest that fairness may be fundamentally resistant to valid operationalization because it is an "essentially contested concept" in W.B. Gallie's sense: a concept whose proper application is inherently disputable despite agreement on general criteria. Different operationalizations don't reflect measurement error or conceptual confusion but embody genuinely different conceptions of what fairness requires. Target specification bias compounds group specification uncertainty: even after determining which intersectional groups to audit, measurement challenges remain because contested normative concepts resist stable formalization.

This reveals a deepening of the dilemma identified in earlier sections. Addressing statistical uncertainty through sparse data methods and addressing ontological uncertainty through philosophical analysis of group constitution are necessary but insufficient. Even with specified groups and adequate samples, measurement validity problems persist because fairness metrics systematically fail to capture what they claim to measure. The next section examines how different normative frameworks generate different answers about which groups matter and how to prioritize them, further compounding these operationalization challenges.

