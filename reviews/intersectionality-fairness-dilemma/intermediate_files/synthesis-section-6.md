## Section 5: Epistemic Justice and the Dilemma's Interaction Effects

The technical sophistication of multicalibration and the conceptual depth of social ontology debates might suggest that the intersectionality problem could be resolved by combining better algorithms with clearer philosophical foundations. This section argues that such optimism is misplaced. Statistical uncertainty and ontological uncertainty do not merely coexist as independent challenges—they interact to create a genuine dilemma where addressing one horn systematically exacerbates the other. Epistemic justice provides the normative lens for understanding why both horns involve structural injustice rather than merely technical inconvenience.

### Subsection 5.1: Epistemic Injustice in Algorithmic Systems

Miranda Fricker's foundational framework establishes epistemic injustice as a distinct category of injustice, identifying two primary forms: testimonial injustice, when prejudice causes deflated credibility assessments of a speaker's testimony, and hermeneutical injustice, when gaps in collective interpretive resources prevent individuals from making sense of their social experiences (Fricker 2007). Both forms harm individuals specifically in their capacity as knowers. While Fricker's original analysis focused on interpersonal contexts, recent work extends the framework to data science and algorithmic systems, revealing how technical design choices can perpetuate epistemic injustices even without individual discriminatory intent.

Symons and Alvarado (2022) demonstrate how data collection practices, algorithmic opacity, and system design choices perpetrate testimonial and hermeneutical injustices through technological infrastructure. Their analysis reveals that sparse data for marginalized groups represents not merely missing information but a systematic undermining of those groups' epistemic standing. When algorithmic systems require large sample sizes that marginalized groups cannot provide, they reproduce hermeneutical injustice by leaving those groups' experiences underrepresented in the collective interpretive resources embedded in machine learning models. The connection between data sparsity and epistemic injustice is not merely metaphorical—it reflects how statistical requirements for reliability can constitute structural barriers to epistemic representation.

Anderson (2012) shifts the focus from individual epistemic virtues to institutional design, arguing that epistemic justice requires social institutions—not just individuals—to correct for identity-prejudicial credibility assessments. This institutional perspective is critical for understanding fairness auditing as an epistemic practice. If cognitive biases are difficult for individuals to overcome but susceptible to institutional correction through properly designed systems of testimonial gathering and assessment, then fairness auditing systems must be designed to counteract systematic credibility deficits for marginalized groups. Sample size requirements and data collection protocols are not neutral technical choices but design decisions with epistemic justice implications. The question becomes: what institutional structures would be needed to address data sparsity without perpetuating epistemic injustice?

Hull (2023) analyzes how the economic structure of data labeling creates hermeneutical injustice by excluding those who label data from participation in determining how their labor and knowledge contributions are conceptualized. This reveals that data collection and curation practices—central to fairness auditing—are sites of epistemic injustice that precede and shape model outputs. Sparse data for marginalized groups may reflect not just statistical under-sampling but systematic exclusion from knowledge production processes. The political-economic structures that determine whose knowledge counts also determine whose data gets collected, in what quantities, and with what conceptual framing.

Milano and Prunkl (2024) introduce the concept of epistemic fragmentation—the isolation of individuals in algorithmically-mediated environments that undermines collective sense-making about algorithmic harms. When data for marginalized groups is sparse and scattered, developing shared hermeneutical resources for understanding and articulating algorithmic patterns becomes difficult. Statistical uncertainty from sparse data compounds hermeneutical injustice by preventing coherent articulation of patterns affecting small groups. The epistemic harm is double: not only do small sample sizes reduce statistical confidence, they also fragment experiences in ways that prevent collective understanding.

This literature establishes that data sparsity constitutes epistemic injustice, not statistical inconvenience. The requirement for large sample sizes to achieve statistical reliability can itself exclude groups from epistemic representation in algorithmic systems. This framing fundamentally changes how we understand the statistical uncertainty problem—it is not merely a technical challenge to be solved through better methods, but a site of structural injustice where statistical requirements systematically disadvantage marginalized communities.

### Subsection 5.2: The Interaction—Statistical Reliability vs. Ontological Adequacy

The core dilemma emerges when we recognize how statistical and ontological uncertainties interact. Kong (2022) frames the problem as a dilemma between infinite regress and fairness gerrymandering: either continuously split groups into finer intersections (creating ever-smaller groups with insufficient data) or arbitrarily select which groups to protect (leaving unprotected groups vulnerable to hidden discrimination). While Kong identifies this dilemma conceptually, the epistemic justice literature reveals its structural dimension.

Zhioua et al. (2025) distinguish sample size bias from underrepresentation bias, demonstrating through extensive experiments that bias from sampling is borne unequally by different groups, potentially exacerbating discrimination against underrepresented populations. This finding reveals the first horn of the interaction: expanding intersectional coverage to address ontological adequacy necessarily reduces per-group sample sizes (assuming fixed data collection budgets), which in turn amplifies bias for those groups. The statistical problem compounds rather than merely adding noise—groups with sparse data suffer both unreliable measurement and amplified discrimination.

Konstantinov and Lampert (2021) provide theoretical grounding, proving that underrepresentation creates fundamental vulnerabilities in fairness-aware learning that cannot be overcome simply by collecting more data. Their PAC learning analysis shows that adversaries can force learners to return overly biased classifiers when protected groups are underrepresented, with the strength of excess bias increasing as underrepresentation worsens. This suggests the statistical uncertainty problem may have theoretical limits beyond what improved methodology can solve. More data helps, but when groups are inherently small (many intersectional groups will be small by definition), fundamental limitations persist.

Jourdan et al. (2023) provide empirical evidence that fairness metrics yield unreliable, diverging results with small samples. Their Monte Carlo simulations on real datasets demonstrate that common fairness metrics have low statistical power and confidence when sample sizes are small, leading to overestimation or underestimation of actual discrimination. This empirical finding confirms the theoretical concern: expanding fairness auditing to more intersectional groups (thereby creating smaller samples per group) produces fairness assessments that are not just uncertain but potentially misleading. The epistemic harm extends beyond missing information to systematically distorted information.

Mhasawade et al. (2024) reveal an additional epistemic dimension: sparse data creates unequal explanation quality across groups. Their investigation of explanation disparities shows that limited sample size, covariate shift, concept shift, and omitted variable bias affect explanation quality differently across demographic groups, with effects more pronounced for neural networks. Groups with sparse data receive lower-quality explanations for algorithmic decisions—a clear form of epistemic injustice where different groups have different access to understanding algorithmic systems.

Singh and Chunara (2023) formalize the trade-off through sample size requirements for disparity estimation. Their framework shows that given fixed data collection budgets, expanding intersectional coverage necessarily reduces precision per group. The budget constraint illuminates the dilemma's economic dimension: auditing more groups costs more (either in reduced precision per group or increased total data collection costs). This is not merely a technical optimization problem but a political economy question about resource allocation for epistemic representation.

Here is the core interaction that constitutes the dilemma: expanding intersectional coverage to address ontological adequacy (ensuring all relevant groups are represented) necessarily reduces per-group sample sizes under realistic budget constraints, undermining statistical reliability and amplifying epistemic injustice for those groups. Conversely, constraining coverage to ensure statistical reliability requires resolving contested ontological questions about which groups exist and matter—but as the social ontology literature demonstrates, no such resolution is available. The dilemma has intractable horns: each attempt to address one dimension systematically worsens the other.

### Subsection 5.3: Critical Perspectives—Is the Dilemma Resolvable?

Critical scholars question whether the intersectionality dilemma can be resolved within current fairness frameworks, arguing that formalization itself may be the limiting factor. Green (2022) distinguishes "formal algorithmic fairness" (restricted to isolated decision-making procedures) from "substantive algorithmic fairness" (analyzing algorithms within social context). He argues that impossibility results, including the intersectionality dilemma, stem from formalist methodology that abstracts away from structural context. Expanding scope to substantive fairness might escape impossibility by considering how algorithms interact with broader social institutions. However, Green's framework provides limited concrete guidance on how to operationalize "substantive fairness" for auditing—critics might argue the distinction diagnoses problems without solving them.

Wachter et al. (2021) provide a stronger claim: fairness cannot be automated because legal and moral conceptions of discrimination fundamentally exceed statistical formalization. Their analysis of EU non-discrimination law shows that legal fairness requires considering intent, context, and structural factors that resist mathematical operationalization. Current fairness metrics measure correlation, not causation or discrimination as legally understood. This suggests the intersectionality dilemma may be one instance of a broader impossibility: fairness as a contested normative concept may resist valid operationalization regardless of group specification method.

Kasirzadeh (2022) draws on Iris Marion Young's work on structural injustice to argue that algorithmic fairness, rooted in distributive justice ideals, cannot address oppressive social structures. Distributive frameworks focus on allocating goods across individuals or groups, but structural oppression operates through social relations, institutional constraints, and systemic patterns that distribution alone cannot remedy. If fairness metrics are fundamentally distributive—measuring how prediction accuracy, error rates, or resource allocations are distributed across groups—then they may be conceptually inappropriate for addressing structural injustice. The intersectionality dilemma might be irresolvable within distributive fairness paradigms not because of statistical or ontological limits but because the problem itself exceeds distributive frameworks.

Hampton (2021) uses Black feminist theory to ground the concept of algorithmic oppression, critiquing the language and assumptions of the fairness, accountability, and transparency community. She highlights the "double bind" of technology for marginalized communities and calls for abolition and empowerment rather than fairness optimization. From this perspective, the intersectionality dilemma may not be resolvable through better fairness metrics because the fairness framework itself inadequately captures Black feminist concerns about oppression. The dilemma reveals not a technical problem requiring algorithmic solutions but a political problem requiring structural transformation. Abolition, not optimization, becomes the appropriate response.

Lopez (2024) introduces "susceptibility to algorithmic disadvantage" as a conceptual framework drawing on relational equality theory. She identifies vertical dimensions (state-individual relations) and horizontal dimensions (intersectional inequalities) that co-constitute and reinforce each other—more than the sum of parts. This parallels the intersectional feminist insight that interlocking oppressions exceed single-axis analysis. Lopez's framework suggests that fairness on single axes may not aggregate to fairness across intersections because intersectional harms emerge from the interaction of dimensions, not their addition. This provides conceptual grounding for understanding why technical solutions addressing statistical challenges separately from ontological challenges will fail—the dimensions interact to produce emergent harms.

These critical perspectives converge on the conclusion that formalization has inherent limits for addressing intersectional algorithmic justice. However, most critiques offer general calls for "substantive fairness," "participatory approaches," or "abolition" without specifying concrete alternatives to current auditing practices. Green's substantive fairness expands scope but remains vague on implementation. Wachter et al. correctly identify automation limits but offer little guidance beyond "complement with human judgment." Kasirzadeh's structural injustice critique diagnoses paradigm limits but provides limited constructive alternatives. Hampton's abolitionist perspective rejects fairness optimization entirely but leaves open questions about near-term governance. Lopez's relational framework offers conceptual clarity about interaction effects but limited operational guidance.

The gap between critical diagnosis and constructive alternatives reflects the dilemma's depth. If the problem arises from the interaction of statistical constraints (fixed budgets, sample size requirements, theoretical limits on learning from sparse data) and ontological constraints (contested group constitution, normative pluralism, practice-based ontology), then purely technical solutions will fail by ignoring ontological dimensions, while purely philosophical solutions will fail by ignoring statistical feasibility. Critics correctly identify formalization limits but struggle to specify alternatives because the dilemma constrains both horns simultaneously.

This leaves open the question: if the intersectionality dilemma is irresolvable within current fairness frameworks, what follows? One response is participatory governance, where affected communities determine which groups matter and acceptable trade-offs between coverage and reliability. Another is accepting fundamental limitations—fairness auditing can provide partial information but not comprehensive justice guarantees. A third is institutional rather than purely algorithmic responses, addressing data scarcity through structural reforms in data collection, resource allocation, and power distribution. None of these responses resolve the dilemma technically, but they may represent more appropriate responses than pursuing optimal algorithmic solutions to structurally intractable problems.
