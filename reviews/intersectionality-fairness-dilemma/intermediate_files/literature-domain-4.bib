@comment{
====================================================================
DOMAIN: Measurement Theory and Construct Validity in ML Fairness
SEARCH_DATE: 2026-01-10
PAPERS_FOUND: 18 total (High: 9, Medium: 7, Low: 2)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex, arXiv
====================================================================

DOMAIN_OVERVIEW:
This domain examines how concepts from measurement theory and construct validity, drawn from the philosophy of science and psychometrics, apply to fairness metrics in machine learning. The central question is whether fairness metrics actually measure what they purport to measure, and what validity challenges arise when operationalizing contested normative concepts like fairness. Key debates center on: (1) the gap between formal fairness definitions and normative goals (e.g., do demographic parity metrics truly capture fairness?); (2) target specification bias, where the operationalized construct differs from what decision-makers actually care about; (3) measurement modeling frameworks that reveal ambiguities in what fairness benchmarks measure; and (4) the limits of formalization for inherently contested concepts. Recent work shows that many fairness metrics exhibit poor construct validity - they lack clear articulations of what is being measured, conflate distinct normative concerns, and often fail to correlate with each other or with human judgments of fairness.

RELEVANCE_TO_PROJECT:
This domain directly supports the paper's core thesis that fairness auditing faces fundamental conceptual challenges in operationalization. The measurement theory literature demonstrates that many current fairness metrics suffer from validity problems - they don't actually measure what they claim to measure. This is precisely the kind of conceptual challenge (not just technical optimization problem) that the research idea emphasizes. Papers on construct validity, target specification bias, and measurement modeling provide the theoretical framework for critiquing current fairness auditing practices.

NOTABLE_GAPS:
While there is substantial work on the validity of individual fairness metrics, there is limited philosophical analysis of how measurement theory principles should guide the design of new metrics. Most work focuses on critique rather than constructive alternatives. Additionally, the connection between construct validity problems and the contested nature of fairness concepts deserves deeper philosophical treatment.

SYNTHESIS_GUIDANCE:
When synthesizing this domain, emphasize the parallel between measurement problems in psychology/social sciences and fairness metrics in ML. The Tal paper on target specification bias provides a crucial bridge between measurement theory and healthcare fairness. Blodgett et al.'s work on measurement modeling offers a comprehensive framework. Consider organizing around: (1) what construct validity means, (2) evidence that fairness metrics lack it, (3) why contested concepts resist valid operationalization.

KEY_POSITIONS:
- Measurement Modeling Critique: 8 papers - Fairness metrics lack clear construct definitions and validity
- Target Specification Bias: 3 papers - Operationalizations mismatch decision-makers' actual concerns
- Metric Incompatibility: 5 papers - Different fairness metrics measure fundamentally different things
- Contextualist Response: 2 papers - Fairness measurement requires domain-specific contextualization
====================================================================
}

@inproceedings{tal2023target,
  author = {Tal, Eran},
  title = {Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare},
  booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2023},
  doi = {10.1145/3600211.3604678},
  arxivId = {2308.02081},
  note = {
  CORE ARGUMENT: Introduces "target specification bias" - a pervasive source of ML bias that arises when the operationalization of the target variable does not match its definition by decision makers. This occurs because decision makers are typically interested in predicting outcomes of counterfactual scenarios rather than actual healthcare scenarios. Drawing on metrology (the science of measurement), Tal argues this bias persists independently of data limitations and health disparities.

  RELEVANCE: This paper provides the most direct philosophical analysis of operationalization problems in fairness metrics, grounded in measurement theory. The concept of target specification bias captures precisely the gap between what fairness metrics formalize and what stakeholders actually care about. The connection to counterfactual reasoning is especially relevant for fairness auditing, where we want to know what would happen under fair treatment, not just what did happen.

  POSITION: Measurement-theoretic critique showing that operationalization failures are conceptual, not merely technical problems. Represents a philosopher of science analyzing ML fairness through the lens of metrology.
  },
  keywords = {measurement-theory, construct-validity, target-bias, High}
}

@inproceedings{blodgett2021stereotyping,
  author = {Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna M.},
  title = {Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  year = {2021},
  pages = {1004--1015},
  doi = {10.18653/v1/2021.acl-long.81},
  note = {
  CORE ARGUMENT: Applies measurement modeling lens from social sciences to NLP fairness benchmarks for stereotyping. Through systematic analysis of four benchmarks for language modeling and coreference resolution, reveals that benchmarks frequently lack clear articulations of what is being measured. Identifies ambiguities and unstated assumptions in how benchmarks conceptualize and operationalize stereotyping, threatening their validity as measurement models.

  RELEVANCE: Provides the most thorough application of measurement theory to fairness evaluation. The "measurement modeling lens" framework is directly applicable to fairness auditing more broadly - it shows how to systematically assess whether a metric actually measures what it claims to measure. The emphasis on conceptualization vs. operationalization is key for understanding construct validity problems.

  POSITION: Applies social science measurement theory to critique fairness benchmarks. Demonstrates that many supposed fairness measurements lack construct validity due to poor conceptualization.
  },
  keywords = {measurement-modeling, construct-validity, fairness-benchmarks, High}
}

@inproceedings{jacobs2020meaning,
  author = {Jacobs, Abigail Z. and Blodgett, Su Lin and Barocas, Solon and Daumé, Hal and Wallach, Hanna M.},
  title = {The meaning and measurement of bias: lessons from natural language processing},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  year = {2020},
  doi = {10.1145/3351095.3375671},
  note = {
  CORE ARGUMENT: Introduces measurement modeling from quantitative social sciences as framework for examining how values enter computational systems. Shows how unobservable theoretical constructs (like "creditworthiness" or "toxicity") are turned into measurable quantities and how this process may introduce fairness-related harms. Demonstrates how to assess construct validity and reliability to detect harms arising from mismatches between constructs and their operationalizations.

  RELEVANCE: Foundational tutorial establishing measurement modeling as the appropriate framework for fairness evaluation. Provides systematic method for unpacking the gap between normative concepts and their operationalizations. The emphasis on unobservable theoretical constructs is crucial - fairness is precisely such a construct, and this paper shows why operationalizing it is conceptually challenging.

  POSITION: Establishes measurement theory as the appropriate framework for understanding fairness in AI. Shows that construct validity problems are central to fairness concerns.
  },
  keywords = {measurement-modeling, construct-validity, debiasing, High}
}

@article{long2020fairness,
  author = {Long, Robert},
  title = {Fairness in machine learning: against false positive rate equality as a measure of fairness},
  journal = {Journal of Moral Philosophy},
  year = {2020},
  doi = {10.1163/17455243-20213439},
  arxivId = {2007.02890},
  note = {
  CORE ARGUMENT: Provides ethical framework for evaluating fairness measures, arguing that false positive rate equality is morally irrelevant and does not actually measure fairness despite initial appearances. Shows that just because a metric is mathematically well-defined and seems intuitively important doesn't mean it tracks a normatively significant property. The inevitable tradeoff between calibration and FPR equality reveals that one of these must not be measuring real fairness.

  RELEVANCE: Exemplifies the conceptual analysis needed to evaluate whether fairness metrics measure what matters. Demonstrates that mathematical rigor doesn't guarantee normative validity. The argument that FPR equality is "morally irrelevant" despite seeming fair is precisely the kind of construct validity critique needed for fairness auditing.

  POSITION: Normative philosophical critique showing that popular fairness metrics may not measure fairness at all. Argues against accepting metrics without philosophical justification.
  },
  keywords = {fairness-metrics, normative-critique, construct-validity, High}
}

@article{baumann2022distributive,
  author = {Baumann, J. and Hertweck, Corinna and Loi, M. and Heitz, Christoph},
  title = {Distributive Justice as the Foundational Premise of Fair ML: Unification, Extension, and Interpretation of Group Fairness Metrics},
  journal = {ArXiv},
  year = {2022},
  volume = {abs/2206.02897},
  doi = {10.48550/arXiv.2206.02897},
  arxivId = {2206.02897},
  note = {
  CORE ARGUMENT: Proposes comprehensive framework linking group fairness metrics to theories of distributive justice. Shows that different fairness metrics differ in how they measure benefit/harm and what moral claims to benefits they assume. This unifying framework reveals the normative choices associated with standard metrics and provides structure for expanding beyond standard parity-based metrics that may harm marginalized groups.

  RELEVANCE: Demonstrates that fairness metrics implicitly embody contested philosophical commitments about distributive justice. Shows that the choice between metrics is not merely technical but reflects fundamental normative disagreements. This supports the claim that fairness operationalization faces inherent conceptual challenges due to the contested nature of fairness itself.

  POSITION: Philosophical unification showing that fairness metrics encode contested normative theories. Argues for making these commitments explicit rather than treating metric choice as purely technical.
  },
  keywords = {fairness-metrics, distributive-justice, normative-foundations, High}
}

@inproceedings{delobelle2022measuring,
  author = {Delobelle, Pieter and Tokpo, E. and Calders, T. and Berendt, Bettina},
  title = {Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics},
  year = {2022},
  pages = {1693--1706},
  doi = {10.18653/v1/2022.naacl-main.122},
  note = {
  CORE ARGUMENT: Systematic empirical evaluation showing that bias/fairness metrics for language models are not compatible with each other and highly depend on template choices, seed selection, and embedding choices. Finds no tangible evidence of intrinsic bias metrics relating to extrinsic bias in downstream tasks. Concludes that fairness evaluation remains challenging because measurement choices are inherently subjective.

  RELEVANCE: Provides empirical evidence that fairness metrics lack construct validity - they don't converge on measuring the same underlying construct. The finding that metric values are artifacts of arbitrary choices (templates, seeds, embeddings) rather than properties of what's being measured undermines their validity. This is exactly the kind of evidence needed to show operationalization is problematic.

  POSITION: Empirical demonstration that fairness metrics are "biased rulers" - they don't reliably measure what they claim to measure. Argues for focusing on downstream task fairness rather than intrinsic metrics.
  },
  keywords = {fairness-metrics, measurement-validity, empirical-evaluation, High}
}

@article{cao2022intrinsic,
  author = {Cao, Yang Trista and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul and Kumar, Varun and Dhamala, J. and Galstyan, A.},
  title = {On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations},
  journal = {ArXiv},
  year = {2022},
  volume = {abs/2203.13928},
  doi = {10.48550/arXiv.2203.13928},
  arxivId = {2203.13928},
  note = {
  CORE ARGUMENT: Extensive correlation study across 19 contextualized language models finds that intrinsic and extrinsic fairness metrics do not necessarily correlate, even when correcting for metric misalignments, evaluation dataset noise, and experimental configuration. This disconnect persists across different bias notions, suggesting intrinsic metrics may not measure what matters for downstream fairness.

  RELEVANCE: Provides strong empirical evidence for construct validity problems - if intrinsic fairness metrics don't predict extrinsic fairness, they may not be measuring fairness at all. This is a measurement validity failure: the operationalization (intrinsic metrics) doesn't track the construct of interest (actual fairness in applications). Critical for arguments about operationalization challenges.

  POSITION: Empirical evidence that common fairness metrics lack predictive validity. Challenges the assumption that measuring "bias" in models translates to measuring unfairness in applications.
  },
  keywords = {fairness-metrics, construct-validity, correlation-study, High}
}

@inproceedings{truong2025valid,
  author = {Truong, K. and Zimmermann, Annette and Heidari, Hoda},
  title = {Toward Valid Measurement Of (Un)fairness For Generative AI: A Proposal For Systematization Through The Lens Of Fair Equality of Chances},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2025},
  doi = {10.48550/arXiv.2507.04641},
  arxivId = {2507.04641},
  note = {
  CORE ARGUMENT: Addresses the neglected systematization of the unfairness construct in Generative AI fairness measurement. Proposes novel framework based on Fair Equality of Chances from political philosophy, decomposing unfairness into harm/benefit, morally arbitrary factors, and morally decisive factors. Argues that lack of proper construct systematization leads to invalid measurements that mischaracterize unfairness by overlooking contextual nuances.

  RELEVANCE: Most recent philosophical work directly addressing construct validity in fairness measurement. Shows that valid measurement requires prior systematization of the construct being measured. The framework demonstrates how political philosophy (Fair Equality of Chances) can guide operationalization, connecting to broader debates about contested concepts and formalization limits.

  POSITION: Argues that construct validity problems in fairness measurement stem from insufficient systematization. Proposes philosophical framework to guide valid operationalization.
  },
  keywords = {construct-validity, measurement-systematization, generative-AI, High}
}

@inproceedings{bean2025measuring,
  author = {Bean, Andrew M. and Kearns, R. and Romanou, Angelika and Hafner, Franziska Sofia and Mayne, Harry and Batzner, Jan and Foroutan, Negar and Schmitz, Chris and Korgul, Karolina and Batra, Hunar and Deb, Oishi and Beharry, Emma and Emde, Cornelius and Foster, Thomas and Gausen, Anna and Grandury, María and Han, Simeng and Hofmann, Valentin and Ibrahim, Lujain and Kim, Hazel and Kirk, Hannah Rose and Lin, Fangru and Liu, Gabrielle Kaili-May and Luettgau, Lennart and Magomere, Jabez and Rystrøm, Jonathan and Sotnikova, Anna and Yang, Yushi and Zhao, Yilun and Bibi, Adel and Bosselut, A. and Clark, Ronald and Cohan, Arman and Foerster, Jakob and Gal, Yarin and Hale, Scott A. and Raji, Inioluwa Deborah and Summerfield, Chris and Torr, Philip H. S. and Ududec, C. and Rocher, Luc and Mahdi, Adam},
  title = {Measuring what Matters: Construct Validity in Large Language Model Benchmarks},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2511.04703},
  doi = {10.48550/arXiv.2511.04703},
  arxivId = {2511.04703},
  note = {
  CORE ARGUMENT: Systematic review of 445 LLM benchmarks by 29 expert reviewers reveals patterns undermining construct validity across benchmarks for phenomena like "safety" and "robustness". Measuring complex abstract phenomena requires strong construct validity - having measures that represent what matters. Provides eight key recommendations and detailed guidance for developing valid LLM benchmarks.

  RELEVANCE: Most comprehensive recent examination of construct validity in AI evaluation. While focused on LLMs, the measurement validity principles apply directly to fairness metrics. The finding that benchmarks lack "clear articulations of what is being measured" parallels Blodgett's critique of fairness benchmarks. Essential for arguments about why operationalizing contested concepts is hard.

  POSITION: Large-scale empirical study showing that AI evaluation broadly suffers from construct validity problems. Emphasizes that measuring abstract phenomena requires careful attention to measurement theory.
  },
  keywords = {construct-validity, LLM-benchmarks, measurement-theory, High}
}

@inproceedings{smith2023scoping,
  author = {Smith, Jessie J. and Beattie, Lex and Cramer, H.},
  title = {Scoping Fairness Objectives and Identifying Fairness Metrics for Recommender Systems: The Practitioners' Perspective},
  booktitle = {Proceedings of the ACM Web Conference 2023},
  year = {2023},
  doi = {10.1145/3543507.3583204},
  note = {
  CORE ARGUMENT: Through literature review of 24 papers and 15 semi-structured practitioner interviews, reveals the complexity practitioners face in operationalizing fairness. The proliferation of fairness metrics creates a complex decision space where practitioners lack guidance on picking metrics appropriate to their context. Decision-tree framework helps scope fairness objectives, but highlights need for more decision-making support.

  RELEVANCE: Provides practitioner perspective on operationalization challenges. Shows that even when metrics are well-defined, practitioners struggle to determine which measure what matters in their context. This is evidence that construct validity problems are not just theoretical - they create real obstacles to deploying fairness auditing in practice.

  POSITION: Practitioner-focused work showing that metric proliferation creates operationalization challenges. Need for decision support suggests metrics alone don't solve fairness problems.
  },
  keywords = {fairness-metrics, practitioner-challenges, operationalization, Medium}
}

@inproceedings{saha2019measuring,
  author = {Saha, Debjani and Schumann, Candice and McElfresh, Duncan C. and Dickerson, John P. and Mazurek, Michelle L. and Tschantz, Michael Carl},
  title = {Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  year = {2019},
  pages = {8377--8387},
  note = {
  CORE ARGUMENT: Empirical study of how non-experts comprehend fairness metrics reveals significant misunderstandings. Even when metrics are mathematically well-defined, stakeholders may misinterpret what they measure. This comprehension gap undermines the ability of fairness metrics to support informed decision-making about system fairness.

  RELEVANCE: Shows that construct validity problems extend beyond metric design to metric interpretation. Even if a metric validly measures some aspect of fairness, stakeholders may not understand what it measures. This is another operationalization challenge - the gap between formal definitions and stakeholder understanding.

  POSITION: Empirical evidence that fairness metrics face interpretation challenges beyond technical definition. Comprehension gaps undermine their practical utility.
  },
  keywords = {fairness-metrics, comprehension, stakeholder-understanding, Medium}
}

@inproceedings{constantin2022algorithmic,
  author = {Constantin, Rares and Dück, Moritz and Alexandrov, Anton and Matošević, Patrik and Keidar, Daphna and El-Assady, Mennatallah},
  title = {How Do Algorithmic Fairness Metrics Align with Human Judgement? A Mixed-Initiative System for Contextualized Fairness Assessment},
  booktitle = {Proceedings of the 2022 IEEE Workshop on TRust and EXpertise in Visual Analytics (TREX)},
  year = {2022},
  pages = {1--7},
  doi = {10.1109/TREX57753.2022.00005},
  note = {
  CORE ARGUMENT: Develops mixed-initiative system (FairAlign) where laypeople assess fairness through interactive visualizations, then compares their judgments with automated metrics. Finds significant gaps between algorithmic metrics and human fairness judgments. Cultural and perceptual biases mean metrics often fail to capture what people perceive as fair or unfair.

  RELEVANCE: Provides empirical evidence that fairness metrics don't align with human judgments - a fundamental construct validity failure. If metrics don't capture human notions of fairness, they're not measuring what matters. This supports the argument that operationalization inevitably loses something important from the normative concept.

  POSITION: Empirical work showing misalignment between algorithmic metrics and human fairness judgments. Argues for incorporating human judgment in fairness assessment.
  },
  keywords = {fairness-metrics, human-judgment, construct-validity, Medium}
}

@article{amigo2023unifying,
  author = {Amigó, Enrique and Deldjoo, Yashar and Mizzaro, Stefano and Bellogín, Alejandro},
  title = {A unifying and general account of fairness measurement in recommender systems},
  journal = {Information Processing \& Management},
  year = {2023},
  volume = {60},
  pages = {103115},
  doi = {10.1016/j.ipm.2022.103115},
  note = {
  CORE ARGUMENT: Proposes general framework for fairness measurement in recommender systems that unifies diverse fairness notions. Shows that many fairness metrics share common mathematical structure but measure different aspects of fairness. Framework helps clarify what each metric measures and when it's appropriate.

  RELEVANCE: Demonstrates that metric diversity partly reflects the multi-faceted nature of fairness - different metrics measure legitimately different things. But this raises a construct validity question: if fairness metrics measure different things, which (if any) measure "fairness" itself? Supports the view that fairness is a contested concept resisting single operationalization.

  POSITION: Unifying framework revealing that metric diversity reflects fairness's conceptual complexity. Helps clarify what different metrics measure.
  },
  keywords = {fairness-metrics, recommender-systems, unifying-framework, Medium}
}

@article{mehrotra2022revisiting,
  author = {Mehrotra, Anay and Sachs, Jeffery and Celis, L. E.},
  title = {Revisiting Group Fairness Metrics: The Effect of Networks},
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  year = {2022},
  volume = {6},
  pages = {1--29},
  doi = {10.1145/3555100},
  note = {
  CORE ARGUMENT: Shows that standard group fairness metrics fail to account for network structure, leading to spurious fairness evaluations. Ignoring networks can miss imbalances in influence and reach or overlook interaction biases. Proposes network-aware fairness metrics that measure access distribution and inter-group interaction patterns.

  RELEVANCE: Demonstrates that context matters for valid measurement - what counts as a valid fairness measure depends on whether network effects are relevant. This supports the view that fairness operationalization requires contextual sensitivity that resists universal formalization. Standard metrics have poor construct validity when networks matter.

  POSITION: Shows that fairness measurement must account for relevant contextual factors like network structure. Standard metrics lack construct validity in networked settings.
  },
  keywords = {fairness-metrics, network-effects, context-dependence, Medium}
}

@inproceedings{sahlgren2024action,
  author = {Sahlgren, Otto},
  title = {Action-guidance and AI ethics: the case of fair machine learning},
  journal = {AI and Ethics},
  year = {2024},
  volume = {5},
  pages = {1019--1031},
  doi = {10.1007/s43681-024-00437-2},
  note = {
  CORE ARGUMENT: Identifies "action-guidance gap" in AI ethics where fairness principles fail to guide practice due to ambiguous operationalization and implementation obstacles. Fair ML frameworks and toolkits prove hard to apply because of unclear operationalization of principles like fairness, equality, and justice, plus practical barriers like lack of evaluation data.

  RELEVANCE: Connects operationalization challenges to practical action-guidance problems. Shows that construct validity issues (ambiguous operationalization) directly undermine the ability of fairness principles to guide development. This is evidence that operationalization problems are not merely academic - they prevent fairness from being implemented.

  POSITION: Identifies operationalization ambiguity as key barrier to implementing fairness in practice. Argues for philosophical work on action-guidance.
  },
  keywords = {action-guidance, operationalization, implementation-challenges, Medium}
}

@inproceedings{xiao2023evaluating,
  author = {Xiao, Ziang and Zhang, Susu and Lai, Vivian and Liao, Q.},
  title = {Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  year = {2023},
  pages = {10967--10982},
  doi = {10.18653/v1/2023.emnlp-main.676},
  arxivId = {2305.14889},
  note = {
  CORE ARGUMENT: Proposes MetricEval framework informed by measurement theory from educational test design for evaluating NLG evaluation metrics. Formalizes sources of measurement error and offers statistical tools for assessing reliability and validity based on empirical data. Allows quantifying uncertainty in metrics to better interpret results.

  RELEVANCE: Demonstrates systematic application of measurement theory to evaluate metrics themselves - meta-evaluation. Shows how to empirically assess whether metrics are valid and reliable. The framework is broadly applicable beyond NLG to fairness metrics. Provides concrete methods for testing construct validity claims.

  POSITION: Applies educational measurement theory to develop systematic framework for metric evaluation. Emphasizes reliability and validity as fundamental metric properties.
  },
  keywords = {measurement-theory, metric-evaluation, validity-assessment, Medium}
}

@article{buijsman2023navigating,
  author = {Buijsman, Stefan},
  title = {Navigating fairness measures and trade-offs},
  journal = {AI and Ethics},
  year = {2023},
  volume = {4},
  pages = {1323--1334},
  doi = {10.1007/s43681-023-00318-0},
  arxivId = {2307.08484},
  note = {
  CORE ARGUMENT: Uses Rawls's justice as fairness to create principled basis for navigating fairness measures and accuracy trade-offs. Shows that mathematical impossibility of optimizing all fairness metrics simultaneously creates need for substantive normative theory to guide metric choice. Proposes focusing on most vulnerable groups and metrics with biggest impact on them.

  RELEVANCE: Addresses the problem that metric incompatibility requires normative theory to guide operationalization choices. Shows that technical considerations alone cannot determine which metrics to use - need philosophical framework. Demonstrates why operationalizing fairness requires engagement with contested normative theories like Rawlsian justice.

  POSITION: Argues that metric incompatibility necessitates normative philosophical theory to guide choices. Proposes Rawlsian framework as substantive solution.
  },
  keywords = {fairness-metrics, metric-tradeoffs, Rawlsian-justice, Medium}
}

@article{benbouzid2023fairness,
  author = {Benbouzid, Bilel},
  title = {Fairness in machine learning from the perspective of sociology of statistics: How machine learning is becoming scientific by turning its back on metrological realism},
  booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2023},
  doi = {10.1145/3593013.3593974},
  note = {
  CORE ARGUMENT: From sociology of statistics perspective, examines how FairML researchers confront dual demands: reliable computational techniques and transparency about politically situated quantification. Shows FairML tends toward "trained judgment" objectivity - reasonably partial justification from designer that is itself politically situated. This represents shift away from metrological realism.

  RELEVANCE: Provides sociological perspective on the politics of fairness measurement. Shows that fairness operationalization is inherently political - involves "politically situated quantification" that cannot be fully formalized. This supports the view that contested concepts resist neutral operationalization because operationalization itself involves normative choices.

  POSITION: Sociological analysis showing fairness measurement is politically situated. Argues against metrological realism in fairness measurement.
  },
  keywords = {sociology-of-statistics, metrological-realism, politics-of-measurement, Low}
}

@inproceedings{andrus2019towards,
  author = {Andrus, McKane and Gilbert, T.},
  title = {Towards a Just Theory of Measurement: A Principled Social Measurement Assurance Program for Machine Learning},
  booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2019},
  doi = {10.1145/3306618.3314275},
  note = {
  CORE ARGUMENT: Proposes social measurement assurance program (sMAP) where ML serves as quality assurance for institutional measurement practices by revealing when measures fail to capture intended constructs. Rather than codifying fairness into ML directly, use ML to expose epistemic fault lines in existing measurement. Applies Rawlsian principles to develop provisional "just theory of measurement."

  RELEVANCE: Offers alternative view of ML fairness as institutional measurement auditing rather than individual decision fairness. Emphasizes that measurement validity is social and political, not merely technical. The sMAP framework shows how measurement failures connect to fairness harms, linking measurement theory to justice theory.

  POSITION: Proposes ML as tool for auditing institutional measurement practices. Connects measurement validity to Rawlsian justice framework.
  },
  keywords = {measurement-theory, social-measurement, Rawlsian-justice, Low}
}
