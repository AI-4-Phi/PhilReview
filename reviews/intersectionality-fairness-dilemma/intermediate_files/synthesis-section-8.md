## Conclusion

The literature reviewed here reveals a fundamental gap in how the algorithmic fairness community conceptualizes the intersectionality problem. Technical ML research has developed sophisticated statistical solutions—multicalibration frameworks that handle exponentially many overlapping groups (Hébert-Johnson et al. 2018), hierarchical models addressing sparse data (Ferrara et al. 2025), and rigorous hypothesis testing procedures for fairness auditing (Cherian and Candès 2023). Meanwhile, philosophical analysis has generated rich debates about social group constitution, with practice-based accounts demonstrating that groups cannot be algorithmically derived from attributes (Haslanger 2012; Mallon 2016; Ásta 2018; Ritchie 2018), measurement theorists revealing systematic construct validity failures in fairness metrics (Tal 2023; Blodgett et al. 2021), and normative theorists showing that different ethical frameworks yield incompatible answers about which groups deserve protection (Shields 2016; Anderson 1999; Hertweck et al. 2024). Yet these sophisticated analyses remain largely disconnected, treating statistical uncertainty and ontological uncertainty as independent optimization challenges rather than interacting constraints.

The intersectionality dilemma emerges precisely from this interaction. Expanding fairness coverage to address ontological adequacy—ensuring all morally relevant intersectional groups are audited—necessarily reduces per-group sample sizes, undermining the statistical reliability that justifies treating algorithmic fairness metrics as authoritative. Conversely, constraining coverage to ensure statistical reliability requires determining which groups genuinely warrant auditing, but philosophical debates reveal this question has no neutral resolution. Practice-based social ontologies show that social groups are constituted through conferral, structural positioning, and social practices that resist algorithmic specification (Ásta 2018; Ritchie 2018). Normative pluralism demonstrates that prioritarian, sufficientarian, and egalitarian frameworks generate systematically different answers about group prioritization (Shields 2016; Arneson 2000). Even measurement theory reveals that fairness—as a contested normative concept—may fundamentally resist valid operationalization (Long 2020; Jacobs et al. 2020).

Kong (2022) comes closest to recognizing this interaction, identifying a "dilemma between infinite regress and fairness gerrymandering": either continuously split groups into smaller subgroups, creating statistical chaos, or arbitrarily select protected groups, enabling strategic manipulation. However, even Kong's analysis does not fully theorize why these horns interact rather than representing separate challenges. The answer lies in epistemic justice: data sparsity for marginalized groups constitutes not merely missing information but systematic undermining of their epistemic standing (Fricker 2007; Symons and Alvarado 2022). Sample size requirements for statistical reliability can perpetuate hermeneutical injustice by excluding groups from the collective interpretive resources embedded in ML systems. Conversely, lowering reliability standards to include more groups risks producing fairness audits that cannot justify their own conclusions, potentially legitimizing rather than challenging discriminatory systems.

This reveals why both technical optimization and critical rejection represent inadequate responses. The multicalibration literature demonstrates that calibration across rich subgroup collections is computationally tractable (Hébert-Johnson et al. 2018; Gopalan et al. 2022)—but only when the class of group-defining functions is given. No amount of algorithmic sophistication can resolve the ontological question of which functions to include. Critical scholars argue that formalization itself is limited, pointing toward "substantive fairness" that attends to structural context (Green 2022) or participatory approaches that center affected communities (Wachter et al. 2021). Yet critics offer limited concrete guidance on how to conduct fairness assessment when formalization is inadequate. If fairness cannot be operationalized through formal metrics, what alternative accountability mechanisms exist?

The research contribution lies in explicitly framing the statistical-ontological interaction as a genuine dilemma with intractable horns, not merely two independent engineering challenges. This framing explains several otherwise puzzling features of the fairness landscape. First, why impossibility results persist despite technical advances: mathematical incompatibilities between fairness metrics (Kleinberg et al. 2017) may reflect deeper normative disagreements about distributive justice frameworks (Baumann et al. 2022), not merely competing statistical objectives. Second, why measurement validity problems remain chronic: if group specification is contested, then construct validity failures in fairness metrics (Blodgett et al. 2021) are not fixable through better conceptualization because the underlying concepts are irreducibly contested. Third, why fairness gerrymandering is endemic: without principled methods for determining which groups exist, strategic group selection remains possible regardless of metric sophistication (Kearns et al. 2018; Tian et al. 2024).

Recognizing the dilemma's structure has concrete implications for fairness practice. First, fairness auditing may require accepting fundamental trade-offs rather than seeking optimal solutions. Expanding intersectional coverage necessarily reduces per-group reliability; there is no statistical method that escapes this constraint under fixed data budgets (Singh and Chunara 2023). Second, participatory approaches to group specification may be necessary but insufficient. Even when affected communities specify which groups matter, statistical feasibility constraints remain. Conversely, technical solutions to sparse data cannot determine which groups warrant the computational investment. Third, institutional rather than purely algorithmic responses may be required. If fairness auditing faces structural limits, accountability mechanisms must look beyond automated metrics to include qualitative assessment, participatory governance, and transparency about metrics' normative assumptions.

The critical perspectives reviewed here suggest the dilemma may be irresolvable within current fairness frameworks. Structural injustice critiques argue that distributive justice frameworks underpinning fairness metrics cannot address oppressive social structures (Kasirzadeh 2022). Black feminist scholars question whether fairness discourse itself is adequate for addressing algorithmic oppression, pointing toward abolitionist alternatives (Hampton 2021). These critiques identify an important possibility: that formalization's limits are not merely technical but reflect deeper incompatibility between quantitative measurement and the normative complexity of justice. However, purely diagnostic critiques leave practitioners without guidance. The intersectionality dilemma framework provides principled analysis of formalization's limits while acknowledging that alternatives remain underspecified.

This literature review positions the research project to make three contributions. First, theoretically, it offers the first systematic analysis of statistical-ontological interaction as constituting a genuine dilemma where addressing one horn exacerbates the other. Second, methodologically, it demonstrates why existing approaches—technical optimization, conceptual analysis, participatory design—are individually insufficient, suggesting hybrid approaches may be needed. Third, practically, it clarifies what fairness auditing can and cannot achieve: automated metrics may provide partial, context-dependent fairness assessment but cannot substitute for substantive political deliberation about which groups matter and why. The dilemma is not an argument for abandoning fairness auditing but for approaching it with appropriate epistemic humility about the limits of formalization and the unavoidability of normative judgment.
