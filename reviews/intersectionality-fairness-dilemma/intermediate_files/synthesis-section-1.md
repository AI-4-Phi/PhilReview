## Introduction

Intersectional fairness auditing in algorithmic systems presents a deceptively simple goal: ensure equitable treatment across groups defined by multiple, overlapping social categories. Yet beneath this straightforward objective lies a fundamental dilemma that has received insufficient recognition in both technical and philosophical literatures. Achieving intersectional fairness requires two distinct capabilities: first, estimating algorithmic performance across demographic groups with statistical reliability; second, specifying which groups warrant consideration in the first place. The first challenge confronts statistical uncertainty arising from sparse data on small intersectional subgroups. The second confronts ontological uncertainty about how social groups are constituted and which categories deserve protection. We argue that these are not independent optimization problems but interacting constraints that form a genuine dilemma.

Crenshaw's (1989) foundational analysis demonstrated how antidiscrimination law systematically failed Black women by treating race and sex as mutually exclusive categories, unable to recognize discrimination targeting their intersection. This legal insight has motivated extensive technical work in machine learning fairness. Recent algorithmic approaches have achieved remarkable sophistication: multicalibration methods can ensure calibrated predictions across exponentially many subgroups without explicit enumeration (Hébert-Johnson et al. 2018), while statistical frameworks provide rigorous hypothesis testing for fairness auditing across rich subgroup collections (Cherian and Candès 2023). Yet these technical solutions share a critical assumption: they take the set of relevant groups as given. Multicalibration requires pre-specifying which group-defining functions to include; differential fairness assumes protected attributes are determined in advance; auditing frameworks test disparities across predetermined subgroups.

The philosophical literature on social ontology reveals why group specification cannot be treated as a simple input to fairness algorithms. Haslanger (2012) argues that race and gender are not attribute-based categories but social positions constituted through systematic relations of subordination and privilege within hierarchical structures. On this political constructionist view, group membership depends fundamentally on structural positioning and social practices, not on possessing observable demographic features. Similarly, Ásta (2018) develops a conferralist framework showing that social properties are constituted through communal meaning-making rather than intrinsic attributes—the same constellation of features can ground different social properties in different contexts. If such practice-based accounts are correct, algorithmic systems cannot derive relevant groups by mechanically combining demographic attributes, because what makes a collection of people a morally salient social group is not reducible to their shared features but involves their positioning within contested social structures.

The interaction between statistical and ontological uncertainty creates the core dilemma. Expanding intersectional coverage to address ontological adequacy—ensuring all relevant group intersections are monitored—necessarily reduces per-group sample sizes, undermining the statistical reliability required for valid fairness measurement. Conversely, constraining coverage to maintain statistical rigor requires determining which groups matter most, a question that ontological debates reveal to be fundamentally contested. Kong (2022) identifies this tension as a "dilemma between infinite regress and fairness gerrymandering": either continuously split groups into finer intersectional categories (facing ever-smaller samples), or arbitrarily select which groups to protect (enabling strategic manipulation of fairness metrics through selective group specification). Yet even Kong's analysis treats statistical sparsity and group selection as conceptually separate problems rather than recognizing their structural interaction.

This review synthesizes literatures that have largely developed in parallel. Section 1 examines technical approaches to intersectional fairness, showing how multicalibration frameworks, sparse data methods, and fairness gerrymandering research have advanced the field while leaving the ontological dimension unaddressed. Section 2 analyzes philosophical debates about intersectionality and social group constitution, revealing deep disagreement between analytical/causal and hermeneutic/emergent interpretations that resist algorithmic resolution. Section 3 investigates measurement validity challenges, demonstrating that fairness metrics face construct validity problems even when groups are specified. Section 4 explores normative pluralism, showing how different ethical frameworks yield incompatible answers about which groups warrant protection. Section 5 synthesizes these strands through the lens of epistemic justice, arguing that the statistical-ontological interaction constitutes a dilemma where addressing one horn exacerbates the other.

Our central claim is that existing work has failed to recognize this interaction as constituting a genuine dilemma. Technical research treats sparse data as an engineering challenge solvable through better methods; philosophical work analyzes group constitution without engaging statistical feasibility constraints; even philosophical-technical bridges identify both dimensions but do not theorize their interaction (Himmelreich et al. 2024; Gohar and Cheng 2023). By framing the statistical-ontological interaction as a dilemma with intractable horns, we provide a principled account of why both technical optimization and purely critical rejection are inadequate responses. The intersectionality problem for algorithmic fairness may be not a matter of insufficient sophistication but a structural feature of attempting to formalize contested normative concepts across indeterminate social categories.
