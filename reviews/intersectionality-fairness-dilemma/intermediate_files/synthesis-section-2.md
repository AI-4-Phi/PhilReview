## Section 1: Technical Approaches to Intersectional Fairness and Their Limits

The machine learning community has developed sophisticated technical solutions to the challenge of ensuring fairness across intersectional groups. These approaches—particularly multicalibration frameworks, differential fairness methods, and statistical auditing techniques—demonstrate remarkable computational and statistical sophistication in handling exponentially many overlapping demographic categories. However, their sophistication reveals a fundamental limitation: all assume that the relevant groups are specified in advance. This section establishes that even the most advanced technical solutions treat group specification as an input, not as a contested normative-ontological question, thereby leaving half of the intersectionality dilemma unaddressed.

### Subsection 1.1: Multicalibration and Rich Subgroup Frameworks

The multicalibration framework represents the most technically sophisticated approach to intersectional fairness in the algorithmic fairness literature. Introduced by Hébert-Johnson et al. (2018), multicalibration requires that a predictor be calibrated not only overall but simultaneously across a rich collection of overlapping subgroups. Unlike traditional calibration, which ensures that predicted probabilities match observed frequencies in aggregate, multicalibration demands this property hold for every subgroup defined by an efficiently computable function. This directly addresses the intersectional fairness challenge: predictions should be calibrated not just for "women" or "Black individuals" separately, but for all intersections like "Black women," "elderly Black women," and so on.

The key technical innovation is demonstrating that achieving multicalibration is computationally tractable despite the exponential explosion of potential subgroups. Hébert-Johnson et al. (2018) provide efficient algorithms that achieve multicalibration without explicitly enumerating all subgroups. This solves what appears to be an insurmountable combinatorial problem: with $k$ binary attributes, there are $2^k$ possible intersections. The multicalibration algorithm sidesteps exhaustive enumeration by identifying subgroups where calibration fails and iteratively correcting predictions for those groups. This algorithmic elegance demonstrates that the statistical side of intersectional fairness—ensuring fairness across exponentially many groups—is not computationally intractable.

Recent extensions have refined the framework to address statistical feasibility. Gopalan et al. (2022) introduce low-degree multicalibration as a middle ground between weak multiaccuracy and strong multicalibration. Their central observation is that achieving full multicalibration requires exponentially growing data, but many desirable fairness properties manifest as low-degree properties requiring only polynomial sample complexity. This addresses the fundamental tension between intersectional coverage and statistical power: as the number of protected groups increases, each individual group becomes smaller, reducing the precision of fairness estimates. Low-degree multicalibration achieves key fairness guarantees while managing this sample complexity explosion.

The framework continues to evolve. Lee et al. (2025) extend multicalibration beyond categorical attributes to handle continuous and mixed-type protected characteristics through distance covariance regularization. This addresses a limitation of earlier approaches that assumed discrete demographic categories—a significant advancement given that many socially relevant attributes (age, income, disability severity) are continuous. Their extension demonstrates ongoing refinement of technical approaches to intersectional complexity.

Yet these sophisticated solutions share a critical assumption: the class of group-defining functions must be specified in advance. Multicalibration requires choosing which functions over attributes define protected groups, but provides no principled method for making this choice. Hébert-Johnson et al. (2018) note that their framework applies to any efficiently computable class of functions, but this flexibility reveals rather than resolves the problem: which functions should be included? The technical machinery assumes this question has been answered, treating it as an input rather than a problem to solve. This is the first glimpse of the ontological dimension that technical approaches cannot address.

### Subsection 1.2: Sparse Data and Statistical Reliability

While multicalibration demonstrates computational tractability, statistical reliability for small intersectional groups remains a fundamental challenge. Recent work explicitly addresses sparse data problems, developing methods to handle the long tail of small subgroups that inevitably emerge when multiple demographic attributes intersect.

Ferrara et al. (2025) propose size-adaptive hypothesis testing that adjusts statistical methodology based on group size. For large groups, they employ standard Central Limit Theorem-based inference; for small groups where asymptotic approximations fail, they use Bayesian Dirichlet-multinomial estimators. This adaptive approach acknowledges a fundamental reality: standard statistical methods that rely on large-sample asymptotics simply fail for small intersectional groups. The solution is methodological pluralism—different statistical techniques for different group sizes—rather than pretending a single approach works universally.

Cherian and Candès (2023) develop a comprehensive statistical inference framework for fairness auditing across rich subgroup collections. Their approach uses multiple hypothesis testing with bootstrap methods to control false discovery rates when simultaneously testing fairness across many groups. This addresses a critical problem: when evaluating thousands of potential intersectional groups, some will appear unfair by chance alone. Rigorous auditing requires careful statistical control to distinguish genuine disparities from random variation. Their framework demonstrates that statistically valid intersectional fairness evaluation is possible, but demands sophisticated multiple testing corrections that account for dependencies between overlapping groups.

However, these statistical solutions cannot eliminate a fundamental trade-off. Zhioua et al. (2025) distinguish two types of sampling bias in fairness evaluation: sample size bias (insufficient observations) and underrepresentation bias (systematic undersampling of certain groups). They show these biases compound for marginalized groups—precisely those groups for which fairness evaluation is most critical. Even with sophisticated statistical methods, expanding intersectional coverage necessarily fragments the dataset into smaller groups, reducing per-group statistical power.

Jourdan et al. (2023) provide empirical evidence of this reliability problem, demonstrating that fairness metrics yield unreliable and diverging results with small samples. They show that conclusions about algorithmic fairness can reverse depending on which metric is used when sample sizes are small—the very situation that prevails for intersectional groups. Similarly, Singh and Chunara (2023) analyze sample size requirements for disparity estimation, revealing that detecting fairness violations requires larger samples than often assumed. Under fixed data budgets, there is an unavoidable trade-off: more intersectional coverage means less precision per group.

This statistical literature establishes that data sparsity is not merely an engineering inconvenience to be solved with better methods. Even with sophisticated statistical techniques—adaptive testing, multiple comparison corrections, Bayesian estimators—expanding the set of protected groups necessarily reduces the reliability of per-group fairness assessments. Statistical rigor cannot eliminate this trade-off; it can only make it explicit and manage it systematically.

### Subsection 1.3: Fairness Gerrymandering and the Group Specification Problem

The fairness gerrymandering literature reveals that the ontological question of which groups to protect has measurable technical consequences, not merely philosophical ones. Fairness gerrymandering occurs when a classifier appears fair according to predefined demographic categories but violates fairness on intersectional subgroups not explicitly evaluated. This demonstrates that group specification choices directly determine whether algorithms are judged fair or unfair.

Kong (2022) provides the most explicit philosophical analysis of this problem in the machine learning literature. She argues that the dominant statistical parity approach to intersectional fairness faces a "dilemma between infinite regress and fairness gerrymandering." Either we continuously split protected groups into ever-finer intersectional subgroups—leading to infinite regress and statistical impossibility as groups become arbitrarily small—or we arbitrarily select which groups to protect, enabling strategic manipulation through gerrymandering. Kong frames this as a fundamental conceptual problem rather than merely a technical optimization challenge, explicitly connecting it to questions about oppression and which groups warrant protection.

Tian et al. (2024) demonstrate this problem empirically with their MultiFair system. They show that algorithms achieving per-attribute fairness (fair for "gender" and "race" separately) systematically fail to achieve fairness for intersectional groups (Black women, Asian men). This is not a theoretical possibility but an observed pattern: naive approaches to multi-attribute fairness leave intersections unprotected. The solution they propose—explicit intersectional modeling—requires pre-specifying which intersections matter, circling back to the group specification problem.

Kearns et al. (2018) prove that preventing fairness gerrymandering requires ensuring fairness across exponentially many subgroups, a problem computationally equivalent to agnostic learning—widely considered intractable for rich hypothesis classes. This establishes a computational complexity bound: if we want to guarantee fairness against all possible ways of gerrymandering through group selection, we face computational barriers that cannot be overcome with better algorithms. The multicalibration framework addresses this by restricting attention to efficiently computable functions, but this restriction is precisely what enables potential gerrymandering through strategic choice of the function class.

Räz (2023) extends the gerrymandering critique beyond group fairness to individual fairness, showing the problem is fundamental rather than specific to group-based approaches. Individual fairness requires treating similar individuals similarly, but Räz proves this can be gerrymandered through strategic choices about the feature space and similarity metric. The manipulation is structural: fairness metrics can be satisfied or violated through choices about problem representation that precede algorithmic design.

These results reveal that fairness gerrymandering is not a bug to be fixed but a consequence of ontological uncertainty. Without a principled method for determining which groups exist or which similarity metrics are appropriate, fairness metrics can always be gamed through strategic selection. The technical literature has identified this problem clearly—Kong's (2022) "dilemma" language is explicit about its fundamental nature—but cannot solve it within technical frameworks because it arises from contested normative-ontological questions about group constitution.

### Subsection 1.4: Recent Philosophical-Technical Syntheses

Recent work has begun to bridge technical machine learning and philosophical analysis, explicitly recognizing that intersectional fairness involves both statistical and normative dimensions. Himmelreich et al. (2024) provide the most direct statement of this dual nature, distinguishing statistical challenges (small intersectional groups) from moral-methodological challenges (which groups matter). They develop desiderata for intersectional fairness solutions and propose hypothesis testing approaches, demonstrating tradeoffs between statistical power and intersectional coverage. Their framework acknowledges that the intersectionality problem cannot be solved through statistical sophistication alone; it requires normative clarity about which groups warrant consideration.

Gohar and Cheng (2023) survey the intersectional fairness landscape, identifying data scarcity as a key challenge alongside computational complexity and metric selection. Their comprehensive review taxonomizes different technical approaches—multicalibration, differential fairness, subgroup fairness—while acknowledging that all face the sparse data problem when applied to intersectional groups. The survey format allows them to map the solution space while highlighting persistent challenges.

However, even these philosophical-technical syntheses treat statistical and ontological dimensions as separate optimization problems rather than as interacting constraints that form a dilemma. Himmelreich et al. (2024) identify both dimensions but propose solutions addressing each independently: better statistical methods for sparse data and clearer normative frameworks for group selection. What remains untheorized is their interaction: expanding groups for normative adequacy exacerbates statistical unreliability; constraining groups for statistical reliability requires resolving contested ontological questions that admit no consensus solution.

The technical literature has achieved remarkable sophistication in addressing computational and statistical challenges. Multicalibration frameworks prove that calibration across exponentially many groups is tractable. Sparse data methods provide statistical rigor for small intersectional groups. Fairness gerrymandering research demonstrates that strategic group selection undermines metrics. Yet all these advances presuppose that group specifications are inputs to the fairness problem. The ontological question—which groups exist, which groups matter, how to specify them—remains unaddressed because computational methods cannot resolve normative-ontological uncertainty. This sets up the second horn of the dilemma, explored in the next section: philosophical debates reveal that group constitution itself is contested in ways that resist algorithmic formalization.
