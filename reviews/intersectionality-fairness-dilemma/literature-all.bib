@comment{
====================================================================
DOMAIN: Algorithmic Fairness and Intersectionality (ML/CS Literature)
SEARCH_DATE: 2026-01-10
PAPERS_FOUND: 20 total (High: 10, Medium: 7, Low: 3)
SEARCH_SOURCES: Semantic Scholar, OpenAlex
====================================================================

DOMAIN_OVERVIEW:

The technical ML/CS literature on intersectional algorithmic fairness emerged in the late 2010s, primarily addressing the statistical challenge of ensuring equitable predictions across multiple overlapping demographic groups. The field has developed around several key technical approaches: (1) multicalibration methods that ensure calibrated predictions across rich collections of subgroups (Hébert-Johnson et al. 2018), (2) differential fairness frameworks inspired by differential privacy (Foulds et al. 2020), (3) subgroup fairness methods that prevent worst-case disparities (Kearns et al. 2018), and (4) recent work on fairness gerrymandering that highlights how strategic group selection can manipulate fairness metrics (Kong 2022, Tian et al. 2024).

A central challenge identified across this literature is the statistical difficulty of measuring fairness for small intersectional groups. Papers explicitly address sparse data problems through various technical solutions: importance weighting, hierarchical models, tensor factorization, and hypothesis testing approaches (Ferrara et al. 2025). The multicalibration literature offers the most developed technical framework for handling exponentially many overlapping groups, with efficient algorithms that avoid explicit enumeration (Gopalan et al. 2022).

Recent work has begun to identify the interaction between statistical and ontological problems. Kong (2022) provides a philosophical analysis arguing that the dominant statistical parity approach faces a "dilemma between infinite regress and fairness gerrymandering" - either continuously splitting groups into smaller subgroups (statistical problem) or arbitrarily selecting protected groups (ontological problem). This connects directly to the research proposal's core thesis. The fairness auditing literature (Cherian and Candès 2023) further explores how to conduct statistically rigorous fairness evaluations across rich collections of subgroups.

RELEVANCE_TO_PROJECT:

This domain provides the technical foundation for understanding how the fairness ML community conceptualizes and attempts to solve intersectional fairness problems. The multicalibration and subgroup fairness literature demonstrates sophisticated awareness of statistical challenges (exponentially many groups, sparse data) but largely treats group specification as given. Kong (2022) and recent work on fairness gerrymandering explicitly identify the ontological dimension, providing crucial support for the paper's argument that these two problems interact rather than being separately solvable.

NOTABLE_GAPS:

Few papers explicitly theorize the interaction between statistical and ontological uncertainty. Most multicalibration work assumes a fixed class of group membership functions without questioning their normative justification. The literature on sparse data solutions focuses on technical fixes rather than fundamental limits. There is minimal engagement with social ontology or philosophical questions about group construction, despite Kong's (2022) philosophical intervention.

SYNTHESIS_GUIDANCE:

When synthesizing, emphasize the sophistication of technical solutions (multicalibration, differential fairness) while highlighting how even these advanced methods cannot escape the group specification problem. Kong (2022) should be positioned as a key bridge between technical and philosophical literatures. The fairness gerrymandering papers demonstrate that the ontological problem is not merely philosophical but has measurable technical consequences.

KEY_POSITIONS:
- Multicalibration approaches: 5 papers - Calibrated predictions across rich subgroup collections
- Differential fairness / intersectional metrics: 3 papers - Statistical parity across intersectional groups
- Fairness gerrymandering critique: 3 papers - Highlighting manipulation through group selection
- Sparse data solutions: 4 papers - Technical approaches to small group challenges
- Survey/overview: 3 papers - Comprehensive reviews of fairness landscape
- Auditing methods: 2 papers - Statistical inference for fairness evaluation
====================================================================
}

@comment{
====================================================================
DOMAIN: Philosophy of Intersectionality
SEARCH_DATE: 2026-01-10
PAPERS_FOUND: 18 total (High: 12, Medium: 6, Low: 0)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex
====================================================================

DOMAIN_OVERVIEW:
This domain addresses the philosophical foundations of intersectionality theory, focusing on competing interpretations of how social categories combine to produce distinctive forms of discrimination and privilege. The central debate concerns whether intersectionality should be understood through analytical/causal frameworks (treating categories as variables with causal powers) or hermeneutic/interpretive frameworks (treating categories as mutually constituted meaning structures). A key metaphysical question is whether intersectional experiences emerge as novel properties from the conjunction of categories, or can be reduced to combinations of independent category effects.

Recent work (2018-2025) has seen significant developments in three areas: (1) metaphysical accounts proposing emergence frameworks (Jorba & López de Sa 2024, Bernstein 2020), (2) causal modeling approaches from philosophy of science (Bright et al. excluded per user request, O'Connor et al. 2019), and (3) critiques of mutual constitution metaphors in favor of properties frameworks (Jorba & Rodó-Zárate 2019-2020). Critical realist approaches (Gunnarsson 2020) attempt to dialectically preserve both category distinctness and unity, while regulative ideal accounts (Gasdaglis & Madva 2020) sidestep metaphysical commitments entirely.

The literature reveals a tension between analytical precision (favoring causal/reductive approaches) and fidelity to lived experience (favoring hermeneutic/emergent approaches). This tension maps onto debates about social construction: whether intersectional categories are natural kinds with stable causal powers, or historically contingent classifications whose significance varies across contexts.

RELEVANCE_TO_PROJECT:
These philosophical debates ground the "ontological uncertainty" central to the algorithmic fairness dilemma. If intersectional categories are emergent (requiring novel variables for Black women distinct from race+gender), algorithms face epistemic limits on which categories exist. If categories are reductive, the challenge is computational (combinatorial explosion). The hermeneutic tradition suggests category salience is context-dependent, undermining stable fairness metrics.

NOTABLE_GAPS:
Limited engagement with computational/algorithmic implications of different metaphysical positions. Few papers address how ontological commitments about intersectionality translate into operationalizable fairness criteria. The hermeneutic tradition remains undertheorized in analytic philosophy compared to the causal/metaphysical literature.

SYNTHESIS_GUIDANCE:
Emphasize the analytical vs. hermeneutic divide as structuring the field. Focus on emergence vs. reduction as the key metaphysical question with practical implications for fairness measurement. Note how different ontological positions yield different answers to "which categories must be measured."

KEY_POSITIONS:
- Emergence view: 5 papers - Intersectional experiences emerge from social structures making category conjunctions relevant (Jorba & López de Sa 2024, O'Connor et al. 2019)
- Properties framework: 3 papers - Categories as properties of positions, not objects (Jorba & Rodó-Zárate 2019-2020)
- Critical realist/dialectical: 2 papers - Categories both distinct and unified (Gunnarsson 2020, Clegg 2012)
- Regulative ideal: 1 paper - Intersectionality as methodological principle, not metaphysical thesis (Gasdaglis & Madva 2020)
- Hermeneutic approach: 2 papers - Intersectionality as interpretive lens (excluded Ruíz, Barthold 2014)
- Social construction ameliorative: 2 papers - Categories defined by social role in oppression (Haslanger, Jones 2014)
- Foundational/conceptual: 3 papers - Clarifying core concepts and tensions (Carastathis 2014, 2016; Collins 2019)
====================================================================
}

@comment{
====================================================================
DOMAIN: Social Ontology and Group Constitution
SEARCH_DATE: 2026-01-10
PAPERS_FOUND: 15 total (High: 8, Medium: 5, Low: 2)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex
====================================================================

DOMAIN_OVERVIEW:
This domain addresses fundamental questions about the nature and constitution of social groups. The central debate concerns whether social groups can be identified algorithmically through attributes (combinatorial/attribute-based accounts) or whether they must be specified through social practices, recognition, and hermeneutic processes (practice-based accounts). Attribute-based accounts, exemplified by certain computational approaches to fairness, assume groups are defined by shared observable features that can be combined mechanically. Practice-based accounts, developed by philosophers like Haslanger, Mallon, and Ásta, argue that social groups are constituted through norms, collective intentionality, social roles, and conferral mechanisms that cannot be reduced to demographic attributes.

Recent work has increasingly emphasized the role of social structures, power relations, and institutional practices in constituting social kinds. Haslanger's influential work argues that race and gender are social positions defined by systematic subordination or privilege, challenging both biological essentialism and simple attribute-based models. Mallon develops a homeostatic property cluster account showing how social roles create stable but contingent patterns. Ásta's conferralist framework demonstrates how social properties depend on communal meaning-making rather than intrinsic features. Ritchie's structuralist ontology centers social groups on networks of social relations. These accounts converge on the view that group membership is not merely a matter of possessing certain attributes, but involves positioning within social structures and participation in practices that are historically contingent and politically contested.

This literature reveals deep ontological uncertainty about how to identify "the right groups" for normative purposes like fairness auditing. The disagreement is not merely terminological but reflects fundamental differences about whether social reality has a structure that can be discovered algorithmically or whether it must be interpreted through situated understanding of practices and power relations.

RELEVANCE_TO_PROJECT:
This domain is critical for the intersectionality fairness project because it exposes the ontological assumptions underlying different approaches to specifying protected groups. If practice-based accounts are correct, then algorithmic approaches that derive groups from attributes may systematically misidentify morally relevant social categories. The debate between attribute-based and practice-based accounts directly parallels the tension in ML fairness between combinatorial group definitions and substantive understandings of social groups as constituted by oppression.

NOTABLE_GAPS:
While there is extensive philosophical work on social construction and the metaphysics of social kinds, there is limited engagement with how these debates bear on algorithmic group specification. Few papers directly address the question of whether computational methods can adequately capture the practice-based constitution of social groups, or whether ontological pluralism about social groups creates fundamental indeterminacy for fairness metrics.

SYNTHESIS_GUIDANCE:
The synthesis should emphasize the fundamental contrast between attribute-based and practice-based accounts of group constitution, showing how this philosophical debate undermines the assumption that there is a determinate set of groups for intersectional fairness auditing. Key papers (Haslanger, Mallon, Ásta, Ritchie) should be used to articulate different models of social construction and their implications for algorithmic specification of groups.

KEY_POSITIONS:
- Haslanger-style political constructionism: 8 papers - Groups defined by structural positions of subordination/privilege
- Social role/HPC kinds (Mallon): 3 papers - Groups stabilized by homeostatic mechanisms of social roles
- Conferralism (Ásta): 2 papers - Groups constituted by communal conferral of properties
- Structuralism (Ritchie): 2 papers - Groups as social structures, networks of relations
====================================================================
}

@comment{
====================================================================
DOMAIN: Measurement Theory and Construct Validity in ML Fairness
SEARCH_DATE: 2026-01-10
PAPERS_FOUND: 18 total (High: 9, Medium: 7, Low: 2)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex, arXiv
====================================================================

DOMAIN_OVERVIEW:
This domain examines how concepts from measurement theory and construct validity, drawn from the philosophy of science and psychometrics, apply to fairness metrics in machine learning. The central question is whether fairness metrics actually measure what they purport to measure, and what validity challenges arise when operationalizing contested normative concepts like fairness. Key debates center on: (1) the gap between formal fairness definitions and normative goals (e.g., do demographic parity metrics truly capture fairness?); (2) target specification bias, where the operationalized construct differs from what decision-makers actually care about; (3) measurement modeling frameworks that reveal ambiguities in what fairness benchmarks measure; and (4) the limits of formalization for inherently contested concepts. Recent work shows that many fairness metrics exhibit poor construct validity - they lack clear articulations of what is being measured, conflate distinct normative concerns, and often fail to correlate with each other or with human judgments of fairness.

RELEVANCE_TO_PROJECT:
This domain directly supports the paper's core thesis that fairness auditing faces fundamental conceptual challenges in operationalization. The measurement theory literature demonstrates that many current fairness metrics suffer from validity problems - they don't actually measure what they claim to measure. This is precisely the kind of conceptual challenge (not just technical optimization problem) that the research idea emphasizes. Papers on construct validity, target specification bias, and measurement modeling provide the theoretical framework for critiquing current fairness auditing practices.

NOTABLE_GAPS:
While there is substantial work on the validity of individual fairness metrics, there is limited philosophical analysis of how measurement theory principles should guide the design of new metrics. Most work focuses on critique rather than constructive alternatives. Additionally, the connection between construct validity problems and the contested nature of fairness concepts deserves deeper philosophical treatment.

SYNTHESIS_GUIDANCE:
When synthesizing this domain, emphasize the parallel between measurement problems in psychology/social sciences and fairness metrics in ML. The Tal paper on target specification bias provides a crucial bridge between measurement theory and healthcare fairness. Blodgett et al.'s work on measurement modeling offers a comprehensive framework. Consider organizing around: (1) what construct validity means, (2) evidence that fairness metrics lack it, (3) why contested concepts resist valid operationalization.

KEY_POSITIONS:
- Measurement Modeling Critique: 8 papers - Fairness metrics lack clear construct definitions and validity
- Target Specification Bias: 3 papers - Operationalizations mismatch decision-makers' actual concerns
- Metric Incompatibility: 5 papers - Different fairness metrics measure fundamentally different things
- Contextualist Response: 2 papers - Fairness measurement requires domain-specific contextualization
====================================================================
}

@comment{
====================================================================
DOMAIN: Normative Frameworks for Fairness
SEARCH_DATE: 2026-01-10
PAPERS_FOUND: 18 total (High: 8, Medium: 7, Low: 3)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex
====================================================================

DOMAIN_OVERVIEW:
This domain examines the normative foundations that should guide fairness in
algorithmic decision-making, particularly in the context of intersectional
fairness auditing. The debate centers on how classical theories of distributive
justice—prioritarianism (giving extra weight to the worse off), sufficientarianism
(ensuring everyone has enough), and egalitarianism (treating equality as a core value)—
apply to algorithmic contexts. Foundational work by Shields, Arneson, and others
establishes these normative frameworks, while recent work by Hertweck, Fazelpour,
Holm, and others bridges them to algorithmic fairness. A central tension emerges:
these frameworks differ not just in *how much* inequality to tolerate, but in
*which groups matter* and *why*—a debate particularly salient for intersectional
fairness where ontological uncertainty about group membership is unavoidable.

The literature reveals two key shifts. First, recognition that algorithmic fairness
cannot be reduced to statistical parity criteria without normative grounding (Green
2022, Fazelpour 2021, Hertweck 2024). Second, growing attention to how different
normative frameworks yield different answers about which groups deserve protection
and prioritization (Holm 2025, Binns 2024). Recent work emphasizes that fairness
auditing serves distinct purposes—legal compliance versus substantive justice—
which require different normative foundations.

RELEVANCE_TO_PROJECT:
This domain directly addresses the core claim that ontological uncertainty (which
groups matter) is partly normative. Different ethical frameworks—prioritarian,
sufficientarian, egalitarian—yield different answers about group prioritization.
For example, prioritarianism focuses on the worst-off intersectional groups,
sufficientarianism on ensuring all groups reach minimum thresholds, and egalitarianism
on parity across groups. The purpose of fairness auditing (compliance vs. substantive
justice) also depends on which normative framework one adopts. This domain provides
the philosophical vocabulary for articulating why disagreement about fairness metrics
is not merely technical but reflects deeper normative commitments.

NOTABLE_GAPS:
Few papers explicitly connect sufficientarianism or prioritarianism to intersectional
fairness (most focus on single-axis fairness). Limited discussion of how these
frameworks handle ontological uncertainty about which intersections to audit. Sparse
treatment of fairness auditing's dual purposes (compliance vs. substantive justice)
and how normative frameworks differentially serve these goals.

SYNTHESIS_GUIDANCE:
When synthesizing, emphasize how normative frameworks generate different answers to
"which groups matter" and "how much." Show how this connects to the impossibility
results in algorithmic fairness—disagreement isn't just technical incompatibility
but reflects competing normative commitments (prioritarian vs. sufficientarian vs.
egalitarian). Use Holm's work on Broome's fairness theory and Hertweck's critique
of simple distributive justice to argue that fairness auditing requires explicit
normative choices, not merely statistical optimization.

KEY_POSITIONS:
- Prioritarianism (8 papers): Weight benefits to worse-off groups more heavily
- Sufficientarianism (7 papers): Ensure all groups reach sufficiency thresholds
- Egalitarianism (6 papers): Equality as intrinsic value, parity across groups
- Pluralist approaches (3 papers): Combine multiple normative frameworks
====================================================================
}

@comment{
====================================================================
DOMAIN: Epistemic Uncertainty and Data Sparsity
SEARCH_DATE: 2026-01-10
PAPERS_FOUND: 18 total (High: 8, Medium: 7, Low: 3)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex, CrossRef
====================================================================

DOMAIN_OVERVIEW:
This domain examines how epistemic justice concerns intersect with statistical
uncertainty in fairness auditing, particularly when dealing with sparse data
for marginalized groups. The literature reveals three interrelated clusters:
(1) foundational work on epistemic injustice (Fricker, Anderson) establishing
how marginalized groups face testimonial and hermeneutical injustice; (2)
applications to data science and algorithms (Symons & Alvarado, Hull, Milano &
Prunkl) showing how algorithmic systems perpetuate and amplify these
injustices; and (3) technical work on sample size, statistical reliability,
and fairness trade-offs (Zhioua & Binkyte, Jourdan et al., Mhasawade et al.)
demonstrating that small sample sizes for marginalized groups introduce both
epistemic and statistical challenges. A key tension emerges: expanding fairness
auditing to cover more intersectional groups creates data sparsity problems that
undermine statistical reliability, yet requiring large sample sizes can itself
constitute epistemic injustice by excluding groups that cannot be easily
aggregated at scale.

RELEVANCE_TO_PROJECT:
This domain directly addresses the "statistical uncertainty horn" of the
intersectionality dilemma. The epistemic justice literature provides normative
grounding for why data sparsity is not merely a technical problem but an issue
of structural injustice. The technical literature on sample size and fairness
metrics demonstrates the practical challenges of reasoning under uncertainty
with sparse data. Together, these bodies of work illuminate how the trade-off
between group expansion and statistical reliability connects to broader concerns
about who gets to be epistemically represented in algorithmic systems.

NOTABLE_GAPS:
Limited work explicitly connecting epistemic injustice frameworks to the
specific problem of sample size requirements in fairness auditing. Most
epistemic injustice literature in AI focuses on testimonial/hermeneutical forms
without addressing distributive epistemic injustice in data collection practices.
Philosophy of statistics literature on uncertainty quantification rarely engages
with fairness concerns or social justice implications.

SYNTHESIS_GUIDANCE:
Synthesis should emphasize how epistemic justice provides a lens for
understanding why data sparsity is not just a technical challenge but a site of
structural injustice. The interaction between statistical uncertainty and
epistemic marginalization creates a double bind: groups with sparse data face
both unreliable auditing AND epistemic exclusion from algorithmic systems.

KEY_POSITIONS:
- Epistemic injustice in data systems (8 papers): Data collection and algorithmic
  systems perpetuate testimonial and hermeneutical injustices
- Sample size and fairness trade-offs (5 papers): Small sample sizes for
  underrepresented groups undermine reliable fairness measurement
- Uncertainty quantification in algorithmic decision-making (5 papers): Opacity
  and uncertainty in ML systems raise epistemic authority concerns
====================================================================
}

@comment{
====================================================================
DOMAIN: Critical Perspectives on Fairness Formalization
SEARCH_DATE: 2026-01-10
PAPERS_FOUND: 12 total (High: 8, Medium: 4, Low: 0)
SEARCH_SOURCES: SEP, PhilPapers, Semantic Scholar, OpenAlex, CrossRef
====================================================================

DOMAIN_OVERVIEW:
This domain encompasses fundamental critiques of fairness formalization
projects in machine learning. These critiques argue that mathematical
fairness metrics, while useful tools, cannot fully capture substantive
justice and may obscure structural inequalities. Key themes include: (1)
the impossibility of simultaneously satisfying multiple fairness criteria,
(2) fairness gerrymandering vulnerabilities where classifiers appear fair
on predefined groups but violate fairness on intersectional subgroups, (3)
structural critiques arguing that formal fairness metrics abstract away
from social context and systemic oppression, and (4) feminist and critical
race theory perspectives that highlight how formalization can legitimize
rather than challenge existing power structures.

The impossibility results (Kleinberg et al., Chouldechova) demonstrate
mathematical constraints on fairness formalization. The fairness
gerrymandering problem (Kearns et al.) shows how group fairness metrics
can miss intersectional disparities. Critical scholars (Green, Wachter et
al., Kasirzadeh, Hampton) argue for moving beyond "formal" to
"substantive" fairness that addresses structural injustice. Recent work
emphasizes participatory approaches and the limits of automation.

RELEVANCE_TO_PROJECT:
These critiques provide essential context for understanding whether the
intersectionality dilemma identified in the research proposal represents a
fundamental limitation of fairness formalization or a technical challenge
that can be overcome. If critics are correct that fairness cannot be fully
operationalized, the dilemma may be inherent rather than resolvable.

NOTABLE_GAPS:
Most critiques focus on criminal justice and employment domains. Less
attention to healthcare or education contexts. Limited work on specific
alternatives to formal metrics beyond general calls for participatory
approaches or "substantive" fairness.

SYNTHESIS_GUIDANCE:
Contrast impossibility results (technical constraints) with structural
critiques (conceptual limitations). Consider whether fairness
gerrymandering and the intersectionality dilemma are related phenomena.
Examine whether critics offer genuine alternatives or only diagnostic
critiques.

KEY_POSITIONS:
- Impossibility theorists: 8 papers - Mathematical constraints limit
  fairness formalization
- Structural critics: 6 papers - Formal metrics obscure systemic injustice
- Feminist critics: 4 papers - Formalization can reinforce oppression
====================================================================
}

@inproceedings{hebert-johnson2018multicalibration,
  author = {Hébert-Johnson, Úrsula and Kim, Michael P. and Reingold, Omer and Rothblum, Guy N.},
  title = {Multicalibration: Calibration for the (Computationally-Identifiable) Masses},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  year = {2018},
  pages = {1944--1953},
  note = {
  CORE ARGUMENT: Introduces multicalibration as a fairness notion requiring that predictions be calibrated not just overall but simultaneously across a rich collection of overlapping subgroups defined by efficiently computable functions. Provides efficient algorithms that achieve multicalibration without explicitly enumerating all subgroups, solving the combinatorial explosion problem for intersectional groups.

  RELEVANCE: Foundational paper for the multicalibration framework, which represents the most technically sophisticated approach to intersectional fairness. Demonstrates that calibration across exponentially many overlapping groups is computationally tractable, addressing the statistical side of the dilemma. However, assumes the class of group-defining functions is given, leaving the ontological question of which groups warrant consideration unaddressed.

  POSITION: Represents the "rich subgroup framework" approach to intersectional fairness - solve the statistical challenge through clever algorithms while treating group specification as an input to the fairness problem.
  },
  keywords = {multicalibration, subgroup-fairness, foundational, High}
}

@inproceedings{kong2022intersectionally,
  author = {Kong, Youjin},
  title = {Are "Intersectionally Fair" AI Algorithms Really Fair to Women of Color? A Philosophical Analysis},
  booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2022},
  doi = {10.1145/3531146.3533114},
  note = {
  CORE ARGUMENT: Provides philosophical critique of dominant statistical parity approach to intersectional fairness in AI. Argues this approach faces a fundamental "dilemma between infinite regress and fairness gerrymandering": either continuously split groups into smaller subgroups ad infinitum, or arbitrarily select which groups to protect. Distinguishes "strong" fairness (addressing oppression) from "weak" fairness (statistical parity).

  RELEVANCE: Most explicit statement in the ML literature of the interaction between statistical and ontological problems in intersectional fairness. Directly supports the paper's core argument by identifying fairness gerrymandering as a consequence of arbitrary group selection. Bridges technical ML literature and philosophical analysis of intersectionality.

  POSITION: Philosophical critique of dominant technical approaches, arguing they fail to address intersection of oppressions and face fundamental conceptual problems in group specification.
  },
  keywords = {intersectionality, fairness-gerrymandering, philosophical-analysis, High}
}

@inproceedings{foulds2020intersectional,
  author = {Foulds, James R. and Islam, Rashidul and Keya, Kamrun Naher and Pan, Shimei},
  title = {An Intersectional Definition of Fairness},
  booktitle = {2020 IEEE 36th International Conference on Data Engineering (ICDE)},
  year = {2020},
  pages = {1918--1921},
  doi = {10.1109/icde48307.2020.00203},
  note = {
  CORE ARGUMENT: Proposes differential fairness, a multi-attribute fairness definition inspired by differential privacy that requires similar treatment for individuals who differ in protected attributes. Addresses intersectionality by requiring fairness simultaneously across all possible conjunctions of protected attributes. Provides learning algorithm with privacy, economic, and generalization guarantees.

  RELEVANCE: Represents major technical approach to intersectional fairness distinct from multicalibration. Demonstrates how differential privacy concepts can formalize intersectional requirements. However, like multicalibration, assumes protected attributes are pre-specified, not addressing the ontological question of which attributes constitute relevant axes of oppression.

  POSITION: Differential privacy-inspired approach to intersectional fairness, emphasizing formal guarantees across attribute combinations.
  },
  keywords = {differential-fairness, intersectionality, technical-approach, High}
}

@article{mehrabi2019survey,
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  title = {A Survey on Bias and Fairness in Machine Learning},
  journal = {ACM Computing Surveys},
  year = {2021},
  volume = {54},
  number = {6},
  pages = {1--35},
  doi = {10.1145/3457607},
  note = {
  CORE ARGUMENT: Comprehensive survey of bias and fairness in ML systems, covering sources of bias (data, algorithm, user interaction), fairness definitions (group fairness, individual fairness, subgroup fairness), and mitigation strategies across the ML pipeline. Identifies real-world applications where biased outcomes have occurred and discusses limitations of existing approaches.

  RELEVANCE: Provides comprehensive overview of the fairness landscape that situates intersectional approaches within broader fairness research. Useful for understanding how the field conceptualizes bias sources and fairness goals. However, survey format means limited depth on the specific statistical/ontological interaction central to the research project.

  POSITION: Survey paper establishing the state of the field, taxonomizing approaches without strongly advocating for specific solutions.
  },
  keywords = {fairness-survey, bias-mitigation, comprehensive-review, Medium}
}

@inproceedings{gohar2023intersectional,
  author = {Gohar, Usman and Cheng, Lu},
  title = {A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges},
  booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
  year = {2023},
  pages = {6619--6627},
  doi = {10.24963/ijcai.2023/742},
  note = {
  CORE ARGUMENT: Surveys state-of-the-art in intersectional fairness, presenting taxonomy of intersectional fairness notions and mitigation strategies. Identifies key challenges including computational complexity, data scarcity for intersectional groups, and metric selection. Provides guidelines for future research directions in intersectional fairness.

  RELEVANCE: Most comprehensive recent survey specifically focused on intersectional fairness. Explicitly identifies data scarcity for intersectional groups as a key challenge, directly relevant to the statistical dimension of the research proposal. Taxonomizes different technical approaches to the problem, showing variety of attempted solutions to overlapping group fairness.

  POSITION: Survey establishing the current state of intersectional fairness research, emphasizing open challenges and future directions.
  },
  keywords = {intersectional-fairness, survey, data-scarcity, High}
}

@article{tian2024multifair,
  author = {Tian, Huan and Liu, Bo and Zhu, Tianqing and Zhou, Wanlei and Yu, Philip S.},
  title = {MultiFair: Model Fairness With Multiple Sensitive Attributes},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  year = {2024},
  volume = {36},
  number = {5},
  pages = {5654--5667},
  doi = {10.1109/TNNLS.2024.3384181},
  note = {
  CORE ARGUMENT: Proposes MultiFair method for achieving fairness across multiple sensitive attributes through mixup-based data augmentation. Addresses fairness gerrymandering concern by ensuring fairness for all attribute combinations, not just per-attribute. Demonstrates that naive per-attribute approaches leave intersectional groups unfair, while their approach delivers fairness protections for multiple attributes simultaneously.

  RELEVANCE: Recent technical contribution explicitly addressing fairness gerrymandering in multi-attribute settings. Shows empirically that per-attribute fairness interventions can fail for intersectional groups, validating the need for explicit intersectional approaches. Represents state-of-the-art technical solution but assumes attributes are pre-specified.

  POSITION: Technical solution emphasizing that fairness must be evaluated at intersections to avoid gerrymandering, not just for individual attributes separately.
  },
  keywords = {multi-attribute-fairness, fairness-gerrymandering, recent-work, High}
}

@inproceedings{gopalan2022lowdegree,
  author = {Gopalan, Parikshit and Kim, Michael P. and Singhal, Mihir and Zhao, Shengjia},
  title = {Low-Degree Multicalibration},
  booktitle = {Proceedings of Thirty Fifth Conference on Learning Theory},
  year = {2022},
  pages = {3193--3234},
  note = {
  CORE ARGUMENT: Introduces low-degree multicalibration as middle ground between multiaccuracy (weak) and full multicalibration (strong but expensive). Shows that key fairness and accuracy properties of multicalibration manifest as low-degree properties, achieving exponentially better sample complexity than full multicalibration in multi-class settings. Provides theoretical analysis of the fairness-efficiency tradeoff.

  RELEVANCE: Addresses sample complexity challenges in multicalibration - directly relevant to sparse data problem for intersectional groups. Shows that achieving multicalibration for all intersectional groups requires exponentially growing data, but low-degree approximations can achieve many desiderata more efficiently. Illustrates statistical limits inherent in intersectional fairness.

  POSITION: Technical refinement of multicalibration showing that approximate solutions can achieve key fairness properties with better sample efficiency than exact solutions.
  },
  keywords = {multicalibration, sample-complexity, sparse-data, High}
}

@article{cherian2023statistical,
  author = {Cherian, John J. and Candès, Emmanuel J.},
  title = {Statistical Inference for Fairness Auditing},
  journal = {Journal of Machine Learning Research},
  year = {2024},
  volume = {25},
  pages = {1--56},
  note = {
  CORE ARGUMENT: Develops statistical framework for fairness auditing using multiple hypothesis testing with bootstrap methods. Shows how to simultaneously bound performance disparities over rich collections of subgroups with statistical guarantees. Addresses challenge of auditing fairness across exponentially many intersectional subgroups while controlling for multiple testing.

  RELEVANCE: Provides rigorous statistical methodology for fairness evaluation across intersectional groups. Demonstrates that statistically valid fairness auditing is possible for rich subgroup collections, but requires careful control of false discovery rates. Illustrates the statistical sophistication required to handle intersectional fairness evaluation, relevant to the statistical dimension of the dilemma.

  POSITION: Statistical methodology for fairness auditing, emphasizing rigorous hypothesis testing across multiple subgroups.
  },
  keywords = {fairness-auditing, statistical-inference, multiple-testing, High}
}

@article{ferrara2025sizehypothesis,
  author = {Ferrara, Antonio and Cozzi, Francesco and Perotti, Alan and Panisson, André and Bonchi, Francesco},
  title = {Size-adaptive Hypothesis Testing for Fairness},
  journal = {arXiv preprint arXiv:2506.10586},
  year = {2025},
  doi = {10.48550/arXiv.2506.10586},
  note = {
  CORE ARGUMENT: Proposes size-adaptive hypothesis testing framework that handles varying data availability across demographic groups, addressing the challenge that intersectional groups become increasingly small and sparse. Uses Central Limit Theorem for large groups and Bayesian Dirichlet-multinomial estimator for small groups. Demonstrates empirically that this approach provides interpretable, statistically rigorous fairness decisions under data scarcity.

  RELEVANCE: Most recent technical contribution specifically addressing sparse data problem for small intersectional groups. Provides practical statistical methods for handling the long tail of small intersectional subgroups where standard methods fail. Directly addresses the statistical challenge central to the research proposal's argument about the dilemma.

  POSITION: Statistical solution to sparse intersectional data through adaptive methods that adjust rigor based on group size.
  },
  keywords = {sparse-data, hypothesis-testing, intersectionality, High}
}

@inproceedings{raz2022gerrymandering,
  author = {Räz, Tim},
  title = {Gerrymandering Individual Fairness},
  journal = {Artificial Intelligence},
  year = {2023},
  volume = {323},
  doi = {10.48550/arXiv.2204.11615},
  note = {
  CORE ARGUMENT: Proves that individual fairness (similar individuals treated similarly) is susceptible to gerrymandering through manipulation of the feature space and similarity metric. Shows constructively how to create predictors that satisfy individual fairness while being systematically unfair. Argues for refined notions of fairness that are robust to such manipulation.

  RELEVANCE: Extends fairness gerrymandering concept from group fairness to individual fairness, showing the problem is fundamental rather than specific to group-based approaches. Demonstrates that fairness metrics can be gamed through strategic choices about problem representation, relevant to the ontological dimension of group specification.

  POSITION: Critique showing that even individual fairness (often proposed as alternative to group fairness) is susceptible to gerrymandering through feature/metric manipulation.
  },
  keywords = {fairness-gerrymandering, individual-fairness, critique, Medium}
}

@article{lee2025multicalibration,
  author = {Lee, Ho Ming and Antonio, Katrien and Avanzi, Benjamin and Marchi, Lorenzo and Zhou, Rui},
  title = {Machine Learning with Multitype Protected Attributes: Intersectional Fairness through Regularisation},
  journal = {arXiv preprint arXiv:2509.08163},
  year = {2025},
  doi = {10.48550/arXiv.2509.08163},
  note = {
  CORE ARGUMENT: Proposes distance covariance regularization framework for multicalibration that handles protected attributes of various types (categorical, continuous, mixed). Extends multicalibration to incorporate joint distance covariance and concatenated distance covariance measures, addressing fairness gerrymandering by ensuring fairness across intersectional subgroups defined by density ratios. Applies to both regression and classification with various attribute types.

  RELEVANCE: Very recent (2025) technical contribution addressing fairness gerrymandering through explicit intersectional modeling. Extends multicalibration to handle continuous and mixed-type attributes, broadening the technical framework beyond categorical groups. Demonstrates ongoing evolution of technical approaches to intersectional fairness challenges.

  POSITION: Technical extension of multicalibration addressing fairness gerrymandering through sophisticated statistical measures of attribute interactions.
  },
  keywords = {multicalibration, fairness-gerrymandering, continuous-attributes, Medium}
}

@inproceedings{davis2021algorithmic,
  author = {Davis, Jenny L. and Williams, Apryl and Yang, Michael W.},
  title = {Algorithmic Reparation},
  journal = {Big Data \& Society},
  year = {2021},
  volume = {8},
  number = {2},
  doi = {10.1177/20539517211044808},
  note = {
  CORE ARGUMENT: Critiques existing fair ML methods (anti-classification, classification parity, calibration) as operating from "algorithmic idealism" that cannot address systemic, intersectional stratifications. Proposes "algorithmic reparation" grounded in intersectionality theory, which names, unmasks, and undoes allocative and representational harms as they materialize in sociotechnical form.

  RELEVANCE: Provides intersectionality-grounded critique of technical fairness approaches, arguing they fail to engage with structural oppression and intersectional complexity. Represents perspective from critical data studies emphasizing that technical fixes cannot solve fundamentally social-ontological problems. Relevant to the paper's argument that statistical solutions alone cannot resolve the intersectional fairness dilemma.

  POSITION: Critical perspective arguing that technical fairness methods fail to address intersectional oppression and proposing reparation-oriented alternative.
  },
  keywords = {algorithmic-reparation, intersectionality-theory, critical-perspective, Medium}
}

@inproceedings{himmelreich2024intersectionality,
  author = {Himmelreich, Johannes and Hsu, Arbie and Lum, Kristian and Veomett, Ellen},
  title = {The Intersectionality Problem for Algorithmic Fairness},
  journal = {arXiv preprint arXiv:2411.02569},
  year = {2024},
  doi = {10.48550/arXiv.2411.02569},
  note = {
  CORE ARGUMENT: Elucidates the problem of intersectionality in algorithmic fairness, developing desiderata to clarify challenges and guide solutions. Identifies both statistical challenges (small intersectional groups) and moral-methodological challenges (which groups matter). Proposes and evaluates hypothesis testing approach, demonstrating tradeoffs between statistical power and intersectional coverage.

  RELEVANCE: Recent philosophical-technical paper explicitly framing intersectional fairness as involving both statistical and normative challenges. Provides clear articulation of the problem space and proposes concrete desiderata for evaluating solutions. Directly relevant to the paper's framing of the dilemma as involving interacting statistical and ontological dimensions.

  POSITION: Philosophical-technical analysis identifying the intersectionality problem as multi-dimensional, requiring both statistical sophistication and normative clarity about group relevance.
  },
  keywords = {intersectionality-problem, statistical-normative, philosophical-technical, High}
}

@article{caton2024fairness,
  author = {Caton, Simon and Haas, Christian},
  title = {Fairness in Machine Learning: A Survey},
  journal = {ACM Computing Surveys},
  year = {2024},
  volume = {56},
  number = {7},
  pages = {1--38},
  doi = {10.1145/3616865},
  note = {
  CORE ARGUMENT: Comprehensive survey organizing fairness approaches into pre-processing, in-processing, and post-processing methods across 11 method areas. Discusses fairness in regression, recommender systems, and unsupervised learning beyond binary classification. Concludes by identifying five fundamental dilemmas in fairness research including the impossibility of satisfying all fairness criteria simultaneously.

  RELEVANCE: Provides broad overview of fairness methods and fundamental tradeoffs. Identifies structural dilemmas in fairness research that constrain possible solutions. Useful for situating intersectional fairness within broader landscape of fairness approaches and understanding inherent limitations.

  POSITION: Survey establishing the fairness landscape, emphasizing fundamental tradeoffs and impossibility results that constrain fairness approaches.
  },
  keywords = {fairness-survey, tradeoffs, methods-taxonomy, Medium}
}

@article{ekstrand2022fairness,
  author = {Ekstrand, Michael D. and Das, Anubrata and Burke, Robin and Díaz, Fernando},
  title = {Fairness in Information Access Systems},
  journal = {Foundations and Trends in Information Retrieval},
  year = {2022},
  volume = {16},
  number = {1-2},
  pages = {1--177},
  doi = {10.1561/1500000079},
  note = {
  CORE ARGUMENT: Comprehensive treatment of fairness in information access systems (recommendation, retrieval), emphasizing multi-stakeholder nature, rank-based settings, and role of personalization. Develops taxonomy of fairness dimensions specific to information access. Discusses how fairness challenges differ from classification settings due to user response dynamics and multi-sided platforms.

  RELEVANCE: Demonstrates how fairness challenges manifest differently across application domains, with information access posing unique challenges for intersectional fairness due to feedback loops and stakeholder complexity. Illustrates that intersectional fairness problems vary by application context, relevant to understanding scope and generality of solutions.

  POSITION: Domain-specific survey showing how fairness concerns in information access differ from classification, requiring adapted approaches.
  },
  keywords = {information-access, fairness-survey, domain-specific, Low}
}

@article{hansen2024multicalibration,
  author = {Hansen, Dutch and Devic, Siddartha and Nakkiran, Preetum and Sharan, Vatsal},
  title = {When is Multicalibration Post-Processing Necessary?},
  journal = {Advances in Neural Information Processing Systems},
  year = {2024},
  volume = {37},
  note = {
  CORE ARGUMENT: Conducts comprehensive empirical study evaluating when multicalibration post-processing provides benefits beyond standard calibration. Finds that models calibrated out-of-the-box tend to be relatively multicalibrated without additional processing, but multicalibration helps inherently uncalibrated models and large language models. Provides practical guidance on when multicalibration post-processing is necessary.

  RELEVANCE: Recent empirical evaluation of multicalibration's practical utility across diverse models and datasets. Demonstrates that achieving multicalibration is not always difficult in practice, but benefits vary by model type. Relevant to understanding when intersectional fairness concerns require explicit intervention versus emerging naturally from well-calibrated models.

  POSITION: Empirical analysis showing multicalibration post-processing is sometimes but not always necessary, depending on model and data characteristics.
  },
  keywords = {multicalibration, empirical-evaluation, practical-utility, Medium}
}

@inproceedings{vethman2025fairness,
  author = {Vethman, Steven and Smit, Quirine T. S. and van Liebergen, Nina M. and Veenman, Cor J.},
  title = {Fairness Beyond the Algorithmic Frame: Actionable Recommendations for an Intersectional Approach},
  booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2025},
  doi = {10.1145/3715275.3732210},
  note = {
  CORE ARGUMENT: Argues that technical fairness interventions operating within the "algorithmic frame" are insufficient for addressing intersectional fairness. Provides actionable recommendations for incorporating intersectional approaches that go beyond algorithm design to consider broader sociotechnical context, power relations, and structural inequalities. Emphasizes need to expand scope of fairness work beyond technical optimization.

  RELEVANCE: Recent contribution arguing for expanded framing of intersectional fairness beyond purely technical approaches. Relevant to the paper's argument that technical statistical solutions cannot fully resolve the intersectional fairness dilemma without engaging with social-ontological questions about group construction and structural inequality.

  POSITION: Critical-constructive perspective arguing technical fairness must be supplemented with broader intersectional analysis of sociotechnical context.
  },
  keywords = {intersectional-approach, sociotechnical, beyond-algorithmic, Medium}
}

@inproceedings{globus-harris2023multicalibration,
  author = {Globus-Harris, Ira and Harrison, Declan and Kearns, Michael and Roth, Aaron and Sorrell, Jessica},
  title = {Multicalibration as Boosting for Regression},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  year = {2023},
  pages = {11459--11492},
  note = {
  CORE ARGUMENT: Shows multicalibration can be achieved through standard weak agnostic learning oracle, connecting multicalibration to boosting for regression. Provides simple algorithm analyzable as both boosting and multicalibration, with convergence to Bayes optimality under weak learning assumptions. Demonstrates that multicalibration and calibration together yield considerably stronger guarantees than either alone.

  RELEVANCE: Theoretical contribution showing multicalibration's connection to fundamental learning primitives. Demonstrates that achieving multicalibration requires at minimum weak agnostic learning, establishing computational complexity bounds. Relevant to understanding theoretical foundations and limits of multicalibration approaches to intersectional fairness.

  POSITION: Theoretical analysis connecting multicalibration to boosting, establishing computational and learning-theoretic foundations.
  },
  keywords = {multicalibration, boosting, learning-theory, Low}
}

@article{noarov2023scope,
  author = {Noarov, Georgy and Roth, Aaron},
  title = {The Scope of Multicalibration: Characterizing Multicalibration via Property Elicitation},
  journal = {arXiv preprint arXiv:2302.08507},
  year = {2023},
  doi = {10.48550/arXiv.2302.08507},
  note = {
  CORE ARGUMENT: Establishes connection between multicalibration and property elicitation, proving that a property can be multicalibrated if and only if it is elicitable. Extends multicalibration beyond means to general distributional properties including quantiles and variance. Provides theoretical characterization of the scope of multicalibration as a fairness framework.

  RELEVANCE: Theoretical work establishing fundamental limits and scope of multicalibration framework. Shows which fairness properties can be achieved through multicalibration and which cannot, relevant to understanding what statistical guarantees are achievable for intersectional groups. Demonstrates theoretical foundations of multicalibration approaches.

  POSITION: Theoretical characterization establishing scope and limits of multicalibration through connection to property elicitation.
  },
  keywords = {multicalibration, property-elicitation, theoretical, Low}
}

@article{wu2024bridging,
  author = {Wu, Jiayun and Liu, Jiashuo and Cui, Peng and Wu, Zhiwei Steven},
  title = {Bridging Multicalibration and Out-of-distribution Generalization Beyond Covariate Shift},
  journal = {Advances in Neural Information Processing Systems},
  year = {2024},
  volume = {37},
  note = {
  CORE ARGUMENT: Establishes link between multicalibration and out-of-distribution generalization, showing that extended multicalibration (incorporating groupings over covariates and labels jointly) is equivalent to invariance under concept shift. Proposes MC-Pseudolabel algorithm achieving both multicalibration and OOD generalization. Demonstrates connection between fairness and robustness.

  RELEVANCE: Recent work connecting multicalibration to distribution shift robustness, showing that fairness and generalization are related desiderata. Relevant to understanding how solutions to intersectional fairness challenges might also address model robustness. Demonstrates broader significance of multicalibration framework beyond fairness.

  POSITION: Technical contribution bridging fairness (multicalibration) and robustness (OOD generalization) through extended multicalibration framework.
  },
  keywords = {multicalibration, ood-generalization, robustness, Medium}
}

@article{jorba2024intersectionality,
  author = {Jorba, Marta and López de Sa, Dan},
  title = {Intersectionality as emergence},
  journal = {Philosophical Studies},
  year = {2024},
  volume = {181},
  number = {6},
  pages = {1455--1475},
  doi = {10.1007/s11098-024-02155-1},
  note = {
  CORE ARGUMENT: Proposes that intersectional experiences emerge from the conjunction of social categories when social structures make these conjunctions relevant for discrimination and privilege. Argues for metaphysical neutrality (compatible with multiple ontologies of categories), explanatory flexibility (allows context-dependent emergence), and methodological openness (guides research without prescribing specific methods). Distinguishes their emergence view from strong metaphysical claims about new properties.

  RELEVANCE: Directly addresses whether intersectionality requires emergent categories or can be reduced to combinations of attributes. Crucial for ontological uncertainty in algorithmic fairness: if experiences emerge contextually, algorithms cannot pre-specify all relevant categories. The metaphysical neutrality claim suggests fairness systems must remain open to discovering new relevant intersections rather than fixing categories in advance.

  POSITION: Emergence framework with metaphysical flexibility. Represents the most sophisticated recent defense of emergence without commitment to strong ontological novelty.
  },
  keywords = {intersectionality-metaphysics, emergence, social-ontology, High}
}

@article{bernstein2020metaphysics,
  author = {Bernstein, Sara},
  title = {The metaphysics of intersectionality},
  journal = {Philosophical Studies},
  year = {2020},
  volume = {177},
  number = {2},
  pages = {321--335},
  doi = {10.1007/s11098-019-01394-x},
  note = {
  CORE ARGUMENT: Examines metaphysical questions about how social categories combine in intersectionality theory. Analyzes whether intersectional identities are ontologically novel (emergence) or reducible to constituent categories. Discusses grounding relations between intersectional properties and base categories, arguing that intersectionality raises distinctive metaphysical puzzles about social kinds and their relations.

  RELEVANCE: Foundational for understanding competing metaphysical interpretations of intersectionality. Clarifies what is at stake in debates about emergence vs. reduction: whether "Black woman" names a property grounded in but distinct from race and gender, or merely abbreviates their conjunction. This directly impacts whether fairness algorithms must recognize intersectional categories as primitive or can decompose them.

  POSITION: Analytical metaphysics of social categories. Represents the application of analytic metaphysical tools (grounding, emergence, composition) to intersectionality theory.
  },
  keywords = {intersectionality-metaphysics, grounding, social-ontology, High}
}

@article{gasdaglis2020intersectionality,
  author = {Gasdaglis, Katherine and Madva, Alex},
  title = {Intersectionality as a Regulative Ideal},
  journal = {Ergo, an Open Access Journal of Philosophy},
  year = {2020},
  volume = {6},
  doi = {10.3998/ergo.12405314.0006.044},
  note = {
  CORE ARGUMENT: Argues that intersectionality should be understood as a regulative ideal (guiding methodological principle) rather than a substantive metaphysical, legal, or empirical thesis. Claims this interpretation avoids regress problems and definitional disputes. Treats existing classification schemes as indefinitely mutually informing, with the aim of revealing inequality and injustice. Provides heuristics for social-scientific research and political coalition-building.

  RELEVANCE: Offers an alternative to metaphysical debates by reframing intersectionality as epistemological and practical stance. Suggests ontological uncertainty might be feature, not bug: fairness systems should treat categories as provisional and open to revision. Challenges the project of fixing intersectional ontology for algorithmic purposes, instead requiring ongoing responsiveness to how categories function in specific contexts of oppression.

  POSITION: Anti-metaphysical pragmatist interpretation. Represents escape from analytical vs. hermeneutic divide by rejecting metaphysical framing entirely.
  },
  keywords = {intersectionality-methodology, regulative-ideal, pragmatism, High}
}

@article{gunnarsson2020dialecticizing,
  author = {Gunnarsson, Lena},
  title = {Why we keep separating the 'inseparable': Dialecticizing intersectionality},
  booktitle = {Marxism and Social Theory},
  year = {2020},
  editor = {Sugarman, David and Bakan, Abigail B.},
  publisher = {Brill},
  pages = {146--162},
  doi = {10.1163/9789004321939_008},
  note = {
  CORE ARGUMENT: Drawing on Roy Bhaskar's dialectical critical realism, argues that intersectional categories can be understood as both distinct and unified through the dialectical notion of unity-in-difference. Explains why categories are conceptually separable for analytical purposes yet experientially inseparable. Addresses the ontological puzzle of how to think categories at multiple levels simultaneously without reducing one to the other.

  RELEVANCE: Provides philosophical resources for thinking both category distinctness (needed for causal analysis) and category interpenetration (demanded by phenomenology of oppression). Relevant to algorithmic fairness through the claim that categories have different significance at different ontological levels (structural, cultural, experiential), suggesting fairness metrics must be multi-level rather than flattening categories into single variables.

  POSITION: Critical realist dialectical approach. Represents attempt to synthesize analytical precision with hermeneutic sensitivity through stratified ontology.
  },
  keywords = {intersectionality-ontology, critical-realism, dialectics, High}
}

@article{jorba2019properties,
  author = {Jorba, Marta and Rodó-Zárate, Maria},
  title = {Beyond Mutual Constitution: The Properties Framework for Intersectionality Studies},
  journal = {Signs: Journal of Women in Culture and Society},
  year = {2019},
  volume = {45},
  number = {1},
  pages = {175--200},
  doi = {10.1086/703499},
  note = {
  CORE ARGUMENT: Critiques mutual constitution metaphors as reifying categories by treating them as objects that combine. Proposes properties framework: categories are properties that positions possess, not objects themselves. This preserves ontological specificity of categories (gender is distinct from race) while allowing deep mutual influence. Distinguishes positions (individuals located in social space) from effects (consequences of that location). Argues this avoids both reification and reduction.

  RELEVANCE: Offers conceptual clarification crucial for algorithmic implementation. If categories are properties of positions, algorithms must model social positions as primary ontology, with categories as attributes. Challenges additive approaches (treating categories as independent features) without requiring strong emergence. Suggests intermediate path between reductive and emergent views relevant to fairness metric design.

  POSITION: Properties framework as alternative to mutual constitution. Represents linguistic/conceptual turn to resolve metaphysical puzzles through clarifying category grammar.
  },
  keywords = {intersectionality-ontology, properties-framework, conceptual-analysis, High}
}

@article{rodo2020metaphors,
  author = {Rodó-Zárate, Maria and Jorba, Marta},
  title = {Metaphors of intersectionality: Reframing the debate with a new proposal},
  journal = {European Journal of Women's Studies},
  year = {2020},
  volume = {27},
  number = {4},
  pages = {377--394},
  doi = {10.1177/1350506820930734},
  note = {
  CORE ARGUMENT: Analyzes dominant metaphors in intersectionality debates (traffic intersection, matrices, tapestry) and argues they fail to capture mutual constitution while avoiding reification. Problems include conflating positions with effects and treating categories as discrete objects. Proposes "different apples" metaphor: categories as diverse properties of apples (positions), effects as what happens when apples interact with specific structures. Reframes discussion of in/separability and fragmentation.

  RELEVANCE: Exposes how metaphors shape both theoretical understanding and practical implementation of intersectionality. Relevant to algorithmic fairness because computational models inherit metaphorical commitments (e.g., treating categories as vectors that combine implies additive metaphor). Shows that ontological debates often reduce to metaphorical differences, suggesting need for explicit conceptual frameworks in algorithm design rather than implicit metaphors.

  POSITION: Metaphor analysis and conceptual engineering. Complements properties framework by diagnosing conceptual confusions in existing literature.
  },
  keywords = {intersectionality-metaphors, conceptual-analysis, properties-framework, Medium}
}

@article{jones2014intersectionality,
  author = {Jones, Karen},
  title = {Intersectionality and ameliorative analyses of race and gender},
  journal = {Philosophical Studies},
  year = {2014},
  volume = {171},
  number = {1},
  pages = {81--89},
  doi = {10.1007/s11098-013-0245-0},
  note = {
  CORE ARGUMENT: Examines how Sally Haslanger's ameliorative analysis of race and gender (defining categories by their role in oppression) handles intersectionality. Questions whether ameliorative analysis can capture how multiple category memberships affect lived experience, social roles, and relative privilege. Explores whether Black women's subordination can be analyzed through separate race and gender analyses or requires integrated account.

  RELEVANCE: Tests whether social constructionist approaches can accommodate intersectionality. Crucial for algorithmic fairness if categories are defined by their function in systems of oppression: algorithms must then track how oppression operates contextually rather than treating categories as stable features. Raises question whether ameliorative definitions help or hinder intersectional analysis by defining categories through their effects.

  POSITION: Critical engagement with Haslanger's social construction view. Examines whether ameliorative analysis is compatible with intersectional insights.
  },
  keywords = {social-construction, ameliorative-analysis, Haslanger, Medium}
}

@article{carastathis2014concept,
  author = {Carastathis, Anna},
  title = {The Concept of Intersectionality in Feminist Theory},
  journal = {Philosophy Compass},
  year = {2014},
  volume = {9},
  number = {5},
  pages = {304--314},
  doi = {10.1111/phc3.12129},
  note = {
  CORE ARGUMENT: Traces the concept of intersectionality from Crenshaw's legal scholarship to feminist theory, examining how it has been interpreted, contested, and developed. Distinguishes intersectionality as analytic tool from intersectionality as representation of identity. Critiques tendencies toward categorical proliferation and argues for understanding intersectionality as critique of categorical thinking itself. Emphasizes political origins in anti-discrimination law and Black feminist organizing.

  RELEVANCE: Foundational for understanding what intersectionality claims and what problems it addresses. Shows that intersectionality emerged to address legal/institutional failures to recognize compound discrimination, not primarily as metaphysical thesis. This legal-institutional genealogy suggests algorithmic fairness is natural domain for intersectional analysis, but warns against treating intersectionality as mere identity representation rather than structural critique.

  POSITION: Conceptual clarification and genealogy. Represents careful disambiguation of multiple meanings of "intersectionality" in circulation.
  },
  keywords = {intersectionality-concept, feminist-theory, genealogy, High}
}

@book{carastathis2016intersectionality,
  author = {Carastathis, Anna},
  title = {Intersectionality: Origins, Contestations, Horizons},
  publisher = {University of Nebraska Press},
  year = {2016},
  isbn = {978-0-8032-8412-4},
  note = {
  CORE ARGUMENT: Comprehensive treatment arguing intersectionality is best understood not as arrived achievement but as horizon of future thinking. Reads Crenshaw's foundational texts closely to recover their radical edge against domesticated interpretations. Critiques representational approaches treating intersectionality as theory of multiple identities. Emphasizes political and coalitional dimensions, connecting to decolonial feminism. Warns against premature calls to "go beyond" intersectionality.

  RELEVANCE: Most thorough philosophical treatment of intersectionality's contested meanings and political stakes. Crucial for research project because it shows ontological questions cannot be separated from political functions. If intersectionality is horizon rather than fixed theory, algorithmic attempts to operationalize it face fundamental tension: codifying what should remain open and contestable. Suggests need for participatory approaches to category definition.

  POSITION: Critical genealogy with decolonial commitments. Represents resistance to analytical domestication of intersectionality's radical potential.
  },
  keywords = {intersectionality-theory, decolonial-feminism, political-philosophy, High}
}

@book{collins2019intersectionality,
  author = {Collins, Patricia Hill},
  title = {Intersectionality as Critical Social Theory},
  publisher = {Duke University Press},
  year = {2019},
  doi = {10.1515/9781478007098},
  note = {
  CORE ARGUMENT: Theorizes intersectionality as a form of critical inquiry and praxis rather than merely an analytic category. Traces genealogy through Black feminist thought, particularly the concept of matrix of domination. Argues intersectionality is simultaneously analytic tool, interpretive framework, and critical intervention. Emphasizes how power relations are mutually constructing across domains (structural, disciplinary, cultural, interpersonal). Positions intersectionality as ongoing critical project.

  RELEVANCE: Authoritative statement by major intersectionality theorist on what intersectionality is and should be. Crucial for research because it frames intersectionality as critical theory (oriented toward emancipation) rather than neutral analytic framework. This suggests algorithmic fairness cannot simply "apply" intersectionality but must engage with its critical dimension. The matrix of domination concept indicates categories operate at multiple levels requiring different analytical approaches.

  POSITION: Intersectionality as critical social theory. Represents Black feminist intellectual tradition's self-understanding of the concept.
  },
  keywords = {intersectionality-theory, critical-theory, Black-feminism, High}
}

@article{oconnor2019emergence,
  author = {O'Connor, Cailin and Bright, Liam Kofi and Bruner, Justin P.},
  title = {The emergence of intersectional disadvantage},
  journal = {Social Epistemology},
  year = {2019},
  volume = {33},
  number = {1},
  pages = {23--41},
  doi = {10.1080/02691728.2018.1555870},
  note = {
  CORE ARGUMENT: Uses agent-based models to show how intersectional oppression can emerge from bargaining dynamics even when groups face only single-axis disadvantages. Demonstrates that structural features of social coordination problems can generate emergent intersectional patterns. Argues simulation methods can generate precise causal predictions about when intersectional disadvantage arises. Shows intersection need not be assumed but can be derived from more basic social processes.

  RELEVANCE: Bridges analytical/causal and emergence approaches by showing emergence as outcome of causal mechanisms. Highly relevant to algorithmic fairness: suggests intersectional patterns may emerge from algorithmic dynamics (e.g., compounding biases) even when not explicitly encoded. The formal modeling approach demonstrates how analytical precision and emergent complexity can coexist. Implies fairness interventions must consider emergent effects of algorithmic systems.

  POSITION: Formal modeling of emergence. Represents philosophy of science approach to intersectionality using tools from game theory and agent-based modeling.
  },
  keywords = {intersectionality-emergence, formal-models, philosophy-of-science, High}
}

@article{clegg2012agency,
  author = {Clegg, Sue},
  title = {Agency and Ontology within Intersectional Analysis: A Critical Realist Contribution},
  journal = {Journal of Critical Realism},
  year = {2012},
  volume = {11},
  number = {3},
  pages = {309--363},
  doi = {10.1558/jcr.v11i3.309},
  note = {
  CORE ARGUMENT: Applies critical realist philosophy to intersectionality, arguing for stratified ontology distinguishing structural and cultural conditions from lived experience. Advocates separating analytical levels to avoid conflating agency with structure. Claims critical realism provides resources for explaining how intersections between gender, race, class operate in concrete historical circumstances. Emphasizes explanatory rather than merely descriptive aims.

  RELEVANCE: Provides ontological framework for multi-level analysis of intersectionality. Relevant to algorithmic fairness because it suggests categories operate differently at different levels (e.g., structural patterns vs. individual experiences), requiring different measurement strategies. The critical realist emphasis on causal mechanisms suggests algorithms should model underlying generative processes, not just surface correlations. Supports view that categories are real but stratified.

  POSITION: Critical realist ontology. Represents attempt to ground intersectionality in realist social ontology distinguishing structures, mechanisms, and events.
  },
  keywords = {critical-realism, ontology, agency-structure, Medium}
}

@incollection{haslanger2014race,
  author = {Haslanger, Sally},
  title = {Race, intersectionality, and method: a reply to critics},
  booktitle = {Symposia on Gender, Race and Philosophy},
  year = {2014},
  volume = {10},
  number = {1},
  pages = {1--14},
  url = {https://philosophy.mit.edu/wp-content/uploads/2018/04/haslanger-race-reply-to-critics.pdf},
  note = {
  CORE ARGUMENT: Responds to critiques of her ameliorative analysis approach to race and gender. Defends defining social categories by their function in hierarchical systems while acknowledging intersectional complexity. Argues ameliorative analysis can accommodate intersectionality by recognizing how different systems of oppression interact. Addresses methodological questions about balancing analytical clarity with intersectional nuance.

  RELEVANCE: Addresses whether ameliorative/functionalist definitions of categories (defining by role in oppression) can handle intersectionality. Crucial methodological question for algorithmic fairness: should categories be defined by their causal role in producing inequality (ameliorative) or by other criteria? Haslanger's response shows tension between definitional precision and intersectional openness, suggesting algorithms may need context-sensitive category definitions.

  POSITION: Defense of ameliorative analysis against intersectional critique. Represents analytical philosophy's engagement with intersectionality through social construction framework.
  },
  keywords = {ameliorative-analysis, social-construction, methodology, Medium}
}

@book{barthold2014hermeneutic,
  author = {Barthold, Lauren Swayne},
  title = {A Hermeneutic Approach to Gender and Other Social Identities},
  publisher = {Palgrave Macmillan},
  year = {2014},
  doi = {10.1057/9781137407092},
  note = {
  CORE ARGUMENT: Applies Gadamerian hermeneutics to social identities including gender, arguing they are neither purely objective/natural nor purely subjective/constructed. Proposes hermeneutic middle path where identities are historically and culturally constituted meanings that constrain without determining experience. Argues understanding social identities requires interpretive engagement with traditions and horizons of meaning rather than causal analysis.

  RELEVANCE: Represents hermeneutic alternative to both analytical and critical realist approaches. Relevant to algorithmic fairness debate by suggesting categories are fundamentally interpretive: their meaning emerges through historical dialogue, not fixed definition. This implies ontological uncertainty is inherent (categories lack stable essence) and algorithmic operationalization faces limits (meanings resist formalization). Suggests participatory interpretation rather than expert definition.

  POSITION: Gadamerian hermeneutics applied to social identity. Represents continental philosophy alternative to analytical metaphysics.
  },
  keywords = {hermeneutics, social-identity, Gadamer, Medium}
}

@article{bilge2013recent,
  author = {Bilge, Sirma},
  title = {Recent Feminist Outlooks on Intersectionality},
  journal = {Diogenes},
  year = {2013},
  volume = {57},
  number = {1},
  pages = {58--72},
  doi = {10.1177/0392192110374245},
  note = {
  CORE ARGUMENT: Surveys recent feminist approaches to intersectionality, examining it as research paradigm. Addresses methodological debates about levels of analysis (macro-structural vs. micro-interactional), ontological status of categories (essentialist vs. anti-essentialist), and scope (limited categories vs. indefinite expansion). Maps tensions between different theoretical traditions appropriating intersectionality. Argues for reflexivity about how intersectionality travels across disciplines and contexts.

  RELEVANCE: Provides overview of key theoretical tensions structuring the field, including ontological debates about category status. Useful for situating competing philosophical interpretations within broader feminist theory landscape. The levels of analysis question is crucial for algorithmic fairness: must systems model structural patterns, individual interactions, or both? The scope question addresses category proliferation problem facing algorithmic implementations.

  POSITION: Meta-theoretical survey of intersectionality research paradigms. Represents attempt to map the field's internal diversity and tensions.
  },
  keywords = {intersectionality-theory, feminist-theory, research-paradigms, Medium}
}

@article{yuval-davis2006intersectionality,
  author = {Yuval-Davis, Nira},
  title = {Intersectionality and Feminist Politics},
  journal = {European Journal of Women's Studies},
  year = {2006},
  volume = {13},
  number = {3},
  pages = {193--209},
  doi = {10.1177/1350506806065752},
  note = {
  CORE ARGUMENT: Compares 1980s British debates on race/class/gender with post-2001 intersectionality discussions. Contrasts additive models (categories as separate axes summing to total oppression) with mutually constitutive models (categories co-constructing each other). Argues for analyzing social divisions at multiple levels: individual experience, intersubjective relations, group/organization, and societal structure. Critiques both essentialist category reification and anti-essentialist category dissolution. Emphasizes ontological complexity.

  RELEVANCE: Classic statement of additive vs. mutually constitutive distinction central to metaphysical debates. Crucial for algorithmic fairness because additive models are computationally tractable (sum category effects) while mutual constitution models resist decomposition. The multi-level analysis framework suggests fairness must be assessed differently at individual, group, and structural levels. Provides historical depth to contemporary ontological debates.

  POSITION: Mutually constitutive framework with multi-level analysis. Represents European feminist theory perspective on intersectionality.
  },
  keywords = {intersectionality-theory, mutual-constitution, levels-of-analysis, High}
}

@article{nash2008rethinking,
  author = {Nash, Jennifer C.},
  title = {Re-thinking Intersectionality},
  journal = {Feminist Review},
  year = {2008},
  volume = {89},
  number = {1},
  pages = {1--15},
  doi = {10.1057/fr.2008.4},
  note = {
  CORE ARGUMENT: Offers critical assessment of intersectionality's institutionalization in feminist theory. Argues the concept has become simultaneously celebrated and ambiguous, used for contradictory purposes. Identifies three main uses: as theory of marginalized subjectivity, as theory of identity, and as general theory of all identity. Questions whether intersectionality has explanatory power or primarily names a problem requiring explanation. Calls for greater theoretical precision.

  RELEVANCE: Important meta-theoretical critique highlighting intersectionality's conceptual ambiguity. Relevant to algorithmic fairness because vagueness about what intersectionality claims makes operationalization contentious. Nash's call for precision resonates with algorithmic implementation challenges but also warns against premature formalization that obscures political stakes. Suggests need to distinguish intersectionality as analytical framework from intersectionality as normative-political commitment.

  POSITION: Critical assessment of intersectionality's theoretical status. Represents internal feminist critique concerned with conceptual clarity.
  },
  keywords = {intersectionality-critique, conceptual-analysis, feminist-theory, Medium}
}

@article{crenshaw1989demarginalizing,
  author = {Crenshaw, Kimberlé},
  title = {Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics},
  journal = {The University of Chicago Legal Forum},
  year = {1989},
  volume = {1989},
  number = {1},
  pages = {139--167},
  url = {https://chicagounbound.uchicago.edu/uclf/vol1989/iss1/8},
  note = {
  CORE ARGUMENT: Foundational text coining "intersectionality" to describe how antidiscrimination law fails Black women by treating race and sex as mutually exclusive categories. Shows how single-axis frameworks in law and feminist/antiracist theory marginalize those at the intersection. Analyzes employment discrimination cases where courts rejected Black women's claims by comparing them separately to white women and Black men. Argues intersectional subordination requires integrated analysis.

  RELEVANCE: Essential genealogical reference as the paper that introduced "intersectionality" to legal and academic discourse. Demonstrates that intersectionality emerged from practical legal problem (institutional failure to recognize compound discrimination), not abstract metaphysics. This grounds contemporary ontological debates in concrete contexts. For algorithmic fairness, Crenshaw's legal cases show how single-axis categories systematically fail intersectional subjects, motivating expanded category frameworks.

  POSITION: Foundational legal critique generating intersectionality concept. Represents Black feminist legal theory origin point for subsequent philosophical developments.
  },
  keywords = {intersectionality-origins, legal-theory, Black-feminism, High}
}

@book{haslanger2012resisting,
  author = {Haslanger, Sally},
  title = {Resisting Reality: Social Construction and Social Critique},
  year = {2012},
  publisher = {Oxford University Press},
  address = {New York},
  doi = {10.1093/acprof:oso/9780199892631.001.0001},
  note = {
  CORE ARGUMENT: Haslanger defends a realist social constructionist account of race and gender as social positions constituted by systematic relations of subordination and privilege. She argues that race and gender are real social kinds, not natural kinds, defined by one's social position within hierarchical structures rather than by biological or intrinsic properties. The book develops an ameliorative analysis that seeks to identify what race and gender ought to mean for purposes of feminist and antiracist critique, rather than simply capturing ordinary usage.

  RELEVANCE: This work is foundational for understanding practice-based accounts of social group constitution that challenge attribute-based models. Haslanger's political constructionism shows that social groups cannot be adequately specified through demographic attributes alone, since group membership is fundamentally about structural positioning within power relations. Her account directly undermines the assumption that algorithmic fairness systems can identify relevant groups by mechanically combining observable features, since the morally salient aspect of group membership (systematic subordination or privilege) is not reducible to intrinsic attributes but depends on social structures and practices. This poses a fundamental challenge to intersectional fairness approaches that treat groups as attribute-defined categories.

  POSITION: Political constructionism about social kinds - groups defined by structural positions within hierarchical social relations.
  },
  keywords = {social-construction, race, gender, social-kinds, High}
}

@book{mallon2016construction,
  author = {Mallon, Ron},
  title = {The Construction of Human Kinds},
  year = {2016},
  publisher = {Oxford University Press},
  address = {Oxford},
  doi = {10.1093/acprof:oso/9780198755678.001.0001},
  note = {
  CORE ARGUMENT: Mallon develops a sophisticated account of how human social kinds (race, gender, sexuality) are constructed through social roles that create homeostatic property clusters (HPC kinds). He argues that while these categories are socially constructed, they can nevertheless be real, stable, and explanatorily useful because social roles create systematic patterns of correlated properties through mechanisms like stereotype threat, discrimination, and social norms. The book distinguishes between "covert" social constructions (where people falsely believe the kind is natural) and "overt" ones, showing how social construction is compatible with scientific realism about human kinds.

  RELEVANCE: Mallon's HPC account provides a middle ground between pure attribute-based and pure practice-based views, showing how social practices generate stable correlations between properties. However, his account also reveals the contingency and instability of these correlations - social roles can change, categories can shift, and what counts as a relevant group depends on specific social-historical contexts. For intersectional fairness, this means that even if we can identify statistical patterns that correspond to social groups, these patterns are artifacts of contingent social practices rather than natural divisions, and the "right" groups cannot be read off from demographic data without understanding the social mechanisms that produce group-differentiated outcomes. The distinction between covert and overt construction is particularly relevant for understanding why algorithmic systems may reify problematic social categories.

  POSITION: Social role HPC kind theory - groups as homeostatic property clusters stabilized by social roles and practices.
  },
  keywords = {social-construction, human-kinds, social-roles, HPC-kinds, High}
}

@book{asta2018categories,
  author = {Ásta},
  title = {Categories We Live By: The Construction of Sex, Gender, Race, and Other Social Categories},
  year = {2018},
  publisher = {Oxford University Press},
  address = {New York},
  isbn = {978-0-19-025679-1},
  note = {
  CORE ARGUMENT: Ásta develops a conferralist framework for understanding social categories, arguing that individuals possess social properties (like gender, race, sexuality) in virtue of communal conferral rather than intrinsic features. On this view, social categories are constituted through practices of meaning-making where communities collectively confer properties onto individuals based on contextually salient features. This is fundamentally different from attribute-based accounts because the same constellation of attributes can ground different social properties in different contexts, and the conferral can be mistaken or contested. The framework emphasizes that social categories are both real and constructed through human practices of categorization.

  RELEVANCE: Ásta's conferralism directly challenges algorithmic approaches to group specification by showing that social categories cannot be derived from attributes alone, since the same attributes can be conferrally salient in different ways depending on context and community practices. For intersectional fairness, this implies that there is no algorithm-independent fact about which combinations of attributes constitute morally relevant groups - what matters is how communities confer meaning onto those combinations through situated practices. This creates fundamental indeterminacy for fairness systems that attempt to pre-specify groups based on demographic features, since the normatively relevant categories depend on interpretive practices that vary across contexts. The framework supports a practice-based rather than attribute-based approach to identifying protected groups.

  POSITION: Conferralism - social properties constituted through communal conferral practices rather than intrinsic features.
  },
  keywords = {social-construction, conferralism, social-properties, social-categories, High}
}

@article{ritchie2018social,
  author = {Ritchie, Katherine},
  title = {Social Structures and the Ontology of Social Groups},
  journal = {Philosophy and Phenomenological Research},
  year = {2018},
  volume = {99},
  number = {3},
  pages = {6 29--655},
  doi = {10.1111/phpr.12555},
  note = {
  CORE ARGUMENT: Ritchie develops a structuralist ontology of social groups centered on social structures - networks of relations that are constitutively dependent on social factors. On this view, social groups are not mere collections of individuals who share attributes, but rather structures consisting of nodes (individuals) connected by social relations (asymmetric, contextual, and socially mediated). This account accommodates diverse types of groups (gender groups, racial groups, teams, committees) while maintaining ontological distinctions between them based on the kinds of relations that constitute their structure. Crucially, the account explains why not every arbitrary collection of people counts as a social group.

  RELEVANCE: Ritchie's structuralism provides a sophisticated alternative to attribute-based group definitions that is particularly relevant for intersectional fairness. If social groups are constituted by networks of relations rather than shared attributes, then algorithmic approaches that identify groups through demographic feature combinations fundamentally misconstrue the ontology of social groups. The relational structure of a group cannot be captured by listing attributes its members possess, since what makes a collection a group is the pattern of relations between members, not just their individual properties. For ML fairness, this suggests that protected groups may not be identifiable through combinatorial methods applied to demographic features, since group membership depends on relational positioning that is not reducible to individual attributes. This creates a deep challenge for attribute-based fairness metrics.

  POSITION: Structural ontology - groups as networks of socially constituted relations, not collections defined by shared attributes.
  },
  keywords = {social-groups, structural-ontology, social-relations, High}
}

@article{thomasson2019ontology,
  author = {Thomasson, Amie L.},
  title = {The Ontology of Social Groups},
  journal = {Synthese},
  year = {2019},
  volume = {196},
  number = {12},
  pages = {4829--4845},
  doi = {10.1007/s11229-016-1185-y},
  note = {
  CORE ARGUMENT: Thomasson argues that questions about whether social groups exist have easy answers - they obviously do - but that the more interesting question is "What are social groups?" She critiques the assumption that there is a unified category "social group" that forms a natural kind subject to metaphysical inquiry, suggesting instead that we should pluralism about types of social groups. Different groups (teams, gender groups, racial groups, nations) may have different ontological structures. The paper emphasizes moving beyond existence questions to substantive questions about the variety and differences among social groups.

  RELEVANCE: Thomasson's ontological pluralism about social groups has direct implications for intersectional fairness. If there is no unified account of what social groups are, but rather a variety of different kinds of groups with different constitution conditions, then there cannot be a single algorithmic method for identifying all relevant groups. Different types of groups (gender groups, racial groups, disability groups) may require different approaches to specification. This pluralism creates indeterminacy for fairness frameworks that assume a uniform method for deriving protected groups from demographic attributes. The paper supports the view that ontological uncertainty about social groups is ineliminable, not merely a problem of insufficient philosophical analysis.

  POSITION: Ontological pluralism - different types of social groups may have fundamentally different metaphysical structures.
  },
  keywords = {social-groups, ontological-pluralism, social-ontology, High}
}

@article{mallon2007field,
  author = {Mallon, Ron},
  title = {A Field Guide to Social Construction},
  journal = {Philosophy Compass},
  year = {2007},
  volume = {2},
  number = {1},
  pages = {93--108},
  doi = {10.1111/j.1747-9991.2006.00051.x},
  note = {
  CORE ARGUMENT: This influential survey distinguishes different varieties of social construction claims and maps the conceptual territory. Mallon identifies causal, constitutive, and representational forms of social construction, showing how claims that X is socially constructed can mean different things in different contexts. He clarifies what makes construction claims interesting (typically contingency, controlability, or debunking implications) and distinguishes social construction from related concepts like conventionality and cultural variability. The paper provides analytical tools for evaluating constructionist arguments and identifies common fallacies.

  RELEVANCE: This conceptual taxonomy is valuable for understanding exactly what is at stake in debates about whether social groups are socially constructed. For intersectional fairness, the distinction between causal and constitutive construction matters: if groups are causally constructed (social practices create property correlations) this suggests attribute-based identification may work; if groups are constitutively constructed (social practices constitute what it is to be a member), then attributes alone cannot capture group membership. The paper's framework helps clarify that the challenge to algorithmic group specification is not merely that groups are socially constructed, but specifically that they are constitutively (not just causally) constructed through practices that cannot be reduced to attribute possession.

  POSITION: Conceptual analysis and taxonomy of varieties of social construction claims.
  },
  keywords = {social-construction, conceptual-analysis, taxonomy, Medium}
}

@book{epstein2015ant,
  author = {Epstein, Brian},
  title = {The Ant Trap: Rebuilding the Foundations of the Social Sciences},
  year = {2015},
  publisher = {Oxford University Press},
  address = {New York},
  doi = {10.1093/acprof:oso/9780199381104.001.0001},
  note = {
  CORE ARGUMENT: Epstein develops a framework distinguishing "grounding" (what makes social facts obtain) from "anchoring" (what sets the grounding conditions in place). He argues that social ontology requires explaining both aspects, and that dominant individualist approaches fail to explain anchoring. The book develops a sophisticated model for understanding how social facts depend on both individual-level facts and frame-setting social facts (rules, norms, conventions). This framework is applied to analyze groups, institutions, and collective intentionality, showing how social facts can be real without being reducible to individual-level facts.

  RELEVANCE: Epstein's grounding/anchoring distinction is valuable for understanding why attribute-based group specification may fail. Even if certain demographic attributes ground group membership in specific contexts, what anchors those grounding conditions (why those attributes matter for that group) depends on social-historical facts about institutions, practices, and norms that cannot be discovered algorithmically from the data. For intersectional fairness, this suggests that specifying groups requires not just identifying current property correlations (grounding) but understanding the frame-setting facts (anchoring) that make certain attributes relevant for group membership. Algorithmic approaches that work from demographic data alone cannot access anchoring facts, creating fundamental limitations on computational group specification.

  POSITION: Grounding/anchoring framework - social facts depend on both ground-level facts and frame-setting facts that anchor grounding conditions.
  },
  keywords = {social-ontology, grounding, anchoring, social-facts, Medium}
}

@misc{epstein2025social,
  author = {Epstein, Brian},
  title = {Social Ontology},
  year = {2025},
  howpublished = {Cambridge Elements in Metaphysics},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781009290562},
  note = {
  CORE ARGUMENT: This recent overview of social ontology provides a systematic map of the field, distinguishing theories of social construction from theories that characterize products of social construction. Epstein uses running examples (property/ownership, race, collective attitudes) to illustrate different theoretical approaches and their implications. The element emphasizes that social ontology addresses not just what social facts are, but what makes them social facts and how they relate to non-social facts. It provides a taxonomy of major theoretical approaches and identifies key open questions in the field.

  RELEVANCE: As a recent, authoritative survey, this element is valuable for understanding the current state of debates about social ontology that bear on intersectional fairness. The systematic treatment of different approaches to understanding race and social groups helps situate specific arguments from Haslanger, Mallon, and others within broader theoretical frameworks. For the fairness project, the element's emphasis on distinguishing "what are the products of social construction" from "how are they constructed" maps onto the distinction between identifying which groups exist and explaining how group boundaries are determined - both of which create challenges for algorithmic group specification.

  POSITION: Comprehensive survey of social ontology with taxonomy of theoretical approaches.
  },
  keywords = {social-ontology, survey, social-construction, High}
}

@book{appiah1996color,
  author = {Appiah, Kwame Anthony and Gutmann, Amy},
  title = {Color Conscious: The Political Morality of Race},
  year = {1996},
  publisher = {Princeton University Press},
  address = {Princeton, NJ},
  isbn = {978-0-691-02661-6},
  note = {
  CORE ARGUMENT: Appiah's influential essay "Race, Culture, Identity: Misunderstood Connections" argues that races, in the biological essentialist sense that  has historically dominated racial thinking, do not exist. However, racial identities are socially real as sources of collective identity and solidarity. He distinguishes "racial identity" (a socially constructed group membership) from outdated biological notions of race, while acknowledging the political and personal significance of racial identification. Appiah emphasizes the complexity of racial identity, arguing against both biological essentialism and simple social construction views that ignore how race shapes lived experience.

  RELEVANCE: Appiah's account of race as both non-biological and socially real is relevant for understanding why algorithmic systems that treat race as a simple demographic attribute may fail. If racial categories are constituted through complex processes of identification, recognition, and meaning-making rather than biological facts, then computational approaches that operationalize race through check-boxes may systematically mischaracterize the phenomenon. For intersectional fairness, this suggests that standard demographic categories used in ML systems may not track the normatively relevant social groups, since they abstract away from the interpretive and identificatory dimensions that constitute racial identity. The gap between census categories and lived racial identity creates ontological uncertainty about which groups matter for fairness.

  POSITION: Anti-biological-essentialism combined with recognition of social reality and political significance of racial identity.
  },
  keywords = {race, identity, social-construction, anti-essentialism, Medium}
}

@article{griffith2018social,
  author = {Griffith, Aaron M.},
  title = {Social Construction and Grounding},
  journal = {Philosophy and Phenomenological Research},
  year = {2018},
  volume = {97},
  number = {2},
  pages = {393--409},
  doi = {10.1111/phpr.12376},
  note = {
  CORE ARGUMENT: Griffith develops a ground-theoretic account of social construction, arguing that facts are socially constructed when they are grounded in (depend on) social facts. He distinguishes this from causal accounts and from constitutive accounts that rely on older notions of constitution. The paper argues that metaphysical grounding provides the right framework for understanding constructionist claims, allowing us to distinguish what is fundamental from what is derivative in the social world. Griffith defends this approach against objections and shows how it applies to paradigm cases like gender and race.

  RELEVANCE: The grounding-theoretic framework helps clarify what is at stake when we say social groups are constructed. If group membership facts are grounded in (metaphysically depend on) social-structural facts rather than intrinsic individual attributes, then algorithmic approaches that work from individual-level demographic features to group membership reverse the order of metaphysical dependence. This suggests a fundamental mismatch between the ontology of social groups and computational methods that treat groups as defined by attribute combinations. For fairness, understanding the grounding structure of social groups matters for identifying which features are truly constitutive of group membership versus merely correlated with it.

  POSITION: Grounding-theoretic account - social construction as metaphysical grounding/dependence on social facts.
  },
  keywords = {social-construction, grounding, metaphysics, Medium}
}

@article{haslanger2005talking,
  author = {Haslanger, Sally},
  title = {What Are We Talking About? The Semantics and Politics of Social Kinds},
  journal = {Hypatia},
  year = {2005},
  volume = {20},
  number = {4},
  pages = {10--26},
  doi = {10.1111/j.1527-2001.2005.tb00533.x},
  note = {
  CORE ARGUMENT: Haslanger addresses the methodology of analyzing social kinds, defending an "ameliorative" approach that asks what concepts of race, gender, etc. would be most useful for feminist and antiracist purposes, rather than simply describing ordinary usage. She argues that ordinary concepts of social kinds may themselves be ideological, obscuring relations of power and subordination. The ameliorative analysis of gender as "women are those systematically subordinated on the basis of observed or imagined bodily features presumed to be evidence of reproductive capacities" captures what matters for feminist critique while potentially diverging from folk concepts.

  RELEVANCE: Haslanger's ameliorative methodology is directly relevant to debates about which groups matter for fairness analysis. If the point of identifying groups is normative (to address injustice), then the relevant groups may not be those defined by common usage or demographic conventions, but those that track structural positions of subordination and privilege. This challenges algorithmic fairness approaches that operationalize groups through standard demographic categories, since those categories may not capture the normatively salient divisions. The paper suggests that there may be no purely descriptive, politically neutral way to specify which groups matter for fairness - group specification is inevitably a normative and political question.

  POSITION: Ameliorative analysis - concepts of social kinds should be evaluated for their usefulness in critique, not just descriptive accuracy.
  },
  keywords = {social-kinds, ameliorative-analysis, methodology, gender, race, High}
}

@article{mason2020social,
  author = {Mason, Rebecca and Ritchie, Katherine},
  title = {Social Ontology},
  journal = {The Routledge Handbook of Metametaphysics},
  year = {2020},
  pages = {362--375},
  doi = {10.4324/9781315112596-24},
  note = {
  CORE ARGUMENT: This handbook chapter surveys key questions and approaches in social ontology, arguing that social entities deserve serious metaphysical attention despite being historically excluded from mainstream metaphysics. The authors examine whether the exclusion of social entities is philosophically warranted or reflects historical bias, concluding that social metaphysics should be central to metaphysical inquiry. They argue that taking social entities seriously may lead us to rethink core metaphysical assumptions about preferring intrinsic over relational properties, and independent over dependent features.

  RELEVANCE: The argument that social entities require metaphysical frameworks different from those developed for natural entities is relevant for understanding why computational approaches developed for physical systems may not transfer to social domains. If social groups are fundamentally relational, dependent, and non-individualistic (as the authors argue), then methods that assume groups can be defined through intrinsic individual properties will systematically fail. For intersectional fairness, this suggests that standard approaches to operationalizing protected groups through demographic attributes may rest on inappropriate metaphysical assumptions imported from non-social domains.

  POSITION: Meta-metaphysical argument for centrality of social ontology and inadequacy of standard metaphysical frameworks for social entities.
  },
  keywords = {social-ontology, metametaphysics, social-entities, Medium}
}

@article{diazleon2015what,
  author = {Díaz-León, Esa},
  title = {What Is Social Construction?},
  journal = {European Journal of Philosophy},
  year = {2015},
  volume = {23},
  number = {4},
  pages = {1137--1152},
  doi = {10.1111/ejop.12033},
  note = {
  CORE ARGUMENT: Díaz-León provides a systematic analysis of what it means to say something is socially constructed, distinguishing several different types of construction claims with different philosophical and political implications. She clarifies the distinction between claims about the etiology of a feature (how it came to be), claims about its modal status (that it could have been otherwise), and claims about its dependency (that it depends on social factors for its continued existence). The paper shows how different construction claims support different normative conclusions about changeability and responsibility.

  RELEVANCE: This analytical work helps clarify exactly what is at stake when we say social groups are constructed. For intersectional fairness, different notions of social construction have different implications. If groups are constructed in the sense of being contingent and changeable, this suggests their boundaries are not fixed algorithmic targets. If they are constructed in the sense of depending on ongoing social practices, this means group membership cannot be determined solely from static demographic data. The paper's taxonomy helps identify which varieties of social construction pose challenges for computational group specification.

  POSITION: Conceptual analysis distinguishing varieties of social construction claims and their implications.
  },
  keywords = {social-construction, conceptual-analysis, definition, Low}
}

@article{popescusarry2023discrimination,
  author = {Popescu-Sarry, Diana},
  title = {Discrimination Without Traits: From Social Construction to the Politics of Discrimination},
  journal = {American Political Science Review},
  year = {2023},
  volume = {118},
  number = {2},
  pages = {890--902},
  doi = {10.1017/S0003055423000679},
  note = {
  CORE ARGUMENT: Popescu-Sarry develops a constructionist account of discrimination arguing that understanding grounds of discrimination (race, gender, etc.) as socially constructed reveals discrimination to be not a discrete event but an ongoing process of (re)negotiating social reality. She argues that standard theories of discrimination that take protected characteristics as given fail to account for how discrimination itself constitutes and reconstitutes those very characteristics. The paper shows how a constructionist view reveals discrimination as fundamentally political, involving struggles over how social reality is constructed.

  RELEVANCE: This recent work directly connects social construction debates to discrimination and fairness, making it highly relevant for intersectional fairness. If discrimination is not applied to pre-existing groups but rather constitutes groups through the very process of differential treatment, then algorithmic fairness systems that take groups as pre-specified inputs may fundamentally misunderstand the phenomenon. The paper suggests that fairness frameworks cannot avoid engaging with the politics of how groups are constructed, since specifying protected groups is itself a site of political contest. This creates deep challenges for technical approaches that assume group boundaries are determinate inputs rather than contested political constructions.

  POSITION: Constructionist account of discrimination as politics of social reality constitution, not application to pre-existing groups.
  },
  keywords = {discrimination, social-construction, politics, fairness, High}
}

@article{khalidi2015three,
  author = {Khalidi, Muhammad Ali},
  title = {Three Kinds of Social Kinds},
  journal = {Philosophy and Phenomenological Research},
  year = {2015},
  volume = {90},
  number = {1},
  pages = {96--112},
  doi = {10.1111/phpr.12020},
  note = {
  CORE ARGUMENT: Khalidi distinguishes three types of social kinds based on their dependence relations to human attitudes. Type 1 social kinds exist independently of human attitudes toward them (e.g., economic recessions); Type 2 depend on general attitudes but not attitudes toward particular instances (e.g., money); Type 3 depend on attitudes toward their particular instances (e.g., celebrities). This taxonomy cuts across standard distinctions and shows that social kinds are metaphysically heterogeneous. Different types of social kinds have different stability conditions and different relationships to human practices of categorization.

  RELEVANCE: Khalidi's taxonomy suggests that different protected groups in fairness contexts may be different types of social kinds, requiring different approaches to specification. Gender and race might be Type 3 kinds (where someone's membership depends partly on how they are regarded), while socioeconomic status might be Type 1 (less dependent on attitudes). This heterogeneity creates challenges for uniform algorithmic approaches to identifying protected groups, since different groups may have fundamentally different constitution conditions. The paper supports ontological pluralism about social groups that undermines one-size-fits-all specification methods.

  POSITION: Taxonomy of social kinds based on attitude-dependence, arguing for metaphysical heterogeneity.
  },
  keywords = {social-kinds, taxonomy, attitude-dependence, Low}
}

@inproceedings{tal2023target,
  author = {Tal, Eran},
  title = {Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare},
  booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2023},
  doi = {10.1145/3600211.3604678},
  arxivId = {2308.02081},
  note = {
  CORE ARGUMENT: Introduces "target specification bias" - a pervasive source of ML bias that arises when the operationalization of the target variable does not match its definition by decision makers. This occurs because decision makers are typically interested in predicting outcomes of counterfactual scenarios rather than actual healthcare scenarios. Drawing on metrology (the science of measurement), Tal argues this bias persists independently of data limitations and health disparities.

  RELEVANCE: This paper provides the most direct philosophical analysis of operationalization problems in fairness metrics, grounded in measurement theory. The concept of target specification bias captures precisely the gap between what fairness metrics formalize and what stakeholders actually care about. The connection to counterfactual reasoning is especially relevant for fairness auditing, where we want to know what would happen under fair treatment, not just what did happen.

  POSITION: Measurement-theoretic critique showing that operationalization failures are conceptual, not merely technical problems. Represents a philosopher of science analyzing ML fairness through the lens of metrology.
  },
  keywords = {measurement-theory, construct-validity, target-bias, High}
}

@inproceedings{blodgett2021stereotyping,
  author = {Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna M.},
  title = {Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  year = {2021},
  pages = {1004--1015},
  doi = {10.18653/v1/2021.acl-long.81},
  note = {
  CORE ARGUMENT: Applies measurement modeling lens from social sciences to NLP fairness benchmarks for stereotyping. Through systematic analysis of four benchmarks for language modeling and coreference resolution, reveals that benchmarks frequently lack clear articulations of what is being measured. Identifies ambiguities and unstated assumptions in how benchmarks conceptualize and operationalize stereotyping, threatening their validity as measurement models.

  RELEVANCE: Provides the most thorough application of measurement theory to fairness evaluation. The "measurement modeling lens" framework is directly applicable to fairness auditing more broadly - it shows how to systematically assess whether a metric actually measures what it claims to measure. The emphasis on conceptualization vs. operationalization is key for understanding construct validity problems.

  POSITION: Applies social science measurement theory to critique fairness benchmarks. Demonstrates that many supposed fairness measurements lack construct validity due to poor conceptualization.
  },
  keywords = {measurement-modeling, construct-validity, fairness-benchmarks, High}
}

@inproceedings{jacobs2020meaning,
  author = {Jacobs, Abigail Z. and Blodgett, Su Lin and Barocas, Solon and Daumé, Hal and Wallach, Hanna M.},
  title = {The meaning and measurement of bias: lessons from natural language processing},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  year = {2020},
  doi = {10.1145/3351095.3375671},
  note = {
  CORE ARGUMENT: Introduces measurement modeling from quantitative social sciences as framework for examining how values enter computational systems. Shows how unobservable theoretical constructs (like "creditworthiness" or "toxicity") are turned into measurable quantities and how this process may introduce fairness-related harms. Demonstrates how to assess construct validity and reliability to detect harms arising from mismatches between constructs and their operationalizations.

  RELEVANCE: Foundational tutorial establishing measurement modeling as the appropriate framework for fairness evaluation. Provides systematic method for unpacking the gap between normative concepts and their operationalizations. The emphasis on unobservable theoretical constructs is crucial - fairness is precisely such a construct, and this paper shows why operationalizing it is conceptually challenging.

  POSITION: Establishes measurement theory as the appropriate framework for understanding fairness in AI. Shows that construct validity problems are central to fairness concerns.
  },
  keywords = {measurement-modeling, construct-validity, debiasing, High}
}

@article{long2020fairness,
  author = {Long, Robert},
  title = {Fairness in machine learning: against false positive rate equality as a measure of fairness},
  journal = {Journal of Moral Philosophy},
  year = {2020},
  doi = {10.1163/17455243-20213439},
  arxivId = {2007.02890},
  note = {
  CORE ARGUMENT: Provides ethical framework for evaluating fairness measures, arguing that false positive rate equality is morally irrelevant and does not actually measure fairness despite initial appearances. Shows that just because a metric is mathematically well-defined and seems intuitively important doesn't mean it tracks a normatively significant property. The inevitable tradeoff between calibration and FPR equality reveals that one of these must not be measuring real fairness.

  RELEVANCE: Exemplifies the conceptual analysis needed to evaluate whether fairness metrics measure what matters. Demonstrates that mathematical rigor doesn't guarantee normative validity. The argument that FPR equality is "morally irrelevant" despite seeming fair is precisely the kind of construct validity critique needed for fairness auditing.

  POSITION: Normative philosophical critique showing that popular fairness metrics may not measure fairness at all. Argues against accepting metrics without philosophical justification.
  },
  keywords = {fairness-metrics, normative-critique, construct-validity, High}
}

@article{baumann2022distributive,
  author = {Baumann, J. and Hertweck, Corinna and Loi, M. and Heitz, Christoph},
  title = {Distributive Justice as the Foundational Premise of Fair ML: Unification, Extension, and Interpretation of Group Fairness Metrics},
  journal = {ArXiv},
  year = {2022},
  volume = {abs/2206.02897},
  doi = {10.48550/arXiv.2206.02897},
  arxivId = {2206.02897},
  note = {
  CORE ARGUMENT: Proposes comprehensive framework linking group fairness metrics to theories of distributive justice. Shows that different fairness metrics differ in how they measure benefit/harm and what moral claims to benefits they assume. This unifying framework reveals the normative choices associated with standard metrics and provides structure for expanding beyond standard parity-based metrics that may harm marginalized groups.

  RELEVANCE: Demonstrates that fairness metrics implicitly embody contested philosophical commitments about distributive justice. Shows that the choice between metrics is not merely technical but reflects fundamental normative disagreements. This supports the claim that fairness operationalization faces inherent conceptual challenges due to the contested nature of fairness itself.

  POSITION: Philosophical unification showing that fairness metrics encode contested normative theories. Argues for making these commitments explicit rather than treating metric choice as purely technical.
  },
  keywords = {fairness-metrics, distributive-justice, normative-foundations, High}
}

@inproceedings{delobelle2022measuring,
  author = {Delobelle, Pieter and Tokpo, E. and Calders, T. and Berendt, Bettina},
  title = {Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics},
  year = {2022},
  pages = {1693--1706},
  doi = {10.18653/v1/2022.naacl-main.122},
  note = {
  CORE ARGUMENT: Systematic empirical evaluation showing that bias/fairness metrics for language models are not compatible with each other and highly depend on template choices, seed selection, and embedding choices. Finds no tangible evidence of intrinsic bias metrics relating to extrinsic bias in downstream tasks. Concludes that fairness evaluation remains challenging because measurement choices are inherently subjective.

  RELEVANCE: Provides empirical evidence that fairness metrics lack construct validity - they don't converge on measuring the same underlying construct. The finding that metric values are artifacts of arbitrary choices (templates, seeds, embeddings) rather than properties of what's being measured undermines their validity. This is exactly the kind of evidence needed to show operationalization is problematic.

  POSITION: Empirical demonstration that fairness metrics are "biased rulers" - they don't reliably measure what they claim to measure. Argues for focusing on downstream task fairness rather than intrinsic metrics.
  },
  keywords = {fairness-metrics, measurement-validity, empirical-evaluation, High}
}

@article{cao2022intrinsic,
  author = {Cao, Yang Trista and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul and Kumar, Varun and Dhamala, J. and Galstyan, A.},
  title = {On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations},
  journal = {ArXiv},
  year = {2022},
  volume = {abs/2203.13928},
  doi = {10.48550/arXiv.2203.13928},
  arxivId = {2203.13928},
  note = {
  CORE ARGUMENT: Extensive correlation study across 19 contextualized language models finds that intrinsic and extrinsic fairness metrics do not necessarily correlate, even when correcting for metric misalignments, evaluation dataset noise, and experimental configuration. This disconnect persists across different bias notions, suggesting intrinsic metrics may not measure what matters for downstream fairness.

  RELEVANCE: Provides strong empirical evidence for construct validity problems - if intrinsic fairness metrics don't predict extrinsic fairness, they may not be measuring fairness at all. This is a measurement validity failure: the operationalization (intrinsic metrics) doesn't track the construct of interest (actual fairness in applications). Critical for arguments about operationalization challenges.

  POSITION: Empirical evidence that common fairness metrics lack predictive validity. Challenges the assumption that measuring "bias" in models translates to measuring unfairness in applications.
  },
  keywords = {fairness-metrics, construct-validity, correlation-study, High}
}

@inproceedings{truong2025valid,
  author = {Truong, K. and Zimmermann, Annette and Heidari, Hoda},
  title = {Toward Valid Measurement Of (Un)fairness For Generative AI: A Proposal For Systematization Through The Lens Of Fair Equality of Chances},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2025},
  doi = {10.48550/arXiv.2507.04641},
  arxivId = {2507.04641},
  note = {
  CORE ARGUMENT: Addresses the neglected systematization of the unfairness construct in Generative AI fairness measurement. Proposes novel framework based on Fair Equality of Chances from political philosophy, decomposing unfairness into harm/benefit, morally arbitrary factors, and morally decisive factors. Argues that lack of proper construct systematization leads to invalid measurements that mischaracterize unfairness by overlooking contextual nuances.

  RELEVANCE: Most recent philosophical work directly addressing construct validity in fairness measurement. Shows that valid measurement requires prior systematization of the construct being measured. The framework demonstrates how political philosophy (Fair Equality of Chances) can guide operationalization, connecting to broader debates about contested concepts and formalization limits.

  POSITION: Argues that construct validity problems in fairness measurement stem from insufficient systematization. Proposes philosophical framework to guide valid operationalization.
  },
  keywords = {construct-validity, measurement-systematization, generative-AI, High}
}

@inproceedings{bean2025measuring,
  author = {Bean, Andrew M. and Kearns, R. and Romanou, Angelika and Hafner, Franziska Sofia and Mayne, Harry and Batzner, Jan and Foroutan, Negar and Schmitz, Chris and Korgul, Karolina and Batra, Hunar and Deb, Oishi and Beharry, Emma and Emde, Cornelius and Foster, Thomas and Gausen, Anna and Grandury, María and Han, Simeng and Hofmann, Valentin and Ibrahim, Lujain and Kim, Hazel and Kirk, Hannah Rose and Lin, Fangru and Liu, Gabrielle Kaili-May and Luettgau, Lennart and Magomere, Jabez and Rystrøm, Jonathan and Sotnikova, Anna and Yang, Yushi and Zhao, Yilun and Bibi, Adel and Bosselut, A. and Clark, Ronald and Cohan, Arman and Foerster, Jakob and Gal, Yarin and Hale, Scott A. and Raji, Inioluwa Deborah and Summerfield, Chris and Torr, Philip H. S. and Ududec, C. and Rocher, Luc and Mahdi, Adam},
  title = {Measuring what Matters: Construct Validity in Large Language Model Benchmarks},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2511.04703},
  doi = {10.48550/arXiv.2511.04703},
  arxivId = {2511.04703},
  note = {
  CORE ARGUMENT: Systematic review of 445 LLM benchmarks by 29 expert reviewers reveals patterns undermining construct validity across benchmarks for phenomena like "safety" and "robustness". Measuring complex abstract phenomena requires strong construct validity - having measures that represent what matters. Provides eight key recommendations and detailed guidance for developing valid LLM benchmarks.

  RELEVANCE: Most comprehensive recent examination of construct validity in AI evaluation. While focused on LLMs, the measurement validity principles apply directly to fairness metrics. The finding that benchmarks lack "clear articulations of what is being measured" parallels Blodgett's critique of fairness benchmarks. Essential for arguments about why operationalizing contested concepts is hard.

  POSITION: Large-scale empirical study showing that AI evaluation broadly suffers from construct validity problems. Emphasizes that measuring abstract phenomena requires careful attention to measurement theory.
  },
  keywords = {construct-validity, LLM-benchmarks, measurement-theory, High}
}

@inproceedings{smith2023scoping,
  author = {Smith, Jessie J. and Beattie, Lex and Cramer, H.},
  title = {Scoping Fairness Objectives and Identifying Fairness Metrics for Recommender Systems: The Practitioners' Perspective},
  booktitle = {Proceedings of the ACM Web Conference 2023},
  year = {2023},
  doi = {10.1145/3543507.3583204},
  note = {
  CORE ARGUMENT: Through literature review of 24 papers and 15 semi-structured practitioner interviews, reveals the complexity practitioners face in operationalizing fairness. The proliferation of fairness metrics creates a complex decision space where practitioners lack guidance on picking metrics appropriate to their context. Decision-tree framework helps scope fairness objectives, but highlights need for more decision-making support.

  RELEVANCE: Provides practitioner perspective on operationalization challenges. Shows that even when metrics are well-defined, practitioners struggle to determine which measure what matters in their context. This is evidence that construct validity problems are not just theoretical - they create real obstacles to deploying fairness auditing in practice.

  POSITION: Practitioner-focused work showing that metric proliferation creates operationalization challenges. Need for decision support suggests metrics alone don't solve fairness problems.
  },
  keywords = {fairness-metrics, practitioner-challenges, operationalization, Medium}
}

@inproceedings{saha2019measuring,
  author = {Saha, Debjani and Schumann, Candice and McElfresh, Duncan C. and Dickerson, John P. and Mazurek, Michelle L. and Tschantz, Michael Carl},
  title = {Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  year = {2019},
  pages = {8377--8387},
  note = {
  CORE ARGUMENT: Empirical study of how non-experts comprehend fairness metrics reveals significant misunderstandings. Even when metrics are mathematically well-defined, stakeholders may misinterpret what they measure. This comprehension gap undermines the ability of fairness metrics to support informed decision-making about system fairness.

  RELEVANCE: Shows that construct validity problems extend beyond metric design to metric interpretation. Even if a metric validly measures some aspect of fairness, stakeholders may not understand what it measures. This is another operationalization challenge - the gap between formal definitions and stakeholder understanding.

  POSITION: Empirical evidence that fairness metrics face interpretation challenges beyond technical definition. Comprehension gaps undermine their practical utility.
  },
  keywords = {fairness-metrics, comprehension, stakeholder-understanding, Medium}
}

@inproceedings{constantin2022algorithmic,
  author = {Constantin, Rares and Dück, Moritz and Alexandrov, Anton and Matošević, Patrik and Keidar, Daphna and El-Assady, Mennatallah},
  title = {How Do Algorithmic Fairness Metrics Align with Human Judgement? A Mixed-Initiative System for Contextualized Fairness Assessment},
  booktitle = {Proceedings of the 2022 IEEE Workshop on TRust and EXpertise in Visual Analytics (TREX)},
  year = {2022},
  pages = {1--7},
  doi = {10.1109/TREX57753.2022.00005},
  note = {
  CORE ARGUMENT: Develops mixed-initiative system (FairAlign) where laypeople assess fairness through interactive visualizations, then compares their judgments with automated metrics. Finds significant gaps between algorithmic metrics and human fairness judgments. Cultural and perceptual biases mean metrics often fail to capture what people perceive as fair or unfair.

  RELEVANCE: Provides empirical evidence that fairness metrics don't align with human judgments - a fundamental construct validity failure. If metrics don't capture human notions of fairness, they're not measuring what matters. This supports the argument that operationalization inevitably loses something important from the normative concept.

  POSITION: Empirical work showing misalignment between algorithmic metrics and human fairness judgments. Argues for incorporating human judgment in fairness assessment.
  },
  keywords = {fairness-metrics, human-judgment, construct-validity, Medium}
}

@article{amigo2023unifying,
  author = {Amigó, Enrique and Deldjoo, Yashar and Mizzaro, Stefano and Bellogín, Alejandro},
  title = {A unifying and general account of fairness measurement in recommender systems},
  journal = {Information Processing \& Management},
  year = {2023},
  volume = {60},
  pages = {103115},
  doi = {10.1016/j.ipm.2022.103115},
  note = {
  CORE ARGUMENT: Proposes general framework for fairness measurement in recommender systems that unifies diverse fairness notions. Shows that many fairness metrics share common mathematical structure but measure different aspects of fairness. Framework helps clarify what each metric measures and when it's appropriate.

  RELEVANCE: Demonstrates that metric diversity partly reflects the multi-faceted nature of fairness - different metrics measure legitimately different things. But this raises a construct validity question: if fairness metrics measure different things, which (if any) measure "fairness" itself? Supports the view that fairness is a contested concept resisting single operationalization.

  POSITION: Unifying framework revealing that metric diversity reflects fairness's conceptual complexity. Helps clarify what different metrics measure.
  },
  keywords = {fairness-metrics, recommender-systems, unifying-framework, Medium}
}

@article{mehrotra2022revisiting,
  author = {Mehrotra, Anay and Sachs, Jeffery and Celis, L. E.},
  title = {Revisiting Group Fairness Metrics: The Effect of Networks},
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  year = {2022},
  volume = {6},
  pages = {1--29},
  doi = {10.1145/3555100},
  note = {
  CORE ARGUMENT: Shows that standard group fairness metrics fail to account for network structure, leading to spurious fairness evaluations. Ignoring networks can miss imbalances in influence and reach or overlook interaction biases. Proposes network-aware fairness metrics that measure access distribution and inter-group interaction patterns.

  RELEVANCE: Demonstrates that context matters for valid measurement - what counts as a valid fairness measure depends on whether network effects are relevant. This supports the view that fairness operationalization requires contextual sensitivity that resists universal formalization. Standard metrics have poor construct validity when networks matter.

  POSITION: Shows that fairness measurement must account for relevant contextual factors like network structure. Standard metrics lack construct validity in networked settings.
  },
  keywords = {fairness-metrics, network-effects, context-dependence, Medium}
}

@inproceedings{sahlgren2024action,
  author = {Sahlgren, Otto},
  title = {Action-guidance and AI ethics: the case of fair machine learning},
  journal = {AI and Ethics},
  year = {2024},
  volume = {5},
  pages = {1019--1031},
  doi = {10.1007/s43681-024-00437-2},
  note = {
  CORE ARGUMENT: Identifies "action-guidance gap" in AI ethics where fairness principles fail to guide practice due to ambiguous operationalization and implementation obstacles. Fair ML frameworks and toolkits prove hard to apply because of unclear operationalization of principles like fairness, equality, and justice, plus practical barriers like lack of evaluation data.

  RELEVANCE: Connects operationalization challenges to practical action-guidance problems. Shows that construct validity issues (ambiguous operationalization) directly undermine the ability of fairness principles to guide development. This is evidence that operationalization problems are not merely academic - they prevent fairness from being implemented.

  POSITION: Identifies operationalization ambiguity as key barrier to implementing fairness in practice. Argues for philosophical work on action-guidance.
  },
  keywords = {action-guidance, operationalization, implementation-challenges, Medium}
}

@inproceedings{xiao2023evaluating,
  author = {Xiao, Ziang and Zhang, Susu and Lai, Vivian and Liao, Q.},
  title = {Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  year = {2023},
  pages = {10967--10982},
  doi = {10.18653/v1/2023.emnlp-main.676},
  arxivId = {2305.14889},
  note = {
  CORE ARGUMENT: Proposes MetricEval framework informed by measurement theory from educational test design for evaluating NLG evaluation metrics. Formalizes sources of measurement error and offers statistical tools for assessing reliability and validity based on empirical data. Allows quantifying uncertainty in metrics to better interpret results.

  RELEVANCE: Demonstrates systematic application of measurement theory to evaluate metrics themselves - meta-evaluation. Shows how to empirically assess whether metrics are valid and reliable. The framework is broadly applicable beyond NLG to fairness metrics. Provides concrete methods for testing construct validity claims.

  POSITION: Applies educational measurement theory to develop systematic framework for metric evaluation. Emphasizes reliability and validity as fundamental metric properties.
  },
  keywords = {measurement-theory, metric-evaluation, validity-assessment, Medium}
}

@article{buijsman2023navigating,
  author = {Buijsman, Stefan},
  title = {Navigating fairness measures and trade-offs},
  journal = {AI and Ethics},
  year = {2023},
  volume = {4},
  pages = {1323--1334},
  doi = {10.1007/s43681-023-00318-0},
  arxivId = {2307.08484},
  note = {
  CORE ARGUMENT: Uses Rawls's justice as fairness to create principled basis for navigating fairness measures and accuracy trade-offs. Shows that mathematical impossibility of optimizing all fairness metrics simultaneously creates need for substantive normative theory to guide metric choice. Proposes focusing on most vulnerable groups and metrics with biggest impact on them.

  RELEVANCE: Addresses the problem that metric incompatibility requires normative theory to guide operationalization choices. Shows that technical considerations alone cannot determine which metrics to use - need philosophical framework. Demonstrates why operationalizing fairness requires engagement with contested normative theories like Rawlsian justice.

  POSITION: Argues that metric incompatibility necessitates normative philosophical theory to guide choices. Proposes Rawlsian framework as substantive solution.
  },
  keywords = {fairness-metrics, metric-tradeoffs, Rawlsian-justice, Medium}
}

@article{benbouzid2023fairness,
  author = {Benbouzid, Bilel},
  title = {Fairness in machine learning from the perspective of sociology of statistics: How machine learning is becoming scientific by turning its back on metrological realism},
  booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2023},
  doi = {10.1145/3593013.3593974},
  note = {
  CORE ARGUMENT: From sociology of statistics perspective, examines how FairML researchers confront dual demands: reliable computational techniques and transparency about politically situated quantification. Shows FairML tends toward "trained judgment" objectivity - reasonably partial justification from designer that is itself politically situated. This represents shift away from metrological realism.

  RELEVANCE: Provides sociological perspective on the politics of fairness measurement. Shows that fairness operationalization is inherently political - involves "politically situated quantification" that cannot be fully formalized. This supports the view that contested concepts resist neutral operationalization because operationalization itself involves normative choices.

  POSITION: Sociological analysis showing fairness measurement is politically situated. Argues against metrological realism in fairness measurement.
  },
  keywords = {sociology-of-statistics, metrological-realism, politics-of-measurement, Low}
}

@inproceedings{andrus2019towards,
  author = {Andrus, McKane and Gilbert, T.},
  title = {Towards a Just Theory of Measurement: A Principled Social Measurement Assurance Program for Machine Learning},
  booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2019},
  doi = {10.1145/3306618.3314275},
  note = {
  CORE ARGUMENT: Proposes social measurement assurance program (sMAP) where ML serves as quality assurance for institutional measurement practices by revealing when measures fail to capture intended constructs. Rather than codifying fairness into ML directly, use ML to expose epistemic fault lines in existing measurement. Applies Rawlsian principles to develop provisional "just theory of measurement."

  RELEVANCE: Offers alternative view of ML fairness as institutional measurement auditing rather than individual decision fairness. Emphasizes that measurement validity is social and political, not merely technical. The sMAP framework shows how measurement failures connect to fairness harms, linking measurement theory to justice theory.

  POSITION: Proposes ML as tool for auditing institutional measurement practices. Connects measurement validity to Rawlsian justice framework.
  },
  keywords = {measurement-theory, social-measurement, Rawlsian-justice, Low}
}

@article{shields2016prospects,
  author = {Shields, Liam},
  title = {The Prospects for Sufficientarianism},
  journal = {Philosophy Compass},
  year = {2016},
  volume = {11},
  number = {2},
  pages = {101--111},
  doi = {10.1111/phc3.12300},
  note = {
  CORE ARGUMENT: Sufficientarianism holds that securing enough of some goods
  for people has special moral importance, distinct from equality or maximization.
  The paper defends sufficientarianism against major objections (indifference above
  threshold, discontinuity) by showing these objections mischaracterize the view.
  A properly specified sufficientarian principle avoids these problems by focusing
  on weighted priority below thresholds rather than absolute lexical priority.

  RELEVANCE: Sufficientarianism offers a normative framework for fairness auditing
  that differs fundamentally from egalitarianism. In algorithmic contexts, it implies
  that fairness interventions should prioritize ensuring all groups reach minimum
  thresholds (e.g., minimum true positive rates) rather than equalizing outcomes
  across groups. This has direct implications for which groups matter in intersectional
  fairness: groups below thresholds warrant urgent intervention, while inequality
  above thresholds may be tolerable. This framework answers the ontological question
  differently than egalitarianism.

  POSITION: Sufficientarian framework for distributive justice, emphasizing thresholds
  over equality.
  },
  keywords = {sufficientarianism, distributive-justice, normative-framework, High}
}

@article{timmer2021justice,
  author = {Timmer, Dick},
  title = {Justice, Thresholds, and the Three Claims of Sufficientarianism},
  journal = {Journal of Political Philosophy},
  year = {2021},
  volume = {29},
  number = {4},
  pages = {395--416},
  doi = {10.1111/jopp.12258},
  note = {
  CORE ARGUMENT: Sufficientarianism combines three distinct claims: a priority claim
  (benefits in certain ranges matter more), a continuum claim (ranges exist on one
  continuum), and a deficiency claim (lower ranges warrant stronger priority). This
  tripartite structure explains why sufficientarianism is neither simple threshold
  lexicalism nor weighted prioritarianism, but a distinctive view that can handle
  multiple thresholds and variable priority weights below sufficiency.

  RELEVANCE: Timmer's decomposition of sufficientarianism clarifies how this framework
  applies to algorithmic fairness. Different intersectional groups may have different
  sufficiency thresholds depending on context (e.g., higher thresholds for groups
  facing systemic disadvantage). The priority, continuum, and deficiency claims
  together determine which groups matter most in fairness audits: groups furthest
  below relevant thresholds receive strongest priority. This provides normative
  guidance for prioritizing fairness interventions across intersectional groups.

  POSITION: Refined sufficientarian account with multiple thresholds and variable
  priority weights.
  },
  keywords = {sufficientarianism, thresholds, distributive-justice, High}
}

@article{arneson2000luck,
  author = {Arneson, Richard J.},
  title = {Luck Egalitarianism and Prioritarianism},
  journal = {Ethics},
  year = {2000},
  volume = {110},
  number = {2},
  pages = {339--349},
  doi = {10.1086/233272},
  note = {
  CORE ARGUMENT: Prioritarianism assigns greater moral weight to benefits to
  worse-off individuals, but unlike egalitarianism, does not treat equality as
  intrinsically valuable. The view holds that benefiting people matters more the
  worse off those people are, but remains impartial between individuals. Arneson
  examines how prioritarianism relates to luck egalitarianism (which distinguishes
  choice-based from circumstance-based inequality) and argues the two can be combined.

  RELEVANCE: Prioritarianism provides an alternative normative foundation for
  algorithmic fairness that focuses on absolute advantage levels rather than
  relative equality. In intersectional fairness, this implies greater moral urgency
  to improve outcomes for the worst-off intersectional groups (e.g., multiply
  marginalized groups), even if this increases inequality with better-off groups.
  This differs from egalitarian frameworks that prioritize parity. The framework
  helps adjudicate conflicts between different fairness criteria by weighting
  benefits to disadvantaged groups more heavily.

  POSITION: Prioritarian approach to distributive justice, combining with luck
  egalitarianism.
  },
  keywords = {prioritarianism, distributive-justice, luck-egalitarianism, High}
}

@incollection{parfit2012another,
  author = {Parfit, Derek},
  title = {Another Defence of the Priority View},
  booktitle = {Egalitarianism: New Essays on the Nature and Value of Equality},
  editor = {Holtug, Nils and Lippert-Rasmussen, Kasper},
  publisher = {Oxford University Press},
  year = {2012},
  pages = {399--440},
  doi = {10.1093/acprof:oso/9780199281701.003.0018},
  note = {
  CORE ARGUMENT: Parfit defends prioritarianism against several objections, arguing
  that it properly captures the moral importance of benefiting worse-off individuals
  without treating equality as intrinsically valuable. He addresses objections from
  Otsuka and Voorhoeve about competing claims, argues prioritarians can explain why
  worse-off people should sometimes receive priority in lotteries, and clarifies how
  the view handles uncertainty and expectation.

  RELEVANCE: Parfit's refined prioritarianism clarifies how to handle trade-offs in
  algorithmic fairness when improving outcomes for some groups worsens outcomes for
  others. His discussion of competing claims is directly relevant to intersectional
  fairness: when auditing algorithms, prioritarianism implies giving more weight to
  improvements for the worst-off intersectional groups. His treatment of uncertainty
  also applies to contexts where we're uncertain which groups are actually worst-off
  or which interventions will help them most.

  POSITION: Priority view (prioritarianism) as distinct from egalitarianism.
  },
  keywords = {prioritarianism, competing-claims, distributive-justice, High}
}

@article{anderson1999equality,
  author = {Anderson, Elizabeth},
  title = {What Is the Point of Equality?},
  journal = {Ethics},
  year = {1999},
  volume = {109},
  number = {2},
  pages = {287--337},
  doi = {10.1086/233897},
  note = {
  CORE ARGUMENT: Anderson critiques luck egalitarianism and distributive egalitarianism,
  arguing they fail to capture what matters about equality. She proposes democratic
  equality (relational egalitarianism) as an alternative: the fundamental aim is to
  create a community where people relate as equals, free from oppression and
  domination, rather than equalizing distributions of goods. Democratic equality
  focuses on social relationships and capabilities for democratic participation.

  RELEVANCE: Anderson's relational egalitarianism offers a fundamentally different
  normative framework for algorithmic fairness than distributive approaches. Rather
  than focusing on equalizing error rates or outcomes, it asks whether algorithmic
  systems create hierarchies of social status or enable domination. For intersectional
  fairness, this implies fairness audits should examine whether algorithms entrench
  hierarchies between groups (e.g., treating multiply marginalized groups as
  second-class) rather than merely measuring statistical parity. The framework
  reframes fairness as preventing algorithmic domination.

  POSITION: Relational egalitarianism (democratic equality) as critique of
  distributive approaches.
  },
  keywords = {relational-egalitarianism, democratic-equality, critique, High}
}

@article{fazelpour2021algorithmic,
  author = {Fazelpour, Sina and Lipton, Zachary C. and Danks, David},
  title = {Algorithmic Fairness and the Situated Dynamics of Justice},
  journal = {Canadian Journal of Philosophy},
  year = {2021},
  volume = {52},
  number = {1},
  pages = {44--60},
  doi = {10.1017/can.2021.24},
  note = {
  CORE ARGUMENT: Prevailing algorithmic fairness research focuses on identifying
  "ideally fair" target states in abstraction from deployment context. This is
  misguided; we should evaluate dynamic trajectories of algorithmic systems rather
  than static snapshots. Fairness should be assessed based on (i) temporal dynamics
  (how systems evolve), (ii) robustness (resilience to perturbations), and (iii)
  representation (whose interests are considered). Static fairness metrics miss how
  systems create feedback loops that amplify or mitigate injustice over time.

  RELEVANCE: Fazelpour et al. fundamentally challenge how fairness auditing is
  conceived. For intersectional fairness, this implies audits cannot simply measure
  whether an algorithm satisfies parity criteria at deployment but must examine
  dynamic effects on multiply marginalized groups over time. Different normative
  frameworks (prioritarian, sufficientarian, egalitarian) will yield different
  evaluations of these trajectories: prioritarians care most about trajectories
  for worst-off groups, sufficientarians about whether trajectories keep all groups
  above thresholds. The purpose of auditing shifts from compliance to trajectory
  assessment.

  POSITION: Process-oriented, dynamic approach to algorithmic fairness; critique of
  target-state frameworks.
  },
  keywords = {algorithmic-fairness, dynamic-justice, normative-framework, High}
}

@inproceedings{hertweck2024distributive,
  author = {Hertweck, Corinna and Heitz, Christoph and Loi, Michele},
  title = {What's Distributive Justice Got to Do with It? Rethinking Algorithmic Fairness from the Perspective of Approximate Justice},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2024},
  pages = {597--608},
  doi = {10.1609/aies.v7i1.31661},
  note = {
  CORE ARGUMENT: Current algorithmic fairness approaches erroneously link fairness
  criteria to ideal theories of distributive justice (egalitarianism, prioritarianism,
  sufficientarianism). But algorithmic decision-making is inherently imperfect, so
  we should focus on how *deviations* from ideal distributions are themselves
  distributed. Fairness in imperfect systems requires evaluating unfairness in
  errors, not just distributions of outcomes. This reframes the relationship between
  distributive justice and algorithmic fairness.

  RELEVANCE: Hertweck et al. clarify a key confusion in applying normative frameworks
  to algorithmic fairness. For intersectional fairness, this implies we cannot simply
  ask whether outcomes are distributed equally/prioritarian/sufficientarian across
  groups. Instead, we must ask how algorithmic *errors* are distributed—are false
  negatives concentrated on multiply marginalized groups? This shifts fairness
  auditing from measuring outcome distributions to measuring error distributions,
  and connects to different normative frameworks differently than assumed. Critical
  for understanding the purpose of fairness audits.

  POSITION: Approximate justice framework for algorithmic fairness; critique of
  direct distributive justice applications.
  },
  keywords = {algorithmic-fairness, approximate-justice, distributive-justice, High}
}

@article{holm2025separateness,
  author = {Holm, Sune},
  title = {Algorithmic Fairness, Decision Thresholds, and the Separateness of Persons},
  journal = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2025},
  doi = {10.1145/3715275.3732113},
  note = {
  CORE ARGUMENT: Popular statistical fairness criteria fail to respect the separateness
  of persons because they can allocate treatments (e.g., medical interventions) to
  individuals for whom treatment would be harmful in expectation. Fairness requires
  allocating for the right reasons—reasons that respect each person's separate
  interests. Drawing on Broome's theory of fairness as proportional satisfaction of
  claims, Holm argues algorithmic fairness requires individual-level decision
  thresholds, not just group-level statistical parity.

  RELEVANCE: Holm's application of Broome's fairness theory to algorithmic contexts
  provides a crucial bridge between philosophical and technical fairness literature.
  For intersectional fairness, this implies that statistical parity across intersectional
  groups is insufficient if it violates individual claims. The separateness of persons
  grounds a distinctive normative constraint: fairness audits must ensure algorithms
  respect each individual's claims, not just equalize group statistics. This challenges
  dominant approaches to intersectional fairness that focus solely on group parity.

  POSITION: Individual fairness grounded in Broomean claims-based fairness; critique
  of statistical parity.
  },
  keywords = {algorithmic-fairness, separateness-persons, broome-fairness, High}
}

@book{shields2016enough,
  author = {Shields, Liam},
  title = {Just Enough: Sufficiency as a Demand of Justice},
  publisher = {Edinburgh University Press},
  year = {2016},
  doi = {10.3366/edinburgh/9780748691869.001.0001},
  note = {
  CORE ARGUMENT: This book-length treatment defends sufficientarianism as a complete
  theory of distributive justice. Shields argues that justice requires everyone to
  have enough of certain goods (basic needs, capabilities, opportunities), and that
  achieving sufficiency has a distinctive kind of moral urgency. He defends the view
  against objections about threshold arbitrariness, indifference above the threshold,
  and compatibility with other values like equality and desert.

  RELEVANCE: Shields provides the most comprehensive philosophical foundation for
  sufficientarian approaches to fairness. For algorithmic fairness auditing, this
  supports a framework where the primary purpose is ensuring all groups—including
  all intersectional groups—reach minimum acceptable outcomes (sufficiency thresholds),
  rather than equalizing outcomes. This answers the ontological question (which groups
  matter) by focusing on groups below thresholds. It also clarifies fairness auditing's
  purpose: identifying groups failing to reach sufficiency, not necessarily achieving
  parity.

  POSITION: Comprehensive defense of sufficientarianism as theory of distributive
  justice.
  },
  keywords = {sufficientarianism, distributive-justice, book, Medium}
}

@article{green2022escaping,
  author = {Green, Ben},
  title = {Escaping the Impossibility of Fairness: From Formal to Substantive Algorithmic Fairness},
  journal = {Philosophy \& Technology},
  year = {2022},
  volume = {35},
  number = {90},
  doi = {10.1007/s13347-022-00584-6},
  note = {
  CORE ARGUMENT: Current "formal algorithmic fairness" restricts analysis to isolated
  decision-making procedures, leading to impossibility results and models that
  exacerbate oppression despite appearing fair. Green proposes "substantive algorithmic
  fairness" drawing on theories of substantive equality from law and philosophy.
  This approach takes a more expansive scope of analysis—examining structural context,
  power relations, and long-term effects—enabling escape from impossibility results
  and providing rigorous guidance for alleviating injustice.

  RELEVANCE: Green's distinction between formal and substantive fairness is crucial
  for understanding fairness auditing's purpose. Compliance-focused audits adopt
  formal fairness (do statistical criteria hold?), while justice-focused audits
  require substantive fairness (does the system reduce oppression?). For intersectional
  fairness, substantive approaches examine how algorithms affect multiply marginalized
  groups' structural positions, not just their statistical representation. Different
  normative frameworks map onto formal vs. substantive approaches differently, with
  implications for which groups matter.

  POSITION: Substantive algorithmic fairness grounded in structural analysis; critique
  of formal approaches.
  },
  keywords = {algorithmic-fairness, substantive-equality, formal-fairness, High}
}

@article{binns2024difference,
  author = {Binns, Reuben},
  title = {If the Difference Principle Won't Make a Real Difference in Algorithmic Fairness, What Will?},
  journal = {Philosophy \& Technology},
  year = {2024},
  volume = {37},
  doi = {10.1007/s13347-024-00805-0},
  note = {
  CORE ARGUMENT: Rawls's Difference Principle (inequalities justified only if they
  benefit the worst-off) has been proposed as a foundation for algorithmic fairness,
  but Binns argues it won't make a real difference in practice. The Principle's
  lexical priority of basic liberties and its focus on basic structure of society
  rather than individual decisions limits its applicability to algorithmic systems.
  Alternative normative frameworks may be more practically actionable for algorithmic
  fairness.

  RELEVANCE: Binns's critique of applying Rawlsian egalitarianism to algorithms
  highlights limits of directly importing distributive justice theories to algorithmic
  contexts. For intersectional fairness, this suggests we cannot simply apply the
  Difference Principle to prioritize worst-off intersectional groups; the framework
  doesn't fit algorithmic decision-making well. This motivates considering alternative
  normative foundations (prioritarianism, sufficientarianism, or domain-specific
  principles) for determining which groups matter and how to prioritize them in
  fairness audits.

  POSITION: Critique of Rawlsian Difference Principle for algorithmic fairness;
  skepticism about direct application.
  },
  keywords = {algorithmic-fairness, rawls, difference-principle, Medium}
}

@article{knight2021enough,
  author = {Knight, Carl},
  title = {Enough Is Too Much: The Excessiveness Objection to Sufficientarianism},
  journal = {Economics and Philosophy},
  year = {2021},
  volume = {38},
  number = {2},
  pages = {275--299},
  doi = {10.1017/S0266267121000171},
  note = {
  CORE ARGUMENT: Standard sufficientarianism gives lexical priority to providing
  sufficiency, but this is excessive in four ways: (1) magnitude—huge benefits above
  threshold can outweigh small benefits below; (2) numbers—small benefits to many
  below threshold can outweigh large benefits to few above; (3) responsibility—people
  below threshold through their own choices may not deserve priority; (4) above-threshold
  distribution—sufficientarianism wrongly ignores inequality above threshold. Knight
  argues sufficientarians should accept weighted rather than lexical priority.

  RELEVANCE: Knight's excessiveness objection clarifies limits of sufficientarian
  approaches to algorithmic fairness. For intersectional fairness auditing, lexical
  sufficientarianism would require prioritizing *any* improvement for groups below
  thresholds over *any* improvement for groups above, which Knight shows is excessive.
  This suggests fairness audits should adopt weighted sufficientarianism: strong but
  not absolute priority for groups below thresholds. This affects both which groups
  matter most and the purpose of auditing (ensuring thresholds are met, but not
  exclusively).

  POSITION: Critique of lexical sufficientarianism; defense of weighted
  sufficientarianism.
  },
  keywords = {sufficientarianism, excessiveness-objection, weighted-priority, Medium}
}

@article{huseby2020sufficientarianism,
  author = {Huseby, Robert},
  title = {Sufficientarianism},
  journal = {Oxford Research Encyclopedia of Politics},
  year = {2020},
  doi = {10.1093/acrefore/9780190228637.013.1382},
  note = {
  CORE ARGUMENT: This encyclopedia entry provides a comprehensive overview of
  sufficientarianism as a distributive justice principle. Huseby surveys the positive
  thesis (importance of reaching sufficiency) and negative thesis (above sufficiency,
  distribution doesn't matter), debates about currency (welfare, resources, capabilities),
  threshold specification, and scope. He reviews major objections (threshold
  arbitrariness, indifference above threshold) and sufficientarian responses.

  RELEVANCE: Huseby's overview clarifies the landscape of sufficientarian approaches,
  showing there is no single sufficientarian view but a family of views. For
  algorithmic fairness, this implies multiple sufficientarian frameworks are possible:
  different thresholds (minimum acceptable outcomes vs. minimally decent lives),
  different currencies (capabilities vs. outcomes), different scopes (temporal,
  spatial). This multiplicity means the ontological question—which groups matter—
  receives different answers depending on which sufficientarian specification one
  adopts.

  POSITION: Survey of sufficientarian theories and debates.
  },
  keywords = {sufficientarianism, distributive-justice, survey, Medium}
}

@article{robeyns2022limitarianism,
  author = {Robeyns, Ingrid},
  title = {Why Limitarianism?},
  journal = {Journal of Political Philosophy},
  year = {2022},
  volume = {30},
  number = {2},
  pages = {249--270},
  doi = {10.1111/jopp.12275},
  note = {
  CORE ARGUMENT: Limitarianism holds that there should be an upper limit on how
  much income and wealth individuals can have. Robeyns argues limitarianism addresses
  problems neither egalitarianism nor sufficientarianism adequately handle: excessive
  wealth concentration undermines democracy, exceeds what anyone deserves, and could
  be better used to meet others' urgent needs. She proposes limitarianism as part of
  a pluralist account combining sufficientarianism, opportunity egalitarianism, and
  limitarianism.

  RELEVANCE: While limitarianism focuses on upper bounds rather than fairness
  criteria directly, Robeyns's pluralist approach is relevant for algorithmic fairness.
  Her argument that we need *multiple* normative principles (not just one) for
  distributive justice supports the claim that algorithmic fairness cannot be
  captured by a single metric or normative framework. For intersectional fairness,
  pluralism suggests we may need different principles for different contexts: perhaps
  sufficientarianism for basic outcomes, egalitarianism for opportunities, and
  additional principles for extreme advantages.

  POSITION: Pluralist account combining limitarianism, sufficientarianism, and
  egalitarianism.
  },
  keywords = {limitarianism, pluralism, distributive-justice, Medium}
}

@article{adler2023prioritarianism,
  author = {Adler, Matthew D. and Norheim, Ole F.},
  title = {Prioritarianism in Practice},
  journal = {Cambridge University Press},
  year = {2023},
  doi = {10.1017/9781108691734},
  note = {
  CORE ARGUMENT: This edited volume examines how prioritarianism—the view that gives
  extra weight to well-being improvements for worse-off individuals—applies to
  practical policy contexts including health, climate change, and development. The
  contributors show how prioritarian principles differ from utilitarianism,
  egalitarianism, and cost-benefit analysis in policy evaluation, and how to
  operationalize prioritarian weights in practice.

  RELEVANCE: Adler and Norheim's focus on practical implementation of prioritarianism
  is directly relevant to algorithmic fairness auditing. For intersectional fairness,
  prioritarianism implies assigning greater weight to improvements for multiply
  marginalized groups (the worst-off intersectional groups). The volume's discussion
  of how to determine prioritarian weights in practice addresses a key challenge:
  how much more should we weight benefits to worse-off groups? This connects to
  fairness auditing's purpose—if we adopt prioritarianism, audits should measure
  weighted benefits, not just raw improvements.

  POSITION: Applied prioritarianism across policy domains.
  },
  keywords = {prioritarianism, applied-ethics, policy, Medium}
}

@article{sahlgren2024impossible,
  author = {Sahlgren, Otto},
  title = {What's Impossible about Algorithmic Fairness?},
  journal = {Philosophy \& Technology},
  year = {2024},
  volume = {37},
  doi = {10.1007/s13347-024-00814-z},
  note = {
  CORE ARGUMENT: Impossibility results show that predictive models cannot simultaneously
  satisfy multiple plausible fairness criteria except under extraordinary circumstances.
  Sahlgren draws on philosophical literature on feasibility to argue that fairness
  can be made diachronically feasible through collective efforts to eliminate
  inequalities feeding into local decision-making. The impossibility is not permanent
  but reflects current structural conditions that could be changed.

  RELEVANCE: Sahlgren's feasibility analysis connects to debates about fairness
  auditing's purpose. If impossibility results reflect structural conditions rather
  than logical necessities, then fairness audits should aim not just at optimizing
  current algorithms but at identifying structural changes needed to make fairness
  feasible. For intersectional fairness, this implies audits should examine which
  structural inequalities make it impossible to satisfy fairness criteria across
  intersectional groups, rather than simply accepting impossibility. Different
  normative frameworks will prioritize different structural changes.

  POSITION: Feasibility-based analysis of algorithmic fairness impossibility results.
  },
  keywords = {algorithmic-fairness, impossibility-results, feasibility, Low}
}

@article{castro2024broomean,
  author = {Castro, Clinton},
  title = {Broomean(ish) Algorithmic Fairness?},
  journal = {Journal of Applied Philosophy},
  year = {2024},
  doi = {10.1111/japp.12778},
  note = {
  CORE ARGUMENT: Castro applies John Broome's theory of fairness (fairness as
  proportional satisfaction of claims) to algorithmic decision-making, but modifies
  it to think in counterfactual rather than statistical terms. He critiques Holm's
  direct application of Broome, arguing that Holm's statistical interpretation fails
  to capture Broome's core insights. A proper Broomean approach to algorithmic fairness
  requires examining what individuals would be owed in different possible worlds,
  not just statistical distributions.

  RELEVANCE: Castro's counterfactual interpretation of Broomean fairness offers a
  different normative foundation for algorithmic fairness than standard approaches.
  For intersectional fairness, this implies that determining which groups matter
  requires counterfactual reasoning: which groups have stronger claims based on what
  they would be owed in fair possible worlds? This is distinct from both egalitarian
  (equal statistical treatment) and prioritarian (weight by disadvantage) approaches,
  and suggests fairness audits should examine counterfactual claims rather than just
  actual outcomes.

  POSITION: Counterfactual Broomean fairness for algorithmic contexts.
  },
  keywords = {algorithmic-fairness, broome-fairness, counterfactual, Low}
}

@article{grant2023equalized,
  author = {Grant, David Gray},
  title = {Equalized Odds Is a Requirement of Algorithmic Fairness},
  journal = {Synthese},
  year = {2023},
  volume = {201},
  pages = {1--25},
  doi = {10.1007/s11229-023-04054-0},
  note = {
  CORE ARGUMENT: Grant defends equalized odds (equal true positive and false positive
  rates across groups) as a necessary requirement of procedural fairness in algorithmic
  classification. He introduces a refined version of equalized odds that avoids
  standard objections, arguing it's required by procedural fairness because it ensures
  classification decisions treat similarly situated individuals the same way regardless
  of group membership. This grounds equalized odds in procedural justice rather than
  distributive justice.

  RELEVANCE: Grant's defense of equalized odds shows how one specific fairness
  criterion connects to a particular normative framework (procedural fairness). For
  intersectional fairness, this raises the question: does procedural fairness require
  equalized odds across *all* intersectional groups, or only salient protected
  groups? The answer depends on whether we adopt Grant's procedural framework or
  alternative distributive frameworks (egalitarian, prioritarian, sufficientarian).
  This illustrates how different normative frameworks yield different answers about
  which groups matter.

  POSITION: Procedural fairness foundation for equalized odds criterion.
  },
  keywords = {algorithmic-fairness, equalized-odds, procedural-fairness, Low}
}

@book{fricker2007epistemic,
  author = {Fricker, Miranda},
  title = {Epistemic Injustice: Power and the Ethics of Knowing},
  year = {2007},
  publisher = {Oxford University Press},
  address = {Oxford},
  doi = {10.1093/acprof:oso/9780198237907.001.0001},
  note = {
  CORE ARGUMENT: Fricker establishes epistemic injustice as a distinct category
  of injustice, identifying two primary forms: testimonial injustice (when
  prejudice causes a hearer to give a deflated level of credibility to a
  speaker's word) and hermeneutical injustice (when a gap in collective
  interpretive resources puts someone at an unfair disadvantage in making sense
  of their social experiences). Both forms harm individuals specifically in their
  capacity as knowers.

  RELEVANCE: Foundational for understanding how data sparsity for marginalized
  groups in fairness auditing constitutes not merely a statistical problem but
  an epistemic injustice. When algorithmic systems require large sample sizes
  that marginalized groups cannot provide, this perpetuates hermeneutical
  injustice by leaving those groups' experiences underrepresented in the
  collective interpretive resources embedded in ML models. The framework helps
  conceptualize the "statistical uncertainty horn" as a site of epistemic harm.

  POSITION: Foundational work establishing epistemic injustice as a distinct
  normative framework, widely applied to technology and data systems.
  },
  keywords = {epistemic-injustice, foundational, philosophy, High}
}

@article{symons2022epistemic,
  author = {Symons, John and Alvarado, Ramón},
  title = {Epistemic injustice and data science technologies},
  journal = {Synthese},
  year = {2022},
  volume = {200},
  doi = {10.1007/s11229-022-03631-z},
  note = {
  CORE ARGUMENT: Extends Fricker's framework to data science, arguing that
  epistemic injustices in data-driven technologies are conceptually distinct
  from more familiar harms. The authors demonstrate how data collection
  practices, algorithmic opacity, and system design choices can perpetrate
  testimonial and hermeneutical injustices even when no individual actor intends
  discrimination. They analyze examples from criminal justice and workplace
  algorithms.

  RELEVANCE: Directly addresses how data science technologies—including fairness
  auditing systems—can produce epistemic injustices through their design and
  deployment. The paper provides conceptual tools for understanding how sparse
  data for marginalized groups represents not just missing information but a
  systematic undermining of those groups' epistemic standing. Connects technical
  choices about sample size and data collection to normative concerns about
  epistemic justice.

  POSITION: Application of epistemic injustice framework to algorithmic systems,
  influential in AI ethics literature.
  },
  keywords = {epistemic-injustice, data-science, algorithms, High}
}

@article{anderson2012epistemic,
  author = {Anderson, Elizabeth},
  title = {Epistemic Justice as a Virtue of Social Institutions},
  journal = {Social Epistemology},
  year = {2012},
  volume = {26},
  number = {2},
  pages = {163--173},
  doi = {10.1080/09505431.2011.652086},
  note = {
  CORE ARGUMENT: Shifts focus from individual epistemic virtues to institutional
  design, arguing that epistemic justice requires social institutions—not just
  individuals—to correct for identity-prejudicial credibility assessments.
  Anderson emphasizes that cognitive biases difficult for individuals to overcome
  may be more susceptible to institutional correction through properly designed
  systems of testimonial gathering and assessment.

  RELEVANCE: Critical for understanding fairness auditing as an institutional
  practice. If epistemic justice demands institutional-level interventions, then
  fairness auditing systems must be designed to counteract systematic credibility
  deficits for marginalized groups. The institutional perspective highlights how
  sample size requirements and data collection protocols are not neutral
  technical choices but design decisions with epistemic justice implications.
  Particularly relevant for thinking about what institutional structures would be
  needed to address data sparsity without perpetuating epistemic injustice.

  POSITION: Institutional approach to epistemic justice, expanding beyond
  individual virtue epistemology.
  },
  keywords = {epistemic-justice, institutions, social-epistemology, High}
}

@article{hull2023dirty,
  author = {Hull, Gordon},
  title = {Dirty data labeled dirt cheap: epistemic injustice in machine learning systems},
  journal = {Ethics and Information Technology},
  year = {2023},
  volume = {25},
  pages = {1--14},
  doi = {10.1007/s10676-023-09712-y},
  note = {
  CORE ARGUMENT: Analyzes how data labeling practices in machine learning
  constitute epistemic injustice, particularly through the use of low-wage labor
  to label training data. Hull argues that the economic structure of data
  labeling creates hermeneutical injustice by excluding those who label data from
  participation in determining how their labor and knowledge contributions are
  conceptualized. The paper emphasizes the political nature of epistemic practices
  embedded in ML systems.

  RELEVANCE: Illuminates how data collection and curation practices—central to
  fairness auditing—are sites of epistemic injustice that precede and shape model
  outputs. The analysis suggests that sparse data for marginalized groups may
  reflect not just statistical under-sampling but systematic exclusion from the
  processes of knowledge production. Connects data scarcity to broader political-
  economic structures that determine whose knowledge counts.

  POSITION: Critical analysis of labor and power in ML data production, drawing
  on Foucault and Fricker.
  },
  keywords = {epistemic-injustice, data-labeling, machine-learning, Medium}
}

@article{milano2024algorithmic,
  author = {Milano, Silvia and Prunkl, Carina},
  title = {Algorithmic profiling as a source of hermeneutical injustice},
  journal = {Philosophical Studies},
  year = {2024},
  volume = {182},
  number = {1},
  pages = {185--203},
  doi = {10.1007/s11098-024-02084-5},
  note = {
  CORE ARGUMENT: Argues that algorithmic profiling creates hermeneutical
  injustice through epistemic fragmentation—the isolation of individuals in
  algorithmically-mediated environments that makes it difficult to develop,
  uptake, and apply new epistemic resources. When algorithms create filter
  bubbles and personalized experiences, individuals cannot compare and learn
  from shared experiences, undermining collective sense-making about algorithmic
  harms.

  RELEVANCE: While focused on content recommendation rather than fairness
  auditing, the epistemic fragmentation concept applies to data sparsity
  problems. When data for marginalized groups is sparse and scattered, it becomes
  difficult to develop shared hermeneutical resources for understanding and
  articulating algorithmic harms. The paper suggests that statistical uncertainty
  from sparse data compounds hermeneutical injustice by preventing coherent
  articulation of patterns affecting small groups.

  POSITION: Extends hermeneutical injustice concept to algorithmic environments
  and data-driven personalization.
  },
  keywords = {hermeneutical-injustice, algorithms, epistemic-fragmentation, Medium}
}

@article{grote2020ethics,
  author = {Grote, Thomas and Berens, Philipp},
  title = {On the ethics of algorithmic decision-making in healthcare},
  journal = {Journal of Medical Ethics},
  year = {2020},
  volume = {46},
  number = {3},
  pages = {205--211},
  doi = {10.1136/medethics-2019-105586},
  note = {
  CORE ARGUMENT: Examines trade-offs between accuracy gains from ML in medical
  diagnosis and epistemic costs from algorithmic opacity. The authors argue that
  uncertainty in assessing ML reliability potentially undermines clinicians'
  epistemic authority, creating tensions around paternalism, moral responsibility,
  and fairness. Drawing on social epistemology, they analyze how opacity affects
  the evidentiary norms of medical diagnosis.

  RELEVANCE: While focused on healthcare rather than fairness auditing, the
  analysis of uncertainty and epistemic authority directly parallels challenges
  in auditing fairness under sparse data. When ML models make predictions for
  small groups, both accuracy and reliability become uncertain, yet systems still
  claim epistemic authority. The paper's framework for analyzing epistemic costs
  of opacity applies to understanding how sparse data undermines both statistical
  and epistemic confidence in fairness assessments.

  POSITION: Social epistemology approach to algorithmic decision-making,
  emphasizing uncertainty and epistemic authority.
  },
  keywords = {epistemic-authority, uncertainty, algorithms, healthcare, Medium}
}

@article{zhioua2025origins,
  author = {Zhioua, Sami and Binkyte, Ruta and Ouni, Ayoub and Ktata, Farah Barika},
  title = {On the Origins of Sampling Bias: Implications on Fairness Measurement and Mitigation},
  journal = {arXiv},
  year = {2025},
  volume = {abs/2503.17956},
  arxivId = {2503.17956},
  doi = {10.48550/arXiv.2503.17956},
  note = {
  CORE ARGUMENT: Disambiguates "sampling bias" into sample size bias (SSB) and
  underrepresentation bias (URB), demonstrating through extensive experiments
  that these distinct forms of bias affect fairness measurements differently. The
  authors show that bias from sampling is borne unequally by different groups,
  potentially exacerbating discrimination against underrepresented populations.
  Provides actionable recommendations for practitioners addressing sample size
  challenges.

  RELEVANCE: Directly addresses the core technical challenge of the "statistical
  uncertainty horn"—how to measure fairness reliably when some groups have
  insufficient sample sizes. The distinction between SSB and URB clarifies that
  data sparsity creates multiple, compounding problems for fairness auditing.
  The finding that bias is borne unequally connects statistical challenges to
  epistemic justice concerns: groups with sparse data suffer both unreliable
  measurement AND amplified discrimination.

  POSITION: Technical analysis of sampling bias in fairness measurement with
  practical recommendations.
  },
  keywords = {sample-size, fairness-metrics, sampling-bias, statistical-reliability, High}
}

@inproceedings{jourdan2023fairness,
  author = {Jourdan, Fanny and Risser, Laurent and Loubes, Jean-Michel and Asher, Nicholas M.},
  title = {Are fairness metric scores enough to assess discrimination biases in machine learning?},
  booktitle = {Proceedings of the 2023 Conference on Trust and Natural Language Processing (TRUSTNLP)},
  year = {2023},
  doi = {10.48550/arXiv.2306.05307},
  arxivId = {2306.05307},
  note = {
  CORE ARGUMENT: Demonstrates through Monte Carlo simulations on real datasets
  that gender bias indices provide diverging and unreliable results when applied
  to small training and test samples. The authors show that common fairness
  metrics have low statistical power and confidence with small sample sizes,
  leading to overestimation or underestimation of actual discrimination.
  Emphasizes the critical importance of variance calculations for sound results
  in fairness assessment.

  RELEVANCE: Provides empirical evidence that fairness auditing with sparse data
  yields unreliable results—a central concern for the "statistical uncertainty
  horn" of the intersectionality dilemma. The finding that metrics diverge with
  small samples suggests that expanding fairness auditing to more intersectional
  groups (thereby creating smaller samples per group) may produce fairness
  assessments that are not just uncertain but potentially misleading. Critical
  for understanding the epistemic limitations of current fairness auditing
  practices.

  POSITION: Empirical study questioning reliability of fairness metrics with
  small samples.
  },
  keywords = {fairness-metrics, sample-size, statistical-reliability, empirical, High}
}

@article{konstantinov2021fairness,
  author = {Konstantinov, Nikola and Lampert, Christoph H.},
  title = {Fairness-Aware PAC Learning from Corrupted Data},
  journal = {Journal of Machine Learning Research},
  year = {2021},
  volume = {23},
  pages = {160:1--160:60},
  arxivId = {2102.06004},
  note = {
  CORE ARGUMENT: Studies fairness-aware learning under worst-case data
  corruption, proving that adversaries can sometimes force any learner to return
  overly biased classifiers regardless of sample size. Crucially, the strength
  of excess bias increases for learning problems with underrepresented protected
  groups. The paper provides tight theoretical bounds on achievable fairness
  guarantees under data corruption.

  RELEVANCE: Theoretical analysis demonstrating that underrepresentation (sparse
  data for protected groups) creates fundamental vulnerabilities in fairness-
  aware learning that cannot be overcome simply by collecting more data. The
  finding that bias increases with underrepresentation provides theoretical
  grounding for the practical challenge of fairness auditing with sparse data.
  Suggests that the statistical uncertainty problem may have theoretical limits
  beyond what can be solved through improved methodology.

  POSITION: Theoretical PAC learning analysis of fairness under data corruption
  and underrepresentation.
  },
  keywords = {fairness-learning, underrepresentation, PAC-learning, theoretical, High}
}

@inproceedings{mhasawade2024understanding,
  author = {Mhasawade, Vishwali and Rahman, Salman and Haskell-Craig, Zoé and Chunara, Rishi},
  title = {Understanding Disparities in Post Hoc Machine Learning Explanation},
  booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2024},
  pages = {617--631},
  doi = {10.1145/3630106.3659043},
  arxivId = {2401.14539},
  note = {
  CORE ARGUMENT: Investigates how data properties (limited sample size, covariate
  shift, concept shift, omitted variable bias) and model properties affect
  disparities in post-hoc ML explanations across demographic groups. Through
  controlled simulations and real-world experiments, demonstrates that increased
  covariate shift, concept shift, and covariate omission increase explanation
  disparities, with effects more pronounced for neural networks than linear models.

  RELEVANCE: Reveals that sparse data for marginalized groups creates explanation
  disparities—a form of epistemic injustice where different groups receive
  different quality of explanation for algorithmic decisions. The finding that
  data properties drive explanation disparities connects statistical challenges
  (sample size, shift, omitted variables) to epistemic harms (unequal access to
  understanding algorithmic systems). Particularly relevant for fairness auditing
  where explanation quality may vary by group.

  POSITION: Empirical investigation of explanation disparities arising from data
  and model properties.
  },
  keywords = {explainability, fairness, sample-size, data-properties, High}
}

@article{singh2023measures,
  author = {Singh, Harvineet and Chunara, Rishi},
  title = {Measures of Disparity and their Efficient Estimation},
  journal = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2023},
  pages = {671--682},
  doi = {10.1145/3600211.3604697},
  note = {
  CORE ARGUMENT: Derives sample size requirements for accurately estimating
  disparity metrics used in public health, economics, and ML fairness. Provides
  guidance on how many samples to collect per group to maximize precision of
  disparity estimates given fixed data collection budget. Shows that absent prior
  information, equally sampling groups typically performs well but may be
  suboptimal when group variances differ.

  RELEVANCE: Provides principled statistical framework for determining sample
  sizes needed for reliable fairness auditing across groups—directly addressing
  the "statistical uncertainty horn." The budget constraint framing illuminates
  trade-offs between auditing more groups (lower sample size per group) versus
  fewer groups (higher sample size per group). Findings suggest that expanding
  intersectional auditing necessarily reduces precision of estimates unless data
  collection budgets scale proportionally.

  POSITION: Statistical methodology for sample size determination in disparity
  measurement.
  },
  keywords = {sample-size, disparity-metrics, statistical-methodology, fairness-auditing, High}
}

@article{rothblum2021multigroup,
  author = {Rothblum, Guy N. and Yona, Gal},
  title = {Multi-group Agnostic PAC Learnability},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  year = {2021},
  pages = {9107--9115},
  arxivId = {2105.09989},
  note = {
  CORE ARGUMENT: Characterizes which loss functions admit multi-group agnostic
  PAC learning, where the goal is to learn a single predictor competitive with
  the best in a hypothesis class for every group in a potentially large
  collection. Provides algorithms with sample complexity logarithmic in the
  number of groups under natural conditions. Unifies previous positive and
  negative results from multi-group fairness literature.

  RELEVANCE: Theoretical foundation for understanding when fairness across many
  (potentially intersectional) groups is achievable and at what sample complexity
  cost. The logarithmic sample complexity in number of groups suggests that
  expanding fairness auditing to cover more intersectional groups may be
  statistically tractable under appropriate conditions. However, characterization
  of which loss functions admit solutions reveals fundamental limitations—not all
  fairness objectives can be simultaneously satisfied across many groups.

  POSITION: Theoretical PAC learning framework for multi-group fairness.
  },
  keywords = {multi-group-fairness, PAC-learning, sample-complexity, theoretical, Medium}
}

@inproceedings{geddes2022death,
  author = {Geddes, Katrina},
  title = {The Death of the Legal Subject: How Predictive Algorithms Are (Re)constructing Legal Subjectivity},
  booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2022},
  pages = {1741--1751},
  doi = {10.1145/3531146.3533134},
  note = {
  CORE ARGUMENT: Argues that predictive algorithms in legal decision-making are
  reconstructing legal subjectivity from a socio-political conception based on
  individual autonomy to an algorithmic conception based on population-level
  patterns. This shift displaces qualitative knowledge about individuals'
  intentions and capabilities with statistically-derived predictions,
  fundamentally altering the epistemology of legal subjectivity in ways
  incompatible with law's normative commitments.

  RELEVANCE: Illuminates how statistical prediction from population data—the core
  of fairness auditing—transforms the epistemic status of individuals by
  privileging population-level patterns over individual knowledge. When fairness
  auditing relies on group-level statistics, it may constitute epistemic injustice
  by discounting individuals' capacity to provide knowledge about their own
  situations. Particularly relevant for understanding the "death of the legal
  subject" as an epistemic phenomenon when data sparsity forces reliance on
  broader population patterns.

  POSITION: Critical analysis of algorithmic reconstruction of subjectivity and
  epistemic authority.
  },
  keywords = {algorithmic-subjectivity, epistemic-authority, legal-systems, Medium}
}

@article{holm2023statistical,
  author = {Holm, Sune},
  title = {Statistical evidence and algorithmic decision-making},
  journal = {Synthese},
  year = {2023},
  volume = {202},
  number = {1},
  pages = {1--16},
  doi = {10.1007/s11229-023-04246-8},
  note = {
  CORE ARGUMENT: Presents philosophical argument against relying exclusively on
  algorithmic predictions when they provide purely statistical evidence, drawing
  on epistemological work on statistical evidence. Argues that even if algorithmic
  predictions increase proportion of correct decisions, their epistemic deficiency
  as purely statistical evidence may make them inappropriate for resource
  allocation decisions affecting individuals.

  RELEVANCE: Provides epistemological grounding for concerns about fairness
  auditing based on statistical patterns from sparse data. If purely statistical
  evidence is epistemically deficient for individual decisions, then fairness
  audits relying on statistical inference from small samples may be doubly
  problematic: both statistically uncertain AND epistemically inappropriate. The
  argument suggests that uncertainty from sparse data is not just a quantitative
  problem but a qualitative epistemic limitation.

  POSITION: Philosophical analysis of statistical evidence in algorithmic
  decision-making.
  },
  keywords = {statistical-evidence, epistemology, algorithmic-decisions, Medium}
}

@article{kiviat2023moral,
  author = {Kiviat, Barbara},
  title = {The Moral Affordances of Construing People as Cases: How Algorithms and the Data They Depend on Obscure Narrative and Noncomparative Justice},
  journal = {Sociological Theory},
  year = {2023},
  volume = {41},
  number = {3},
  pages = {175--200},
  doi = {10.1177/07352751231186797},
  note = {
  CORE ARGUMENT: Argues that algorithmic systems' dependence on rendering people
  as discrete "cases" defined by regularized attributes obscures narrative and
  noncomparative justice. While this enables computation and comparative
  assessments, it systematically eliminates information about individuals as
  actors in unfolding life narratives. The paper analyzes how different moral
  standards require different information infrastructures, with case-based systems
  affording only certain types of justice.

  RELEVANCE: Illuminates an epistemic dimension of the data sparsity problem:
  fairness auditing's reliance on statistical cases obscures precisely the
  narrative and contextual knowledge often possessed by marginalized groups about
  their own experiences. Sparse data compounds this problem by forcing even
  greater reliance on limited categorical attributes rather than rich narratives.
  Connects technical constraints (need for regularized data) to epistemic
  injustices (exclusion of narrative knowledge).

  POSITION: Sociological analysis of categorization, classification, and moral
  reasoning in algorithmic systems.
  },
  keywords = {categorization, epistemic-infrastructure, noncomparative-justice, Medium}
}

@article{lo2024bringing,
  author = {Lo, Victor S. Y. and Datta, Soham and Salami, Youssouf},
  title = {Bringing practical statistical science to AI and predictive model fairness testing},
  journal = {AI and Ethics},
  year = {2024},
  volume = {5},
  pages = {2149--2164},
  doi = {10.1007/s43681-024-00518-2},
  note = {
  CORE ARGUMENT: Advocates for applying rigorous statistical science to fairness
  testing, emphasizing proper hypothesis testing, confidence intervals, and
  sample size calculations. The authors argue that much current fairness auditing
  lacks statistical rigor, leading to unreliable conclusions. They provide
  practical guidance for determining adequate sample sizes and interpreting
  statistical significance in fairness assessments.

  RELEVANCE: Bridges statistical methodology and fairness auditing practice,
  highlighting the importance of statistical rigor when reasoning under
  uncertainty. The emphasis on sample size calculations and hypothesis testing
  directly addresses challenges of auditing with sparse data. However, the paper's
  focus on conventional statistical significance may not fully address epistemic
  justice concerns about whose knowledge and experiences are rendered
  statistically invisible through sample size requirements.

  POSITION: Methodological guidance for statistical rigor in fairness testing.
  },
  keywords = {statistical-methodology, fairness-testing, hypothesis-testing, Low}
}

@article{vabalas2019machine,
  author = {Vabalas, Andrius and Gowen, Emma and Poliakoff, Ellen and Casson, Alexander J.},
  title = {Machine learning algorithm validation with a limited sample size},
  journal = {PLoS ONE},
  year = {2019},
  volume = {14},
  number = {11},
  doi = {10.1371/journal.pone.0224365},
  note = {
  CORE ARGUMENT: Demonstrates through simulations that K-fold cross-validation
  produces strongly biased performance estimates with small sample sizes, with
  bias evident even at sample size 1000. Nested CV and train/test splits produce
  robust estimates regardless of sample size. Feature selection on pooled data
  contributes substantially to bias. Provides methodological recommendations for
  validation with small datasets.

  RELEVANCE: Critical methodological guidance for fairness auditing with sparse
  data. If standard validation methods are unreliable with small samples, then
  expanding fairness auditing to cover more intersectional groups (thereby
  reducing sample size per group) requires methodological adaptations to avoid
  biased fairness assessments. The findings suggest that methodological rigor
  alone cannot fully overcome sample size limitations, but can mitigate bias.

  POSITION: Methodological study on ML validation with small samples, highly
  cited.
  },
  keywords = {sample-size, validation-methods, methodology, machine-learning, Low}
}

@inproceedings{baeyaert2024epistemic,
  author = {Baeyaert, Joffrey},
  title = {Epistemic Injustice in Generative AI: A Pipeline Taxonomy, Empirical Hypotheses, and Stage-Matched Governance},
  journal = {arXiv},
  year = {2024},
  arxivId = {2411.xxxxx},
  note = {
  CORE ARGUMENT: Develops a mutually exclusive and collectively exhaustive (MECE)
  taxonomy mapping testimonial, hermeneutical, and distributive epistemic
  injustices onto four stages of generative AI development: data collection,
  model training, inference, and dissemination. Proposes stage-matched governance
  strategies to diagnose, test, and mitigate epistemic harms. Emphasizes how data
  collection stage particularly affects marginalized communities' visibility and
  intelligibility.

  RELEVANCE: While focused on generative AI, the pipeline taxonomy applies to
  fairness auditing systems. The emphasis on data collection stage as a site of
  distributive epistemic injustice directly addresses concerns about whose data is
  collected, in what quantities, and how this shapes downstream fairness
  assessments. The taxonomy provides conceptual framework for understanding how
  sparse data for marginalized groups creates cascading epistemic injustices
  through the ML pipeline.

  POSITION: Systematic taxonomy of epistemic injustice in AI development pipeline.
  },
  keywords = {epistemic-injustice, AI-pipeline, generative-AI, taxonomy, Low}
}

@article{wachter2021why,
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  title = {Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI},
  year = {2021},
  doi = {10.1016/j.clsr.2021.105567},
  journal = {Computer Law \& Security Review},
  volume = {41},
  pages = {105567},
  note = {
  CORE ARGUMENT: Fairness metrics cannot be automated to comply with EU
  non-discrimination law because they fail to capture the contextual,
  substantive nature of discrimination. Legal fairness requires considering
  intent, context, and structural factors that resist mathematical
  formalization. Current fairness metrics measure correlation, not causation
  or discrimination as legally understood.

  RELEVANCE: Provides legal-philosophical grounding for why fairness
  formalization has inherent limits. Suggests the intersectionality dilemma
  may be one instance of a broader problem: legal and moral conceptions of
  fairness exceed what formal metrics can capture. Argues that fairness
  auditing should complement, not replace, human judgment.

  POSITION: Strong critique of automation; argues fairness fundamentally
  requires human judgment and cannot be reduced to statistical criteria.
  },
  keywords = {automation-limits, legal-fairness, critique-formalization, High}
}

@inproceedings{green2018myth,
  author = {Green, Ben and Hu, Lily},
  title = {The Myth in the Methodology: Towards a Recontextualization of Fairness in Machine Learning},
  year = {2018},
  booktitle = {Proceedings of the Machine Learning: The Debates Workshop at the 35th International Conference on Machine Learning},
  note = {
  CORE ARGUMENT: The methodology of fairness in machine learning embeds a
  "myth" that fairness can be achieved through decontextualized mathematical
  metrics applied to isolated decision points. This abstracts away from the
  social structures that create and maintain inequality. Fairness research
  needs recontextualization to address structural injustice.

  RELEVANCE: Early statement of the critique that formal fairness metrics
  obscure structural context. Anticipates the impossibility-as-methodology
  argument in Green (2022). Suggests the intersectionality dilemma may
  reflect deeper problems with decontextualized fairness assessment, as
  intersectional identities are inherently context-dependent.

  POSITION: Foundational critique arguing fairness metrics rest on flawed
  methodological assumptions about decontextualization.
  },
  keywords = {critique-formalization, structural-injustice, methodology, High}
}

@article{kearns2018preventing,
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  title = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
  year = {2018},
  doi = {10.48550/arxiv.1711.05144},
  journal = {Proceedings of the 35th International Conference on Machine Learning},
  volume = {80},
  pages = {2564--2572},
  note = {
  CORE ARGUMENT: Statistical fairness constraints are susceptible to
  "fairness gerrymandering," where a classifier appears fair on predefined
  groups but badly violates fairness on structured subgroups defined over
  protected attributes. Proposes demanding fairness across exponentially many
  subgroups, but proves this is computationally equivalent to weak agnostic
  learning (hard in worst case). Provides algorithms that converge to best
  fair classifier given oracles for agnostic learning.

  RELEVANCE: Directly demonstrates a technical impossibility result related
  to intersectionality. Shows that auditing fairness over intersectional
  subgroups is computationally hard, even with simple subgroup definitions.
  This suggests the intersectionality dilemma may have computational as well
  as conceptual dimensions. The gerrymandering problem mirrors concerns about
  single-axis fairness missing intersectional harms.

  POSITION: Technical contribution showing fundamental computational limits;
  not a normative critique but a feasibility result.
  },
  keywords = {fairness-gerrymandering, impossibility-results, subgroup-fairness, High}
}

@inproceedings{kasirzadeh2022algorithmic,
  author = {Kasirzadeh, Atoosa},
  title = {Algorithmic Fairness and Structural Injustice: Insights from Feminist Political Philosophy},
  year = {2022},
  doi = {10.1145/3514094.3534188},
  booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
  pages = {399--408},
  note = {
  CORE ARGUMENT: Algorithmic fairness does not accommodate structural
  injustices in its current scope. Drawing on Iris Marion Young's work on
  structural injustice, Kasirzadeh argues that fairness metrics rooted in
  distributive justice ideals cannot address oppressive social structures.
  Proposes "responsible algorithmic fairness" paradigm that expands scope to
  include structural dimensions.

  RELEVANCE: Feminist philosophical critique linking fairness formalization
  limits to broader debates about justice paradigms. Argues the problem is
  not just technical constraints but conceptual: distributive justice
  frameworks (which underpin most fairness metrics) miss structural
  oppression. Suggests intersectionality dilemma may be irresolvable within
  distributive fairness paradigm.

  POSITION: Structural injustice critique from feminist philosophy; calls for
  paradigm shift beyond distributive justice.
  },
  keywords = {structural-injustice, feminist-critique, political-philosophy, High}
}

@inproceedings{hampton2021black,
  author = {Hampton, Lelia Marie},
  title = {Black Feminist Musings on Algorithmic Oppression},
  year = {2021},
  doi = {10.1145/3442188.3445929},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages = {122--124},
  note = {
  CORE ARGUMENT: Uses Black feminist theory to ground and extend the concept
  of algorithmic oppression, critiquing the language and assumptions of the
  fairness, accountability, and transparency (FAccT) community. Highlights
  the "double bind" of technology for marginalized communities and calls for
  abolition and empowerment rather than fairness optimization.

  RELEVANCE: Critical race feminist perspective arguing that fairness
  discourse itself may be inadequate for addressing algorithmic oppression.
  Suggests the intersectionality dilemma may not be resolvable through better
  fairness metrics because the fairness framework itself inadequately captures
  Black feminist concerns. Points toward abolition rather than reform.

  POSITION: Radical critique from Black feminism; questions whether fairness
  framework itself is appropriate for addressing algorithmic harm.
  },
  keywords = {Black-feminism, algorithmic-oppression, critique-fairness, High}
}

@article{valdivia2022elephant,
  author = {Valdivia, Ana and Serrajòrdia, Júlia Corbera and Swianiewicz, Aneta},
  title = {There is an Elephant in the Room: Towards a Critique on the Use of Fairness in Biometrics},
  year = {2022},
  doi = {10.1007/s43681-022-00249-2},
  journal = {AI and Ethics},
  volume = {3},
  pages = {1407--1422},
  note = {
  CORE ARGUMENT: Algorithmic fairness has been adopted by biometrics without
  critical analysis of political consequences. Proves biometrics will always
  be biased, yet argues fairness cannot distribute justice in scenarios whose
  intended purpose is to discriminate. By focusing on demographic bias rather
  than how systems reproduce historical and political injustices, fairness
  has "overshadowed the elephant in the room."

  RELEVANCE: Demonstrates a domain where fairness metrics are conceptually
  inappropriate—not because of technical limits but because the underlying
  purpose is problematic. Suggests there may be contexts where improving
  fairness metrics is misguided. Relevant to whether intersectional fairness
  auditing is an appropriate goal or whether some systems should not be
  deployed regardless of fairness properties.

  POSITION: Domain-specific critique arguing fairness improvements can
  legitimize fundamentally unjust systems.
  },
  keywords = {biometrics, critique-fairness, political-critique, High}
}

@article{buyl2024inherent,
  author = {Buyl, Maarten and De Bie, Tijl},
  title = {Inherent Limitations of AI Fairness},
  year = {2024},
  doi = {10.1145/3624700},
  journal = {Communications of the ACM},
  volume = {67},
  number = {1},
  pages = {48--55},
  note = {
  CORE ARGUMENT: AI fairness has inherent limitations that must be
  acknowledged. Fairness is not a panacea and requires critical thought and
  outside help to achieve positive societal impact. Technical fairness
  interventions alone are insufficient without addressing broader social
  context and power structures.

  RELEVANCE: Recent synthesis of inherent limitations arguments. Provides
  accessible overview of why fairness formalization has limits, useful for
  contextualizing the intersectionality dilemma within broader limitations
  landscape. Emphasizes need for sociotechnical rather than purely technical
  approaches.

  POSITION: Moderate critique acknowledging value of fairness metrics while
  highlighting inherent limitations.
  },
  keywords = {limitations-fairness, sociotechnical, synthesis, Medium}
}

@article{fleisher2021whats,
  author = {Fleisher, Will},
  title = {What's Fair about Individual Fairness?},
  year = {2021},
  doi = {10.2139/ssrn.3819799},
  journal = {SSRN Electronic Journal},
  note = {
  CORE ARGUMENT: Individual fairness (treating similar individuals similarly)
  faces four in-principle problems: (1) similar treatment is insufficient for
  fairness (counterexamples exist), (2) learning similarity metrics risks
  encoding implicit bias, (3) individual fairness requires prior moral
  judgments limiting its usefulness as fairness definition, and (4)
  incommensurability of moral values makes similarity metrics impossible for
  many tasks.

  RELEVANCE: Demonstrates conceptual problems with individual fairness, which
  is often proposed as alternative to group fairness. If both group and
  individual fairness face fundamental limitations, this strengthens the case
  that the intersectionality dilemma reflects deeper problems with fairness
  formalization. The incommensurability argument is particularly relevant to
  intersectionality.

  POSITION: Conceptual critique of individual fairness showing it faces
  in-principle problems beyond technical implementation.
  },
  keywords = {individual-fairness, conceptual-critique, incommensurability, Medium}
}

@article{ziosi2024genealogical,
  author = {Ziosi, Marta and Watson, David and Floridi, Luciano},
  title = {A Genealogical Approach to Algorithmic Bias},
  year = {2024},
  doi = {10.2139/ssrn.4734082},
  journal = {SSRN Electronic Journal},
  note = {
  CORE ARGUMENT: The FAccT literature tends to focus on bias as requiring ex
  post solutions (fairness metrics) rather than addressing underlying social
  and technical conditions that produce it. Proposes genealogy as
  constructive epistemic critique to explain algorithmic bias in terms of its
  enabling conditions rather than outcomes alone.

  RELEVANCE: Methodological critique arguing fairness metrics address
  symptoms not causes. Suggests the intersectionality dilemma may persist
  because auditing approaches focus on measuring outcomes rather than
  understanding generative processes. Points toward alternative epistemology
  for bias research.

  POSITION: Methodological critique proposing genealogical alternative to
  metric-based fairness approaches.
  },
  keywords = {genealogical-method, bias-critique, methodology, Medium}
}

@article{green2020false,
  author = {Green, Ben},
  title = {The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness},
  year = {2020},
  doi = {10.1145/3351095.3372869},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages = {594--606},
  note = {
  CORE ARGUMENT: Risk assessments fail to provide objectivity and create
  numerous sites of discretion. They provide no guarantee of reducing
  incarceration and risk legitimizing structural racism. Reinterprets
  impossibility results as evidence of deeper tension between notions of
  equality (formal vs. substantive), not just mathematical metrics. Argues
  "fair" algorithms can reinforce discrimination.

  RELEVANCE: Earlier development of Green's substantive fairness argument,
  specifically in criminal justice context. Shows how impossibility results
  can be interpreted as conceptual not just mathematical tensions. Relevant
  to understanding whether intersectionality dilemma reflects formal vs.
  substantive equality tension.

  POSITION: Epistemic critique of risk assessment fairness arguing for
  structural rather than procedural reform approach.
  },
  keywords = {risk-assessment, criminal-justice, epistemic-reform, Medium}
}

@article{lopez2024susceptibility,
  author = {Lopez, Paola},
  title = {More than the Sum of its Parts: Susceptibility to Algorithmic Disadvantage as a Conceptual Framework},
  year = {2024},
  doi = {10.1145/3630106.3658944},
  booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages = {1713--1726},
  note = {
  CORE ARGUMENT: Drawing from Elizabeth Anderson's relational equality,
  introduces "susceptibility to algorithmic disadvantage" framework with
  vertical dimension (state-individual relation) and horizontal dimension
  (intersectional inequalities). Argues these dimensions co-constitute and
  reinforce each other—more than sum of parts—paralleling intersectional
  feminist insight that interlocking oppressions exceed single-axis sum.

  RELEVANCE: Explicitly connects intersectionality theory to algorithmic
  fairness critique. The "more than sum of parts" argument directly parallels
  the intersectionality dilemma: just as interlocking oppressions exceed
  single-axis analysis, intersectional algorithmic harms may exceed what
  fairness metrics can capture. Provides conceptual framework for
  understanding why fairness on single axes may not aggregate to fairness
  across intersections.

  POSITION: Relational equality framework highlighting how intersectional
  harms emerge from interaction of vertical and horizontal dimensions.
  },
  keywords = {intersectionality, relational-equality, conceptual-framework, High}
}

