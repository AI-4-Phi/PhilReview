---
title: The Intersectionality Dilemma for Algorithmic Fairness
date: '2026-01-10'
---

## Introduction

Intersectional fairness auditing in algorithmic systems presents a deceptively simple goal: ensure equitable treatment across groups defined by multiple, overlapping social categories. Yet beneath this straightforward objective lies a fundamental dilemma that has received insufficient recognition in both technical and philosophical literatures. Achieving intersectional fairness requires two distinct capabilities: first, estimating algorithmic performance across demographic groups with statistical reliability; second, specifying which groups warrant consideration in the first place. The first challenge confronts statistical uncertainty arising from sparse data on small intersectional subgroups. The second confronts ontological uncertainty about how social groups are constituted and which categories deserve protection. We argue that these are not independent optimization problems but interacting constraints that form a genuine dilemma.

Crenshaw's (1989) foundational analysis demonstrated how antidiscrimination law systematically failed Black women by treating race and sex as mutually exclusive categories, unable to recognize discrimination targeting their intersection. This legal insight has motivated extensive technical work in machine learning fairness. Recent algorithmic approaches have achieved remarkable sophistication: multicalibration methods can ensure calibrated predictions across exponentially many subgroups without explicit enumeration (Hébert-Johnson et al. 2018), while statistical frameworks provide rigorous hypothesis testing for fairness auditing across rich subgroup collections (Cherian and Candès 2023). Yet these technical solutions share a critical assumption: they take the set of relevant groups as given. Multicalibration requires pre-specifying which group-defining functions to include; differential fairness assumes protected attributes are determined in advance; auditing frameworks test disparities across predetermined subgroups.

The philosophical literature on social ontology reveals why group specification cannot be treated as a simple input to fairness algorithms. Haslanger (2012) argues that race and gender are not attribute-based categories but social positions constituted through systematic relations of subordination and privilege within hierarchical structures. On this political constructionist view, group membership depends fundamentally on structural positioning and social practices, not on possessing observable demographic features. Similarly, Ásta (2018) develops a conferralist framework showing that social properties are constituted through communal meaning-making rather than intrinsic attributes—the same constellation of features can ground different social properties in different contexts. If such practice-based accounts are correct, algorithmic systems cannot derive relevant groups by mechanically combining demographic attributes, because what makes a collection of people a morally salient social group is not reducible to their shared features but involves their positioning within contested social structures.

The interaction between statistical and ontological uncertainty creates the core dilemma. Expanding intersectional coverage to address ontological adequacy—ensuring all relevant group intersections are monitored—necessarily reduces per-group sample sizes, undermining the statistical reliability required for valid fairness measurement. Conversely, constraining coverage to maintain statistical rigor requires determining which groups matter most, a question that ontological debates reveal to be fundamentally contested. Kong (2022) identifies this tension as a "dilemma between infinite regress and fairness gerrymandering": either continuously split groups into finer intersectional categories (facing ever-smaller samples), or arbitrarily select which groups to protect (enabling strategic manipulation of fairness metrics through selective group specification). Yet even Kong's analysis treats statistical sparsity and group selection as conceptually separate problems rather than recognizing their structural interaction.

This review synthesizes literatures that have largely developed in parallel. Section 1 examines technical approaches to intersectional fairness, showing how multicalibration frameworks, sparse data methods, and fairness gerrymandering research have advanced the field while leaving the ontological dimension unaddressed. Section 2 analyzes philosophical debates about intersectionality and social group constitution, revealing deep disagreement between analytical/causal and hermeneutic/emergent interpretations that resist algorithmic resolution. Section 3 investigates measurement validity challenges, demonstrating that fairness metrics face construct validity problems even when groups are specified. Section 4 explores normative pluralism, showing how different ethical frameworks yield incompatible answers about which groups warrant protection. Section 5 synthesizes these strands through the lens of epistemic justice, arguing that the statistical-ontological interaction constitutes a dilemma where addressing one horn exacerbates the other.

Our central claim is that existing work has failed to recognize this interaction as constituting a genuine dilemma. Technical research treats sparse data as an engineering challenge solvable through better methods; philosophical work analyzes group constitution without engaging statistical feasibility constraints; even philosophical-technical bridges identify both dimensions but do not theorize their interaction (Himmelreich et al. 2024; Gohar and Cheng 2023). By framing the statistical-ontological interaction as a dilemma with intractable horns, we provide a principled account of why both technical optimization and purely critical rejection are inadequate responses. The intersectionality problem for algorithmic fairness may be not a matter of insufficient sophistication but a structural feature of attempting to formalize contested normative concepts across indeterminate social categories.

## Section 1: Technical Approaches to Intersectional Fairness and Their Limits

The machine learning community has developed sophisticated technical solutions to the challenge of ensuring fairness across intersectional groups. These approaches—particularly multicalibration frameworks, differential fairness methods, and statistical auditing techniques—demonstrate remarkable computational and statistical sophistication in handling exponentially many overlapping demographic categories. However, their sophistication reveals a fundamental limitation: all assume that the relevant groups are specified in advance. This section establishes that even the most advanced technical solutions treat group specification as an input, not as a contested normative-ontological question, thereby leaving half of the intersectionality dilemma unaddressed.

### Subsection 1.1: Multicalibration and Rich Subgroup Frameworks

The multicalibration framework represents the most technically sophisticated approach to intersectional fairness in the algorithmic fairness literature. Introduced by Hébert-Johnson et al. (2018), multicalibration requires that a predictor be calibrated not only overall but simultaneously across a rich collection of overlapping subgroups. Unlike traditional calibration, which ensures that predicted probabilities match observed frequencies in aggregate, multicalibration demands this property hold for every subgroup defined by an efficiently computable function. This directly addresses the intersectional fairness challenge: predictions should be calibrated not just for "women" or "Black individuals" separately, but for all intersections like "Black women," "elderly Black women," and so on.

The key technical innovation is demonstrating that achieving multicalibration is computationally tractable despite the exponential explosion of potential subgroups. Hébert-Johnson et al. (2018) provide efficient algorithms that achieve multicalibration without explicitly enumerating all subgroups. This solves what appears to be an insurmountable combinatorial problem: with $k$ binary attributes, there are $2^k$ possible intersections. The multicalibration algorithm sidesteps exhaustive enumeration by identifying subgroups where calibration fails and iteratively correcting predictions for those groups. This algorithmic elegance demonstrates that the statistical side of intersectional fairness—ensuring fairness across exponentially many groups—is not computationally intractable.

Recent extensions have refined the framework to address statistical feasibility. Gopalan et al. (2022) introduce low-degree multicalibration as a middle ground between weak multiaccuracy and strong multicalibration. Their central observation is that achieving full multicalibration requires exponentially growing data, but many desirable fairness properties manifest as low-degree properties requiring only polynomial sample complexity. This addresses the fundamental tension between intersectional coverage and statistical power: as the number of protected groups increases, each individual group becomes smaller, reducing the precision of fairness estimates. Low-degree multicalibration achieves key fairness guarantees while managing this sample complexity explosion.

The framework continues to evolve. Lee et al. (2025) extend multicalibration beyond categorical attributes to handle continuous and mixed-type protected characteristics through distance covariance regularization. This addresses a limitation of earlier approaches that assumed discrete demographic categories—a significant advancement given that many socially relevant attributes (age, income, disability severity) are continuous. Their extension demonstrates ongoing refinement of technical approaches to intersectional complexity.

Yet these sophisticated solutions share a critical assumption: the class of group-defining functions must be specified in advance. Multicalibration requires choosing which functions over attributes define protected groups, but provides no principled method for making this choice. Hébert-Johnson et al. (2018) note that their framework applies to any efficiently computable class of functions, but this flexibility reveals rather than resolves the problem: which functions should be included? The technical machinery assumes this question has been answered, treating it as an input rather than a problem to solve. This is the first glimpse of the ontological dimension that technical approaches cannot address.

### Subsection 1.2: Sparse Data and Statistical Reliability

While multicalibration demonstrates computational tractability, statistical reliability for small intersectional groups remains a fundamental challenge. Recent work explicitly addresses sparse data problems, developing methods to handle the long tail of small subgroups that inevitably emerge when multiple demographic attributes intersect.

Ferrara et al. (2025) propose size-adaptive hypothesis testing that adjusts statistical methodology based on group size. For large groups, they employ standard Central Limit Theorem-based inference; for small groups where asymptotic approximations fail, they use Bayesian Dirichlet-multinomial estimators. This adaptive approach acknowledges a fundamental reality: standard statistical methods that rely on large-sample asymptotics simply fail for small intersectional groups. The solution is methodological pluralism—different statistical techniques for different group sizes—rather than pretending a single approach works universally.

Cherian and Candès (2023) develop a comprehensive statistical inference framework for fairness auditing across rich subgroup collections. Their approach uses multiple hypothesis testing with bootstrap methods to control false discovery rates when simultaneously testing fairness across many groups. This addresses a critical problem: when evaluating thousands of potential intersectional groups, some will appear unfair by chance alone. Rigorous auditing requires careful statistical control to distinguish genuine disparities from random variation. Their framework demonstrates that statistically valid intersectional fairness evaluation is possible, but demands sophisticated multiple testing corrections that account for dependencies between overlapping groups.

However, these statistical solutions cannot eliminate a fundamental trade-off. Zhioua et al. (2025) distinguish two types of sampling bias in fairness evaluation: sample size bias (insufficient observations) and underrepresentation bias (systematic undersampling of certain groups). They show these biases compound for marginalized groups—precisely those groups for which fairness evaluation is most critical. Even with sophisticated statistical methods, expanding intersectional coverage necessarily fragments the dataset into smaller groups, reducing per-group statistical power.

Jourdan et al. (2023) provide empirical evidence of this reliability problem, demonstrating that fairness metrics yield unreliable and diverging results with small samples. They show that conclusions about algorithmic fairness can reverse depending on which metric is used when sample sizes are small—the very situation that prevails for intersectional groups. Similarly, Singh and Chunara (2023) analyze sample size requirements for disparity estimation, revealing that detecting fairness violations requires larger samples than often assumed. Under fixed data budgets, there is an unavoidable trade-off: more intersectional coverage means less precision per group.

This statistical literature establishes that data sparsity is not merely an engineering inconvenience to be solved with better methods. Even with sophisticated statistical techniques—adaptive testing, multiple comparison corrections, Bayesian estimators—expanding the set of protected groups necessarily reduces the reliability of per-group fairness assessments. Statistical rigor cannot eliminate this trade-off; it can only make it explicit and manage it systematically.

### Subsection 1.3: Fairness Gerrymandering and the Group Specification Problem

The fairness gerrymandering literature reveals that the ontological question of which groups to protect has measurable technical consequences, not merely philosophical ones. Fairness gerrymandering occurs when a classifier appears fair according to predefined demographic categories but violates fairness on intersectional subgroups not explicitly evaluated. This demonstrates that group specification choices directly determine whether algorithms are judged fair or unfair.

Kong (2022) provides the most explicit philosophical analysis of this problem in the machine learning literature. She argues that the dominant statistical parity approach to intersectional fairness faces a "dilemma between infinite regress and fairness gerrymandering." Either we continuously split protected groups into ever-finer intersectional subgroups—leading to infinite regress and statistical impossibility as groups become arbitrarily small—or we arbitrarily select which groups to protect, enabling strategic manipulation through gerrymandering. Kong frames this as a fundamental conceptual problem rather than merely a technical optimization challenge, explicitly connecting it to questions about oppression and which groups warrant protection.

Tian et al. (2024) demonstrate this problem empirically with their MultiFair system. They show that algorithms achieving per-attribute fairness (fair for "gender" and "race" separately) systematically fail to achieve fairness for intersectional groups (Black women, Asian men). This is not a theoretical possibility but an observed pattern: naive approaches to multi-attribute fairness leave intersections unprotected. The solution they propose—explicit intersectional modeling—requires pre-specifying which intersections matter, circling back to the group specification problem.

Kearns et al. (2018) prove that preventing fairness gerrymandering requires ensuring fairness across exponentially many subgroups, a problem computationally equivalent to agnostic learning—widely considered intractable for rich hypothesis classes. This establishes a computational complexity bound: if we want to guarantee fairness against all possible ways of gerrymandering through group selection, we face computational barriers that cannot be overcome with better algorithms. The multicalibration framework addresses this by restricting attention to efficiently computable functions, but this restriction is precisely what enables potential gerrymandering through strategic choice of the function class.

Räz (2023) extends the gerrymandering critique beyond group fairness to individual fairness, showing the problem is fundamental rather than specific to group-based approaches. Individual fairness requires treating similar individuals similarly, but Räz proves this can be gerrymandered through strategic choices about the feature space and similarity metric. The manipulation is structural: fairness metrics can be satisfied or violated through choices about problem representation that precede algorithmic design.

These results reveal that fairness gerrymandering is not a bug to be fixed but a consequence of ontological uncertainty. Without a principled method for determining which groups exist or which similarity metrics are appropriate, fairness metrics can always be gamed through strategic selection. The technical literature has identified this problem clearly—Kong's (2022) "dilemma" language is explicit about its fundamental nature—but cannot solve it within technical frameworks because it arises from contested normative-ontological questions about group constitution.

### Subsection 1.4: Recent Philosophical-Technical Syntheses

Recent work has begun to bridge technical machine learning and philosophical analysis, explicitly recognizing that intersectional fairness involves both statistical and normative dimensions. Himmelreich et al. (2024) provide the most direct statement of this dual nature, distinguishing statistical challenges (small intersectional groups) from moral-methodological challenges (which groups matter). They develop desiderata for intersectional fairness solutions and propose hypothesis testing approaches, demonstrating tradeoffs between statistical power and intersectional coverage. Their framework acknowledges that the intersectionality problem cannot be solved through statistical sophistication alone; it requires normative clarity about which groups warrant consideration.

Gohar and Cheng (2023) survey the intersectional fairness landscape, identifying data scarcity as a key challenge alongside computational complexity and metric selection. Their comprehensive review taxonomizes different technical approaches—multicalibration, differential fairness, subgroup fairness—while acknowledging that all face the sparse data problem when applied to intersectional groups. The survey format allows them to map the solution space while highlighting persistent challenges.

However, even these philosophical-technical syntheses treat statistical and ontological dimensions as separate optimization problems rather than as interacting constraints that form a dilemma. Himmelreich et al. (2024) identify both dimensions but propose solutions addressing each independently: better statistical methods for sparse data and clearer normative frameworks for group selection. What remains untheorized is their interaction: expanding groups for normative adequacy exacerbates statistical unreliability; constraining groups for statistical reliability requires resolving contested ontological questions that admit no consensus solution.

The technical literature has achieved remarkable sophistication in addressing computational and statistical challenges. Multicalibration frameworks prove that calibration across exponentially many groups is tractable. Sparse data methods provide statistical rigor for small intersectional groups. Fairness gerrymandering research demonstrates that strategic group selection undermines metrics. Yet all these advances presuppose that group specifications are inputs to the fairness problem. The ontological question—which groups exist, which groups matter, how to specify them—remains unaddressed because computational methods cannot resolve normative-ontological uncertainty. This sets up the second horn of the dilemma, explored in the next section: philosophical debates reveal that group constitution itself is contested in ways that resist algorithmic formalization.

## Section 2: Ontological Uncertainty—The Social Construction of Intersectional Groups

While technical ML approaches to intersectional fairness have achieved computational tractability through multicalibration and sparse data methods, these solutions uniformly assume that the set of protected groups can be specified in advance. This assumption confronts a fundamental challenge from philosophy: deep disagreement about how social groups are constituted and which intersectional categories exist. Philosophical debates reveal competing frameworks—attribute-based accounts treating groups as combinations of demographic features versus practice-based accounts arguing groups are constituted through social roles, conferral, and structural positioning—that yield incompatible answers about which intersections warrant recognition. This ontological uncertainty is not resolvable through better conceptual analysis; it reflects fundamental disagreement about social reality's structure.

### Subsection 2.1: Intersectionality—Emergence vs. Reduction

Intersectionality theory itself divides over a foundational metaphysical question: whether intersectional experiences emerge as novel phenomena from the conjunction of social categories, or can be reduced to combinations of independent category effects. Jorba and López de Sa (2024) develop the most sophisticated recent defense of emergence, proposing that intersectional experiences arise when social structures make category conjunctions relevant for discrimination and privilege. Crucially, their account maintains "metaphysical neutrality"—compatible with multiple ontologies of base categories—while arguing for "explanatory flexibility" that allows context-dependent emergence. On this view, "Black woman" is not merely a label for the conjunction of being Black and being a woman; rather, the category emerges when specific social structures treat this conjunction distinctly. The emergence framework has direct implications for algorithmic group specification: if relevant categories emerge contextually from how social structures operate, algorithms cannot pre-specify all intersections through combinatorial enumeration of attributes. Fairness systems must remain open to discovering which intersections matter in specific contexts.

Bernstein (2020) sharpens the metaphysical stakes by asking whether intersectional identities are ontologically novel or reducible to constituent categories. Her analysis examines grounding relations: does "Black woman" as a social kind depend on (is grounded in) the categories "Black" and "woman," or does it constitute an independent kind with its own causal powers? This is not merely terminological. If intersectional categories are strongly emergent with novel properties, fairness metrics must recognize them as primitive; if they are reducible, combinatorial approaches suffice. The philosophical debate parallels a technical choice in ML fairness: whether to treat intersections as derived features (attribute combinations) or independent variables requiring separate measurement.

Agent-based modeling research by O'Connor, Bright, and Bruner (2019) demonstrates that intersectional disadvantage can emerge from bargaining dynamics even when only single-axis disadvantages are explicitly encoded. Their simulations show that structural features of coordination problems generate emergent intersectional patterns—Black women face compound disadvantage exceeding the sum of racial and gender effects—without assuming intersectionality at the outset. This finding bridges analytical precision and emergent complexity, suggesting that fairness interventions must consider how algorithmic systems may generate intersectional harms through interaction effects even when intersections aren't explicitly represented. The formal modeling substantiates emergence claims through causal mechanisms rather than phenomenological description.

The emergence position confronts an additive alternative. Yuval-Davis (2006) traces this debate through British feminist theory, contrasting models where categories function as "separate axes summing to total oppression" with "mutually constitutive" frameworks where categories co-construct each other's meaning. Additive models are computationally tractable—fairness metrics can sum disparities across dimensions—but Yuval-Davis argues they "fail to capture how categories shape each other's significance in specific contexts." A Black woman's gender oppression differs qualitatively from a white woman's because race and gender are not independent variables but mutually constituting dimensions of experience. This creates a fundamental tension for algorithmic implementation: mutual constitution resists the decomposition into independent features that computational methods require.

Critical voices within Black feminist theory resist the entire project of formalizing intersectionality. Collins (2019) positions intersectionality as "critical inquiry and praxis" rather than neutral analytic framework, emphasizing how power relations are "mutually constructing across domains—structural, disciplinary, cultural, interpersonal." On this view, treating intersectionality as a technical problem of measuring compound effects domesticates its radical potential as critique of categorical thinking itself. Carastathis (2016) similarly warns against "premature calls to operationalize intersectionality," arguing it should remain a "horizon of future thinking" rather than an "arrived achievement" subject to codification. These critiques suggest that algorithmic attempts to specify intersectional groups face not just technical challenges but resistance from the very theoretical tradition they invoke: intersectionality may be fundamentally at odds with the formalization fairness metrics require.

The emergence-reduction debate is not merely academic. If intersectional categories emerge contextually when social structures make conjunctions salient (Jorba and López de Sa 2024), then there is no fixed set of groups for fairness auditing—relevant categories shift across contexts. If categories are mutually constitutive rather than additive (Yuval-Davis 2006), then combinatorial approaches that enumerate attribute combinations will systematically fail to identify morally relevant groups. And if intersectionality is critical inquiry rather than neutral framework (Collins 2019, Carastathis 2016), then no technical operationalization can be politically innocent. This ontological openness—the possibility that new intersections emerge, that category meanings shift, that formalization distorts—constitutes irreducible uncertainty for algorithmic group specification.

### Subsection 2.2: Social Ontology—Attribute-Based vs. Practice-Based Group Constitution

Beneath debates about intersectionality lie foundational questions in social ontology about how any social groups are constituted. The dominant framework in ML fairness—treating groups as defined by shared demographic attributes that can be combined mechanically—reflects what philosophers call an "attribute-based" account. Practice-based alternatives developed by Haslanger, Mallon, Ásta, and Ritchie argue instead that social groups are constituted through norms, roles, conferral, and structural positioning that cannot be reduced to observable features. This philosophical divide has direct consequences: if practice-based accounts are correct, algorithmic approaches that derive groups from attributes systematically misidentify morally relevant categories.

Haslanger's (2012) political constructionism provides the most influential challenge to attribute-based models. On her view, race and gender are "social positions constituted by systematic relations of subordination and privilege," not collections of individuals who share intrinsic properties. What makes someone a woman, on this ameliorative analysis, is being "systematically subordinated on the basis of observed or imagined bodily features presumed to be evidence of reproductive capacities" (Haslanger 2005, 12). Crucially, group membership is defined not by possessing certain attributes but by occupying a position within hierarchical social structures. Two individuals with identical observable features might occupy different social positions depending on how they are treated, recognized, and positioned by institutions. This structural definition directly undermines attribute-based algorithmic specification: the morally salient aspect of group membership (systematic subordination or privilege) is not reducible to demographic features measurable in datasets but depends on social relations and practices that algorithms cannot directly observe.

Mallon (2016) develops a sophisticated middle position through his account of social kinds as homeostatic property clusters (HPC) stabilized by social roles. On this view, social categories like race and gender are real and explanatorily useful because social roles create systematic correlations between properties—not through biology but through mechanisms like stereotype threat, discrimination, and social norms. A social role generates stable patterns where certain attributes cluster together, making categories scientifically tractable despite their social construction. However, Mallon's framework also reveals the contingency and context-dependence of these correlations: social roles can shift, categories can be reconfigured, and what counts as a relevant grouping depends on historically specific practices. For algorithmic fairness, this means that even when statistical patterns correspond to social groups, these patterns are artifacts of contingent social structures rather than natural divisions. The "right" groups cannot be read off from demographic correlations without understanding the social mechanisms that produce group-differentiated outcomes—mechanisms that vary across contexts and can change over time.

Ásta's (2018) conferralist framework pushes further, arguing that individuals possess social properties "in virtue of communal conferral rather than intrinsic features." On this view, social categories are constituted through practices where communities collectively confer properties onto individuals based on contextually salient characteristics. The same constellation of attributes can ground different social properties in different contexts. Crucially, conferral can be contested, mistaken, or refused—someone classified as a woman by institutional systems may reject that classification, and which classification matters depends on which community's conferral is operative in that context. For intersectional fairness, conferralism creates fundamental indeterminacy: there is no algorithm-independent fact about which combinations of attributes constitute morally relevant groups, since what matters is how communities confer meaning onto combinations through situated interpretive practices. A demographic category like "Hispanic woman" might be conferred differently across institutional contexts (census, employment, criminal justice), and these differences are not mere measurement error but reflect genuinely distinct social properties constituted through different conferral practices.

Ritchie (2018) offers perhaps the most radical departure from attribute-based models through her structural ontology. On this view, social groups are not collections of individuals who share attributes but "networks of relations constitutively dependent on social factors" (629). What makes a collection a social group is not shared properties of members but the pattern of relations between them. Gender groups, racial groups, and other social categories are structures—nodes (individuals) connected by asymmetric, contextual, socially mediated relations—rather than classes defined by common features. This structural account fundamentally challenges combinatorial approaches to intersectionality: if groups are constituted by relational patterns rather than attribute combinations, then specifying groups through demographic feature enumeration misconstrues their ontology. The relational positioning that defines group membership is not captured by individual-level attributes measurable in datasets. An algorithm that identifies "Black women" by selecting individuals with race=Black and gender=woman may capture a statistical category but miss the social structure—the network of relations—that constitutes the group.

Thomasson (2019) complicates matters further by arguing for ontological pluralism about social groups. Different types of groups—gender groups, racial groups, disability groups, socioeconomic classes—may have fundamentally different constitution conditions. Some might be attribute-based, others practice-based, still others structurally constituted. If this pluralism is correct, there cannot be a single algorithmic method for identifying all protected groups. What works for specifying income brackets (arguably attribute-based) will fail for gender (plausibly conferral-based on Ásta's account) and race (structural positioning on Ritchie's view). This pluralism creates irreducible ontological uncertainty: not only is there disagreement about how any particular group is constituted, but there is meta-level disagreement about whether all groups share a common ontological structure.

Recent work by Popescu-Sarry (2023) radicalizes the constructionist critique by arguing that discrimination does not apply to pre-existing groups but rather constitutes groups through differential treatment. On this view, protected characteristics are not given inputs to fairness analysis but contested outputs of discriminatory practices that (re)negotiate social reality. If discrimination itself constructs the groups it targets—treating people as members creates membership—then algorithmic fairness systems that take groups as pre-specified fundamentally misunderstand the phenomenon. The politics of group specification cannot be bracketed as a preliminary technical question because specifying which groups exist is inseparable from the ongoing process through which discrimination constitutes social reality.

These practice-based accounts converge on a challenge to attribute-based algorithmic specification: social groups cannot be adequately captured by demographic feature combinations because group membership is constituted through social structures (Haslanger), contingent role-based mechanisms (Mallon), communal conferral practices (Ásta), relational networks (Ritchie), or discriminatory processes themselves (Popescu-Sarry). These constitution processes resist algorithmic discovery from demographic data. While computational methods can identify statistical patterns, they cannot determine which patterns correspond to morally relevant groups without substantive social-ontological commitments that remain contested. This is ontological uncertainty in its deepest form: not merely disagreement about which groups exist, but fundamental conflict about what it is for a group to exist.

### Subsection 2.3: Methodological Tensions—Ameliorative Analysis and Regulative Ideals

The ontological debates between attribute-based and practice-based accounts are compounded by methodological questions about how to conduct conceptual analysis of social kinds. Should we describe how social categories function in ordinary usage, or should we design improved concepts for emancipatory purposes? This methodological divide—between descriptive and ameliorative analysis—suggests that ontological questions about which groups exist may be partly normative rather than purely empirical or metaphysical, further deepening uncertainty.

Haslanger's (2005) ameliorative approach explicitly rejects purely descriptive analysis of social kinds. Her methodology asks: what concepts of race, gender, and other categories would be most useful for feminist and antiracist critique? This yields definitions potentially diverging from ordinary usage—defining women as "those systematically subordinated on the basis of observed or imagined bodily features" captures what matters for feminist politics but may not match folk concepts. The justification is that ordinary concepts may themselves be ideological, obscuring power relations that critical analysis aims to expose. For algorithmic fairness, ameliorative analysis implies that relevant groups may not be those defined by demographic conventions or common usage but those tracking structural positions of subordination and privilege. Standard demographic categories used in datasets—themselves products of institutional practices like census design—may not capture normatively salient divisions.

Jones (2014) examines whether ameliorative analysis can accommodate intersectionality, questioning whether Black women's subordination requires an integrated account or can be analyzed through separate race and gender analyses. Her discussion reveals a tension: if categories are defined amelioratively by their role in oppression, and intersectional positions face distinctive forms of oppression irreducible to constituent categories, then ameliorative analysis should recognize intersections as primitive. But which intersections? The ameliorative framework lacks resources for determining which conjunctions constitute independent sites of oppression versus which are reducible to component effects. This indeterminacy within ameliorative analysis itself shows that even philosophically sophisticated approaches to group specification face unresolved questions about intersectional boundaries.

Gasdaglis and Madva (2020) propose a different methodological stance that sidesteps ontological commitments entirely. They argue intersectionality should be understood as a "regulative ideal"—a guiding methodological principle rather than substantive metaphysical or empirical thesis. On this interpretation, intersectionality directs us to remain attentive to how categories interact in producing inequality without requiring metaphysical claims about emergence, grounding, or constitution. Existing classification schemes should be treated as "indefinitely mutually informing" and "provisional," always open to revision as we discover new patterns of injustice. This pragmatist move avoids regress problems (which intersections count?) by refusing to fix categories in advance.

The regulative ideal interpretation has attractive features for algorithmic fairness: it suggests systems should treat groups as provisional hypotheses subject to ongoing revision rather than fixed ontological facts. However, it also reveals a fundamental tension: fairness auditing requires specifying which groups to measure, yet regulative ideals provide methodological guidance without delivering determinate groups. A system designer cannot audit "all groups that matter" without some way of determining which groups those are. The regulative approach may show that ontological uncertainty is a feature rather than a bug—categories should remain contestable—but this feature is incompatible with algorithmic systems requiring fixed group definitions for computational tractability.

These methodological debates compound ontological uncertainty in a specific way: they suggest there may be no politically neutral, purely descriptive answer to which groups exist. If the relevant question is ameliorative (which categories serve emancipatory purposes?), then group specification becomes a normative rather than empirical question—and normative questions admit of plural reasonable answers. If intersectionality functions as regulative ideal rather than fixed theory, then any attempt to codify groups for algorithmic purposes violates its methodological spirit by prematurely closing what should remain open. The ontological uncertainty revealed by social ontology is thus not merely a gap in our knowledge but reflects the normative and political dimensions of social classification itself.

---

**Section Summary**: Philosophical examination of social groups reveals deep ontological uncertainty irreducible to better conceptual analysis. Intersectionality theory divides between emergence frameworks (intersections as novel categories) and reductive approaches (intersections as attribute combinations), with critical traditions resisting formalization entirely. Social ontology offers competing accounts—attribute-based models assuming groups are defined by shared features versus practice-based accounts arguing constitution through roles, conferral, relations, and structural positioning. These disagreements are not terminological but reflect fundamental conflicts about social reality's structure. Methodological debates add further complexity: ameliorative analysis makes group specification partly normative, while regulative approaches reject fixed categorization. For algorithmic fairness, this ontological uncertainty means there is no determinate, politically neutral set of groups to specify—different metaphysical and normative commitments yield different answers about which intersections exist and matter.

## Section 3: Measurement Validity—Do Fairness Metrics Measure What They Claim?

Even when intersectional groups are successfully specified, fairness auditing confronts a deeper problem: the metrics used to evaluate systems may fail to measure what they purport to measure. Drawing on measurement theory from the philosophy of science and psychometrics, recent work reveals systematic construct validity failures in fairness metrics. These failures manifest as target specification bias, metric incompatibility, and misalignment between algorithmic measures and human fairness judgments. This section demonstrates that fairness—as a contested normative concept—resists stable operationalization, compounding the dilemma established in earlier sections. Measurement challenges persist even after group specification is resolved.

### Subsection 3.1: Construct Validity and Target Specification Bias

Measurement theory distinguishes between unobservable theoretical constructs (like "intelligence," "toxicity," or "fairness") and their operationalizations through measurable proxies. Valid measurement requires that operationalizations actually capture the construct of interest, a property called construct validity (Jacobs et al. 2020). Recent work applying this framework to fairness metrics reveals troubling validity gaps.

Tal (2023) introduces "target specification bias" to describe a pervasive source of algorithmic bias in healthcare fairness: the operationalization of the target variable fails to match decision-makers' actual concerns. Drawing on metrology—the science of measurement—Tal argues this bias arises because stakeholders are typically interested in predicting outcomes of counterfactual scenarios (what would happen under fair treatment?) rather than actual scenarios (what did happen given existing disparities?). Crucially, this mismatch "persists independently of data limitations and health disparities" (Tal 2023, p. 2). It is a conceptual failure, not a technical one that better data or algorithms could resolve. When fairness auditing operationalizes concepts like "qualified applicant" or "creditworthy borrower" using historical data shaped by discrimination, the resulting metrics measure past discriminatory patterns rather than counterfactual fairness.

This conceptual gap between construct and operationalization pervades fairness benchmarking. Blodgett et al. (2021) apply a "measurement modeling lens" from quantitative social sciences to examine four NLP fairness benchmarks for stereotyping. Their systematic analysis reveals that benchmarks "frequently lack clear articulations of what is being measured" (Blodgett et al. 2021, p. 1004). The frameworks conflate distinct concepts—for instance, treating stereotyping, bias, and discrimination as interchangeable when they represent different theoretical constructs. They also contain unstated assumptions about how stereotyping should be conceptualized and operationalized, creating ambiguities that threaten validity. Without clear construct definitions, evaluators cannot assess whether deviations from expected metric values represent actual fairness problems or measurement artifacts.

Jacobs et al. (2020) formalize this analysis, showing how unobservable theoretical constructs like "creditworthiness" are transformed into measurable quantities and how this process introduces fairness-related harms. They demonstrate that mismatches between constructs and operationalizations create systematic errors: measures intended to capture one property (e.g., risk) instead capture confounded properties (e.g., risk plus historical discrimination). The measurement modeling framework they propose—adapted from educational testing—provides tools for detecting such mismatches through reliability and validity assessments. Yet even with sophisticated measurement methods, the fundamental challenge remains: fairness is precisely the kind of contested normative construct that resists determinate operationalization.

The most comprehensive recent examination of construct validity in AI evaluation comes from Bean et al. (2025), who systematically reviewed 445 large language model benchmarks with 29 expert reviewers. They find pervasive patterns undermining construct validity across benchmarks for complex phenomena like "safety" and "robustness": vague construct definitions, poor alignment between what benchmarks claim to measure and what they actually test, and lack of validation that benchmarks capture properties that matter for real-world performance. Their core insight applies directly to fairness metrics: "measuring complex abstract phenomena requires strong construct validity—having measures that represent what matters" (Bean et al. 2025, p. 1). Without this validity, even mathematically rigorous metrics may not measure fairness at all.

### Subsection 3.2: Empirical Evidence of Metric Unreliability

Theoretical arguments about construct validity find support in empirical studies revealing that fairness metrics are incompatible, unstable, and misaligned with human judgments. These findings suggest that fairness is not a unitary construct admitting single valid operationalizations.

Delobelle et al. (2022) provide systematic evidence that bias metrics for pre-trained language models are "biased rulers"—measurement instruments whose outputs depend heavily on arbitrary methodological choices rather than properties of what's being measured. Comparing multiple intrinsic bias metrics across models, they find that metric values are highly sensitive to template choices, random seed selection, and word embedding methods. Different metrics yield contradictory conclusions about the same model: one metric shows high bias where another shows low bias. Crucially, they find "no tangible evidence of intrinsic bias metrics relating to extrinsic bias in downstream tasks" (Delobelle et al. 2022, p. 1693), suggesting that what intrinsic metrics measure may not track actual fairness in applications. This represents a fundamental construct validity failure: if the operationalization doesn't predict outcomes that matter, it's not measuring the right thing.

Cao et al. (2022) extend this finding through an extensive correlation study across 19 contextualized language models, examining whether intrinsic and extrinsic fairness metrics align. Even after correcting for metric misalignments, evaluation dataset noise, and experimental configurations, they find that the two types of metrics "do not necessarily correlate" (Cao et al. 2022, p. 1). This disconnect persists across different bias notions and model architectures, suggesting a systematic problem: intrinsic metrics may not measure what matters for downstream fairness. The implication is profound—efforts to "debias" models based on intrinsic metrics may not reduce actual unfairness in applications.

Beyond metric incompatibility, fairness metrics fail to align with human fairness judgments. Constantin et al. (2022) developed a mixed-initiative system (FairAlign) where laypeople assess fairness through interactive visualizations, then compared their judgments with automated metrics. They find significant gaps between algorithmic measures and human perceptions of fairness, with cultural and perceptual factors creating divergence. If fairness metrics don't capture what people perceive as fair or unfair, they lack face validity—a basic form of construct validity indicating whether measures appear to capture the intended construct. Similarly, Saha et al. (2019) demonstrate that even when metrics are mathematically well-defined, non-experts systematically misinterpret what they measure, creating comprehension gaps that undermine informed decision-making.

The measurement validity challenge extends beyond metric design to fundamental questions about what fairness metrics can and should measure. Long (2020) provides philosophical analysis showing that false positive rate (FPR) equality—a widely used fairness criterion—is "morally irrelevant" despite being mathematically well-defined and intuitively appealing. Long argues that FPR equality seems fair only through confusion: it appears to equalize burdens across groups, but the specific burden it equalizes (incorrect positive predictions) has no inherent moral significance. The inevitable mathematical tradeoff between calibration and FPR equality reveals that one of these must not be measuring real fairness, since genuine fairness properties should not conflict. Long's analysis exemplifies the philosophical work required to assess construct validity: mathematical rigor doesn't guarantee normative validity.

### Subsection 3.3: Distributive Justice Foundations and Metric Diversity

The proliferation of incompatible fairness metrics partly reflects fairness's conceptual complexity and contested normative foundations. Different metrics operationalize fundamentally different conceptions of what fairness requires, often embedding implicit philosophical commitments about distributive justice.

Baumann et al. (2022) provide a unifying framework linking group fairness metrics to theories of distributive justice. They demonstrate that standard fairness metrics differ systematically in how they conceptualize benefits and harms and what moral claims to benefits they assume. Demographic parity treats equal probability of favorable outcomes as the relevant benefit; equalized odds focuses on equal error rates; and calibration emphasizes equal predictive value of scores. These differences are not merely technical variations but reflect contested normative commitments: "different fairness metrics differ in how they measure benefit/harm and what moral claims to benefits they assume" (Baumann et al. 2022, p. 1). Choosing between metrics requires taking stances on disputed questions in distributive justice—questions that political philosophers have debated for centuries without consensus.

This normative pluralism creates irreducible operationalization challenges. Truong et al. (2025) argue that construct validity problems in fairness measurement stem from "insufficient systematization" of the unfairness construct. They propose a framework based on Fair Equality of Chances from political philosophy, decomposing unfairness into harms/benefits, morally arbitrary factors (like race or gender), and morally decisive factors (like qualifications). This systematization aims to guide valid operationalization by clarifying what fairness metrics should measure. Yet their proposal illustrates the challenge: Fair Equality of Chances itself represents one contested philosophical framework among many. Rawlsian, utilitarian, sufficientarian, and relational egalitarian theories would systematize fairness differently, yielding different operationalizations.

Buijsman (2023) confronts this pluralism directly, arguing that the mathematical impossibility of simultaneously optimizing all fairness metrics creates need for substantive normative theory to navigate metric choices. Drawing on Rawls's theory of justice as fairness, Buijsman proposes prioritizing metrics that have the biggest impact on the most vulnerable groups. This represents a principled approach to operationalization: "metric incompatibility requires normative theory to guide choices" (Buijsman 2023, p. 1323). Yet implementing this approach requires resolving contested questions about which groups are most vulnerable (connecting to the group specification problem from earlier sections) and what forms of disadvantage matter most—precisely the questions on which normative theories diverge.

The measurement challenge extends to practical implementation. Smith et al. (2023) interviewed fairness practitioners working on recommender systems and found they face overwhelming complexity in operationalizing fairness: "The proliferation of fairness metrics creates a complex decision space where practitioners lack guidance on picking metrics appropriate to their context" (Smith et al. 2023, p. 1). Even when metrics are well-defined mathematically, determining which measure what matters in specific contexts requires contextual judgments that resist formalization. Similarly, Sahlgren (2024) identifies an "action-guidance gap" in AI ethics where fairness principles fail to guide practice due to "ambiguous operationalization" of concepts like fairness, equality, and justice (Sahlgren 2024, p. 1019). Construct validity issues directly undermine fairness implementation.

These findings suggest that fairness may be fundamentally resistant to valid operationalization because it is an "essentially contested concept" in W.B. Gallie's sense: a concept whose proper application is inherently disputable despite agreement on general criteria. Different operationalizations don't reflect measurement error or conceptual confusion but embody genuinely different conceptions of what fairness requires. Target specification bias compounds group specification uncertainty: even after determining which intersectional groups to audit, measurement challenges remain because contested normative concepts resist stable formalization.

This reveals a deepening of the dilemma identified in earlier sections. Addressing statistical uncertainty through sparse data methods and addressing ontological uncertainty through philosophical analysis of group constitution are necessary but insufficient. Even with specified groups and adequate samples, measurement validity problems persist because fairness metrics systematically fail to capture what they claim to measure. The next section examines how different normative frameworks generate different answers about which groups matter and how to prioritize them, further compounding these operationalization challenges.

## Section 4: Normative Pluralism—Which Groups Matter and Why?

Technical approaches to intersectional fairness assume the question "which groups matter?" has a determinate, value-neutral answer discoverable from data or demographic attributes. Yet philosophical analysis reveals this question is fundamentally normative: different ethical frameworks yield systematically different answers about which intersectional groups warrant protection and how to prioritize them. This normative dimension compounds the ontological uncertainty examined in Section 2, showing that contestation about group specification is not merely metaphysical but reflects deeper disagreement about the purposes of fairness auditing itself.

### Distributive Justice Frameworks—Priorities and Thresholds

Classical theories of distributive justice—prioritarianism, sufficientarianism, and egalitarianism—differ not merely in how much inequality to tolerate but in which groups matter and why. These differences have direct implications for intersectional fairness auditing.

Sufficientarianism holds that securing sufficiency has special moral importance distinct from equality or maximization. As Shields (2016) argues, properly specified sufficientarian principles focus on weighted priority below thresholds rather than absolute lexical priority, avoiding standard objections about indifference above thresholds. In algorithmic contexts, this implies fairness interventions should prioritize ensuring all intersectional groups reach minimum thresholds—minimum acceptable true positive rates, minimum access to benefits, minimum protection from harms—rather than equalizing outcomes across groups. Groups below thresholds warrant urgent intervention; inequality above thresholds may be tolerable. Timmer (2021) refines this framework by decomposing sufficientarianism into three claims: a priority claim (benefits in certain ranges matter more), a continuum claim (ranges exist on one continuum), and a deficiency claim (lower ranges warrant stronger priority). This tripartite structure allows for multiple thresholds and variable priority weights, suggesting different intersectional groups may have different sufficiency thresholds depending on context—perhaps higher thresholds for groups facing systemic disadvantage. The framework answers the ontological question "which groups matter?" by focusing analytical attention on groups below relevant thresholds rather than attempting comprehensive coverage of all possible intersections.

Prioritarianism offers a contrasting answer. This view assigns greater moral weight to benefits for worse-off individuals without treating equality as intrinsically valuable (Arneson 2000). Unlike egalitarianism, prioritarianism is "impartial between individuals" but non-linear in valuation: benefiting people matters more the worse off those people are. For intersectional fairness, this implies greater moral urgency to improve outcomes for the worst-off intersectional groups—multiply marginalized groups—even if this increases inequality with better-off groups. As Parfit (2012) clarifies in his defense of the priority view, the framework properly captures the moral importance of benefiting disadvantaged individuals without requiring that we treat equality itself as valuable. His discussion of competing claims directly addresses trade-offs in algorithmic fairness: when improving outcomes for some groups worsens outcomes for others, prioritarianism dictates weighting improvements for the worst-off groups more heavily. Critically, prioritarianism yields a different group prioritization than sufficientarianism: while sufficientarians focus on groups below thresholds (wherever they fall in the distribution), prioritarians care most about the absolute worst-off groups regardless of whether they've crossed sufficiency thresholds.

Relational egalitarianism, most prominently defended by Anderson (1999), offers a fundamentally different normative framework that rejects distributive approaches altogether. Anderson critiques luck egalitarianism and distributive egalitarianism for failing to capture what matters about equality, proposing democratic equality as an alternative: the fundamental aim is creating a community where people relate as equals, free from oppression and domination, rather than equalizing distributions of goods. This framework focuses on social relationships and capabilities for democratic participation. Applied to algorithmic fairness, relational egalitarianism asks not whether algorithms achieve statistical parity across intersectional groups but whether they create hierarchies of social status or enable domination. For intersectional fairness auditing, this reframes the central question: rather than measuring error rates or outcome distributions, audits should examine whether algorithms entrench hierarchies between groups—treating multiply marginalized groups as second-class—or undermine their standing as equal participants in democratic society. Anderson's framework thus yields a categorically different answer to "which groups matter": groups facing potential algorithmic domination, not merely groups with unequal distributions.

These competing frameworks generate incompatible group prioritizations. Fazelpour, Lipton, and Danks (2021) further complicate this picture by arguing that fairness should be assessed based on dynamic trajectories rather than static snapshots: temporal evolution, robustness to perturbations, and whose interests are represented matter more than point-in-time parity. Different normative frameworks evaluate these trajectories differently: prioritarians care most about trajectories for worst-off groups, sufficientarians about whether trajectories keep all groups above thresholds over time, egalitarians about whether trajectories maintain parity, relationalists about whether trajectories preserve equal standing. This shifts fairness auditing's purpose from compliance checking to trajectory assessment—a fundamentally different enterprise.

### Applications to Algorithmic Fairness

Recent work directly bridging normative philosophy and algorithmic fairness reveals how these theoretical differences constrain technical implementations. Hertweck, Heitz, and Loi (2024) argue that current algorithmic fairness approaches erroneously link fairness criteria to ideal theories of distributive justice. Since algorithmic decision-making is inherently imperfect, "we should focus on how *deviations* from ideal distributions are themselves distributed" (p. 600). Fairness in imperfect systems requires evaluating how unfairness in errors is distributed, not just distributions of outcomes. For intersectional fairness, this implies we cannot simply ask whether outcomes are distributed equally, prioritarian, or sufficientarian across groups; we must ask how algorithmic *errors* are distributed—are false negatives concentrated on multiply marginalized groups? This reframes the relationship between normative theories and fairness auditing: different frameworks apply differently to error distributions than to outcome distributions.

Holm (2025) makes a related but distinct argument by applying Broome's theory of fairness as proportional satisfaction of claims to algorithmic contexts. He argues that popular statistical fairness criteria fail to respect the separateness of persons because they can allocate treatments—medical interventions, educational opportunities—to individuals for whom treatment would be harmful in expectation, purely to achieve group-level statistical parity. Drawing on Broome's claims-based fairness theory, Holm contends that "fairness requires allocating for the right reasons—reasons that respect each person's separate interests" (p. 4). This grounds a distinctive normative constraint: fairness audits must ensure algorithms respect each individual's claims, not merely equalize group statistics. For intersectional fairness, Holm's Broomean framework challenges dominant approaches focused solely on group parity across intersections. The separateness of persons demands individual-level decision thresholds, raising the question: can we even meaningfully audit fairness across exponentially many intersectional groups without violating individuals' separate claims? Castro (2024) refines this Broomean approach further by arguing for counterfactual rather than statistical interpretation: determining which groups matter requires counterfactual reasoning about what different groups would be owed in fair possible worlds, not just examining actual outcome distributions.

Green (2022) introduces a different axis of normative disagreement: formal versus substantive algorithmic fairness. Current "formal algorithmic fairness" restricts analysis to isolated decision-making procedures, leading to impossibility results and models that may exacerbate oppression despite appearing fair. Green proposes "substantive algorithmic fairness" drawing on theories of substantive equality from law and philosophy, taking "a more expansive scope of analysis—examining structural context, power relations, and long-term effects" (p. 4). This distinction maps onto different purposes for fairness auditing: compliance-focused audits adopt formal fairness (do statistical criteria hold?), while justice-focused audits require substantive fairness (does the system reduce oppression?). For intersectional fairness, substantive approaches examine how algorithms affect multiply marginalized groups' structural positions, not just their statistical representation. Different normative frameworks map onto this formal-substantive distinction differently: egalitarian and sufficientarian frameworks might be implementable in formal mode (measure parity or threshold achievement), while relational egalitarian frameworks necessarily require substantive mode (assess domination and social standing).

Binns (2024) identifies limits to directly importing distributive justice theories into algorithmic contexts. He argues that Rawls's Difference Principle—inequalities justified only if they benefit the worst-off—won't "make a real difference" in algorithmic fairness practice despite its theoretical appeal. The Principle's lexical priority of basic liberties and its focus on the basic structure of society rather than individual decisions limits applicability to algorithmic systems. For intersectional fairness, this suggests we cannot simply apply the Difference Principle to prioritize worst-off intersectional groups; the framework doesn't fit the problem structure. This motivates considering alternative normative foundations—prioritarianism, sufficientarianism, or domain-specific principles—for determining which groups matter and how to prioritize them in fairness audits.

### Normative Pluralism as Compounding Ontological Uncertainty

The proliferation of competing normative frameworks reveals that ontological uncertainty about which groups to audit is partly irreducible normative disagreement. Sufficientarians identify groups below thresholds as mattering most; prioritarians focus on absolutely worst-off groups; egalitarians treat all groups equally; relationalists prioritize groups facing domination; Broomean approaches examine individual claims rather than group statistics; substantive approaches look to structural position rather than statistical parity. These are not merely different metrics for the same underlying fairness concept but reflect fundamental disagreement about what fairness *is* and what purposes fairness auditing should serve.

This normative pluralism compounds the metaphysical disagreements examined in Section 2. Even if we resolved debates between attribute-based and practice-based social ontology—even if we agreed on whether intersectional categories emerge from social structures or can be derived from demographic attributes—we would still face normative disagreement about which of the resulting groups warrant auditing attention. A prioritarian using practice-based ontology would identify different groups than a sufficientarian using the same ontology, because they are looking for different things: worst-off groups versus groups below thresholds. Conversely, two prioritarians disagreeing about group ontology (one attribute-based, one practice-based) would generate different group lists because they disagree about what constitutes group membership, even though they share normative commitments.

The interaction between normative and ontological uncertainty creates a dilemma within the dilemma. Attempting to resolve ontological questions about which intersections exist requires confronting normative questions about which normative framework should guide the inquiry—yet these normative frameworks themselves are contested. Robeyns (2022) argues for normative pluralism explicitly: no single principle suffices for distributive justice; we need limitarianism (upper bounds on wealth), sufficientarianism (minimum thresholds), and egalitarianism (equal opportunities) working together. If she is right, then determining which intersectional groups matter requires combining multiple frameworks—but these frameworks may yield conflicting group prioritizations in specific cases, leaving the ontological question unresolved.

Recent work on impossibility results reveals another dimension of this normative entanglement. Sahlgren (2024) draws on philosophical literature on feasibility to argue that impossibility results reflect current structural conditions that could be changed through collective efforts, not permanent logical necessities. If fairness impossibility is diachronically rather than synchronically feasible, then fairness audits should aim not at optimizing current algorithms but at identifying structural changes needed to make fairness feasible across intersectional groups. Yet different normative frameworks prioritize different structural changes: prioritarians care about reducing absolute disadvantage of worst-off groups, egalitarians about reducing relative inequality, relationalists about eliminating domination. The purpose of auditing itself depends on which normative framework guides the analysis.

This normative dimension of the group specification problem remains largely unaddressed in both technical and philosophical fairness literature. Technical work treats metric choice as a technical decision about which mathematical criterion best captures fairness, occasionally acknowledging that different metrics embody different fairness intuitions. But the recognition that these "intuitions" reflect contested normative frameworks—prioritarian, sufficientarian, egalitarian, relational—with systematically different implications for which groups matter has not been integrated into technical approaches. Philosophical work, conversely, debates the merits of these frameworks in abstraction from algorithmic implementation constraints, rarely examining how normative choices interact with statistical feasibility and ontological uncertainty to determine the scope of fairness auditing.

The literature on normative frameworks thus reveals a fundamental challenge: determining which intersectional groups warrant auditing is not a value-neutral technical or metaphysical question but requires contestable normative commitments about the purposes of fairness and the structure of justice. These normative disagreements are not resolvable through better conceptual analysis or more sophisticated algorithms; they reflect enduring philosophical controversy. Section 5 examines how this normative pluralism interacts with the statistical and ontological challenges identified in previous sections to create a genuine dilemma for intersectional fairness auditing.

## Section 5: Epistemic Justice and the Dilemma's Interaction Effects

The technical sophistication of multicalibration and the conceptual depth of social ontology debates might suggest that the intersectionality problem could be resolved by combining better algorithms with clearer philosophical foundations. This section argues that such optimism is misplaced. Statistical uncertainty and ontological uncertainty do not merely coexist as independent challenges—they interact to create a genuine dilemma where addressing one horn systematically exacerbates the other. Epistemic justice provides the normative lens for understanding why both horns involve structural injustice rather than merely technical inconvenience.

### Subsection 5.1: Epistemic Injustice in Algorithmic Systems

Miranda Fricker's foundational framework establishes epistemic injustice as a distinct category of injustice, identifying two primary forms: testimonial injustice, when prejudice causes deflated credibility assessments of a speaker's testimony, and hermeneutical injustice, when gaps in collective interpretive resources prevent individuals from making sense of their social experiences (Fricker 2007). Both forms harm individuals specifically in their capacity as knowers. While Fricker's original analysis focused on interpersonal contexts, recent work extends the framework to data science and algorithmic systems, revealing how technical design choices can perpetuate epistemic injustices even without individual discriminatory intent.

Symons and Alvarado (2022) demonstrate how data collection practices, algorithmic opacity, and system design choices perpetrate testimonial and hermeneutical injustices through technological infrastructure. Their analysis reveals that sparse data for marginalized groups represents not merely missing information but a systematic undermining of those groups' epistemic standing. When algorithmic systems require large sample sizes that marginalized groups cannot provide, they reproduce hermeneutical injustice by leaving those groups' experiences underrepresented in the collective interpretive resources embedded in machine learning models. The connection between data sparsity and epistemic injustice is not merely metaphorical—it reflects how statistical requirements for reliability can constitute structural barriers to epistemic representation.

Anderson (2012) shifts the focus from individual epistemic virtues to institutional design, arguing that epistemic justice requires social institutions—not just individuals—to correct for identity-prejudicial credibility assessments. This institutional perspective is critical for understanding fairness auditing as an epistemic practice. If cognitive biases are difficult for individuals to overcome but susceptible to institutional correction through properly designed systems of testimonial gathering and assessment, then fairness auditing systems must be designed to counteract systematic credibility deficits for marginalized groups. Sample size requirements and data collection protocols are not neutral technical choices but design decisions with epistemic justice implications. The question becomes: what institutional structures would be needed to address data sparsity without perpetuating epistemic injustice?

Hull (2023) analyzes how the economic structure of data labeling creates hermeneutical injustice by excluding those who label data from participation in determining how their labor and knowledge contributions are conceptualized. This reveals that data collection and curation practices—central to fairness auditing—are sites of epistemic injustice that precede and shape model outputs. Sparse data for marginalized groups may reflect not just statistical under-sampling but systematic exclusion from knowledge production processes. The political-economic structures that determine whose knowledge counts also determine whose data gets collected, in what quantities, and with what conceptual framing.

Milano and Prunkl (2024) introduce the concept of epistemic fragmentation—the isolation of individuals in algorithmically-mediated environments that undermines collective sense-making about algorithmic harms. When data for marginalized groups is sparse and scattered, developing shared hermeneutical resources for understanding and articulating algorithmic patterns becomes difficult. Statistical uncertainty from sparse data compounds hermeneutical injustice by preventing coherent articulation of patterns affecting small groups. The epistemic harm is double: not only do small sample sizes reduce statistical confidence, they also fragment experiences in ways that prevent collective understanding.

This literature establishes that data sparsity constitutes epistemic injustice, not statistical inconvenience. The requirement for large sample sizes to achieve statistical reliability can itself exclude groups from epistemic representation in algorithmic systems. This framing fundamentally changes how we understand the statistical uncertainty problem—it is not merely a technical challenge to be solved through better methods, but a site of structural injustice where statistical requirements systematically disadvantage marginalized communities.

### Subsection 5.2: The Interaction—Statistical Reliability vs. Ontological Adequacy

The core dilemma emerges when we recognize how statistical and ontological uncertainties interact. Kong (2022) frames the problem as a dilemma between infinite regress and fairness gerrymandering: either continuously split groups into finer intersections (creating ever-smaller groups with insufficient data) or arbitrarily select which groups to protect (leaving unprotected groups vulnerable to hidden discrimination). While Kong identifies this dilemma conceptually, the epistemic justice literature reveals its structural dimension.

Zhioua et al. (2025) distinguish sample size bias from underrepresentation bias, demonstrating through extensive experiments that bias from sampling is borne unequally by different groups, potentially exacerbating discrimination against underrepresented populations. This finding reveals the first horn of the interaction: expanding intersectional coverage to address ontological adequacy necessarily reduces per-group sample sizes (assuming fixed data collection budgets), which in turn amplifies bias for those groups. The statistical problem compounds rather than merely adding noise—groups with sparse data suffer both unreliable measurement and amplified discrimination.

Konstantinov and Lampert (2021) provide theoretical grounding, proving that underrepresentation creates fundamental vulnerabilities in fairness-aware learning that cannot be overcome simply by collecting more data. Their PAC learning analysis shows that adversaries can force learners to return overly biased classifiers when protected groups are underrepresented, with the strength of excess bias increasing as underrepresentation worsens. This suggests the statistical uncertainty problem may have theoretical limits beyond what improved methodology can solve. More data helps, but when groups are inherently small (many intersectional groups will be small by definition), fundamental limitations persist.

Jourdan et al. (2023) provide empirical evidence that fairness metrics yield unreliable, diverging results with small samples. Their Monte Carlo simulations on real datasets demonstrate that common fairness metrics have low statistical power and confidence when sample sizes are small, leading to overestimation or underestimation of actual discrimination. This empirical finding confirms the theoretical concern: expanding fairness auditing to more intersectional groups (thereby creating smaller samples per group) produces fairness assessments that are not just uncertain but potentially misleading. The epistemic harm extends beyond missing information to systematically distorted information.

Mhasawade et al. (2024) reveal an additional epistemic dimension: sparse data creates unequal explanation quality across groups. Their investigation of explanation disparities shows that limited sample size, covariate shift, concept shift, and omitted variable bias affect explanation quality differently across demographic groups, with effects more pronounced for neural networks. Groups with sparse data receive lower-quality explanations for algorithmic decisions—a clear form of epistemic injustice where different groups have different access to understanding algorithmic systems.

Singh and Chunara (2023) formalize the trade-off through sample size requirements for disparity estimation. Their framework shows that given fixed data collection budgets, expanding intersectional coverage necessarily reduces precision per group. The budget constraint illuminates the dilemma's economic dimension: auditing more groups costs more (either in reduced precision per group or increased total data collection costs). This is not merely a technical optimization problem but a political economy question about resource allocation for epistemic representation.

Here is the core interaction that constitutes the dilemma: expanding intersectional coverage to address ontological adequacy (ensuring all relevant groups are represented) necessarily reduces per-group sample sizes under realistic budget constraints, undermining statistical reliability and amplifying epistemic injustice for those groups. Conversely, constraining coverage to ensure statistical reliability requires resolving contested ontological questions about which groups exist and matter—but as the social ontology literature demonstrates, no such resolution is available. The dilemma has intractable horns: each attempt to address one dimension systematically worsens the other.

### Subsection 5.3: Critical Perspectives—Is the Dilemma Resolvable?

Critical scholars question whether the intersectionality dilemma can be resolved within current fairness frameworks, arguing that formalization itself may be the limiting factor. Green (2022) distinguishes "formal algorithmic fairness" (restricted to isolated decision-making procedures) from "substantive algorithmic fairness" (analyzing algorithms within social context). He argues that impossibility results, including the intersectionality dilemma, stem from formalist methodology that abstracts away from structural context. Expanding scope to substantive fairness might escape impossibility by considering how algorithms interact with broader social institutions. However, Green's framework provides limited concrete guidance on how to operationalize "substantive fairness" for auditing—critics might argue the distinction diagnoses problems without solving them.

Wachter et al. (2021) provide a stronger claim: fairness cannot be automated because legal and moral conceptions of discrimination fundamentally exceed statistical formalization. Their analysis of EU non-discrimination law shows that legal fairness requires considering intent, context, and structural factors that resist mathematical operationalization. Current fairness metrics measure correlation, not causation or discrimination as legally understood. This suggests the intersectionality dilemma may be one instance of a broader impossibility: fairness as a contested normative concept may resist valid operationalization regardless of group specification method.

Kasirzadeh (2022) draws on Iris Marion Young's work on structural injustice to argue that algorithmic fairness, rooted in distributive justice ideals, cannot address oppressive social structures. Distributive frameworks focus on allocating goods across individuals or groups, but structural oppression operates through social relations, institutional constraints, and systemic patterns that distribution alone cannot remedy. If fairness metrics are fundamentally distributive—measuring how prediction accuracy, error rates, or resource allocations are distributed across groups—then they may be conceptually inappropriate for addressing structural injustice. The intersectionality dilemma might be irresolvable within distributive fairness paradigms not because of statistical or ontological limits but because the problem itself exceeds distributive frameworks.

Hampton (2021) uses Black feminist theory to ground the concept of algorithmic oppression, critiquing the language and assumptions of the fairness, accountability, and transparency community. She highlights the "double bind" of technology for marginalized communities and calls for abolition and empowerment rather than fairness optimization. From this perspective, the intersectionality dilemma may not be resolvable through better fairness metrics because the fairness framework itself inadequately captures Black feminist concerns about oppression. The dilemma reveals not a technical problem requiring algorithmic solutions but a political problem requiring structural transformation. Abolition, not optimization, becomes the appropriate response.

Lopez (2024) introduces "susceptibility to algorithmic disadvantage" as a conceptual framework drawing on relational equality theory. She identifies vertical dimensions (state-individual relations) and horizontal dimensions (intersectional inequalities) that co-constitute and reinforce each other—more than the sum of parts. This parallels the intersectional feminist insight that interlocking oppressions exceed single-axis analysis. Lopez's framework suggests that fairness on single axes may not aggregate to fairness across intersections because intersectional harms emerge from the interaction of dimensions, not their addition. This provides conceptual grounding for understanding why technical solutions addressing statistical challenges separately from ontological challenges will fail—the dimensions interact to produce emergent harms.

These critical perspectives converge on the conclusion that formalization has inherent limits for addressing intersectional algorithmic justice. However, most critiques offer general calls for "substantive fairness," "participatory approaches," or "abolition" without specifying concrete alternatives to current auditing practices. Green's substantive fairness expands scope but remains vague on implementation. Wachter et al. correctly identify automation limits but offer little guidance beyond "complement with human judgment." Kasirzadeh's structural injustice critique diagnoses paradigm limits but provides limited constructive alternatives. Hampton's abolitionist perspective rejects fairness optimization entirely but leaves open questions about near-term governance. Lopez's relational framework offers conceptual clarity about interaction effects but limited operational guidance.

The gap between critical diagnosis and constructive alternatives reflects the dilemma's depth. If the problem arises from the interaction of statistical constraints (fixed budgets, sample size requirements, theoretical limits on learning from sparse data) and ontological constraints (contested group constitution, normative pluralism, practice-based ontology), then purely technical solutions will fail by ignoring ontological dimensions, while purely philosophical solutions will fail by ignoring statistical feasibility. Critics correctly identify formalization limits but struggle to specify alternatives because the dilemma constrains both horns simultaneously.

This leaves open the question: if the intersectionality dilemma is irresolvable within current fairness frameworks, what follows? One response is participatory governance, where affected communities determine which groups matter and acceptable trade-offs between coverage and reliability. Another is accepting fundamental limitations—fairness auditing can provide partial information but not comprehensive justice guarantees. A third is institutional rather than purely algorithmic responses, addressing data scarcity through structural reforms in data collection, resource allocation, and power distribution. None of these responses resolve the dilemma technically, but they may represent more appropriate responses than pursuing optimal algorithmic solutions to structurally intractable problems.

## Section 6: Research Gaps and the Dilemma's Novelty

The literature reviewed reveals sophisticated engagement with both the statistical and ontological dimensions of intersectional fairness, yet a critical gap remains. Existing work treats these as separate optimization problems—one technical, one philosophical—rather than recognizing their interaction as constituting a genuine dilemma where addressing one horn exacerbates the other. This section identifies four specific gaps that define the research opportunity.

### Gap 1: No Recognition of Statistical-Ontological Interaction as Dilemma

The most fundamental gap is the failure to recognize that statistical uncertainty and ontological uncertainty interact as a dilemma rather than as independent challenges. The technical ML literature has developed impressive solutions to statistical problems: multicalibration enables calibrated predictions across exponentially many subgroups without explicit enumeration (Hébert-Johnson et al. 2018), low-degree approximations address sample complexity challenges (Gopalan et al. 2022), size-adaptive hypothesis testing handles varying data availability (Ferrara et al. 2025), and statistical inference frameworks provide rigorous auditing across rich subgroup collections (Cherian and Candès 2023). These advances demonstrate that the statistical dimension—ensuring reliable fairness measurement despite sparse data—has sophisticated technical responses.

Parallel to this technical progress, the philosophy literature has explored ontological questions about group constitution without engaging statistical feasibility constraints. Debates about whether intersectional categories emerge contextually from social structures (Jorba and López de Sa 2024) or reduce to constituent categories (Bernstein 2020), whether groups are defined by attributes or practices (Haslanger 2012; Ásta 2018; Ritchie 2018), and whether intersectionality should be understood through analytical or hermeneutic frameworks (Carastathis 2016; Collins 2019) proceed largely in isolation from questions about whether these metaphysical positions can be algorithmically implemented. Even work applying social ontology to discrimination (Popescu-Sarry 2023) rarely considers how practice-based group constitution undermines computational group specification.

The few papers bridging these domains identify both dimensions but do not theorize their interaction. Himmelreich et al. (2024) explicitly frame the intersectionality problem as involving "statistical challenges (small intersectional groups) and moral-methodological challenges (which groups matter)," developing desiderata to clarify both. Yet their analysis treats these as separate constraints to be jointly satisfied, not as horns of a dilemma where satisfying one makes satisfying the other harder. Gohar and Cheng's (2023) survey identifies data scarcity alongside computational complexity and metric selection as key challenges, but presents them as a list of problems rather than interacting constraints.

Kong (2022) comes closest to recognizing the interaction, identifying a "dilemma between infinite regress and fairness gerrymandering": either continuously split groups into smaller subgroups ad infinitum (creating ever-sparser data), or arbitrarily select which groups to protect (ontological gerrymandering). However, Kong frames this primarily as a philosophical problem about the statistical parity approach's conceptual adequacy, not as an intractable interaction between statistical reliability requirements and ontological indeterminacy. The statistical consequences of infinite group subdivision and the ontological arbitrariness of group selection are presented as parallel problems arising from the same flawed approach, not as interacting constraints that create a genuine dilemma.

This gap matters because if the interaction constitutes a genuine dilemma rather than two independent challenges, then existing approaches addressing either horn in isolation are systematically inadequate. Technical solutions that assume groups are given (multicalibration, differential fairness) cannot resolve ontological uncertainty about which groups exist. Philosophical analyses of group constitution that ignore statistical feasibility cannot guide fairness auditing when sample sizes impose hard constraints on how finely groups can be subdivided. The interaction is the problem: expanding intersectional coverage for ontological adequacy creates statistical unreliability; constraining coverage for statistical reliability requires resolving contested ontological questions that philosophy reveals have no neutral resolution.

### Gap 2: Limited Engagement with Statistical Feasibility in Social Ontology

The philosophy of intersectionality has produced rich conceptual analyses of how social categories combine, yet remarkably little of this work considers algorithmic implementation constraints or statistical feasibility. Debates about emergence versus reduction (Jorba and López de Sa 2024; Bernstein 2020), analytical versus hermeneutic interpretations (Carastathis 2016; Collins 2019), and mutually constitutive versus additive models (Yuval-Davis 2006) proceed without asking whether the resulting ontological commitments can be operationalized in fairness auditing systems that require statistical inference from finite data.

Social ontology debates about attribute-based versus practice-based group constitution similarly neglect computational implications. Haslanger's (2012) political constructionism defines race and gender as social positions constituted by systematic subordination and privilege, not intrinsic attributes. Mallon's (2016) social role HPC kinds show how categories are contingent artifacts of practices stabilized by homeostatic mechanisms. Ásta's (2018) conferralism argues that social properties depend on communal conferral, with the same attributes grounding different properties in different contexts. Ritchie's (2018) structuralism treats groups as networks of social relations rather than collections defined by shared features. These sophisticated accounts converge on the view that group membership cannot be derived algorithmically from demographic attributes—yet none directly engage with how this constrains fairness auditing.

The gap is not that social ontologists should become technical ML researchers, but rather that the algorithmic infeasibility of certain ontological positions deserves explicit theoretical attention. If Ásta is correct that conferralism makes group boundaries context-dependent on communal meaning-making, this implies fairness systems cannot pre-specify groups from demographic data. If Ritchie is correct that groups are constituted by relational networks rather than shared attributes, algorithmic approaches treating groups as attribute-defined categories fundamentally misconstrue what groups are. These implications matter for evaluating whether fairness auditing is possible, yet they remain undertheorized.

Measurement theory applications to fairness provide an instructive contrast. Papers like Tal (2023), Blodgett et al. (2021), and Bean et al. (2025) explicitly analyze whether fairness benchmarks validly measure what they purport to measure, connecting measurement validity challenges to construct specification problems. Similar analysis is needed for the connection between social ontology and group specification: if practice-based accounts are correct, does this make valid algorithmic group specification impossible, or merely require different computational approaches? The philosophical debates have direct bearing on technical feasibility, but the implications remain implicit.

### Gap 3: Normative Foundations Left Implicit in Technical Fairness Work

Technical fairness literature generally treats metric choice as a technical decision to be optimized rather than a normative choice reflecting contested ethical commitments. Yet the distributive justice literature shows that different normative frameworks—prioritarianism (Arneson 2000; Parfit 2012), sufficientarianism (Shields 2016; Timmer 2021), egalitarianism (Anderson 1999), and pluralist combinations (Robeyns 2022)—yield systematically different answers about which groups warrant protection and how to prioritize them. Prioritarians focus fairness interventions on the worst-off intersectional groups, weighting improvements for multiply marginalized communities most heavily. Sufficientarians prioritize ensuring all groups reach minimum acceptable thresholds, with strongest urgency for groups falling below sufficiency levels. Egalitarians pursue parity across groups. These frameworks differ not just in how much inequality to tolerate, but in which groups matter and why.

Recent philosophical work has begun making these connections explicit. Baumann et al. (2022) show that different fairness metrics embody contested philosophical commitments about distributive justice—what counts as benefit or harm, what moral claims to benefits people have. Hertweck et al. (2024) argue that algorithmic fairness should focus on distribution of errors rather than outcomes, reframing how distributive justice principles apply. Holm (2025) applies Broome's claims-based fairness theory to argue that statistical parity violates the separateness of persons by allocating treatments for the wrong reasons. These papers demonstrate that metric choice cannot be settled by technical optimization alone but requires engagement with normative theory.

However, this insight rarely penetrates technical work on intersectional fairness. Multicalibration literature treats the class of subgroup-defining functions as a technical parameter rather than recognizing that choosing which functions to include reflects normative judgments about which groups deserve fairness protections. Work on sparse data solutions focuses on statistical methodology without asking whether requiring large sample sizes for reliability constitutes epistemic injustice (Fricker 2007; Anderson 2012) by excluding groups that cannot be easily aggregated. Fairness gerrymandering research demonstrates that strategic group selection undermines metrics (Kearns et al. 2018; Kong 2022; Tian et al. 2024), yet doesn't fully theorize that the ontological question—which groups exist—is partly normative, not purely metaphysical or empirical.

The gap is consequential because normative disagreements compound ontological uncertainty. If which groups matter is partly an ethical question, then different stakeholders operating from different normative frameworks will legitimately disagree about which intersectional groups warrant auditing. A prioritarian may focus on the multiply marginalized; a sufficientarian on groups below thresholds; an egalitarian on all groups equally. These are not technical disagreements resolvable through better metrics, but substantive ethical disagreements about the purpose of fairness auditing. Technical optimization cannot resolve which normative framework to adopt.

### Gap 4: Critical Perspectives Without Constructive Alternatives

Critical perspectives on fairness formalization have identified fundamental limitations yet offer limited concrete alternatives beyond general programmatic calls. Green's (2022) distinction between formal and substantive algorithmic fairness argues that impossibility results stem from formalist methodology restricting analysis to isolated decision procedures. By expanding scope to examine structural context and power relations, substantive approaches can escape impossibility theorems. However, "substantive fairness" remains somewhat programmatic—it describes what fairness analysis should attend to (structural conditions, long-term effects, distributional patterns) without specifying operational alternatives to current fairness auditing practices.

Wachter et al.'s (2021) impossibility thesis argues that fairness cannot be automated because legal and moral conceptions of discrimination exceed statistical formalization, requiring contextual judgment about intent, causation, and structural factors. This is an important corrective to techno-optimism, yet it leaves practitioners without clear guidance: if fairness cannot be automated, what role should computational fairness assessment play? The paper emphasizes limitations without developing alternatives.

Kasirzadeh's (2022) structural injustice critique argues that fairness metrics rooted in distributive justice ideals cannot address oppressive structures, drawing on Iris Marion Young's political philosophy. This represents a genuine paradigm challenge—if the problem is structural oppression rather than maldistribution, then fairness metrics optimizing distributions miss the point. Yet the paper's proposed "responsible algorithmic fairness" remains abstract, calling for expanding scope without specifying what institutional or technical practices would realize this expansion.

Hampton's (2021) Black feminist critique is perhaps most radical, questioning whether fairness discourse itself adequately captures algorithmic oppression or merely provides liberal cover for extractive systems. The call for abolition rather than algorithmic reform represents a fundamental rejection of fairness optimization. However, as a position statement it offers limited concrete guidance for practitioners committed to working within existing institutional contexts where algorithmic systems will be deployed regardless.

The gap is not that critics should provide fully specified technical alternatives—critique can be valuable without proposing solutions. Rather, the gap is in developing a principled understanding of formalization's limits. By framing the intersectionality dilemma as arising from the interaction of statistical and ontological constraints, the present research provides more granular analysis of why fairness formalization is limited. Not all of fairness resists operationalization for the same reasons. The dilemma helps explain which aspects (intersectional group specification under sparse data) face fundamental constraints versus which might be addressable through better methods.

Critical perspectives rightly identify that impossibility results reflect deeper problems than metric incompatibility, but they have not fully theorized what those deeper problems are. Is the issue that fairness is inherently contested (Green 2022), that discrimination law requires contextual judgment (Wachter et al. 2021), that distributive justice cannot capture structural oppression (Kasirzadeh 2022), or that fairness discourse itself is ideologically compromised (Hampton 2021)? These are importantly different diagnoses suggesting different responses. The intersectionality dilemma contributes by identifying a specific structural problem—the interaction between epistemological limits (sparse data) and ontological limits (contested group constitution)—that current approaches cannot resolve because addressing either horn worsens the other.

### Summary: The Missing Interaction

Across all four gaps, the common pattern is treatment of statistical and ontological challenges as separate problems rather than interacting constraints. Technical literature assumes groups can be specified; philosophical literature ignores statistical feasibility; normative foundations remain implicit; critical perspectives diagnose limitations without specifying their sources. What the literature does not address is the interaction itself: expanding groups for ontological adequacy necessarily reduces per-group sample sizes, undermining statistical reliability. Constraining groups for statistical reliability requires resolving contested ontological questions about which groups exist—but philosophy reveals these questions have no neutral resolution, being compounded by normative disagreements about which groups matter.

This is not merely a matter of two hard problems that must both be solved. It is a dilemma where the solutions are in tension. Better statistical methods for sparse data (Ferrara et al. 2025; Cherian and Candès 2023) enable more reliable auditing of whatever groups are specified, but cannot determine which groups should be specified. Sophisticated social ontology (Haslanger 2012; Ásta 2018; Ritchie 2018) clarifies what groups are, but reveals they cannot be algorithmically derived from attributes because constitution depends on practices, conferral, and relational structures. Normative frameworks (Shields 2016; Arneson 2000; Anderson 1999) provide principled bases for prioritizing groups, but different frameworks yield different prioritizations and no framework has neutral justification. Critical perspectives (Green 2022; Kasirzadeh 2022) identify formalization's limits, but do not explain precisely why fairness resists operationalization.

The research contribution is recognizing and articulating this interaction. Existing literature identifies the pieces—statistical challenges, ontological debates, normative pluralism, formalization limits—but misses how they fit together as a dilemma. The intersectionality problem for algorithmic fairness is not that we lack good enough statistics (we have sophisticated methods) or that we lack philosophical clarity about groups (social ontology is well-developed) or that we lack ethical frameworks (distributive justice provides options). The problem is that these domains interact to create intractable trade-offs: ensuring statistical reliability while respecting ontological adequacy and accommodating normative pluralism may be impossible because satisfying each constraint makes satisfying the others harder. This is the gap the literature leaves unaddressed.

## Conclusion

The literature reviewed here reveals a fundamental gap in how the algorithmic fairness community conceptualizes the intersectionality problem. Technical ML research has developed sophisticated statistical solutions—multicalibration frameworks that handle exponentially many overlapping groups (Hébert-Johnson et al. 2018), hierarchical models addressing sparse data (Ferrara et al. 2025), and rigorous hypothesis testing procedures for fairness auditing (Cherian and Candès 2023). Meanwhile, philosophical analysis has generated rich debates about social group constitution, with practice-based accounts demonstrating that groups cannot be algorithmically derived from attributes (Haslanger 2012; Mallon 2016; Ásta 2018; Ritchie 2018), measurement theorists revealing systematic construct validity failures in fairness metrics (Tal 2023; Blodgett et al. 2021), and normative theorists showing that different ethical frameworks yield incompatible answers about which groups deserve protection (Shields 2016; Anderson 1999; Hertweck et al. 2024). Yet these sophisticated analyses remain largely disconnected, treating statistical uncertainty and ontological uncertainty as independent optimization challenges rather than interacting constraints.

The intersectionality dilemma emerges precisely from this interaction. Expanding fairness coverage to address ontological adequacy—ensuring all morally relevant intersectional groups are audited—necessarily reduces per-group sample sizes, undermining the statistical reliability that justifies treating algorithmic fairness metrics as authoritative. Conversely, constraining coverage to ensure statistical reliability requires determining which groups genuinely warrant auditing, but philosophical debates reveal this question has no neutral resolution. Practice-based social ontologies show that social groups are constituted through conferral, structural positioning, and social practices that resist algorithmic specification (Ásta 2018; Ritchie 2018). Normative pluralism demonstrates that prioritarian, sufficientarian, and egalitarian frameworks generate systematically different answers about group prioritization (Shields 2016; Arneson 2000). Even measurement theory reveals that fairness—as a contested normative concept—may fundamentally resist valid operationalization (Long 2020; Jacobs et al. 2020).

Kong (2022) comes closest to recognizing this interaction, identifying a "dilemma between infinite regress and fairness gerrymandering": either continuously split groups into smaller subgroups, creating statistical chaos, or arbitrarily select protected groups, enabling strategic manipulation. However, even Kong's analysis does not fully theorize why these horns interact rather than representing separate challenges. The answer lies in epistemic justice: data sparsity for marginalized groups constitutes not merely missing information but systematic undermining of their epistemic standing (Fricker 2007; Symons and Alvarado 2022). Sample size requirements for statistical reliability can perpetuate hermeneutical injustice by excluding groups from the collective interpretive resources embedded in ML systems. Conversely, lowering reliability standards to include more groups risks producing fairness audits that cannot justify their own conclusions, potentially legitimizing rather than challenging discriminatory systems.

This reveals why both technical optimization and critical rejection represent inadequate responses. The multicalibration literature demonstrates that calibration across rich subgroup collections is computationally tractable (Hébert-Johnson et al. 2018; Gopalan et al. 2022)—but only when the class of group-defining functions is given. No amount of algorithmic sophistication can resolve the ontological question of which functions to include. Critical scholars argue that formalization itself is limited, pointing toward "substantive fairness" that attends to structural context (Green 2022) or participatory approaches that center affected communities (Wachter et al. 2021). Yet critics offer limited concrete guidance on how to conduct fairness assessment when formalization is inadequate. If fairness cannot be operationalized through formal metrics, what alternative accountability mechanisms exist?

The research contribution lies in explicitly framing the statistical-ontological interaction as a genuine dilemma with intractable horns, not merely two independent engineering challenges. This framing explains several otherwise puzzling features of the fairness landscape. First, why impossibility results persist despite technical advances: mathematical incompatibilities between fairness metrics (Kleinberg et al. 2017) may reflect deeper normative disagreements about distributive justice frameworks (Baumann et al. 2022), not merely competing statistical objectives. Second, why measurement validity problems remain chronic: if group specification is contested, then construct validity failures in fairness metrics (Blodgett et al. 2021) are not fixable through better conceptualization because the underlying concepts are irreducibly contested. Third, why fairness gerrymandering is endemic: without principled methods for determining which groups exist, strategic group selection remains possible regardless of metric sophistication (Kearns et al. 2018; Tian et al. 2024).

Recognizing the dilemma's structure has concrete implications for fairness practice. First, fairness auditing may require accepting fundamental trade-offs rather than seeking optimal solutions. Expanding intersectional coverage necessarily reduces per-group reliability; there is no statistical method that escapes this constraint under fixed data budgets (Singh and Chunara 2023). Second, participatory approaches to group specification may be necessary but insufficient. Even when affected communities specify which groups matter, statistical feasibility constraints remain. Conversely, technical solutions to sparse data cannot determine which groups warrant the computational investment. Third, institutional rather than purely algorithmic responses may be required. If fairness auditing faces structural limits, accountability mechanisms must look beyond automated metrics to include qualitative assessment, participatory governance, and transparency about metrics' normative assumptions.

The critical perspectives reviewed here suggest the dilemma may be irresolvable within current fairness frameworks. Structural injustice critiques argue that distributive justice frameworks underpinning fairness metrics cannot address oppressive social structures (Kasirzadeh 2022). Black feminist scholars question whether fairness discourse itself is adequate for addressing algorithmic oppression, pointing toward abolitionist alternatives (Hampton 2021). These critiques identify an important possibility: that formalization's limits are not merely technical but reflect deeper incompatibility between quantitative measurement and the normative complexity of justice. However, purely diagnostic critiques leave practitioners without guidance. The intersectionality dilemma framework provides principled analysis of formalization's limits while acknowledging that alternatives remain underspecified.

This literature review positions the research project to make three contributions. First, theoretically, it offers the first systematic analysis of statistical-ontological interaction as constituting a genuine dilemma where addressing one horn exacerbates the other. Second, methodologically, it demonstrates why existing approaches—technical optimization, conceptual analysis, participatory design—are individually insufficient, suggesting hybrid approaches may be needed. Third, practically, it clarifies what fairness auditing can and cannot achieve: automated metrics may provide partial, context-dependent fairness assessment but cannot substitute for substantive political deliberation about which groups matter and why. The dilemma is not an argument for abandoning fairness auditing but for approaching it with appropriate epistemic humility about the limits of formalization and the unavoidability of normative judgment.
