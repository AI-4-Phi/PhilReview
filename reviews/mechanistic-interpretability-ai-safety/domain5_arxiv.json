[search_arxiv.py] Searching arXiv: 'all:interpretability adversarial robustness AND cat:cs.LG' (category=cs.LG) (year=2023), limit=25
[search_arxiv.py] Search complete: 2 papers found
[search_arxiv.py] Cached results (cache key: arxiv_1fa6293696c0477f)
{
  "status": "success",
  "source": "arxiv",
  "query": "all:interpretability adversarial robustness AND cat:cs.LG",
  "results": [
    {
      "arxiv_id": "2306.11066",
      "title": "Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding",
      "authors": [
        "Venkata Prabhakara Sarath Nookala",
        "Gaurav Verma",
        "Subhabrata Mukherjee",
        "Srijan Kumar"
      ],
      "abstract": "State-of-the-art few-shot learning (FSL) methods leverage prompt-based fine-tuning to obtain remarkable results for natural language understanding (NLU) tasks. While much of the prior FSL methods focus on improving downstream task performance, there is a limited understanding of the adversarial robustness of such methods. In this work, we conduct an extensive study of several state-of-the-art FSL methods to assess their robustness to adversarial perturbations. To better understand the impact of various factors towards robustness (or the lack of it), we evaluate prompt-based FSL methods against fully fine-tuned models for aspects such as the use of unlabeled data, multiple prompts, number of few-shot examples, model size and type. Our results on six GLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL methods lead to a notable relative drop in task performance (i.e., are less robust) in the face of adversarial perturbations. However, using (i) unlabeled data for prompt-based FSL and (ii) multiple prompts flip the trend. We further demonstrate that increasing the number of few-shot examples and model size lead to increased adversarial robustness of vanilla FSL methods. Broadly, our work sheds light on the adversarial robustness evaluation of prompt-based FSL methods for NLU tasks.",
      "published": "2023-06-19",
      "updated": "2023-06-21",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2306.11066v2",
      "url": "https://arxiv.org/abs/2306.11066"
    },
    {
      "arxiv_id": "2306.05952",
      "title": "Overcoming Adversarial Attacks for Human-in-the-Loop Applications",
      "authors": [
        "Ryan McCoppin",
        "Marla Kennedy",
        "Platon Lukyanenko",
        "Sean Kennedy"
      ],
      "abstract": "Including human analysis has the potential to positively affect the robustness of Deep Neural Networks and is relatively unexplored in the Adversarial Machine Learning literature. Neural network visual explanation maps have been shown to be prone to adversarial attacks. Further research is needed in order to select robust visualizations of explanations for the image analyst to evaluate a given model. These factors greatly impact Human-In-The-Loop (HITL) evaluation tools due to their reliance on adversarial images, including explanation maps and measurements of robustness. We believe models of human visual attention may improve interpretability and robustness of human-machine imagery analysis systems. Our challenge remains, how can HITL evaluation be robust in this adversarial landscape?",
      "published": "2023-06-09",
      "updated": "2023-08-25",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2306.05952v2",
      "url": "https://arxiv.org/abs/2306.05952"
    }
  ],
  "count": 2,
  "errors": []
}
