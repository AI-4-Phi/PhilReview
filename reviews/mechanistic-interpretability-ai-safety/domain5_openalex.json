[search_openalex.py] Searching OpenAlex: 'interpretability trust transparency safety' (year=2023-2025), limit=40
[search_openalex.py] Retrieved 40 papers...
[search_openalex.py] Search complete: 40 papers found
[search_openalex.py] Cached results (cache key: openalex_a79b7bcc27eb8ed3)
{
  "status": "success",
  "source": "openalex",
  "query": "interpretability trust transparency safety",
  "results": [
    {
      "openalex_id": "W4313585122",
      "doi": "10.3390/s23020565",
      "title": "Metaverse in Healthcare Integrated with Explainable AI and Blockchain: Enabling Immersiveness, Ensuring Trust, and Providing Patient Data Security",
      "authors": [
        {
          "name": "Sikandar Ali",
          "openalex_id": "A5101739837",
          "orcid": "https://orcid.org/0000-0002-8479-4084",
          "institutions": [
            "Inje University"
          ]
        },
        {
          "name": "Abdullah",
          "openalex_id": "A5073100288",
          "institutions": [
            "Inje University"
          ]
        },
        {
          "name": "Tagne Poupi Theodore Armand",
          "openalex_id": "A5076827703",
          "orcid": "https://orcid.org/0000-0002-5933-3163",
          "institutions": [
            "Inje University"
          ]
        },
        {
          "name": "Ali Athar",
          "openalex_id": "A5034925484",
          "orcid": "https://orcid.org/0000-0002-2331-7663",
          "institutions": [
            "Inje University"
          ]
        },
        {
          "name": "Ali Hussain",
          "openalex_id": "A5003020971",
          "orcid": "https://orcid.org/0000-0001-6208-6100",
          "institutions": [
            "Inje University"
          ]
        },
        {
          "name": "Maisam Ali",
          "openalex_id": "A5081765557",
          "orcid": "https://orcid.org/0000-0003-4185-1963",
          "institutions": [
            "Inje University"
          ]
        },
        {
          "name": "Muhammad Yaseen",
          "openalex_id": "A5016189883",
          "orcid": "https://orcid.org/0000-0002-2128-6680",
          "institutions": [
            "Inje University"
          ]
        },
        {
          "name": "Moon-Il Joo",
          "openalex_id": "A5113942151",
          "institutions": [
            "Inje University"
          ]
        },
        {
          "name": "Hee\u2010Cheol Kim",
          "openalex_id": "A5102020002",
          "orcid": "https://orcid.org/0000-0002-5399-7647",
          "institutions": [
            "Inje University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-04",
      "abstract": "Digitization and automation have always had an immense impact on healthcare. It embraces every new and advanced technology. Recently the world has witnessed the prominence of the metaverse which is an emerging technology in digital space. The metaverse has huge potential to provide a plethora of health services seamlessly to patients and medical professionals with an immersive experience. This paper proposes the amalgamation of artificial intelligence and blockchain in the metaverse to provide better, faster, and more secure healthcare facilities in digital space with a realistic experience. Our proposed architecture can be summarized as follows. It consists of three environments, namely the doctor\u2019s environment, the patient\u2019s environment, and the metaverse environment. The doctors and patients interact in a metaverse environment assisted by blockchain technology which ensures the safety, security, and privacy of data. The metaverse environment is the main part of our proposed architecture. The doctors, patients, and nurses enter this environment by registering on the blockchain and they are represented by avatars in the metaverse environment. All the consultation activities between the doctor and the patient will be recorded and the data, i.e., images, speech, text, videos, clinical data, etc., will be gathered, transferred, and stored on the blockchain. These data are used for disease prediction and diagnosis by explainable artificial intelligence (XAI) models. The GradCAM and LIME approaches of XAI provide logical reasoning for the prediction of diseases and ensure trust, explainability, interpretability, and transparency regarding the diagnosis and prediction of diseases. Blockchain technology provides data security for patients while enabling transparency, traceability, and immutability regarding their data. These features of blockchain ensure trust among the patients regarding their data. Consequently, this proposed architecture ensures transparency and trust regarding both the diagnosis of diseases and the data security of the patient. We also explored the building block technologies of the metaverse. Furthermore, we also investigated the advantages and challenges of a metaverse in healthcare.",
      "cited_by_count": 220,
      "type": "article",
      "source": {
        "name": "Sensors",
        "type": "journal",
        "issn": [
          "1424-8220"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/1424-8220/23/2/565/pdf?version=1673494756"
      },
      "topics": [
        "Blockchain Technology Applications and Security",
        "IoT and Edge/Fog Computing",
        "Brain Tumor Detection and Classification"
      ],
      "referenced_works_count": 89,
      "url": "https://openalex.org/W4313585122"
    },
    {
      "openalex_id": "W4391044499",
      "doi": "10.1007/s00204-023-03666-2",
      "title": "Artificial intelligence (AI)\u2014it\u2019s the end of the tox as we know it (and I feel fine)*",
      "authors": [
        {
          "name": "Nicole Kleinstreuer",
          "openalex_id": "A5084541486",
          "orcid": "https://orcid.org/0000-0002-7914-3682",
          "institutions": [
            "National Institute of Environmental Health Sciences"
          ]
        },
        {
          "name": "Thomas H\u00e4rtung",
          "openalex_id": "A5091124551",
          "orcid": "https://orcid.org/0000-0003-1359-7689",
          "institutions": [
            "Johns Hopkins University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-01-20",
      "abstract": null,
      "cited_by_count": 68,
      "type": "review",
      "source": {
        "name": "Archives of Toxicology",
        "type": "journal",
        "issn": [
          "0340-5761",
          "1432-0738"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s00204-023-03666-2.pdf"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Machine Learning in Healthcare",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 93,
      "url": "https://openalex.org/W4391044499"
    },
    {
      "openalex_id": "W4375958700",
      "doi": "10.48550/arxiv.2305.04388",
      "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
      "authors": [
        {
          "name": "Miles Turpin",
          "openalex_id": "A5113139169"
        },
        {
          "name": "Julian Michael",
          "openalex_id": "A5072681909"
        },
        {
          "name": "Ethan Perez",
          "openalex_id": "A5091112967"
        },
        {
          "name": "Samuel R. Bowman",
          "openalex_id": "A5112713734"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-05-07",
      "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always \"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",
      "cited_by_count": 72,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2305.04388"
      },
      "topics": [
        "Topic Modeling",
        "Explainable Artificial Intelligence (XAI)",
        "Machine Learning in Materials Science"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4375958700"
    },
    {
      "openalex_id": "W4400311768",
      "doi": "10.1016/j.engappai.2024.108854",
      "title": "An explainable artificial-intelligence-aided safety factor prediction of road embankments",
      "authors": [
        {
          "name": "Azam Abdollahi",
          "openalex_id": "A5085832346",
          "orcid": "https://orcid.org/0000-0002-6394-4899",
          "institutions": [
            "Lakehead University"
          ]
        },
        {
          "name": "Deli Li",
          "openalex_id": "A5103159053",
          "orcid": "https://orcid.org/0000-0001-6107-6883",
          "institutions": [
            "Lakehead University"
          ]
        },
        {
          "name": "Jian Deng",
          "openalex_id": "A5088099250",
          "orcid": "https://orcid.org/0000-0002-1487-1646",
          "institutions": [
            "Lakehead University"
          ]
        },
        {
          "name": "Ali Amini",
          "openalex_id": "A5060550822",
          "orcid": "https://orcid.org/0000-0002-6555-8393",
          "institutions": [
            "Amirkabir University of Technology"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-07-04",
      "abstract": "Despite the widespread application of data-centric techniques in Geotechnical Engineering, there is a rising need for building trust in the artificial intelligence (AI)-driven safety assessment of road embankments due to its so-called \"black-box\" nature. In addition, from the lens of limit equilibrium approaches, e.g., Bishop, Fellenius, Janbu and Morgenstern\u2013Price, and finite element method, it is essential to carefully examine the interplay of both topological and physical/mechanical properties during the safety factor (FoS) predictions. First, aside from having conventional geotechnical inputs for soil in core and foundation and the height of embankments, this paper codifies geometric features innovatively. The number of slope types with different ratios including 1:1, 1.5:1 and 2:1 as well as the number of berms is introduced. Second, a pool of 19 machine learning (ML) techniques is effortlessly trained on the dataset using an automated ML (AutoML) pipeline to identify the most optimized ML algorithm. Finally, to achieve post-hoc interpretability for the internal mechanism of the input\u2013output relationship unbiasedly, a game-theory-based explainable AI (XAI) method called Shapley additive explanations (SHAP) values is applied. SHAP-aided importance analysis provides human-interpretable insights and indicates height, California bearing ratio, slope type 2:1 and cohesion as the most influential parameters. Exclusively, analyzing hazardous embankments by classifying main and joint contributors exhibits a complex and highly variable influence on the FoS. This paper harnesses the power of XAI tools to enhance reliability and transparency for the rapid FoS prediction of slopes. It targets geotechnical researchers, practitioners, decision-makers, and the general public for the first time.",
      "cited_by_count": 39,
      "type": "article",
      "source": {
        "name": "Engineering Applications of Artificial Intelligence",
        "type": "journal",
        "issn": [
          "0952-1976",
          "1873-6769"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://doi.org/10.1016/j.engappai.2024.108854"
      },
      "topics": [
        "Dam Engineering and Safety",
        "Geotechnical Engineering and Analysis",
        "Landslides and related hazards"
      ],
      "referenced_works_count": 92,
      "url": "https://openalex.org/W4400311768"
    },
    {
      "openalex_id": "W4387586973",
      "doi": "10.2139/ssrn.4600029",
      "title": "A Brief Review of Explainable Artificial Intelligence in Healthcare",
      "authors": [
        {
          "name": "Zahra Sadeghi",
          "openalex_id": "A5100738713",
          "orcid": "https://orcid.org/0000-0002-1360-7499",
          "institutions": [
            "Dalhousie University"
          ]
        },
        {
          "name": "Roohallah Alizadehsani",
          "openalex_id": "A5083567791",
          "orcid": "https://orcid.org/0000-0003-0898-5054",
          "institutions": [
            "Deakin University"
          ]
        },
        {
          "name": "Mehmet Akif \u00c7if\u00e7i",
          "openalex_id": "A5052911404",
          "orcid": "https://orcid.org/0000-0002-6439-8826",
          "institutions": [
            "TU Wien",
            "Band\u0131rma Onyedi Eyl\u00fcl University"
          ]
        },
        {
          "name": "Samina Kausar",
          "openalex_id": "A5078332598",
          "orcid": "https://orcid.org/0000-0003-1330-7266",
          "institutions": [
            "University of Azad Jammu and Kashmir"
          ]
        },
        {
          "name": "Rizwan Rehman",
          "openalex_id": "A5060973743",
          "orcid": "https://orcid.org/0000-0002-4725-6877",
          "institutions": [
            "Dibrugarh University"
          ]
        },
        {
          "name": "Priyakshi Mahanta",
          "openalex_id": "A5110437936",
          "institutions": [
            "Dibrugarh University"
          ]
        },
        {
          "name": "Pranjal Kumar Bora",
          "openalex_id": "A5049604729",
          "orcid": "https://orcid.org/0000-0002-7350-7721",
          "institutions": [
            "Dibrugarh University"
          ]
        },
        {
          "name": "Ammar Almasri",
          "openalex_id": "A5026803234",
          "orcid": "https://orcid.org/0000-0003-3289-3328"
        },
        {
          "name": "Rami S. Alkhawaldeh",
          "openalex_id": "A5022163394",
          "orcid": "https://orcid.org/0000-0002-2413-7074",
          "institutions": [
            "University of Jordan"
          ]
        },
        {
          "name": "Sadiq Hussain",
          "openalex_id": "A5062872897",
          "orcid": "https://orcid.org/0000-0002-9840-4796",
          "institutions": [
            "Dibrugarh University"
          ]
        },
        {
          "name": "Bilal Alata\u015f",
          "openalex_id": "A5034084300",
          "orcid": "https://orcid.org/0000-0002-3513-0329",
          "institutions": [
            "F\u0131rat University"
          ]
        },
        {
          "name": "Afshin Shoeibi",
          "openalex_id": "A5029791697",
          "orcid": "https://orcid.org/0000-0003-0635-6799",
          "institutions": [
            "Ferdowsi University of Mashhad"
          ]
        },
        {
          "name": "Hossein Moosaei",
          "openalex_id": "A5041433521",
          "orcid": "https://orcid.org/0000-0002-0640-2161",
          "institutions": [
            "Jan Evangelista Purkyn\u011b University in \u00dast\u00ed nad Labem"
          ]
        },
        {
          "name": "Milan Hlad\u00edk",
          "openalex_id": "A5081693462",
          "orcid": "https://orcid.org/0000-0002-7340-8491",
          "institutions": [
            "Charles University"
          ]
        },
        {
          "name": "Saeid Nahavandi",
          "openalex_id": "A5015293969",
          "orcid": "https://orcid.org/0000-0002-0360-5270",
          "institutions": [
            "Swinburne University of Technology"
          ]
        },
        {
          "name": "Panos M. Pardalo",
          "openalex_id": "A5093053718",
          "institutions": [
            "University of Florida"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-01",
      "abstract": null,
      "cited_by_count": 28,
      "type": "review",
      "source": {
        "name": "SSRN Electronic Journal",
        "type": "repository",
        "issn": [
          "1556-5068"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://doi.org/10.2139/ssrn.4600029"
      },
      "topics": [
        "Machine Learning in Healthcare"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4387586973"
    },
    {
      "openalex_id": "W4386958277",
      "doi": "10.1186/s12909-023-04698-z",
      "title": "Revolutionizing healthcare: the role of artificial intelligence in clinical practice",
      "authors": [
        {
          "name": "Shuroug A. Alowais",
          "openalex_id": "A5005352524",
          "orcid": "https://orcid.org/0000-0002-3266-5774",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Sahar S. Alghamdi",
          "openalex_id": "A5050298886",
          "orcid": "https://orcid.org/0000-0002-2770-218X",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Nada Alsuhebany",
          "openalex_id": "A5081991977",
          "orcid": "https://orcid.org/0000-0003-4077-4521",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Tariq Alqahtani",
          "openalex_id": "A5066001851",
          "orcid": "https://orcid.org/0009-0007-1094-6835",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Abdulrahman Alshaya",
          "openalex_id": "A5020760203",
          "orcid": "https://orcid.org/0000-0002-5262-5841",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Sumaya N. Almohareb",
          "openalex_id": "A5084394960",
          "orcid": "https://orcid.org/0000-0003-3392-8369",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Atheer Aldairem",
          "openalex_id": "A5086178563",
          "orcid": "https://orcid.org/0009-0001-0924-4672",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Mohammed Alrashed",
          "openalex_id": "A5069678285",
          "orcid": "https://orcid.org/0000-0002-5203-8962",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Khalid Bin Saleh",
          "openalex_id": "A5112910027",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Hisham A. Badreldin",
          "openalex_id": "A5088912799",
          "orcid": "https://orcid.org/0000-0001-7182-4347",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Majed S. Al Yami",
          "openalex_id": "A5005774540",
          "orcid": "https://orcid.org/0000-0003-2308-8407",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Shmeylan Al Harbi",
          "openalex_id": "A5017442221",
          "orcid": "https://orcid.org/0000-0003-4437-8761",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Abdulkareem Albekairy",
          "openalex_id": "A5103246868",
          "orcid": "https://orcid.org/0000-0002-0205-6484",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-09-22",
      "abstract": null,
      "cited_by_count": 2127,
      "type": "review",
      "source": {
        "name": "BMC Medical Education",
        "type": "journal",
        "issn": [
          "1472-6920"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://bmcmededuc.biomedcentral.com/counter/pdf/10.1186/s12909-023-04698-z"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Ethics in Clinical Research",
        "Machine Learning in Healthcare"
      ],
      "referenced_works_count": 117,
      "url": "https://openalex.org/W4386958277"
    },
    {
      "openalex_id": "W4399619259",
      "doi": "10.1109/ojcoms.2024.3413790",
      "title": "Explainable Artificial Intelligence for Resilient Security Applications in the Internet of Things",
      "authors": [
        {
          "name": "Mohammed Tanvir Masud",
          "openalex_id": "A5042129214",
          "orcid": "https://orcid.org/0000-0003-4951-3205",
          "institutions": [
            "UNSW Sydney",
            "University of Canberra"
          ]
        },
        {
          "name": "Marwa Keshk",
          "openalex_id": "A5056050345",
          "orcid": "https://orcid.org/0000-0001-5749-0408",
          "institutions": [
            "UNSW Sydney"
          ]
        },
        {
          "name": "Nour Moustafa",
          "openalex_id": "A5089327837",
          "orcid": "https://orcid.org/0000-0001-6127-9349",
          "institutions": [
            "UNSW Sydney",
            "University of Canberra"
          ]
        },
        {
          "name": "Igor Linkov",
          "openalex_id": "A5066135816",
          "orcid": "https://orcid.org/0000-0002-0823-8107",
          "institutions": [
            "U.S. Army Engineer Research and Development Center",
            "United States Army"
          ]
        },
        {
          "name": "Darren K. Emge",
          "openalex_id": "A5099115600"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-06-13",
      "abstract": "The performance of Artificial Intelligence (AI) systems reaches or even exceeds that of humans in an increasing number of complicated tasks. Highly effective non-linear AI models are generally employed in a black-box form nested in their complex structures, which means that no information as to what precisely helps them reach appropriate predictions is provided. The lack of transparency and interpretability in existing Artificial Intelligence techniques would reduce human users&#x2019; trust in the models used for cyber defence, especially in current scenarios where cyber resilience is becoming increasingly diverse and challenging. Explainable AI (XAI) should be incorporated into developing cybersecurity models to deliver explainable models with high accuracy that human users can understand, trust, and manage. This paper explores the following concepts related to XAI. A summary of current literature on XAI is discussed. Recent taxonomies that help explain different machine learning algorithms are discussed. These include deep learning techniques developed and studied extensively in other IoT taxonomies. The outputs of AI models are crucial for cybersecurity, as experts require more than simple binary outputs for examination to enable the cyber resilience of IoT systems. Examining the available XAI applications and safety-related threat models to explain resilience towards IoT systems also summarises the difficulties and gaps in XAI concerning cybersecurity. Finally, various technical issues and trends are explained, and future studies on technology, applications, security, and privacy are presented, emphasizing the ideas of explainable AI models.",
      "cited_by_count": 20,
      "type": "article",
      "source": {
        "name": "IEEE Open Journal of the Communications Society",
        "type": "journal",
        "issn": [
          "2644-125X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://doi.org/10.1109/ojcoms.2024.3413790"
      },
      "topics": [
        "Network Security and Intrusion Detection",
        "Anomaly Detection Techniques and Applications",
        "Brain Tumor Detection and Classification"
      ],
      "referenced_works_count": 190,
      "url": "https://openalex.org/W4399619259"
    },
    {
      "openalex_id": "W4384071683",
      "doi": "10.1038/s41586-023-06291-2",
      "title": "Large language models encode clinical knowledge",
      "authors": [
        {
          "name": "Karan Singhal",
          "openalex_id": "A5027454515",
          "orcid": "https://orcid.org/0000-0001-9002-7490",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Shekoofeh Azizi",
          "openalex_id": "A5047463591",
          "orcid": "https://orcid.org/0000-0002-7447-6031",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Tao Tu",
          "openalex_id": "A5059213795",
          "orcid": "https://orcid.org/0000-0003-3420-7889",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "S. Sara Mahdavi",
          "openalex_id": "A5063201022",
          "orcid": "https://orcid.org/0000-0001-6823-598X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Jason Lee",
          "openalex_id": "A5100657725",
          "orcid": "https://orcid.org/0000-0003-4042-795X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Hyung Won Chung",
          "openalex_id": "A5051828575",
          "orcid": "https://orcid.org/0000-0002-1280-9953",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Nathan Scales",
          "openalex_id": "A5030765685",
          "orcid": "https://orcid.org/0000-0002-9535-7138",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Ajay Kumar Tanwani",
          "openalex_id": "A5088063475",
          "orcid": "https://orcid.org/0000-0002-6365-8315",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Heather Cole-Lewis",
          "openalex_id": "A5069557194",
          "orcid": "https://orcid.org/0000-0002-7275-1810",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Stephen Pfohl",
          "openalex_id": "A5021812637",
          "orcid": "https://orcid.org/0000-0003-0551-9664",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Perry W. Payne",
          "openalex_id": "A5014637990",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Martin Seneviratne",
          "openalex_id": "A5058677067",
          "orcid": "https://orcid.org/0000-0003-0435-3738",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Paul Gamble",
          "openalex_id": "A5090718376",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Christopher Kelly",
          "openalex_id": "A5026540467",
          "orcid": "https://orcid.org/0000-0002-1246-844X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Abubakr Babiker",
          "openalex_id": "A5066029226",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Nathanael Sch\u00e4rli",
          "openalex_id": "A5007588003",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Aakanksha Chowdhery",
          "openalex_id": "A5055969617",
          "orcid": "https://orcid.org/0000-0002-0628-5225",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "P. Mansfield",
          "openalex_id": "A5086361722",
          "orcid": "https://orcid.org/0000-0003-4969-0543",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Dina Demner\u2010Fushman",
          "openalex_id": "A5046764593",
          "institutions": [
            "United States National Library of Medicine"
          ]
        },
        {
          "name": "Blaise Ag\u00fcera y Arcas",
          "openalex_id": "A5044698998",
          "orcid": "https://orcid.org/0000-0003-2256-9823",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Dale R. Webster",
          "openalex_id": "A5060000122",
          "orcid": "https://orcid.org/0000-0002-3023-8824",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Greg S. Corrado",
          "openalex_id": "A5068955381",
          "orcid": "https://orcid.org/0000-0001-8817-0992",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Yossi Matias",
          "openalex_id": "A5065128060",
          "orcid": "https://orcid.org/0000-0003-3960-6002",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Katherine Chou",
          "openalex_id": "A5070366042",
          "orcid": "https://orcid.org/0000-0002-0318-7857",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Juraj Gottweis",
          "openalex_id": "A5057932939",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Nenad Toma\u0161ev",
          "openalex_id": "A5057195145",
          "orcid": "https://orcid.org/0000-0003-1624-0220",
          "institutions": [
            "DeepMind (United Kingdom)"
          ]
        },
        {
          "name": "Yun Liu",
          "openalex_id": "A5078784976",
          "orcid": "https://orcid.org/0000-0003-4079-8275",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Alvin Rajkomar",
          "openalex_id": "A5022388476",
          "orcid": "https://orcid.org/0000-0001-5750-5016",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Jo\u00eblle Barral",
          "openalex_id": "A5043862316",
          "orcid": "https://orcid.org/0009-0009-0432-5148",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Christopher Semturs",
          "openalex_id": "A5010171106",
          "orcid": "https://orcid.org/0000-0001-6108-2773",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Alan Karthikesalingam",
          "openalex_id": "A5003509342",
          "orcid": "https://orcid.org/0000-0001-5074-898X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Vivek Natarajan",
          "openalex_id": "A5103234563",
          "orcid": "https://orcid.org/0000-0001-7849-2074",
          "institutions": [
            "Google (United States)"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-07-12",
      "abstract": "Abstract Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model 1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM 2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA 3 , MedMCQA 4 , PubMedQA 5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics 6 ), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today\u2019s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.",
      "cited_by_count": 2248,
      "type": "article",
      "source": {
        "name": "Nature",
        "type": "journal",
        "issn": [
          "0028-0836",
          "1476-4687"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.nature.com/articles/s41586-023-06291-2.pdf"
      },
      "topics": [
        "Topic Modeling",
        "Artificial Intelligence in Healthcare and Education",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 91,
      "url": "https://openalex.org/W4384071683"
    },
    {
      "openalex_id": "W4386142022",
      "doi": "10.1007/s12559-023-10179-8",
      "title": "Interpreting Black-Box Models: A Review on Explainable Artificial Intelligence",
      "authors": [
        {
          "name": "Vikas Hassija",
          "openalex_id": "A5058693080",
          "orcid": "https://orcid.org/0000-0002-3199-8753",
          "institutions": [
            "KIIT University"
          ]
        },
        {
          "name": "Vinay Chamola",
          "openalex_id": "A5005020243",
          "orcid": "https://orcid.org/0000-0002-6730-3060",
          "institutions": [
            "Birla Institute of Technology and Science, Pilani"
          ]
        },
        {
          "name": "A. Mahapatra",
          "openalex_id": "A5114077165",
          "institutions": [
            "Birla Institute of Technology and Science, Pilani"
          ]
        },
        {
          "name": "Abhinandan Singal",
          "openalex_id": "A5058997114",
          "institutions": [
            "Jaypee Institute of Information Technology"
          ]
        },
        {
          "name": "Divyansh Goel",
          "openalex_id": "A5026248849",
          "institutions": [
            "Jaypee Institute of Information Technology"
          ]
        },
        {
          "name": "Kaizhu Huang",
          "openalex_id": "A5026022035",
          "orcid": "https://orcid.org/0000-0002-3034-9639",
          "institutions": [
            "Duke Kunshan University"
          ]
        },
        {
          "name": "Simone Scardapane",
          "openalex_id": "A5022153057",
          "orcid": "https://orcid.org/0000-0003-0881-8344",
          "institutions": [
            "Sapienza University of Rome"
          ]
        },
        {
          "name": "Indro Spinelli",
          "openalex_id": "A5019615617",
          "orcid": "https://orcid.org/0000-0003-1963-3548",
          "institutions": [
            "Istituto Nazionale di Fisica Nucleare, Sezione di Roma I"
          ]
        },
        {
          "name": "Mufti Mahmud",
          "openalex_id": "A5027525633",
          "orcid": "https://orcid.org/0000-0002-2037-8348",
          "institutions": [
            "Nottingham Trent University"
          ]
        },
        {
          "name": "Amir Hussain",
          "openalex_id": "A5062211930",
          "orcid": "https://orcid.org/0000-0002-8080-082X",
          "institutions": [
            "Edinburgh Napier University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-08-24",
      "abstract": null,
      "cited_by_count": 1082,
      "type": "review",
      "source": {
        "name": "Cognitive Computation",
        "type": "journal",
        "issn": [
          "1866-9956",
          "1866-9964"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s12559-023-10179-8.pdf"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Imbalanced Data Classification Techniques",
        "Machine Learning in Healthcare"
      ],
      "referenced_works_count": 135,
      "url": "https://openalex.org/W4386142022"
    },
    {
      "openalex_id": "W4408462792",
      "doi": "10.3389/frsc.2025.1561404",
      "title": "Explainable AI and monocular vision for enhanced UAV navigation in smart cities: prospects and challenges",
      "authors": [
        {
          "name": "Shumaila Javaid",
          "openalex_id": "A5097139339"
        },
        {
          "name": "Muhammad Asghar Khan",
          "openalex_id": "A5014348689",
          "orcid": "https://orcid.org/0000-0002-1351-898X"
        },
        {
          "name": "Hamza Fahim",
          "openalex_id": "A5076643026",
          "orcid": "https://orcid.org/0000-0001-6537-7691"
        },
        {
          "name": "Bin He",
          "openalex_id": "A5026502807",
          "orcid": "https://orcid.org/0000-0002-2347-0561"
        },
        {
          "name": "Nasir Saeed",
          "openalex_id": "A5042602326",
          "orcid": "https://orcid.org/0000-0002-5123-5139"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-03-14",
      "abstract": "Explainable Artificial Intelligence (XAI) is increasingly pivotal in Unmanned Aerial Vehicle (UAV) operations within smart cities, enhancing trust and transparency in AI-driven systems by addressing the 'black-box' limitations of traditional Machine Learning (ML) models. This paper provides a comprehensive overview of the evolution of UAV navigation and control systems, tracing the transition from conventional methods such as GPS and inertial navigation to advanced AI- and ML-driven approaches. It investigates the transformative role of XAI in UAV systems, particularly in safety-critical applications where interpretability is essential. A key focus of this study is the integration of XAI into monocular vision-based navigation frameworks, which, despite their cost-effectiveness and lightweight design, face challenges such as depth perception ambiguities and limited fields of view. Embedding XAI techniques enhances the reliability and interpretability of these systems, providing clearer insights into navigation paths, obstacle detection, and avoidance strategies. This advancement is crucial for UAV adaptability in dynamic urban environments, including infrastructure changes, traffic congestion, and environmental monitoring. Furthermore, this work examines how XAI frameworks foster transparency and trust in UAV decision-making for high-stakes applications such as urban planning and disaster response. It explores critical challenges, including scalability, adaptability to evolving conditions, balancing explainability with performance, and ensuring robustness in adverse environments. Additionally, it highlights the emerging potential of integrating vision models with Large Language Models (LLMs) to further enhance UAV situational awareness and autonomous decision-making. Accordingly, this study provides actionable insights to advance next-generation UAV technologies, ensuring reliability and transparency. The findings underscore XAI's role in bridging existing research gaps and accelerating the deployment of intelligent, explainable UAV systems for future smart cities.",
      "cited_by_count": 10,
      "type": "article",
      "source": {
        "name": "Frontiers in Sustainable Cities",
        "type": "journal",
        "issn": [
          "2624-9634"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.frontiersin.org/journals/sustainable-cities/articles/10.3389/frsc.2025.1561404/pdf"
      },
      "topics": [
        "Remote Sensing and LiDAR Applications",
        "Automated Road and Building Extraction",
        "Advanced Neural Network Applications"
      ],
      "referenced_works_count": 110,
      "url": "https://openalex.org/W4408462792"
    },
    {
      "openalex_id": "W4390732409",
      "doi": "10.52549/ijeei.v11i4.5151",
      "title": "A Review on Explainable Artificial Intelligence Methods, Applications, and Challenges",
      "authors": [
        {
          "name": "Belghachi Mohammed",
          "openalex_id": "A5060752670",
          "orcid": "https://orcid.org/0009-0009-7067-4493",
          "institutions": [
            "University of Bechar"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-12-19",
      "abstract": "Normal 0 21 false false false MS X-NONE AR-SA /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Tableau Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin:0cm; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Times New Roman\",\"serif\"; mso-ansi-language:EN-US; mso-fareast-language:EN-US;} Explainable Artificial Intelligence (XAI) has emerged as a critical area of research and development in the field of artificial intelligence. This abstract provides an overview of XAI, covering its methods, applications, and challenges. XAI Methods: XAI methods aim to enhance the transparency and interpretability of complex machine learning models. Model-agnostic techniques like LIME and model-specific methods like SHAP have gained prominence in providing explanations for AI predictions. The field also explores interpretable deep learning architectures and approaches to make neural networks more transparent. XAI Applications: XAI finds applications across diverse domains. In healthcare, XAI assists in interpreting medical diagnoses and treatment recommendations. In finance, it aids in risk assessment and regulatory compliance. XAI is crucial in autonomous vehicles to explain decision-making processes, contributing to safety and trust. In customer service, it improves chatbot interactions by providing understandable responses. Moreover, XAI has relevance in agriculture, manufacturing, energy efficiency, education, content recommendation, and more. XAI Challenges: Despite its significance, XAI faces several challenges. Balancing model complexity with interpretability remains a fundamental trade-off. Detecting and mitigating bias in AI systems is crucial, especially in sensitive domains. Ensuring ethical considerations, data privacy, and user consent are paramount. Challenges also include providing explanations for high-stakes decisions, addressing the need for human oversight, and adapting to international and cultural norms. In conclusion, XAI plays a pivotal role in making AI systems more transparent, fair, and accountable. As it continues to evolve, it is poised to shape the future of AI by enabling users to understand and trust AI systems, fostering responsible AI development, and addressing ethical and practical challenges in various applications.",
      "cited_by_count": 10,
      "type": "review",
      "source": {
        "name": "Indonesian Journal of Electrical Engineering and Informatics (IJEEI)",
        "type": "journal",
        "issn": [
          "2089-3272"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "http://section.iaesonline.com/index.php/IJEEI/article/download/5151/877"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 37,
      "url": "https://openalex.org/W4390732409"
    },
    {
      "openalex_id": "W4407041321",
      "doi": "10.3390/s25030856",
      "title": "Exploring the Unseen: A Survey of Multi-Sensor Fusion and the Role of Explainable AI (XAI) in Autonomous Vehicles",
      "authors": [
        {
          "name": "De Jong Yeong",
          "openalex_id": "A5086213784",
          "orcid": "https://orcid.org/0000-0002-4626-8040",
          "institutions": [
            "Munster Technological University",
            "Science Foundation Ireland"
          ]
        },
        {
          "name": "Krishna Panduru",
          "openalex_id": "A5085987953",
          "orcid": "https://orcid.org/0000-0001-5554-3465",
          "institutions": [
            "Munster Technological University",
            "Science Foundation Ireland"
          ]
        },
        {
          "name": "J. L. Walsh",
          "openalex_id": "A5101657276",
          "orcid": "https://orcid.org/0000-0002-6756-3700",
          "institutions": [
            "Munster Technological University",
            "Science Foundation Ireland"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-01-31",
      "abstract": "Autonomous vehicles (AVs) rely heavily on multi-sensor fusion to perceive their environment and make critical, real-time decisions by integrating data from various sensors such as radar, cameras, Lidar, and GPS. However, the complexity of these systems often leads to a lack of transparency, posing challenges in terms of safety, accountability, and public trust. This review investigates the intersection of multi-sensor fusion and explainable artificial intelligence (XAI), aiming to address the challenges of implementing accurate and interpretable AV systems. We systematically review cutting-edge multi-sensor fusion techniques, along with various explainability approaches, in the context of AV systems. While multi-sensor fusion technologies have achieved significant advancement in improving AV perception, the lack of transparency and explainability in autonomous decision-making remains a primary challenge. Our findings underscore the necessity of a balanced approach to integrating XAI and multi-sensor fusion in autonomous driving applications, acknowledging the trade-offs between real-time performance and explainability. The key challenges identified span a range of technical, social, ethical, and regulatory aspects. We conclude by underscoring the importance of developing techniques that ensure real-time explainability, specifically in high-stakes applications, to stakeholders without compromising safety and accuracy, as well as outlining future research directions aimed at bridging the gap between high-performance multi-sensor fusion and trustworthy explainability in autonomous driving systems.",
      "cited_by_count": 10,
      "type": "review",
      "source": {
        "name": "Sensors",
        "type": "journal",
        "issn": [
          "1424-8220"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://doi.org/10.3390/s25030856"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Explainable Artificial Intelligence (XAI)",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 195,
      "url": "https://openalex.org/W4407041321"
    },
    {
      "openalex_id": "W4401379778",
      "doi": "10.1109/jiot.2024.3439228",
      "title": "A Trustable Federated Learning Framework for Rapid Fire Smoke Detection at the Edge in Smart Home Environments",
      "authors": [
        {
          "name": "Aryan Nikul Patel",
          "openalex_id": "A5111282672",
          "institutions": [
            "Vellore Institute of Technology University"
          ]
        },
        {
          "name": "Gautam Srivastava",
          "openalex_id": "A5041541232",
          "orcid": "https://orcid.org/0000-0001-9851-4103",
          "institutions": [
            "Brandon University",
            "Chitkara University",
            "Lebanese American University",
            "China Medical University"
          ]
        },
        {
          "name": "Praveen Kumar Reddy Maddikunta",
          "openalex_id": "A5064982530",
          "orcid": "https://orcid.org/0000-0003-4209-2495",
          "institutions": [
            "Vellore Institute of Technology University"
          ]
        },
        {
          "name": "Ramalingam Murugan",
          "openalex_id": "A5076488341",
          "orcid": "https://orcid.org/0000-0001-7770-8566",
          "institutions": [
            "Vellore Institute of Technology University"
          ]
        },
        {
          "name": "Gokul Yenduri",
          "openalex_id": "A5065197324",
          "orcid": "https://orcid.org/0000-0001-9146-8378",
          "institutions": [
            "VIT-AP University",
            "SRM University"
          ]
        },
        {
          "name": "Thippa Reddy Gadekallu",
          "openalex_id": "A5041854978",
          "institutions": [
            "Zhejiang A & F University",
            "Chitkara University",
            "Lovely Professional University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-08-06",
      "abstract": "With the rapid growth of the Internet of Things, sensors have become integral components of smart homes, enabling real-time monitoring and control of various aspects ranging from energy consumption to security. In this context, we cannot underestimate the importance of sensor-based data in ensuring the safety and well-being of occupants, particularly in scenarios involving early detection of fire outbreaks. We propose a novel federated learning (FL) Framework in this study to address the crucial issue of rapid fire smoke detection at the edge of smart home environments. The proposed framework employs three distinct FL algorithms, namely, federated averaging, federated adaptive moment estimation, and federated proximal, for global aggregation of machine learning predictions based on data from various IoT sensors. This framework allows for early prediction by utilizing the computational capabilities at the edge, thereby improving the responsiveness and efficiency of fire safety systems. Furthermore, to improve trust and transparency in the FL framework, explainable artificial intelligence techniques, such as local interpretable model-agnostic explanations (LIMEs) and Shapley additive explanations (SHAP), are integrated. We unveil pivotal features driving predictive outcomes through LIME and SHAP analyses, offering users valuable insights into model decision-making processes.",
      "cited_by_count": 12,
      "type": "article",
      "source": {
        "name": "IEEE Internet of Things Journal",
        "type": "journal",
        "issn": [
          "2327-4662",
          "2372-2541"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Fire Detection and Safety Systems",
        "Evacuation and Crowd Dynamics"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4401379778"
    },
    {
      "openalex_id": "W4400108396",
      "doi": "10.1201/9781003502432-2",
      "title": "Explainable artificial intelligence",
      "authors": [
        {
          "name": "Manoj Kumar Mahto",
          "openalex_id": "A5032121939",
          "orcid": "https://orcid.org/0000-0002-8258-055X"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-06-28",
      "abstract": "Artificial Intelligence (AI) has made significant strides across various domains, but the opacity of many AI models, especially in critical sectors like healthcare, finance, and autonomous vehicles, has raised concerns. To address this, Explainable Artificial Intelligence (XAI) has emerged, aiming to shed light on AI decision-making and provide human-comprehensible explanations. Understanding XAI is crucial as it can lead to more transparent, trustworthy, and accountable AI systems. XAI seeks to make complex AI models interpretable, bridging the gap left by black-box models like deep neural networks. Various XAI approaches cater to different use cases, with the choice depending on specific domain requirements. However, integrating XAI into autonomous vehicles poses unique challenges, necessitating solutions that maintain real-time decision-making without compromising safety. Achieving interpretability in deep learning models commonly used in autonomous vehicles is also challenging, requiring novel tailored approaches. Moreover, presenting explanations in a clear, concise manner is essential for user trust. Legal and ethical considerations arise when integrating XAI into autonomous vehicles, requiring comprehensive validation and adherence to regulatory standards. Rigorous evaluation, including quantitative and qualitative measures, is imperative to ensure effectiveness and prevent misleading explanations. XAI holds promise for enhancing transparency and interpretability across various fields, but its integration into autonomous vehicles requires addressing specific hurdles while maintaining real-time capabilities and user-centricity, ultimately fostering public trust and acceptance in transformative technologies like autonomous driving.",
      "cited_by_count": 9,
      "type": "book-chapter",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4400108396"
    },
    {
      "openalex_id": "W4391290519",
      "doi": "10.58414/scientifictemper.2023.14.4.39",
      "title": "Developing interpretable models and techniques for explainable AI in decision-making",
      "authors": [
        {
          "name": "Jayaganesh Jagannathan",
          "openalex_id": "A5077332545"
        },
        {
          "name": "Agrawal Rajesh K",
          "openalex_id": "A5102610156"
        },
        {
          "name": "Neelam Labhade-Kumar",
          "openalex_id": "A5085167545",
          "orcid": "https://orcid.org/0000-0002-4571-1135"
        },
        {
          "name": "Ravi Rastogi",
          "openalex_id": "A5013418004",
          "orcid": "https://orcid.org/0000-0002-1118-3020",
          "institutions": [
            "Deen Dayal Upadhyaya Gorakhpur University"
          ]
        },
        {
          "name": "Manu Vasudevan Unni",
          "openalex_id": "A5030841435",
          "orcid": "https://orcid.org/0000-0001-7543-7010"
        },
        {
          "name": "K. K. Baseer",
          "openalex_id": "A5082933799"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-12-31",
      "abstract": "The rapid proliferation of artificial intelligence (AI) technologies across various industries and decision-making processes has undeniably transformed the way of approaching complex problems and tasks. AI systems have proven their prowess in areas such as healthcare, finance, and autonomous systems, revolutionizing how decisions are made. Nevertheless, this proliferation of AI has raised critical concerns regarding the transparency, accountability, and fairness of these systems, as many of the state-of-the-art AI models often resemble complex black boxes. These intricate models, particularly deep learning neural networks, harbor non-linear relationships that are difficult for human users to decipher, thereby raising concerns about bias, fairness, and overall trustworthiness in AI-driven decisions. The urgency of this issue is underscored by the realization that AI should not merely be accurate; it should also be interpretable. Explainable AI (XAI) has emerged as a vital field of research, emphasizing the development of models and techniques that render AI systems comprehensible and transparent in their decision-making processes. This paper investigates into the relevance and significance of XAI across various domains, including healthcare, finance, and autonomous systems, where the ability to understand the rationale behind AI decisions is paramount. In healthcare, where AI assists in diagnosis and treatment, the interpretability of AI models is crucial for clinicians to make informed decisions. In finance, applications like credit scoring and investment analysis demand transparent AI to ensure fairness and accountability. In the realm of autonomous systems, transparency is indispensable to guarantee safety and compliance with regulations. Moreover, government agencies in areas such as law enforcement and social services require interpretable AI to maintain ethical standards and accountability. This paper also highlights the diverse array of research efforts in the XAI domain, spanning from model-specific interpretability methods to more general approaches aimed at unveiling complex AI models. Interpretable models like decision trees and rule-based systems have gained attention for their inherent transparency, while integrating explanation layers into deep neural networks strives to balance accuracy with interpretability. The study emphasizes the significance of this burgeoning field in bridging the gap between AI's advanced capabilities and human users' need for comprehensible AI systems. It seeks to contribute to this field by exploring the design, development, and practical applications of interpretable AI models and techniques, with the ultimate goal of enhancing the trust and understanding of AI-driven decisions.",
      "cited_by_count": 5,
      "type": "article",
      "source": {
        "name": "THE SCIENTIFIC TEMPER",
        "type": "journal",
        "issn": [
          "0976-8653",
          "2231-6396"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "https://scientifictemper.com/index.php/tst/article/download/868/595"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 21,
      "url": "https://openalex.org/W4391290519"
    },
    {
      "openalex_id": "W4399667893",
      "doi": "10.62839/ajfra.v01i01.46-61",
      "title": "AI and Medical Negligence",
      "authors": [
        {
          "name": "George Mensah",
          "openalex_id": "A5107967497",
          "orcid": "https://orcid.org/0009-0006-6280-5738"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-06-14",
      "abstract": "This paper examines the legal and ethical implications of artificial intelligence (AI) in medical negligence in Ghana, focusing on the adequacy of existing legal frameworks in addressing the unique challenges posed by AI-assisted healthcare delivery. Using the IRAC approach, the paper identifies key issues such as the blurring of lines of responsibility, lack of transparency and interpretability of AI systems, and potential for bias. The analysis reveals that Ghana's current laws, including the Public Health Act, 2012 (Act 851) and the Health Professions Regulatory Bodies Act, 2013 (Act 857), may not adequately address AI-related medical negligence. The paper recommends establishing multi-stakeholder committees, developing interim guidelines, adapting existing standards of care, promoting transparency and accountability, and fostering ongoing education and public engagement. The findings and recommendations contribute to the global policy debate on AI liability in medical negligence, emphasizing the need for collaborative, adaptive, and comprehensive legal and regulatory frameworks that prioritize patient safety, rights, and trust.",
      "cited_by_count": 6,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "http://dx.doi.org/10.62839/ajfra.v01i01.46-61"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 20,
      "url": "https://openalex.org/W4399667893"
    },
    {
      "openalex_id": "W4405256978",
      "doi": "10.3390/ai5040138",
      "title": "Explainable Machine Learning in Critical Decision Systems: Ensuring Safe Application and Correctness",
      "authors": [
        {
          "name": "Julius Wiggerthale",
          "openalex_id": "A5115088695",
          "institutions": [
            "Furtwangen University"
          ]
        },
        {
          "name": "Christoph Reich",
          "openalex_id": "A5088114580",
          "orcid": "https://orcid.org/0000-0001-9831-2181",
          "institutions": [
            "Furtwangen University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-12-11",
      "abstract": "Machine learning (ML) is increasingly used to support or automate decision processes in critical decision systems such as self driving cars or systems for medical diagnosis. These systems require decisions in which human lives are at stake and the decisions should therefore be well founded and very reliable. This need for reliability contrasts with the black-box nature of many ML models, making it difficult to ensure that they always behave as intended. In face of the high stakes involved, the resulting uncertainty is a significant challenge. Explainable artificial intelligence (XAI) addresses the issue by making black-box models more interpretable, often to increase user trust. However, many current XAI applications focus more on transparency and usability than on enhancing safety of ML applications. In this work, we therefore conduct a systematic literature review to examine how XAI can be leveraged to increase safety of ML applications in critical decision systems. We strive to find out for what purposes XAI is currently used in critical decision systems, what are the most common XAI techniques in critical decision systems and how XAI can be harnessed to increase safety of ML applications in critical decision systems. Using the SPAR-4-SLR protocol, we are able to answer these questions and provide a foundational resource for researchers and practitioners seeking to mitigate risks of ML applications. Essentially, we identify promising approaches of XAI which go beyond increasing trust to actively ensure correctness of decisions. Our findings propose a three-layered framework to enhance safety of ML in critical decision systems by means of XAI. The approach consists of Reliability, Validation and Verification. Furthermore, we point out gaps in research and propose future directions of XAI research for enhancing safety of ML applications in critical decision systems.",
      "cited_by_count": 5,
      "type": "article",
      "source": {
        "name": "AI",
        "type": "journal",
        "issn": [
          "2673-2688"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2673-2688/5/4/138/pdf?version=1733903788"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Adversarial Robustness in Machine Learning",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 107,
      "url": "https://openalex.org/W4405256978"
    },
    {
      "openalex_id": "W4412156868",
      "doi": "10.1186/s12910-025-01243-z",
      "title": "Evaluating accountability, transparency, and bias in AI-assisted healthcare decision- making: a qualitative study of healthcare professionals' perspectives in the UK.",
      "authors": [
        {
          "name": "Saoudi Ce Nouis",
          "openalex_id": "A5118886495",
          "institutions": [
            "Aston University",
            "Worcestershire Royal Hospital"
          ]
        },
        {
          "name": "Victoria Uren",
          "openalex_id": "A5037549599",
          "orcid": "https://orcid.org/0000-0002-1303-5574",
          "institutions": [
            "Aston University"
          ]
        },
        {
          "name": "Sunit Jariwala",
          "openalex_id": "A5007053950",
          "orcid": "https://orcid.org/0000-0001-6560-7418",
          "institutions": [
            "Worcestershire Royal Hospital"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-07-08",
      "abstract": "Abstract Background While artificial intelligence (AI) has emerged as a powerful tool for enhancing diagnostic accuracy and streamlining workflows, key ethical questions remain insufficiently explored\u2014particularly around accountability, transparency, and bias. These challenges become especially critical in domains such as pathology and blood sciences, where opaque AI algorithms and non-representative datasets can impact clinical outcomes. The present work focuses on a single NHS context and does not claim broader generalization. Methods We conducted a local qualitative study across multiple healthcare facilities in a single NHS Trust in the West Midlands, United Kingdom, to investigate healthcare professionals\u2019 experiences and perceptions of AI-assisted decision-making. Forty participants\u2014including clinicians, healthcare administrators, and AI developers\u2014took part in semi-structured interviews or focus groups. Transcribed data were analyzed using Braun and Clarke\u2019s thematic analysis framework, allowing us to identify core themes relating to the benefits of AI, ethical challenges, and potential mitigation strategies. Results Participants reported notable gains in diagnostic efficiency and resource allocation, underscoring AI\u2019s potential to reduce turnaround times for routine tests and enhance detection of abnormalities. Nevertheless, accountability surfaced as a pervasive concern: while clinicians felt ultimately liable for patient outcomes, they also relied on AI-generated insights, prompting questions about liability if systems malfunctioned. Transparency emerged as another major theme, with clinicians emphasizing the difficulty of trusting \u201cblack box\u201d models that lack clear rationale or interpretability\u2014particularly for rare or complex cases. Bias was repeatedly cited, especially when algorithms underperformed in minority patient groups or in identifying atypical presentations. These issues raised doubts about the fairness and reliability of AIassisted diagnoses. Conclusions Although AI demonstrates promise for improving efficiency and patient care, unresolved ethical complexities around accountability, transparency, and bias may erode stakeholder confidence and compromise patient safety. Participants called for clearer regulatory frameworks, inclusive training datasets, and stronger clinician\u2013developer collaboration. Future research should incorporate patient perspectives, investigate long-term impacts of AI-driven clinical decisions, and refine ethical guidelines to ensure equitable, responsible AI deployment. Trial registration : Not applicable.",
      "cited_by_count": 9,
      "type": "article",
      "source": {
        "name": "PubMed",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/12235780"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 1,
      "url": "https://openalex.org/W4412156868"
    },
    {
      "openalex_id": "W4383346782",
      "doi": "10.1038/s41746-023-00873-0",
      "title": "The imperative for regulatory oversight of large language models (or generative AI) in healthcare",
      "authors": [
        {
          "name": "Bertalan Mesk\u00f3",
          "openalex_id": "A5064145018",
          "orcid": "https://orcid.org/0000-0002-7005-7083"
        },
        {
          "name": "Eric J. Topol",
          "openalex_id": "A5084515381",
          "orcid": "https://orcid.org/0000-0002-1478-4729",
          "institutions": [
            "Scripps Research Institute"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-07-06",
      "abstract": null,
      "cited_by_count": 751,
      "type": "review",
      "source": {
        "name": "npj Digital Medicine",
        "type": "journal",
        "issn": [
          "2398-6352"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.nature.com/articles/s41746-023-00873-0.pdf"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Ethics in Clinical Research",
        "Autopsy Techniques and Outcomes"
      ],
      "referenced_works_count": 11,
      "url": "https://openalex.org/W4383346782"
    },
    {
      "openalex_id": "W4391718848",
      "doi": "10.1007/s11554-023-01411-7",
      "title": "Explaining decisions of a light-weight deep neural network for real-time coronary artery disease classification in magnetic resonance imaging",
      "authors": [
        {
          "name": "Talha Iqbal",
          "openalex_id": "A5020286253",
          "orcid": "https://orcid.org/0000-0001-9505-2732",
          "institutions": [
            "Ollscoil na Gaillimhe \u2013 University of Galway"
          ]
        },
        {
          "name": "Aaleen Khalid",
          "openalex_id": "A5101229411",
          "institutions": [
            "Ollscoil na Gaillimhe \u2013 University of Galway"
          ]
        },
        {
          "name": "Ihsan Ullah",
          "openalex_id": "A5010571416",
          "orcid": "https://orcid.org/0000-0002-7964-5199",
          "institutions": [
            "Ollscoil na Gaillimhe \u2013 University of Galway"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-02-10",
      "abstract": null,
      "cited_by_count": 11,
      "type": "article",
      "source": {
        "name": "Journal of Real-Time Image Processing",
        "type": "journal",
        "issn": [
          "1861-8200",
          "1861-8219"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s11554-023-01411-7.pdf"
      },
      "topics": [
        "Cardiac Imaging and Diagnostics",
        "Advanced X-ray and CT Imaging",
        "Artificial Intelligence in Healthcare"
      ],
      "referenced_works_count": 44,
      "url": "https://openalex.org/W4391718848"
    },
    {
      "openalex_id": "W4392871967",
      "doi": "10.1016/b978-0-443-13671-9.00001-6",
      "title": "Ethics and regulations for AI in radiology",
      "authors": [
        {
          "name": "Filippo Pesapane",
          "openalex_id": "A5042782968",
          "orcid": "https://orcid.org/0000-0002-0374-5054",
          "institutions": [
            "European Institute of Oncology"
          ]
        },
        {
          "name": "Paul Summers",
          "openalex_id": "A5067850637",
          "orcid": "https://orcid.org/0000-0002-5085-1095",
          "institutions": [
            "European Institute of Oncology",
            "Istituti di Ricovero e Cura a Carattere Scientifico"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-01-01",
      "abstract": null,
      "cited_by_count": 8,
      "type": "book-chapter",
      "source": {
        "name": "Elsevier eBooks",
        "type": "ebook platform",
        "issn": null
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Medical Imaging and Analysis",
        "Autopsy Techniques and Outcomes"
      ],
      "referenced_works_count": 53,
      "url": "https://openalex.org/W4392871967"
    },
    {
      "openalex_id": "W4408365138",
      "doi": "10.54660/.ijfmr.2024.5.1.26-32",
      "title": "Explainable AI in Robotics: A Critical Review and Implementation Strategies for Transparent Decision-Making",
      "authors": [
        {
          "name": "Abiodun Sunday Adebayo",
          "openalex_id": "A5036628807"
        },
        {
          "name": "Olanrewaju Oluwaseun Ajayi",
          "openalex_id": "A5116173266"
        },
        {
          "name": "Naomi Chukwurah",
          "openalex_id": "A5116577020"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-01-01",
      "abstract": "The rapid advancement of AI-driven robotic systems has introduced significant challenges related to transparency and trust, particularly in safety-critical applications. This review paper critically examines the current approaches to Explainable AI (xAI) in robotics, emphasizing the inherent trade-offs between performance and transparency. While high-performance AI models are essential for complex robotic tasks, their opacity often undermines trust and limits adoption. To address this, the paper proposes a comprehensive framework for implementing xAI in robotics, including strategies such as modular architecture, hybrid models, and human-centered design. The paper also discusses key design considerations and evaluation metrics that ensure a balance between interpretability and operational effectiveness. Finally, the paper reflects on the implications of these strategies for the future of robotics. It suggests avenues for further research to enhance the integration of xAI, aiming to create more trustworthy and reliable robotic systems.",
      "cited_by_count": 3,
      "type": "review",
      "source": {
        "name": "Journal of Frontiers in Multidisciplinary Research",
        "type": "journal",
        "issn": [
          "3050-9718",
          "3050-9726"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4408365138"
    },
    {
      "openalex_id": "W4401168187",
      "doi": "10.1007/s43681-024-00536-0",
      "title": "Minimum levels of interpretability for artificial moral agents",
      "authors": [
        {
          "name": "Avish Vijayaraghavan",
          "openalex_id": "A5097605828",
          "orcid": "https://orcid.org/0009-0007-2821-1917",
          "institutions": [
            "Imperial College London"
          ]
        },
        {
          "name": "Cosmin Badea",
          "openalex_id": "A5065053618",
          "orcid": "https://orcid.org/0000-0002-9808-2475",
          "institutions": [
            "Imperial College London"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-07-31",
      "abstract": "Abstract As artificial intelligence (AI) models continue to scale up, they are becoming more capable and integrated into various forms of decision-making systems. For models involved in moral decision-making (MDM), also known as artificial moral agents (AMA), interpretability provides a way to trust and understand the agent\u2019s internal reasoning mechanisms for effective use and error correction. In this paper, we bridge the technical approaches to interpretability with construction of AMAs to establish minimal safety requirements for deployed AMAs. We begin by providing an overview of AI interpretability in the context of MDM, thereby framing different levels of interpretability (or transparency) in relation to the different ways of constructing AMAs. Introducing the concept of the Minimum Level of Interpretability (MLI) and drawing on examples from the field, we explore two overarching questions: whether a lack of model transparency prevents trust and whether model transparency helps us sufficiently understand AMAs. Finally, we conclude by recommending specific MLIs for various types of agent constructions, aiming to facilitate their safe deployment in real-world scenarios.",
      "cited_by_count": 2,
      "type": "article",
      "source": {
        "name": "AI and Ethics",
        "type": "journal",
        "issn": [
          "2730-5953",
          "2730-5961"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s43681-024-00536-0.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 109,
      "url": "https://openalex.org/W4401168187"
    },
    {
      "openalex_id": "W4406642232",
      "doi": "10.20944/preprints202501.1423.v1",
      "title": "Exploring the Unseen: A Survey of Multi-Sensor Fusion and the Role of Explainable AI (XAI) in Autonomous Vehicles",
      "authors": [
        {
          "name": "De Jong Yeong",
          "openalex_id": "A5086213784",
          "orcid": "https://orcid.org/0000-0002-4626-8040"
        },
        {
          "name": "Krishna Panduru",
          "openalex_id": "A5085987953",
          "orcid": "https://orcid.org/0000-0001-5554-3465"
        },
        {
          "name": "J. L. Walsh",
          "openalex_id": "A5101657276",
          "orcid": "https://orcid.org/0000-0002-6756-3700"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-01-20",
      "abstract": "Autonomous vehicles (AVs) rely heavily on multi-sensor fusion to perceive their environment and make critical, real-time decisions by integrating data from various sensors such as radar, cameras, Lidar, and GPS. However, the complexity of these systems often leads to a lack of transparency, posing challenges in terms of safety, accountability, and public trust. This review investigates the intersection of multi-sensor fusion and explainable artificial intelligence (XAI), aiming to address the challenges of implementing accurate and interpretable AV systems. We systematically review cutting-edge multi-sensor fusion techniques, along with various explainability approaches, in the context of AV systems. While multi-sensor fusion technologies have achieved significant advancement in improving AV perception, the lack of transparency and explainability in autonomous decision-making remains a primary challenge. Our findings underscore the necessity of a balanced approach to integrating XAI and multi-sensor fusion in autonomous driving applications, acknowledging the trade-offs between real-time performance and explainability. The key challenges identified span a range of technical, social, ethical, and regulatory aspects. We conclude by underscoring the importance of developing techniques that ensure real-time explainability, specifically in high-stakes applications, to stakeholders without compromising safety and accuracy, as well as outlining future research directions aim at bridging the gap between high-performance multi-sensor fusion and trustworthy explainability in autonomous driving systems.",
      "cited_by_count": 5,
      "type": "preprint",
      "source": {
        "name": "Preprints.org",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://doi.org/10.20944/preprints202501.1423.v1"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4406642232"
    },
    {
      "openalex_id": "W4413312092",
      "doi": "10.1073/pnas.2506316122",
      "title": "Sparse autoencoders uncover biologically interpretable features in protein language model representations",
      "authors": [
        {
          "name": "Onkar Singh Gujral",
          "openalex_id": "A5077434967",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Mihir Bafna",
          "openalex_id": "A5007356332",
          "orcid": "https://orcid.org/0000-0001-8182-5888",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Eric J. Alm",
          "openalex_id": "A5091119492",
          "orcid": "https://orcid.org/0000-0001-8294-9364",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Bonnie Berger",
          "openalex_id": "A5044078921",
          "orcid": "https://orcid.org/0000-0002-2724-7228",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-08-19",
      "abstract": "Foundation models in biology\u2014particularly protein language models (PLMs)\u2014have enabled ground-breaking predictions in protein structure, function, and beyond. However, the \u201cblack-box\u201d nature of these representations limits transparency and explainability, posing challenges for human\u2013AI collaboration and leaving open questions about their human-interpretable features. Here, we leverage sparse autoencoders (SAEs) and a variant, transcoders, from natural language processing to extract, in a completely unsupervised fashion, interpretable sparse features present in both protein-level and amino acid (AA)-level representations from ESM2, a popular PLM. Unlike other approaches such as training probes for features, the extraction of features by the SAE is performed without any supervision. We find that many sparse features extracted from SAEs trained on protein-level representations are tightly associated with Gene Ontology (GO) terms across all levels of the GO hierarchy. We also use Anthropic\u2019s Claude to automate the interpretation of sparse features for both protein-level and AA-level representations and find that many of these features correspond to specific protein families and functions such as the NAD Kinase, IUNH, and the PTH family, as well as proteins involved in methyltransferase activity and in olfactory and gustatory sensory perception. We show that sparse features are more interpretable than ESM2 neurons across all our trained SAEs and transcoders. These findings demonstrate that SAEs offer a promising unsupervised approach for disentangling biologically relevant information present in PLM representations, thus aiding interpretability. This work opens the door to safety, trust, and explainability of PLMs and their applications, and paves the way to extracting meaningful biological insights across increasingly powerful models in the life sciences.",
      "cited_by_count": 5,
      "type": "article",
      "source": {
        "name": "Proceedings of the National Academy of Sciences",
        "type": "journal",
        "issn": [
          "0027-8424",
          "1091-6490"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.pnas.org/doi/pdf/10.1073/pnas.2506316122"
      },
      "topics": [
        "Machine Learning in Bioinformatics",
        "RNA and protein synthesis mechanisms",
        "Bioinformatics and Genomic Networks"
      ],
      "referenced_works_count": 24,
      "url": "https://openalex.org/W4413312092"
    },
    {
      "openalex_id": "W4383913712",
      "doi": "10.1111/1748-8583.12524",
      "title": "Human resource management in the age of generative artificial intelligence: Perspectives and research directions on ChatGPT",
      "authors": [
        {
          "name": "Pawan Budhwar",
          "openalex_id": "A5055856506",
          "orcid": "https://orcid.org/0000-0001-8915-6172",
          "institutions": [
            "Aston University"
          ]
        },
        {
          "name": "Soumyadeb Chowdhury",
          "openalex_id": "A5067653484",
          "orcid": "https://orcid.org/0000-0002-8074-248X",
          "institutions": [
            "\u00c9cole Sup\u00e9rieure de Commerce de Toulouse"
          ]
        },
        {
          "name": "Geoffrey Wood",
          "openalex_id": "A5034762665",
          "orcid": "https://orcid.org/0000-0001-9709-1823",
          "institutions": [
            "Western University",
            "Cranfield University",
            "University of Bath",
            "Trinity College Dublin"
          ]
        },
        {
          "name": "Herman Aguinis",
          "openalex_id": "A5067660877",
          "orcid": "https://orcid.org/0000-0002-3485-9484",
          "institutions": [
            "George Washington University"
          ]
        },
        {
          "name": "Greg J. Bamber",
          "openalex_id": "A5018194210",
          "orcid": "https://orcid.org/0000-0001-6646-3065",
          "institutions": [
            "Monash University",
            "Tilburg University",
            "Rutgers, The State University of New Jersey"
          ]
        },
        {
          "name": "Jose R. Beltran",
          "openalex_id": "A5044503602",
          "orcid": "https://orcid.org/0000-0002-3886-8142",
          "institutions": [
            "Rutgers, The State University of New Jersey"
          ]
        },
        {
          "name": "Paul Boselie",
          "openalex_id": "A5023651869",
          "orcid": "https://orcid.org/0000-0001-8507-7258",
          "institutions": [
            "Utrecht University"
          ]
        },
        {
          "name": "Fang Lee Cooke",
          "openalex_id": "A5025486785",
          "orcid": "https://orcid.org/0000-0003-0337-6591",
          "institutions": [
            "Monash University"
          ]
        },
        {
          "name": "Stephanie Decker",
          "openalex_id": "A5064172786",
          "orcid": "https://orcid.org/0000-0003-0547-9594",
          "institutions": [
            "University of Birmingham"
          ]
        },
        {
          "name": "Angelo S. DeNisi",
          "openalex_id": "A5065954019",
          "institutions": [
            "Tulane University"
          ]
        },
        {
          "name": "Prasanta Kumar Dey",
          "openalex_id": "A5046908030",
          "orcid": "https://orcid.org/0000-0002-9984-5374",
          "institutions": [
            "Aston University"
          ]
        },
        {
          "name": "David Guest",
          "openalex_id": "A5091214938",
          "orcid": "https://orcid.org/0000-0003-0457-6077",
          "institutions": [
            "King's College London",
            "King's College School"
          ]
        },
        {
          "name": "Andrew J. Knoblich",
          "openalex_id": "A5066797749",
          "institutions": [
            "University of North Carolina at Charlotte"
          ]
        },
        {
          "name": "Ashish Malik",
          "openalex_id": "A5108813058",
          "orcid": "https://orcid.org/0000-0002-2185-7674",
          "institutions": [
            "Hunter Water"
          ]
        },
        {
          "name": "Jaap Paauwe",
          "openalex_id": "A5081370774",
          "orcid": "https://orcid.org/0000-0003-3633-9607",
          "institutions": [
            "Tilburg University"
          ]
        },
        {
          "name": "Savvas Papagiannidis",
          "openalex_id": "A5006679927",
          "orcid": "https://orcid.org/0000-0003-0799-491X",
          "institutions": [
            "Newcastle University"
          ]
        },
        {
          "name": "Charmi Patel",
          "openalex_id": "A5088161716",
          "orcid": "https://orcid.org/0000-0002-0235-9784",
          "institutions": [
            "University of Reading"
          ]
        },
        {
          "name": "Vijay Pereira",
          "openalex_id": "A5069901933",
          "orcid": "https://orcid.org/0000-0001-6755-0793",
          "institutions": [
            "NEOMA Business School"
          ]
        },
        {
          "name": "Shuang Ren",
          "openalex_id": "A5084581187",
          "orcid": "https://orcid.org/0000-0002-8768-8447",
          "institutions": [
            "Queen's University Belfast"
          ]
        },
        {
          "name": "Steven G. Rogelberg",
          "openalex_id": "A5000877507",
          "institutions": [
            "University of North Carolina at Charlotte"
          ]
        },
        {
          "name": "Mark N. K. Saunders",
          "openalex_id": "A5054244780",
          "orcid": "https://orcid.org/0000-0001-5176-8317",
          "institutions": [
            "University of Birmingham"
          ]
        },
        {
          "name": "Rosalie L. Tung",
          "openalex_id": "A5014893902",
          "institutions": [
            "Simon Fraser University"
          ]
        },
        {
          "name": "Arup Varma",
          "openalex_id": "A5005292478",
          "orcid": "https://orcid.org/0000-0002-6530-5564",
          "institutions": [
            "Loyola University Chicago"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-07-01",
      "abstract": "Abstract ChatGPT and its variants that use generative artificial intelligence (AI) models have rapidly become a focal point in academic and media discussions about their potential benefits and drawbacks across various sectors of the economy, democracy, society, and environment. It remains unclear whether these technologies result in job displacement or creation, or if they merely shift human labour by generating new, potentially trivial or practically irrelevant, information and decisions. According to the CEO of ChatGPT, the potential impact of this new family of AI technology could be as big as \u201cthe printing press\u201d, with significant implications for employment, stakeholder relationships, business models, and academic research, and its full consequences are largely undiscovered and uncertain. The introduction of more advanced and potent generative AI tools in the AI market, following the launch of ChatGPT, has ramped up the \u201cAI arms race\u201d, creating continuing uncertainty for workers, expanding their business applications, while heightening risks related to well\u2010being, bias, misinformation, context insensitivity, privacy issues, ethical dilemmas, and security. Given these developments, this perspectives editorial offers a collection of perspectives and research pathways to extend HRM scholarship in the realm of generative AI. In doing so, the discussion synthesizes the literature on AI and generative AI, connecting it to various aspects of HRM processes, practices, relationships, and outcomes, thereby contributing to shaping the future of HRM research.",
      "cited_by_count": 617,
      "type": "article",
      "source": {
        "name": "Human Resource Management Journal",
        "type": "journal",
        "issn": [
          "0954-5395",
          "1748-8583"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/1748-8583.12524"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "AI and HR Technologies",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 107,
      "url": "https://openalex.org/W4383913712"
    },
    {
      "openalex_id": "W4393353064",
      "doi": "10.3390/bioengineering11040337",
      "title": "The Role of AI in Hospitals and Clinics: Transforming Healthcare in the 21st Century",
      "authors": [
        {
          "name": "Shiva Maleki Varnosfaderani",
          "openalex_id": "A5000215375",
          "orcid": "https://orcid.org/0000-0003-4539-2362",
          "institutions": [
            "Wayne State University"
          ]
        },
        {
          "name": "Mohamad Forouzanfar",
          "openalex_id": "A5069636604",
          "orcid": "https://orcid.org/0000-0001-8849-0144",
          "institutions": [
            "Institut Universitaire de G\u00e9riatrie de Montr\u00e9al",
            "Universit\u00e9 du Qu\u00e9bec \u00e0 Montr\u00e9al",
            "\u00c9cole de Technologie Sup\u00e9rieure"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-03-29",
      "abstract": "As healthcare systems around the world face challenges such as escalating costs, limited access, and growing demand for personalized care, artificial intelligence (AI) is emerging as a key force for transformation. This review is motivated by the urgent need to harness AI\u2019s potential to mitigate these issues and aims to critically assess AI\u2019s integration in different healthcare domains. We explore how AI empowers clinical decision-making, optimizes hospital operation and management, refines medical image analysis, and revolutionizes patient care and monitoring through AI-powered wearables. Through several case studies, we review how AI has transformed specific healthcare domains and discuss the remaining challenges and possible solutions. Additionally, we will discuss methodologies for assessing AI healthcare solutions, ethical challenges of AI deployment, and the importance of data privacy and bias mitigation for responsible technology use. By presenting a critical assessment of AI\u2019s transformative potential, this review equips researchers with a deeper understanding of AI\u2019s current and future impact on healthcare. It encourages an interdisciplinary dialogue between researchers, clinicians, and technologists to navigate the complexities of AI implementation, fostering the development of AI-driven solutions that prioritize ethical standards, equity, and a patient-centered approach.",
      "cited_by_count": 533,
      "type": "article",
      "source": {
        "name": "Bioengineering",
        "type": "journal",
        "issn": [
          "2306-5354"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2306-5354/11/4/337/pdf?version=1711712822"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Artificial Intelligence in Healthcare",
        "COVID-19 diagnosis using AI"
      ],
      "referenced_works_count": 202,
      "url": "https://openalex.org/W4393353064"
    },
    {
      "openalex_id": "W4407901361",
      "doi": "10.70445/gjcsai.1.2.2025.1-22",
      "title": "AI-Powered Food Contaminant Detection: A Review of Machine Learning Approaches",
      "authors": [
        {
          "name": "Khurram Shehzad",
          "openalex_id": "A5054699238",
          "orcid": "https://orcid.org/0000-0001-7835-5264"
        },
        {
          "name": "Akhtar Munir",
          "openalex_id": "A5103042756",
          "orcid": "https://orcid.org/0000-0003-4302-147X"
        },
        {
          "name": "Umair Ali",
          "openalex_id": "A5087846599",
          "orcid": "https://orcid.org/0000-0003-4468-5382"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-02-01",
      "abstract": "Food safety is being transformed by artificial intelligence (AI), which is boosting contamination detection, real time monitoring and transparency of food supply chain. AI based techniques like machine learning, deep learning and computer vision help to detect chemical, microbial and physical contaminants in food more accurately and efficiently. These advancements have led processes to be automated, minimize the impact of human error and facilitate better decision taking. Other innovations include rapid, automated detection and traceability using AI driven spectroscopy, sensor based monitoring and block chain integration. Challenges in adopting AI, however, include fragmented and proprietary data, lack of model interpretability, the sheer implementation costs, and regulatory hurdles. Implementing AI has cost and technical challenges for small and medium sized businesses. Also, the AI models must be explainable and FMV compliant to provide the necessary transparency and reliability. Future research will consist of building upon the AI models developed in this thesis, incorporation of AI with IoT and edge computing for real time monitoring as well as setting up of ethical and regulatory frameworks. Trust in AI driven food safety will be developed with standardized AI regulations, unbiased predictions, and data privacy protections. Although AI presents some hurdles, it has the power to contribute in building a much safer, more efficient and transparent global food supply chain.",
      "cited_by_count": 4,
      "type": "review",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Advanced Chemical Sensor Technologies",
        "Identification and Quantification in Food",
        "Spectroscopy and Chemometric Analyses"
      ],
      "referenced_works_count": 64,
      "url": "https://openalex.org/W4407901361"
    },
    {
      "openalex_id": "W4408186943",
      "doi": "10.3390/technologies13030107",
      "title": "Harnessing Metacognition for Safe and Responsible AI",
      "authors": [
        {
          "name": "Peter B. Walker",
          "openalex_id": "A5103368788",
          "orcid": "https://orcid.org/0000-0003-0746-6295",
          "institutions": [
            "Defense Health Agency"
          ]
        },
        {
          "name": "Jonathan Haase",
          "openalex_id": "A5080449654",
          "orcid": "https://orcid.org/0000-0001-8891-6512",
          "institutions": [
            "Evergreen Health Medical Center"
          ]
        },
        {
          "name": "Melissa L. Mehalick",
          "openalex_id": "A5080836385",
          "orcid": "https://orcid.org/0000-0002-6295-4307",
          "institutions": [
            "Defense Health Agency"
          ]
        },
        {
          "name": "Christopher T. Steele",
          "openalex_id": "A5104308933"
        },
        {
          "name": "Dale W. Russell",
          "openalex_id": "A5035155600",
          "orcid": "https://orcid.org/0000-0003-4289-1270",
          "institutions": [
            "Uniformed Services University of the Health Sciences"
          ]
        },
        {
          "name": "Ian N. Davidson",
          "openalex_id": "A5103494040",
          "institutions": [
            "University of California, Davis"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-03-06",
      "abstract": "The rapid advancement of artificial intelligence (AI) technologies has transformed various sectors, significantly enhancing processes and augmenting human capabilities. However, these advancements have also introduced critical concerns related to the safety, ethics, and responsibility of AI systems. To address these challenges, the principles of the robustness, interpretability, controllability, and ethical alignment framework are essential. This paper explores the integration of metacognition\u2014defined as \u201cthinking about thinking\u201d\u2014into AI systems as a promising approach to meeting these requirements. Metacognition enables AI systems to monitor, control, and regulate the system\u2019s cognitive processes, thereby enhancing their ability to self-assess, correct errors, and adapt to changing environments. By embedding metacognitive processes within AI, this paper proposes a framework that enhances the transparency, accountability, and adaptability of AI systems, fostering trust and mitigating risks associated with autonomous decision-making. Additionally, the paper examines the current state of AI safety and responsibility, discusses the applicability of metacognition to AI, and outlines a mathematical framework for incorporating metacognitive strategies into active learning processes. The findings aim to contribute to the development of safe, responsible, and ethically aligned AI systems.",
      "cited_by_count": 4,
      "type": "article",
      "source": {
        "name": "Technologies",
        "type": "journal",
        "issn": [
          "2227-7080"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2227-7080/13/3/107/pdf?version=1741272483"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4408186943"
    },
    {
      "openalex_id": "W4405550552",
      "doi": "10.1186/s12871-024-02840-y",
      "title": "The anesthesiologist\u2019s guide to critically assessing machine learning research: a narrative review",
      "authors": [
        {
          "name": "Filipa Os\u00f3rio",
          "openalex_id": "A5119809479",
          "orcid": "https://orcid.org/0000-0001-7235-4295",
          "institutions": [
            "Fundaci\u00f3n Valle del Lili"
          ]
        },
        {
          "name": "Sergio Alzate\u2010Ricaurte",
          "openalex_id": "A5031256480",
          "orcid": "https://orcid.org/0000-0002-6711-127X",
          "institutions": [
            "Fundaci\u00f3n Valle del Lili"
          ]
        },
        {
          "name": "Tomas Eduardo Mejia Vallecilla",
          "openalex_id": "A5115535507",
          "institutions": [
            "Fundaci\u00f3n Valle del Lili"
          ]
        },
        {
          "name": "Gustavo Cruz",
          "openalex_id": "A5103135534",
          "orcid": "https://orcid.org/0000-0003-1438-2284",
          "institutions": [
            "Fundaci\u00f3n Valle del Lili",
            "Icesi University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-12-18",
      "abstract": "Artificial Intelligence (AI), especially Machine Learning (ML), has developed systems capable of performing tasks that require human intelligence. In anesthesiology and other medical fields, AI applications can improve the precision and efficiency of daily clinical practice, and can also facilitate a personalized approach to patient care, which can lead to improved outcomes and quality of care. ML has been successfully applied in various settings of daily anesthesiology practice, such as predicting acute kidney injury, optimizing anesthetic doses, and managing postoperative nausea and vomiting. The critical evaluation of ML models in healthcare is crucial to assess their validity, safety, and clinical applicability. Evaluation metrics allow an objective statistical assessment of model performance. Tools such as Shapley Values (SHAP) help interpret how individual variables contribute to model predictions. Transparency in reporting is key in maintaining trust in these technologies and to ensure their use follows ethical principles, aiming to reduce safety concerns while also benefiting patients. Understanding evaluation metrics is essential, as they provide detailed information on model performance and their ability to discriminate between individual class rates. This article offers a comprehensive framework in assessing the validity, applicability, and limitations of models, guiding responsible and effective integration of ML technologies into clinical practice. A balance between innovation, patient safety and ethical considerations must be pursued.",
      "cited_by_count": 3,
      "type": "review",
      "source": {
        "name": "BMC Anesthesiology",
        "type": "journal",
        "issn": [
          "1471-2253"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://bmcanesthesiol.biomedcentral.com/counter/pdf/10.1186/s12871-024-02840-y"
      },
      "topics": [
        "Cardiac, Anesthesia and Surgical Outcomes",
        "Hemodynamic Monitoring and Therapy",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 67,
      "url": "https://openalex.org/W4405550552"
    },
    {
      "openalex_id": "W4362655598",
      "doi": "10.48550/arxiv.2304.01543",
      "title": "A Brief Review of Explainable Artificial Intelligence in Healthcare",
      "authors": [
        {
          "name": "Zahra Sadeghi",
          "openalex_id": "A5100738713",
          "orcid": "https://orcid.org/0000-0002-1360-7499"
        },
        {
          "name": "Roohallah Alizadehsani",
          "openalex_id": "A5083567791",
          "orcid": "https://orcid.org/0000-0003-0898-5054"
        },
        {
          "name": "Mehmet Akif \u00c7if\u00e7i",
          "openalex_id": "A5052911404",
          "orcid": "https://orcid.org/0000-0002-6439-8826"
        },
        {
          "name": "Samina Kausar",
          "openalex_id": "A5078332598",
          "orcid": "https://orcid.org/0000-0003-1330-7266"
        },
        {
          "name": "Rizwan Rehman",
          "openalex_id": "A5060973743",
          "orcid": "https://orcid.org/0000-0002-4725-6877"
        },
        {
          "name": "Priyakshi Mahanta",
          "openalex_id": "A5110437936"
        },
        {
          "name": "Pranjal Kumar Bora",
          "openalex_id": "A5049604729",
          "orcid": "https://orcid.org/0000-0002-7350-7721"
        },
        {
          "name": "Ammar Almasri",
          "openalex_id": "A5026803234",
          "orcid": "https://orcid.org/0000-0003-3289-3328"
        },
        {
          "name": "Rami S. Alkhawaldeh",
          "openalex_id": "A5022163394",
          "orcid": "https://orcid.org/0000-0002-2413-7074"
        },
        {
          "name": "Sadiq Hussain",
          "openalex_id": "A5062872897",
          "orcid": "https://orcid.org/0000-0002-9840-4796"
        },
        {
          "name": "Bilal Alata\u015f",
          "openalex_id": "A5034084300",
          "orcid": "https://orcid.org/0000-0002-3513-0329"
        },
        {
          "name": "Afshin Shoeibi",
          "openalex_id": "A5029791697",
          "orcid": "https://orcid.org/0000-0003-0635-6799"
        },
        {
          "name": "Hossein Moosaei",
          "openalex_id": "A5041433521",
          "orcid": "https://orcid.org/0000-0002-0640-2161"
        },
        {
          "name": "Milan Hlad\u00edk",
          "openalex_id": "A5081693462",
          "orcid": "https://orcid.org/0000-0002-7340-8491"
        },
        {
          "name": "Saeid Nahavandi",
          "openalex_id": "A5015293969",
          "orcid": "https://orcid.org/0000-0002-0360-5270"
        },
        {
          "name": "P\u00e3nos M. Pardalos",
          "openalex_id": "A5063808461",
          "orcid": "https://orcid.org/0000-0003-2824-101X"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-04-04",
      "abstract": "XAI refers to the techniques and methods for building AI applications which assist end users to interpret output and predictions of AI models. Black box AI applications in high-stakes decision-making situations, such as medical domain have increased the demand for transparency and explainability since wrong predictions may have severe consequences. Model explainability and interpretability are vital successful deployment of AI models in healthcare practices. AI applications' underlying reasoning needs to be transparent to clinicians in order to gain their trust. This paper presents a systematic review of XAI aspects and challenges in the healthcare domain. The primary goals of this study are to review various XAI methods, their challenges, and related machine learning models in healthcare. The methods are discussed under six categories: Features-oriented methods, global methods, concept models, surrogate models, local pixel-based methods, and human-centric methods. Most importantly, the paper explores XAI role in healthcare problems to clarify its necessity in safety-critical applications. The paper intends to establish a comprehensive understanding of XAI-related applications in the healthcare field by reviewing the related experimental results. To facilitate future research for filling research gaps, the importance of XAI models from different viewpoints and their limitations are investigated.",
      "cited_by_count": 4,
      "type": "review",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2304.01543"
      },
      "topics": [
        "Machine Learning in Healthcare",
        "Explainable Artificial Intelligence (XAI)",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4362655598"
    },
    {
      "openalex_id": "W4380685454",
      "doi": "10.3390/app13127082",
      "title": "Re-Thinking Data Strategy and Integration for Artificial Intelligence: Concepts, Opportunities, and Challenges",
      "authors": [
        {
          "name": "Abdulaziz Aldoseri",
          "openalex_id": "A5053777601",
          "orcid": "https://orcid.org/0009-0003-4367-0450",
          "institutions": [
            "Qatar University"
          ]
        },
        {
          "name": "Khalifa N. Al\u2010Khalifa",
          "openalex_id": "A5111995971",
          "institutions": [
            "Qatar University"
          ]
        },
        {
          "name": "A.M.S. Hamouda",
          "openalex_id": "A5029670996",
          "orcid": "https://orcid.org/0000-0001-8022-4319",
          "institutions": [
            "Qatar University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-06-13",
      "abstract": "The use of artificial intelligence (AI) is becoming more prevalent across industries such as healthcare, finance, and transportation. Artificial intelligence is based on the analysis of large datasets and requires a continuous supply of high-quality data. However, using data for AI is not without challenges. This paper comprehensively reviews and critically examines the challenges of using data for AI, including data quality, data volume, privacy and security, bias and fairness, interpretability and explainability, ethical concerns, and technical expertise and skills. This paper examines these challenges in detail and offers recommendations on how companies and organizations can address them. By understanding and addressing these challenges, organizations can harness the power of AI to make smarter decisions and gain competitive advantage in the digital age. It is expected, since this review article provides and discusses various strategies for data challenges for AI over the last decade, that it will be very helpful to the scientific research community to create new and novel ideas to rethink our approaches to data strategies for AI.",
      "cited_by_count": 485,
      "type": "article",
      "source": {
        "name": "Applied Sciences",
        "type": "journal",
        "issn": [
          "2076-3417"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2076-3417/13/12/7082/pdf?version=1686659661"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Artificial Intelligence in Healthcare and Education",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 132,
      "url": "https://openalex.org/W4380685454"
    },
    {
      "openalex_id": "W4361806707",
      "doi": "10.48550/arxiv.2303.16537",
      "title": "LMExplainer: Grounding Knowledge and Explaining Language Models",
      "authors": [
        {
          "name": "Zichen Chen",
          "openalex_id": "A5104078124"
        },
        {
          "name": "Ambuj K. Singh",
          "openalex_id": "A5036639779",
          "orcid": "https://orcid.org/0000-0002-1997-7140"
        },
        {
          "name": "Misha Sra",
          "openalex_id": "A5029380651"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-03-29",
      "abstract": "Language models (LMs) like GPT-4 are important in AI applications, but their opaque decision-making process reduces user trust, especially in safety-critical areas. We introduce LMExplainer, a novel knowledge-grounded explainer that clarifies the reasoning process of LMs through intuitive, human-understandable explanations. By leveraging a graph attention network (GAT) with a large-scale knowledge graph (KG), LMExplainer not only precisely narrows the reasoning space to focus on the most relevant knowledge but also grounds its reasoning in structured, verifiable knowledge to reduce hallucinations and enhance interpretability. LMExplainer effectively generates human-understandable explanations to enhance transparency and streamline the decision-making process. Additionally, by incorporating debugging into the explanation, it offers expertise suggestions that improve LMs from a developmental perspective. Thus, LMExplainer stands as an enhancement in making LMs more accessible and understandable to users. We evaluate LMExplainer on benchmark datasets such as CommonsenseQA and OpenBookQA, demonstrating that it outperforms most existing methods. By comparing the explanations generated by LMExplainer with those of other models, we show that our approach offers more comprehensive and clearer explanations of the reasoning process. LMExplainer provides a deeper understanding of the inner workings of LMs, advancing towards more reliable, transparent, and equitable AI.",
      "cited_by_count": 5,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2303.16537"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Topic Modeling",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4361806707"
    },
    {
      "openalex_id": "W4313648826",
      "doi": "10.3390/s23020634",
      "title": "Survey of Explainable AI Techniques in Healthcare",
      "authors": [
        {
          "name": "Ahmad Chaddad",
          "openalex_id": "A5024700033",
          "orcid": "https://orcid.org/0000-0003-3402-9576",
          "institutions": [
            "\u00c9cole de Technologie Sup\u00e9rieure",
            "Guilin University of Electronic Technology"
          ]
        },
        {
          "name": "Jihao Peng",
          "openalex_id": "A5009757263",
          "institutions": [
            "Guilin University of Electronic Technology"
          ]
        },
        {
          "name": "Jian Xu",
          "openalex_id": "A5007939424",
          "orcid": "https://orcid.org/0000-0003-2348-125X",
          "institutions": [
            "Guilin University of Electronic Technology"
          ]
        },
        {
          "name": "Ahmed Bouridane",
          "openalex_id": "A5033623398",
          "orcid": "https://orcid.org/0000-0002-1474-2772",
          "institutions": [
            "University of Sharjah"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-05",
      "abstract": "Artificial intelligence (AI) with deep learning models has been widely applied in numerous domains, including medical imaging and healthcare tasks. In the medical field, any judgment or decision is fraught with risk. A doctor will carefully judge whether a patient is sick before forming a reasonable explanation based on the patient\u2019s symptoms and/or an examination. Therefore, to be a viable and accepted tool, AI needs to mimic human judgment and interpretation skills. Specifically, explainable AI (XAI) aims to explain the information behind the black-box model of deep learning that reveals how the decisions are made. This paper provides a survey of the most recent XAI techniques used in healthcare and related medical imaging applications. We summarize and categorize the XAI types, and highlight the algorithms used to increase interpretability in medical imaging topics. In addition, we focus on the challenging XAI problems in medical applications and provide guidelines to develop better interpretations of deep learning models using XAI concepts in medical image and text analysis. Furthermore, this survey provides future directions to guide developers and researchers for future prospective investigations on clinical topics, particularly on applications with medical imaging.",
      "cited_by_count": 417,
      "type": "review",
      "source": {
        "name": "Sensors",
        "type": "journal",
        "issn": [
          "1424-8220"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/1424-8220/23/2/634/pdf?version=1672988781"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Artificial Intelligence in Healthcare and Education",
        "Radiomics and Machine Learning in Medical Imaging"
      ],
      "referenced_works_count": 120,
      "url": "https://openalex.org/W4313648826"
    },
    {
      "openalex_id": "W4411188043",
      "doi": "10.1007/s10916-025-02211-1",
      "title": "SHAP-Driven Feature Analysis Approach for Epileptic Seizure Prediction",
      "authors": [
        {
          "name": "Mohsin Hasan",
          "openalex_id": "A5102482758",
          "orcid": "https://orcid.org/0009-0006-6013-3712",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        },
        {
          "name": "Wenjuan Wu",
          "openalex_id": "A5044992396",
          "orcid": "https://orcid.org/0009-0000-7768-543X",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        },
        {
          "name": "Xufeng Zhao",
          "openalex_id": "A5055848711",
          "orcid": "https://orcid.org/0000-0002-9423-5366",
          "institutions": [
            "Nanjing University of Aeronautics and Astronautics"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-06-10",
      "abstract": null,
      "cited_by_count": 3,
      "type": "article",
      "source": {
        "name": "Journal of Medical Systems",
        "type": "journal",
        "issn": [
          "0148-5598",
          "1573-689X"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "EEG and Brain-Computer Interfaces",
        "Machine Learning in Healthcare",
        "Brain Tumor Detection and Classification"
      ],
      "referenced_works_count": 35,
      "url": "https://openalex.org/W4411188043"
    },
    {
      "openalex_id": "W4412175615",
      "doi": "10.18535/ijecs.v14i07.5181",
      "title": "Explainable AI in Edge Devices: A Lightweight Framework for Real-Time Decision Transparency",
      "authors": [
        {
          "name": "Mohammed AlNusif",
          "openalex_id": "A5118893450"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-07-02",
      "abstract": "The increasing deployment of Artificial Intelligence (AI) models on edge devices\u2014such as Raspberry Pi, NVIDIA Jetson Nano, and Google Coral TPU\u2014has revolutionized real-time decision-making in critical domains including healthcare, autonomous vehicles, and surveillance. However, these edge-based AI systems often function as opaque \"black boxes,\" making it difficult for end-users to understand, verify, or trust their decisions. This lack of interpretability not only undermines user confidence but also poses serious challenges for ethical accountability, regulatory compliance (e.g., GDPR, HIPAA), and safety in mission-critical applications. To address these limitations, this study proposes a lightweight, modular framework that enables the integration of Explainable AI (XAI) techniques into resource-constrained edge environments. We explore and benchmark several state-of-the-art XAI methods\u2014including SHAP (SHapley Additive Explanations), LIME (Local Interpretable Model-agnostic Explanations), and Saliency Maps\u2014by evaluating their performance in terms of inference latency, memory usage, interpretability score, and user trust across real-world edge devices. Multiple lightweight AI models (such as MobileNetV2, TinyBERT, and XGBoost) are trained and deployed on three benchmark datasets: CIFAR-10, EdgeMNIST, and UCI Human Activity Recognition. Experimental results demonstrate that while SHAP offers high-quality explanations, it imposes significant computational overhead, making it suitable for moderately powered platforms like Jetson Nano. In contrast, LIME achieves a balanced trade-off between transparency and resource efficiency, making it the most viable option for real-time inference on lower-end devices like Raspberry Pi. Saliency Maps, though computationally lightweight, deliver limited interpretability, particularly for non-visual data tasks. Furthermore, two real-world case studies\u2014one in smart health monitoring and the other in drone-based surveillance\u2014validate the framework's applicability. In both scenarios, the integration of XAI significantly enhanced user trust and decision reliability without breaching latency thresholds. Ultimately, this paper contributes a scalable, device-agnostic solution for embedding explainability into edge intelligence, enabling transparent AI decisions at the point of data generation. This advancement is crucial for the future of trustworthy edge AI, particularly in regulated and high-risk environments.",
      "cited_by_count": 2,
      "type": "article",
      "source": {
        "name": "International Journal Of Engineering And Computer Science",
        "type": "journal",
        "issn": [
          "2319-7242"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "https://ijecs.in/index.php/ijecs/article/download/5181/4358"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 17,
      "url": "https://openalex.org/W4412175615"
    },
    {
      "openalex_id": "W4390829176",
      "doi": "10.3390/app14020675",
      "title": "Balancing Privacy and Progress: A Review of Privacy Challenges, Systemic Oversight, and Patient Perceptions in AI-Driven Healthcare",
      "authors": [
        {
          "name": "S. Williamson",
          "openalex_id": "A5031754157",
          "orcid": "https://orcid.org/0009-0001-2785-9734",
          "institutions": [
            "University of North Texas"
          ]
        },
        {
          "name": "Victor R. Prybutok",
          "openalex_id": "A5010238940",
          "orcid": "https://orcid.org/0000-0003-3810-9039",
          "institutions": [
            "Decision Sciences (United States)",
            "University of North Texas"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-01-12",
      "abstract": "Integrating Artificial Intelligence (AI) in healthcare represents a transformative shift with substantial potential for enhancing patient care. This paper critically examines this integration, confronting significant ethical, legal, and technological challenges, particularly in patient privacy, decision-making autonomy, and data integrity. A structured exploration of these issues focuses on Differential Privacy as a critical method for preserving patient confidentiality in AI-driven healthcare systems. We analyze the balance between privacy preservation and the practical utility of healthcare data, emphasizing the effectiveness of encryption, Differential Privacy, and mixed-model approaches. The paper navigates the complex ethical and legal frameworks essential for AI integration in healthcare. We comprehensively examine patient rights and the nuances of informed consent, along with the challenges of harmonizing advanced technologies like blockchain with the General Data Protection Regulation (GDPR). The issue of algorithmic bias in healthcare is also explored, underscoring the urgent need for effective bias detection and mitigation strategies to build patient trust. The evolving roles of decentralized data sharing, regulatory frameworks, and patient agency are discussed in depth. Advocating for an interdisciplinary, multi-stakeholder approach and responsive governance, the paper aims to align healthcare AI with ethical principles, prioritize patient-centered outcomes, and steer AI towards responsible and equitable enhancements in patient care.",
      "cited_by_count": 340,
      "type": "review",
      "source": {
        "name": "Applied Sciences",
        "type": "journal",
        "issn": [
          "2076-3417"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2076-3417/14/2/675/pdf?version=1705396255"
      },
      "topics": [
        "Ethics in Clinical Research",
        "Artificial Intelligence in Healthcare and Education",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 38,
      "url": "https://openalex.org/W4390829176"
    },
    {
      "openalex_id": "W4402129402",
      "doi": "10.3390/jimaging10090215",
      "title": "Concrete Crack Detection and Segregation: A Feature Fusion, Crack Isolation, and Explainable AI-Based Approach",
      "authors": [
        {
          "name": "Reshma Ahmed Swarna",
          "openalex_id": "A5003848105",
          "institutions": [
            "Bangladesh University",
            "Mawlana Bhashani Science and Technology University"
          ]
        },
        {
          "name": "Muhammad Minoar Hossain",
          "openalex_id": "A5090107767",
          "orcid": "https://orcid.org/0000-0002-3936-5539",
          "institutions": [
            "Bangladesh University",
            "Mawlana Bhashani Science and Technology University"
          ]
        },
        {
          "name": "Mst Rokeya Khatun",
          "openalex_id": "A5038741740",
          "orcid": "https://orcid.org/0000-0002-4662-1262",
          "institutions": [
            "Bangladesh University"
          ]
        },
        {
          "name": "Mohammad Motiur Rahman",
          "openalex_id": "A5072317896",
          "orcid": "https://orcid.org/0000-0003-4417-8276",
          "institutions": [
            "Mawlana Bhashani Science and Technology University"
          ]
        },
        {
          "name": "Arslan Munir",
          "openalex_id": "A5024203425",
          "orcid": "https://orcid.org/0000-0002-3126-8945",
          "institutions": [
            "Florida Atlantic University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-08-31",
      "abstract": "Scientific knowledge of image-based crack detection methods is limited in understanding their performance across diverse crack sizes, types, and environmental conditions. Builders and engineers often face difficulties with image resolution, detecting fine cracks, and differentiating between structural and non-structural issues. Enhanced algorithms and analysis techniques are needed for more accurate assessments. Hence, this research aims to generate an intelligent scheme that can recognize the presence of cracks and visualize the percentage of cracks from an image along with an explanation. The proposed method fuses features from concrete surface images through a ResNet-50 convolutional neural network (CNN) and curvelet transform handcrafted (HC) method, optimized by linear discriminant analysis (LDA), and the eXtreme gradient boosting (XGB) classifier then uses these features to recognize cracks. This study evaluates several CNN models, including VGG-16, VGG-19, Inception-V3, and ResNet-50, and various HC techniques, such as wavelet transform, counterlet transform, and curvelet transform for feature extraction. Principal component analysis (PCA) and LDA are assessed for feature optimization. For classification, XGB, random forest (RF), adaptive boosting (AdaBoost), and category boosting (CatBoost) are tested. To isolate and quantify the crack region, this research combines image thresholding, morphological operations, and contour detection with the convex hulls method and forms a novel algorithm. Two explainable AI (XAI) tools, local interpretable model-agnostic explanations (LIMEs) and gradient-weighted class activation mapping++ (Grad-CAM++) are integrated with the proposed method to enhance result clarity. This research introduces a novel feature fusion approach that enhances crack detection accuracy and interpretability. The method demonstrates superior performance by achieving 99.93% and 99.69% accuracy on two existing datasets, outperforming state-of-the-art methods. Additionally, the development of an algorithm for isolating and quantifying crack regions represents a significant advancement in image processing for structural analysis. The proposed approach provides a robust and reliable tool for real-time crack detection and assessment in concrete structures, facilitating timely maintenance and improving structural safety. By offering detailed explanations of the model\u2019s decisions, the research addresses the critical need for transparency in AI applications, thus increasing trust and adoption in engineering practice.",
      "cited_by_count": 7,
      "type": "article",
      "source": {
        "name": "Journal of Imaging",
        "type": "journal",
        "issn": [
          "2313-433X"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://doi.org/10.3390/jimaging10090215"
      },
      "topics": [
        "Infrastructure Maintenance and Monitoring",
        "Concrete Corrosion and Durability",
        "Asphalt Pavement Performance Evaluation"
      ],
      "referenced_works_count": 46,
      "url": "https://openalex.org/W4402129402"
    },
    {
      "openalex_id": "W4386222752",
      "doi": "10.3390/diagnostics13172760",
      "title": "Redefining Radiology: A Review of Artificial Intelligence Integration in Medical Imaging",
      "authors": [
        {
          "name": "Reabal Najjar",
          "openalex_id": "A5028466190",
          "orcid": "https://orcid.org/0000-0001-7169-7077",
          "institutions": [
            "ACT Government",
            "Act Health",
            "Services Australia"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-08-25",
      "abstract": "This comprehensive review unfolds a detailed narrative of Artificial Intelligence (AI) making its foray into radiology, a move that is catalysing transformational shifts in the healthcare landscape. It traces the evolution of radiology, from the initial discovery of X-rays to the application of machine learning and deep learning in modern medical image analysis. The primary focus of this review is to shed light on AI applications in radiology, elucidating their seminal roles in image segmentation, computer-aided diagnosis, predictive analytics, and workflow optimisation. A spotlight is cast on the profound impact of AI on diagnostic processes, personalised medicine, and clinical workflows, with empirical evidence derived from a series of case studies across multiple medical disciplines. However, the integration of AI in radiology is not devoid of challenges. The review ventures into the labyrinth of obstacles that are inherent to AI-driven radiology\u2014data quality, the \u2019black box\u2019 enigma, infrastructural and technical complexities, as well as ethical implications. Peering into the future, the review contends that the road ahead for AI in radiology is paved with promising opportunities. It advocates for continuous research, embracing avant-garde imaging technologies, and fostering robust collaborations between radiologists and AI developers. The conclusion underlines the role of AI as a catalyst for change in radiology, a stance that is firmly rooted in sustained innovation, dynamic partnerships, and a steadfast commitment to ethical responsibility.",
      "cited_by_count": 538,
      "type": "review",
      "source": {
        "name": "Diagnostics",
        "type": "journal",
        "issn": [
          "2075-4418"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2075-4418/13/17/2760/pdf?version=1692969539"
      },
      "topics": [
        "Radiomics and Machine Learning in Medical Imaging",
        "Artificial Intelligence in Healthcare and Education",
        "Medical Imaging and Analysis"
      ],
      "referenced_works_count": 107,
      "url": "https://openalex.org/W4386222752"
    },
    {
      "openalex_id": "W4315880904",
      "doi": "10.1186/s12911-023-02103-9",
      "title": "Ethics and governance of trustworthy medical artificial intelligence",
      "authors": [
        {
          "name": "Jie Zhang",
          "openalex_id": "A5100436646",
          "orcid": "https://orcid.org/0000-0001-8082-2933",
          "institutions": [
            "Nanjing University of Chinese Medicine"
          ]
        },
        {
          "name": "Zongming Zhang",
          "openalex_id": "A5103246273",
          "orcid": "https://orcid.org/0000-0002-7108-2432",
          "institutions": [
            "Nanjing University of Chinese Medicine"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-13",
      "abstract": "Abstract Background The growing application of artificial intelligence (AI) in healthcare has brought technological breakthroughs to traditional diagnosis and treatment, but it is accompanied by many risks and challenges. These adverse effects are also seen as ethical issues and affect trustworthiness in medical AI and need to be managed through identification, prognosis and monitoring. Methods We adopted a multidisciplinary approach and summarized five subjects that influence the trustworthiness of medical AI: data quality, algorithmic bias, opacity, safety and security, and responsibility attribution, and discussed these factors from the perspectives of technology, law, and healthcare stakeholders and institutions. The ethical framework of ethical values-ethical principles-ethical norms is used to propose corresponding ethical governance countermeasures for trustworthy medical AI from the ethical, legal, and regulatory aspects. Results Medical data are primarily unstructured, lacking uniform and standardized annotation, and data quality will directly affect the quality of medical AI algorithm models. Algorithmic bias can affect AI clinical predictions and exacerbate health disparities. The opacity of algorithms affects patients\u2019 and doctors\u2019 trust in medical AI, and algorithmic errors or security vulnerabilities can pose significant risks and harm to patients. The involvement of medical AI in clinical practices may threaten doctors \u2018and patients\u2019 autonomy and dignity. When accidents occur with medical AI, the responsibility attribution is not clear. All these factors affect people\u2019s trust in medical AI. Conclusions In order to make medical AI trustworthy, at the ethical level, the ethical value orientation of promoting human health should first and foremost be considered as the top-level design. At the legal level, current medical AI does not have moral status and humans remain the duty bearers. At the regulatory level, strengthening data quality management, improving algorithm transparency and traceability to reduce algorithm bias, and regulating and reviewing the whole process of the AI industry to control risks are proposed. It is also necessary to encourage multiple parties to discuss and assess AI risks and social impacts, and to strengthen international cooperation and communication.",
      "cited_by_count": 307,
      "type": "article",
      "source": {
        "name": "BMC Medical Informatics and Decision Making",
        "type": "journal",
        "issn": [
          "1472-6947"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://bmcmedinformdecismak.biomedcentral.com/counter/pdf/10.1186/s12911-023-02103-9"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Ethics in Clinical Research",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 70,
      "url": "https://openalex.org/W4315880904"
    }
  ],
  "count": 40,
  "errors": []
}
