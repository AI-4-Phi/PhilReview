[search_arxiv.py] Searching arXiv: 'all:neural network circuits features AND cat:cs.AI' (category=cs.AI) (year=2023), limit=20
[search_arxiv.py] Search complete: 5 papers found
[search_arxiv.py] Cached results (cache key: arxiv_c1600f21250e1a9c)
{
  "status": "success",
  "source": "arxiv",
  "query": "all:neural network circuits features AND cat:cs.AI",
  "results": [
    {
      "arxiv_id": "2307.05639",
      "title": "Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks",
      "authors": [
        "Danny D'Agostino",
        "Ilija Ilievski",
        "Christine Annette Shoemaker"
      ],
      "abstract": "Providing a model that achieves a strong predictive performance and is simultaneously interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the radial basis function neural network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the prediction task enhancing the model interpretability. We conducted numerical experiments for regression, classification, and feature selection tasks, comparing our model against popular machine learning models, the state-of-the-art deep learning-based embedding feature selection techniques, and a transformer model for tabular data. Our results demonstrate that the proposed model does not only yield an attractive prediction performance compared to the competitors but also provides meaningful and interpretable results that potentially could assist the decision-making process in real-world applications. A PyTorch implementation of the model is available on GitHub at the following link. https://github.com/dannyzx/Gaussian-RBFNN",
      "published": "2023-07-11",
      "updated": "2024-05-11",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "stat.ML"
      ],
      "doi": "10.1016/j.neunet.2024.106335",
      "journal_ref": "Neural Networks, Volume 176, 2024, 106335",
      "pdf_url": "https://arxiv.org/pdf/2307.05639v2",
      "url": "https://arxiv.org/abs/2307.05639"
    },
    {
      "arxiv_id": "2311.14601",
      "title": "A Metalearned Neural Circuit for Nonparametric Bayesian Inference",
      "authors": [
        "Jake C. Snell",
        "Gianluca Bencomo",
        "Thomas L. Griffiths"
      ],
      "abstract": "Most applications of machine learning to classification assume a closed set of balanced classes. This is at odds with the real world, where class occurrence statistics often follow a long-tailed power-law distribution and it is unlikely that all classes are seen in a single sample. Nonparametric Bayesian models naturally capture this phenomenon, but have significant practical barriers to widespread adoption, namely implementation complexity and computational inefficiency. To address this, we present a method for extracting the inductive bias from a nonparametric Bayesian model and transferring it to an artificial neural network. By simulating data with a nonparametric Bayesian prior, we can metalearn a sequence model that performs inference over an unlimited set of classes. After training, this \"neural circuit\" has distilled the corresponding inductive bias and can successfully perform sequential inference over an open set of classes. Our experimental results show that the metalearned neural circuit achieves comparable or better performance than particle filter-based methods for inference in these models while being faster and simpler to use than methods that explicitly incorporate Bayesian nonparametric inference.",
      "published": "2023-11-24",
      "updated": "2023-11-24",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.NE",
        "stat.ML"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2311.14601v1",
      "url": "https://arxiv.org/abs/2311.14601"
    },
    {
      "arxiv_id": "2306.14753",
      "title": "The Deep Arbitrary Polynomial Chaos Neural Network or how Deep Artificial Neural Networks could benefit from Data-Driven Homogeneous Chaos Theory",
      "authors": [
        "Sergey Oladyshkin",
        "Timothy Praditia",
        "Ilja Kr\u00f6ker",
        "Farid Mohammadi",
        "Wolfgang Nowak",
        "Sebastian Otte"
      ],
      "abstract": "Artificial Intelligence and Machine learning have been widely used in various fields of mathematical computing, physical modeling, computational science, communication science, and stochastic analysis. Approaches based on Deep Artificial Neural Networks (DANN) are very popular in our days. Depending on the learning task, the exact form of DANNs is determined via their multi-layer architecture, activation functions and the so-called loss function. However, for a majority of deep learning approaches based on DANNs, the kernel structure of neural signal processing remains the same, where the node response is encoded as a linear superposition of neural activity, while the non-linearity is triggered by the activation functions. In the current paper, we suggest to analyze the neural signal processing in DANNs from the point of view of homogeneous chaos theory as known from polynomial chaos expansion (PCE). From the PCE perspective, the (linear) response on each node of a DANN could be seen as a $1^{st}$ degree multi-variate polynomial of single neurons from the previous layer, i.e. linear weighted sum of monomials. From this point of view, the conventional DANN structure relies implicitly (but erroneously) on a Gaussian distribution of neural signals. Additionally, this view revels that by design DANNs do not necessarily fulfill any orthogonality or orthonormality condition for a majority of data-driven applications. Therefore, the prevailing handling of neural signals in DANNs could lead to redundant representation as any neural signal could contain some partial information from other neural signals. To tackle that challenge, we suggest to employ the data-driven generalization of PCE theory known as arbitrary polynomial chaos (aPC) to construct a corresponding multi-variate orthonormal representations on each node of a DANN to obtain Deep arbitrary polynomial chaos neural networks.",
      "published": "2023-06-26",
      "updated": "2023-06-26",
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "stat.ML"
      ],
      "doi": "10.1016/j.neunet.2023.06.036",
      "journal_ref": "Neural Networks Volume 166, September 2023, Pages 85-104",
      "pdf_url": "https://arxiv.org/pdf/2306.14753v1",
      "url": "https://arxiv.org/abs/2306.14753"
    },
    {
      "arxiv_id": "2304.09590",
      "title": "Parallel Neural Networks in Golang",
      "authors": [
        "Daniela Kalwarowskyj",
        "Erich Schikuta"
      ],
      "abstract": "This paper describes the design and implementation of parallel neural networks (PNNs) with the novel programming language Golang. We follow in our approach the classical Single-Program Multiple-Data (SPMD) model where a PNN is composed of several sequential neural networks, which are trained with a proportional share of the training dataset. We used for this purpose the MNIST dataset, which contains binary images of handwritten digits. Our analysis focusses on different activation functions and optimizations in the form of stochastic gradients and initialization of weights and biases. We conduct a thorough performance analysis, where network configurations and different performance factors are analyzed and interpreted. Golang and its inherent parallelization support proved very well for parallel neural network simulation by considerable decreased processing times compared to sequential variants.",
      "published": "2023-04-19",
      "updated": "2023-04-19",
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.DC"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2304.09590v1",
      "url": "https://arxiv.org/abs/2304.09590"
    },
    {
      "arxiv_id": "2307.06084",
      "title": "Neuromorphic analog circuits for robust on-chip always-on learning in spiking neural networks",
      "authors": [
        "Arianna Rubino",
        "Matteo Cartiglia",
        "Melika Payvand",
        "Giacomo Indiveri"
      ],
      "abstract": "Mixed-signal neuromorphic systems represent a promising solution for solving extreme-edge computing tasks without relying on external computing resources. Their spiking neural network circuits are optimized for processing sensory data on-line in continuous-time. However, their low precision and high variability can severely limit their performance. To address this issue and improve their robustness to inhomogeneities and noise in both their internal state variables and external input signals, we designed on-chip learning circuits with short-term analog dynamics and long-term tristate discretization mechanisms. An additional hysteretic stop-learning mechanism is included to improve stability and automatically disable weight updates when necessary, to enable continuous always-on learning. We designed a spiking neural network with these learning circuits in a prototype chip using a 180 nm CMOS technology. Simulation and silicon measurement results from the prototype chip are presented. These circuits enable the construction of large-scale spiking neural networks with online learning capabilities for real-world edge computing tasks.",
      "published": "2023-07-12",
      "updated": "2023-07-12",
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "doi": "10.1109/AICAS57966.2023.10168620",
      "journal_ref": "2023 IEEE 5th International Conference on Artificial Intelligence Circuits and Systems (AICAS)",
      "pdf_url": "https://arxiv.org/pdf/2307.06084v1",
      "url": "https://arxiv.org/abs/2307.06084"
    }
  ],
  "count": 5,
  "errors": []
}
