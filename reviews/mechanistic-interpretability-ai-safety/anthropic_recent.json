[s2_search.py] Searching Semantic Scholar: 'Anthropic interpretability' (year=2023-2025), limit=20
[s2_search.py] Retrieved 20/20 papers...
[s2_search.py] Cached results (cache key: s2_4664711bddc09d0e)
{
  "status": "success",
  "source": "semantic_scholar",
  "query": "Anthropic interpretability",
  "results": [
    {
      "paperId": "9da32528cbf21fa8b129aabe5cdeded21ba082e0",
      "title": "Explaining Large Language Models with gSMILE",
      "authors": [
        {
          "name": "Zeinab Dehghani",
          "authorId": "2336873391"
        },
        {
          "name": "Mohammed Naveed Akram",
          "authorId": "33759196"
        },
        {
          "name": "K. Aslansefat",
          "authorId": "2377794490"
        },
        {
          "name": "Adil Khan",
          "authorId": "2336920066"
        },
        {
          "name": "Y. Papadopoulos",
          "authorId": "2318628033"
        }
      ],
      "year": 2025,
      "abstract": "Large Language Models (LLMs) such as GPT, LLaMA, and Claude achieve remarkable performance in text generation but remain opaque in their decision-making processes, limiting trust and accountability in high-stakes applications. We present gSMILE (generative SMILE), a model-agnostic, perturbation-based framework for token-level interpretability in LLMs. Extending the SMILE methodology, gSMILE uses controlled prompt perturbations, Wasserstein distance metrics, and weighted linear surrogates to identify input tokens with the most significant impact on the output. This process enables the generation of intuitive heatmaps that visually highlight influential tokens and reasoning paths. We evaluate gSMILE across leading LLMs (OpenAI's gpt-3.5-turbo-instruct, Meta's LLaMA 3.1 Instruct Turbo, and Anthropic's Claude 2.1) using attribution fidelity, attribution consistency, attribution stability, attribution faithfulness, and attribution accuracy as metrics. Results show that gSMILE delivers reliable human-aligned attributions, with Claude 2.1 excelling in attention fidelity and GPT-3.5 achieving the highest output consistency. These findings demonstrate gSMILE's ability to balance model performance and interpretability, enabling more transparent and trustworthy AI systems.",
      "citationCount": 1,
      "doi": null,
      "arxivId": "2505.21657",
      "url": "https://www.semanticscholar.org/paper/9da32528cbf21fa8b129aabe5cdeded21ba082e0",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "281b88071874dc23456584179740b6d81db44117",
      "title": "Using physics-inspired Singular Learning Theory to understand grokking&other phase transitions in modern neural networks",
      "authors": [
        {
          "name": "Anish Lakkapragada",
          "authorId": "2123594942"
        }
      ],
      "year": 2025,
      "abstract": "Classical statistical inference and learning theory often fail to explain the success of modern neural networks. A key reason is that these models are non-identifiable (singular), violating core assumptions behind PAC bounds and asymptotic normality. Singular learning theory (SLT), a physics-inspired framework grounded in algebraic geometry, has gained popularity for its ability to close this theory-practice gap. In this paper, we empirically study SLT in toy settings relevant to interpretability and phase transitions. First, we understand the SLT free energy $\\mathcal{F}_n$ by testing an Arrhenius-style rate hypothesis using both a grokking modulo-arithmetic model and Anthropic's Toy Models of Superposition. Second, we understand the local learning coefficient $\\lambda_{\\alpha}$ by measuring how it scales with problem difficulty across several controlled network families (polynomial regressors, low-rank linear networks, and low-rank autoencoders). Our experiments recover known scaling laws while others yield meaningful deviations from theoretical expectations. Overall, our paper illustrates the many merits of SLT for understanding neural network phase transitions, and poses open research questions for the field.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2512.00686",
      "url": "https://www.semanticscholar.org/paper/281b88071874dc23456584179740b6d81db44117",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "8b750488d139f9beba0815ff8f46ebe15ebb3e58",
      "title": "Mechanistic Interpretability for AI Safety - A Review",
      "authors": [
        {
          "name": "Leonard Bereska",
          "authorId": "87881370"
        },
        {
          "name": "E. Gavves",
          "authorId": "2304222"
        }
      ],
      "year": 2024,
      "abstract": "Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We examine benefits in understanding, control, alignment, and risks such as capability gains and dual-use concerns. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.",
      "citationCount": 286,
      "doi": "10.48550/arXiv.2404.14082",
      "arxivId": "2404.14082",
      "url": "https://www.semanticscholar.org/paper/8b750488d139f9beba0815ff8f46ebe15ebb3e58",
      "venue": "Trans. Mach. Learn. Res.",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2404.14082"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "10145b2238569436754c4d9be3f9c7db501cc65c",
      "title": "Kolmogorov-Arnold Networks for Time Series: Bridging Predictive Power and Interpretability",
      "authors": [
        {
          "name": "Kunpeng Xu",
          "authorId": "2113452108"
        },
        {
          "name": "Lifei Chen",
          "authorId": "2265432298"
        },
        {
          "name": "Shengrui Wang",
          "authorId": "2265424708"
        }
      ],
      "year": 2024,
      "abstract": "Kolmogorov-Arnold Networks (KAN) is a groundbreaking model recently proposed by the MIT team, representing a revolutionary approach with the potential to be a game-changer in the field. This innovative concept has rapidly garnered worldwide interest within the AI community. Inspired by the Kolmogorov-Arnold representation theorem, KAN utilizes spline-parametrized univariate functions in place of traditional linear weights, enabling them to dynamically learn activation patterns and significantly enhancing interpretability. In this paper, we explore the application of KAN to time series forecasting and propose two variants: T-KAN and MT-KAN. T-KAN is designed to detect concept drift within time series and can explain the nonlinear relationships between predictions and previous time steps through symbolic regression, making it highly interpretable in dynamically changing environments. MT-KAN, on the other hand, improves predictive performance by effectively uncovering and leveraging the complex relationships among variables in multivariate time series. Experiments validate the effectiveness of these approaches, demonstrating that T-KAN and MT-KAN significantly outperform traditional methods in time series forecasting tasks, not only enhancing predictive accuracy but also improving model interpretability. This research opens new avenues for adaptive forecasting models, highlighting the potential of KAN as a powerful and interpretable tool in predictive analytics.",
      "citationCount": 101,
      "doi": "10.48550/arXiv.2406.02496",
      "arxivId": "2406.02496",
      "url": "https://www.semanticscholar.org/paper/10145b2238569436754c4d9be3f9c7db501cc65c",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.02496"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d9bf49d90e1c646ade1c535f8e93d2c7413da14b",
      "title": "Rethinking Interpretability in the Era of Large Language Models",
      "authors": [
        {
          "name": "Chandan Singh",
          "authorId": "2261286637"
        },
        {
          "name": "J. Inala",
          "authorId": "1827015"
        },
        {
          "name": "Michel Galley",
          "authorId": "2253458981"
        },
        {
          "name": "Rich Caruana",
          "authorId": "2282539419"
        },
        {
          "name": "Jianfeng Gao",
          "authorId": "2256227181"
        }
      ],
      "year": 2024,
      "abstract": "Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs. In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations.",
      "citationCount": 102,
      "doi": "10.48550/arXiv.2402.01761",
      "arxivId": "2402.01761",
      "url": "https://www.semanticscholar.org/paper/d9bf49d90e1c646ade1c535f8e93d2c7413da14b",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.01761"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "ebdf9fa931574e034c15cc871fe56b621d7c7e96",
      "title": "Unlocking the black box: an in-depth review on interpretability, explainability, and reliability in deep learning",
      "authors": [
        {
          "name": "Emrullah \u015eahin",
          "authorId": "2187116340"
        },
        {
          "name": "Naciye Nur Arslan",
          "authorId": "2163920854"
        },
        {
          "name": "Durmu\u015f \u00d6zdemir",
          "authorId": "2279498085"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 106,
      "doi": "10.1007/s00521-024-10437-2",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/ebdf9fa931574e034c15cc871fe56b621d7c7e96",
      "venue": "Neural computing & applications (Print)",
      "journal": {
        "name": "Neural Computing and Applications",
        "pages": "859 - 965",
        "volume": "37"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "2ac231b9cff4f5f9054d86c9b540429d4dd687f4",
      "title": "A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models",
      "authors": [
        {
          "name": "Daking Rai",
          "authorId": "2203429265"
        },
        {
          "name": "Yilun Zhou",
          "authorId": "2309554954"
        },
        {
          "name": "Shi Feng",
          "authorId": "2309897784"
        },
        {
          "name": "Abulhair Saparov",
          "authorId": "2407368"
        },
        {
          "name": "Ziyu Yao",
          "authorId": "2307416803"
        }
      ],
      "year": 2024,
      "abstract": "Mechanistic interpretability (MI) is an emerging sub-field of interpretability that seeks to understand a neural network model by reverse-engineering its internal computations. Recently, MI has garnered significant attention for interpreting transformer-based language models (LMs), resulting in many novel insights yet introducing new challenges. However, there has not been work that comprehensively reviews these insights and challenges, particularly as a guide for newcomers to this field. To fill this gap, we provide a comprehensive survey from a task-centric perspective, organizing the taxonomy of MI research around specific research questions or tasks. We outline the fundamental objects of study in MI, along with the techniques, evaluation methods, and key findings for each task in the taxonomy. In particular, we present a task-centric taxonomy as a roadmap for beginners to navigate the field by helping them quickly identify impactful problems in which they are most interested and leverage MI for their benefit. Finally, we discuss the current gaps in the field and suggest potential future directions for MI research.",
      "citationCount": 79,
      "doi": "10.48550/arXiv.2407.02646",
      "arxivId": "2407.02646",
      "url": "https://www.semanticscholar.org/paper/2ac231b9cff4f5f9054d86c9b540429d4dd687f4",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2407.02646"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "f680d47a51a0e470fcb228bf0110c026535ead1b",
      "title": "Progress measures for grokking via mechanistic interpretability",
      "authors": [
        {
          "name": "Neel Nanda",
          "authorId": "2051128902"
        },
        {
          "name": "Lawrence Chan",
          "authorId": "2072836382"
        },
        {
          "name": "Tom Lieberum",
          "authorId": "2162470507"
        },
        {
          "name": "Jess Smith",
          "authorId": "2200391337"
        },
        {
          "name": "J. Steinhardt",
          "authorId": "5164568"
        }
      ],
      "year": 2023,
      "abstract": "Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \\textit{progress measures} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.",
      "citationCount": 607,
      "doi": "10.48550/arXiv.2301.05217",
      "arxivId": "2301.05217",
      "url": "https://www.semanticscholar.org/paper/f680d47a51a0e470fcb228bf0110c026535ead1b",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2301.05217"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "eefbd8b384a58f464827b19e30a6920ba976def9",
      "title": "Towards Automated Circuit Discovery for Mechanistic Interpretability",
      "authors": [
        {
          "name": "Arthur Conmy",
          "authorId": "2131632310"
        },
        {
          "name": "Augustine N. Mavor-Parker",
          "authorId": "2000605969"
        },
        {
          "name": "Aengus Lynch",
          "authorId": "2174176979"
        },
        {
          "name": "Stefan Heimersheim",
          "authorId": "2256989665"
        },
        {
          "name": "Adri\u00e0 Garriga-Alonso",
          "authorId": "1388513000"
        }
      ],
      "year": 2023,
      "abstract": "Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery.",
      "citationCount": 432,
      "doi": "10.48550/arXiv.2304.14997",
      "arxivId": "2304.14997",
      "url": "https://www.semanticscholar.org/paper/eefbd8b384a58f464827b19e30a6920ba976def9",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2304.14997"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c5d82b27897633d6c3b2e452a0dc6c019d4a1565",
      "title": "Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control",
      "authors": [
        {
          "name": "Aleksandar Makelov",
          "authorId": "17775913"
        },
        {
          "name": "Georg Lange",
          "authorId": "2268489534"
        },
        {
          "name": "Neel Nanda",
          "authorId": "2051128902"
        }
      ],
      "year": 2024,
      "abstract": "Disentangling model activations into meaningful features is a central problem in interpretability. However, the absence of ground-truth for these features in realistic scenarios makes validating recent approaches, such as sparse dictionary learning, elusive. To address this challenge, we propose a framework for evaluating feature dictionaries in the context of specific tasks, by comparing them against \\emph{supervised} feature dictionaries. First, we demonstrate that supervised dictionaries achieve excellent approximation, control, and interpretability of model computations on the task. Second, we use the supervised dictionaries to develop and contextualize evaluations of unsupervised dictionaries along the same three axes. We apply this framework to the indirect object identification (IOI) task using GPT-2 Small, with sparse autoencoders (SAEs) trained on either the IOI or OpenWebText datasets. We find that these SAEs capture interpretable features for the IOI task, but they are less successful than supervised features in controlling the model. Finally, we observe two qualitative phenomena in SAE training: feature occlusion (where a causally relevant concept is robustly overshadowed by even slightly higher-magnitude ones in the learned features), and feature over-splitting (where binary features split into many smaller, less interpretable features). We hope that our framework will provide a useful step towards more objective and grounded evaluations of sparse dictionary learning methods.",
      "citationCount": 58,
      "doi": "10.48550/arXiv.2405.08366",
      "arxivId": "2405.08366",
      "url": "https://www.semanticscholar.org/paper/c5d82b27897633d6c3b2e452a0dc6c019d4a1565",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2405.08366"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d9a449e1123ca37375c9977f51b7ea6129905803",
      "title": "RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations",
      "authors": [
        {
          "name": "Jing Huang",
          "authorId": "2145739230"
        },
        {
          "name": "Zhengxuan Wu",
          "authorId": "47039337"
        },
        {
          "name": "Christopher Potts",
          "authorId": "2280333621"
        },
        {
          "name": "Mor Geva",
          "authorId": "22245981"
        },
        {
          "name": "Atticus Geiger",
          "authorId": "80833908"
        }
      ],
      "year": 2024,
      "abstract": "Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.",
      "citationCount": 54,
      "doi": "10.48550/arXiv.2402.17700",
      "arxivId": "2402.17700",
      "url": "https://www.semanticscholar.org/paper/d9a449e1123ca37375c9977f51b7ea6129905803",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.17700"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "59b988fda9c1737465921a9bade731d511500718",
      "title": "The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability",
      "authors": [
        {
          "name": "Aaron Mueller",
          "authorId": "2261670263"
        },
        {
          "name": "Jannik Brinkmann",
          "authorId": "2228224955"
        },
        {
          "name": "Millicent Li",
          "authorId": "2314738780"
        },
        {
          "name": "Samuel Marks",
          "authorId": "2225941937"
        },
        {
          "name": "Koyena Pal",
          "authorId": "2053245673"
        },
        {
          "name": "Nikhil Prakash",
          "authorId": "2284985448"
        },
        {
          "name": "Can Rager",
          "authorId": "2257034392"
        },
        {
          "name": "Aruna Sankaranarayanan",
          "authorId": "2314691448"
        },
        {
          "name": "Arnab Sen Sharma",
          "authorId": "1429844787"
        },
        {
          "name": "Jiuding Sun",
          "authorId": "2314741402"
        },
        {
          "name": "Eric Todd",
          "authorId": "145290788"
        },
        {
          "name": "David Bau",
          "authorId": "2284996653"
        },
        {
          "name": "Yonatan Belinkov",
          "authorId": "2083259"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 43,
      "doi": "10.48550/arXiv.2408.01416",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/59b988fda9c1737465921a9bade731d511500718",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2408.01416"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "6fefd138be7a96ad6fd5bc448b6be28f69c8198a",
      "title": "Microsoft Copilot and Anthropic Claude AI in education and library service",
      "authors": [
        {
          "name": "Adebowale Jeremy Adetayo",
          "authorId": "122124095"
        },
        {
          "name": "Mariam Oyinda Aborisade",
          "authorId": "2264980426"
        },
        {
          "name": "Basheer Abiodun Sanni",
          "authorId": "2134629237"
        }
      ],
      "year": 2024,
      "abstract": "\nPurpose\nThis study aims to explore the collaborative potential of Microsoft Copilot and Anthropic Claude AI as an assistive technology in education and library services. The research delves into technical architectures and various use cases for both tools, proposing integration strategies within educational and library environments. The paper also addresses challenges such as algorithmic bias, hallucination and data rights.\n\n\nDesign/methodology/approach\nThe study used a literature review approach combined with the proposal of integration strategies across education and library settings.\n\n\nFindings\nThe collaborative framework between Copilot and Claude AI offers a comprehensive solution for transforming education and library services. The study identifies the seamless combination of real-time internet access, information retrieval and advanced comprehension features as key findings. In addition, challenges such as algorithmic bias and data rights are addressed, emphasizing the need for responsible AI governance, transparency and continuous improvement.\n\n\nOriginality/value\nContribute to the field by exploring the unique collaborative framework of Copilot and Claude AI in a specific context, emphasizing responsible AI governance and addressing existing gaps.\n",
      "citationCount": 31,
      "doi": "10.1108/lhtn-01-2024-0002",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/6fefd138be7a96ad6fd5bc448b6be28f69c8198a",
      "venue": "Library Hi Tech News",
      "journal": {
        "name": "Library Hi Tech News"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "21459b853e75341e257960bbdf304a43265a52b8",
      "title": "Enhancing interpretability and accuracy of AI models in healthcare: a comprehensive review on challenges and future directions",
      "authors": [
        {
          "name": "Mohammad Ennab",
          "authorId": "2173959360"
        },
        {
          "name": "Hamid Mcheick",
          "authorId": "2332976568"
        }
      ],
      "year": 2024,
      "abstract": "Artificial Intelligence (AI) has demonstrated exceptional performance in automating critical healthcare tasks, such as diagnostic imaging analysis and predictive modeling, often surpassing human capabilities. The integration of AI in healthcare promises substantial improvements in patient outcomes, including faster diagnosis and personalized treatment plans. However, AI models frequently lack interpretability, leading to significant challenges concerning their performance and generalizability across diverse patient populations. These opaque AI technologies raise serious patient safety concerns, as non-interpretable models can result in improper treatment decisions due to misinterpretations by healthcare providers. Our systematic review explores various AI applications in healthcare, focusing on the critical assessment of model interpretability and accuracy. We identify and elucidate the most significant limitations of current AI systems, such as the black-box nature of deep learning models and the variability in performance across different clinical settings. By addressing these challenges, our objective is to provide healthcare providers with well-informed strategies to develop innovative and safe AI solutions. This review aims to ensure that future AI implementations in healthcare not only enhance performance but also maintain transparency and patient safety.",
      "citationCount": 68,
      "doi": "10.3389/frobt.2024.1444763",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/21459b853e75341e257960bbdf304a43265a52b8",
      "venue": "Frontiers Robotics AI",
      "journal": {
        "name": "Frontiers in Robotics and AI",
        "volume": "11"
      },
      "publicationTypes": [
        "Review",
        "JournalArticle"
      ]
    },
    {
      "paperId": "bacb0d91bd31ae81206fa4e25f995747515dbc5a",
      "title": "Disentangled Graph Variational Auto-Encoder for Multimodal Recommendation With Interpretability",
      "authors": [
        {
          "name": "Xin Zhou",
          "authorId": "2257593840"
        },
        {
          "name": "Chunyan Miao",
          "authorId": "2257348767"
        }
      ],
      "year": 2024,
      "abstract": "Multimodal recommender systems amalgamate multimodal information (<italic>e.g.</italic>, textual descriptions, images) into a collaborative filtering framework to provide more accurate recommendations. While the incorporation of multimodal information could enhance the interpretability of these systems, current multimodal models represent users and items utilizing entangled numerical vectors, rendering them arduous to interpret. To address this, we propose a Disentangled Graph Variational Auto-Encoder (<inline-formula><tex-math notation=\"LaTeX\">$\\mathtt{DGVAE}$</tex-math></inline-formula>) that aims to enhance both model and recommendation interpretability. <inline-formula><tex-math notation=\"LaTeX\">$\\mathtt{DGVAE}$</tex-math></inline-formula> initially projects multimodal information into textual contents, such as converting images to text, by harnessing state-of-the-art multimodal pre-training technologies. It then constructs a frozen item-item graph and encodes the contents and interactions into two sets of disentangled representations utilizing a simplified residual graph convolutional network. <inline-formula><tex-math notation=\"LaTeX\">$\\mathtt{DGVAE}$</tex-math></inline-formula> further regularizes these disentangled representations through mutual information maximization, aligning the representations derived from the interactions between users and items with those learned from textual content. This alignment facilitates the interpretation of user binary interactions via text. Our empirical analysis conducted on three real-world datasets demonstrates that <inline-formula><tex-math notation=\"LaTeX\">$\\mathtt{DGVAE}$</tex-math></inline-formula> significantly surpasses the performance of state-of-the-art baselines by a margin of 10.02%. We also furnish a case study from a real-world dataset to illustrate the interpretability of <inline-formula><tex-math notation=\"LaTeX\">$\\mathtt{DGVAE}$</tex-math></inline-formula>.",
      "citationCount": 28,
      "doi": "10.1109/TMM.2024.3369875",
      "arxivId": "2402.16110",
      "url": "https://www.semanticscholar.org/paper/bacb0d91bd31ae81206fa4e25f995747515dbc5a",
      "venue": "IEEE transactions on multimedia",
      "journal": {
        "name": "IEEE Transactions on Multimedia",
        "pages": "7543-7554",
        "volume": "26"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c626f4c2ad1ff8501de1fd930deb887e6358c9ba",
      "title": "Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders",
      "authors": [
        {
          "name": "Luke Marks",
          "authorId": "2257345417"
        },
        {
          "name": "Alisdair Paren",
          "authorId": "2329106161"
        },
        {
          "name": "David Krueger",
          "authorId": "2262214707"
        },
        {
          "name": "Fazl Barez",
          "authorId": "2143198655"
        }
      ],
      "year": 2024,
      "abstract": "Sparse Autoencoders (SAEs) have shown promise in improving the interpretability of neural network activations, but can learn features that are not features of the input, limiting their effectiveness. We propose \\textsc{Mutual Feature Regularization} \\textbf{(MFR)}, a regularization technique for improving feature learning by encouraging SAEs trained in parallel to learn similar features. We motivate \\textsc{MFR} by showing that features learned by multiple SAEs are more likely to correlate with features of the input. By training on synthetic data with known features of the input, we show that \\textsc{MFR} can help SAEs learn those features, as we can directly compare the features learned by the SAE with the input features for the synthetic data. We then scale \\textsc{MFR} to SAEs that are trained to denoise electroencephalography (EEG) data and SAEs that are trained to reconstruct GPT-2 Small activations. We show that \\textsc{MFR} can improve the reconstruction loss of SAEs by up to 21.21\\% on GPT-2 Small, and 6.67\\% on EEG data. Our results suggest that the similarity between features learned by different SAEs can be leveraged to improve SAE training, thereby enhancing performance and the usefulness of SAEs for model interpretability.",
      "citationCount": 16,
      "doi": "10.48550/arXiv.2411.01220",
      "arxivId": "2411.01220",
      "url": "https://www.semanticscholar.org/paper/c626f4c2ad1ff8501de1fd930deb887e6358c9ba",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2411.01220"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "cd0c5a606e4068a142543679e802a0c1d337436b",
      "title": "Interpretability Gone Bad: The Role of Bounded Rationality in How Practitioners Understand Machine Learning",
      "authors": [
        {
          "name": "Harmanpreet Kaur",
          "authorId": "2294201173"
        },
        {
          "name": "Matthew R. Conrad",
          "authorId": "2294199428"
        },
        {
          "name": "Davis Rule",
          "authorId": "2299093740"
        },
        {
          "name": "Cliff Lampe",
          "authorId": "145856934"
        },
        {
          "name": "Eric Gilbert",
          "authorId": "2280910689"
        }
      ],
      "year": 2024,
      "abstract": "While interpretability tools are intended to help people better understand machine learning (ML), we find that they can, in fact, impair understanding. This paper presents a pre-registered, controlled experiment showing that ML practitioners (N=119) spent 5x less time on task, and were 17% less accurate about the data and model, when given access to interpretability tools. We present bounded rationality as the theoretical reason behind these findings. Bounded rationality presumes human departures from perfect rationality, and it is often effectuated by satisficing, i.e., an inclination towards \"good enough\" understanding. Adding interactive elements---a strategy often employed to promote deliberative thinking and engagement, and tested in our experiment---also does not help. We discuss implications for interpretability designers and researchers related to how cognitive and contextual factors can affect the effectiveness of interpretability tool use.",
      "citationCount": 17,
      "doi": "10.1145/3637354",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/cd0c5a606e4068a142543679e802a0c1d337436b",
      "venue": "Proc. ACM Hum. Comput. Interact.",
      "journal": {
        "name": "Proceedings of the ACM on Human-Computer Interaction",
        "pages": "1 - 34",
        "volume": "8"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c93fa2c9b37b5c46cadfda1bf3c3b7fe7fb0e560",
      "title": "Fine-Grained Interpretability for EEG Emotion Recognition: Concat-Aided Grad-CAM and Systematic Brain Functional Network",
      "authors": [
        {
          "name": "Bingxiu Liu",
          "authorId": "2329416757"
        },
        {
          "name": "Jifeng Guo",
          "authorId": "15563174"
        },
        {
          "name": "C. L. P. Chen",
          "authorId": "2152857598"
        },
        {
          "name": "Xia Wu",
          "authorId": "2154604753"
        },
        {
          "name": "T. Zhang",
          "authorId": "38144094"
        }
      ],
      "year": 2024,
      "abstract": "EEG emotion recognition plays a significant role in various mental health services. Deep learning-based methods perform excellently, but still suffer from interpretability. Although methods such as Gradient-weighted Class Activation Mapping(Grad-CAM) can cope with the above problem, their coarse granularity cannot accurately reveal the mechanism to promote emotional intelligence. In this paper, fine-grained interpretability is proposed, called Concat-aided Grad-CAM. Specifically, the multi-level feature mapping before the fully connected layer is concatenated to obtain the gradients of the target concept so that the discriminant information can be directly located in the high-precision area. Unlike coarse-grained interpretability methods applied in EEG emotion recognition, it can accurately highlight the EEG channels related to emotion rather than an obscure area. In addition, a systematic brain functional network is proposed to reveal the relationship between those channels and to further improve emotion recognition performance. The channels with greater contributions are connected, and those connections are learned by dynamic graph convolutional networks, while the others are independent to eliminate interference. Experiments on four EEG emotion recognition datasets manifest that Concat-aided Grad-CAM can be interpreted by the fine-grained. In addition, it has been shown that the learned brain functional network can improve the performance of the baselines. Significantly, the experiment results achieve state-of-the-art performance in multiple experiments.",
      "citationCount": 46,
      "doi": "10.1109/TAFFC.2023.3288885",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/c93fa2c9b37b5c46cadfda1bf3c3b7fe7fb0e560",
      "venue": "IEEE Transactions on Affective Computing",
      "journal": {
        "name": "IEEE Transactions on Affective Computing",
        "pages": "671-684",
        "volume": "15"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f3658afcd181e4078e1e96ff86eac224fd92faab",
      "title": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability",
      "authors": [
        {
          "name": "ZhongXiang Sun",
          "authorId": "2109820280"
        },
        {
          "name": "Xiaoxue Zang",
          "authorId": "2055666765"
        },
        {
          "name": "Kai Zheng",
          "authorId": "2293395261"
        },
        {
          "name": "Yang Song",
          "authorId": "2293392741"
        },
        {
          "name": "Jun Xu",
          "authorId": "2293399145"
        },
        {
          "name": "Xiao Zhang",
          "authorId": "2293646334"
        },
        {
          "name": "Weijie Yu",
          "authorId": "2118684861"
        },
        {
          "name": "Han Li",
          "authorId": "2326496399"
        }
      ],
      "year": 2024,
      "abstract": "Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) utilize external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when the Knowledge FFNs in LLMs overemphasize parametric knowledge in the residual stream, while Copying Heads fail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we propose ReDeEP, a novel method that detects hallucinations by decoupling LLM's utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads.",
      "citationCount": 54,
      "doi": "10.48550/arXiv.2410.11414",
      "arxivId": "2410.11414",
      "url": "https://www.semanticscholar.org/paper/f3658afcd181e4078e1e96ff86eac224fd92faab",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2410.11414"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5d7860e7e3ee853b6b55f1449db21f517f0b7298",
      "title": "Uncertainty Quantification and Interpretability for Clinical Trial Approval Prediction",
      "authors": [
        {
          "name": "Yingzhou Lu",
          "authorId": "2276324714"
        },
        {
          "name": "Tianyi Chen",
          "authorId": "2278853538"
        },
        {
          "name": "Nan Hao",
          "authorId": "2278427512"
        },
        {
          "name": "Capucine Van Rechem",
          "authorId": "48704734"
        },
        {
          "name": "Jintai Chen",
          "authorId": "2292561029"
        },
        {
          "name": "Tianfan Fu",
          "authorId": "2292583857"
        }
      ],
      "year": 2024,
      "abstract": "Background: Clinical trial is a crucial step in the development of a new therapy (e.g., medication) and is remarkably expensive and time-consuming. Forecasting the approval of clinical trials accurately would enable us to circumvent trials destined to fail, thereby allowing us to allocate more resources to therapies with better chances. However, existing approval prediction algorithms did not quantify the uncertainty and provide interpretability, limiting their usage in real-world clinical trial management. Methods: This paper quantifies uncertainty and improves interpretability in clinical trial approval predictions. We devised a selective classification approach and integrated it with the Hierarchical Interaction Network, the state-of-the-art clinical trial prediction model. Selective classification, encompassing a spectrum of methods for uncertainty quantification, empowers the model to withhold decision-making in the face of samples marked by ambiguity or low confidence. This approach not only amplifies the accuracy of predictions for the instances it chooses to classify but also notably enhances the model\u2019s interpretability. Results: Comprehensive experiments demonstrate that incorporating uncertainty markedly enhances the model\u2019s performance. Specifically, the proposed method achieved 32.37%, 21.43%, and 13.27% relative improvement on area under the precision\u2013recall curve over the base model (Hierarchical Interaction Network) in phase I, II, and III trial approval predictions, respectively. For phase III trials, our method reaches 0.9022 area under the precision\u2013recall curve scores. In addition, we show a case study of interpretability that helps domain experts to understand model\u2019s outcome. The code is publicly available at https://github.com/Vincent-1125/Uncertainty-Quantification-on-Clinical-Trial-Outcome-Prediction. Conclusion: Our approach not only measures model uncertainty but also greatly improves interpretability and performance for clinical trial approval prediction.",
      "citationCount": 43,
      "doi": "10.34133/hds.0126",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/5d7860e7e3ee853b6b55f1449db21f517f0b7298",
      "venue": "Health Data Science",
      "journal": {
        "name": "Health Data Science",
        "volume": "4"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 20,
  "errors": []
}
