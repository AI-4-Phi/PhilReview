[s2_search.py] Searching Semantic Scholar: 'mechanistic explanation neural networks' (year=2020-2025) (field=Philosophy), limit=30
[s2_search.py] Retrieved 30/30 papers...
[s2_search.py] Cached results (cache key: s2_e51b7ad17ddee9e3)
{
  "status": "success",
  "source": "semantic_scholar",
  "query": "mechanistic explanation neural networks",
  "results": [
    {
      "paperId": "862cebfcb07ae56b00bba93c7294b048e040a897",
      "title": "Common Pitfalls When Explaining AI and Why Mechanistic Explanation Is a Hard Problem",
      "authors": [
        {
          "name": "Daniel C. Elton",
          "authorId": "31838510"
        }
      ],
      "year": 2021,
      "abstract": null,
      "citationCount": 2,
      "doi": "10.1007/978-981-16-2377-6_38",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/862cebfcb07ae56b00bba93c7294b048e040a897",
      "venue": "International Congress on Information and Communication Technology",
      "journal": {
        "pages": "401-408"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4d70f8267c6795d78c29511658c0663cf78e0bff",
      "title": "AI3SD Video: Neural Networks and Explanatory Opacity",
      "authors": [
        {
          "name": "W. McNeill",
          "authorId": "2075786970"
        }
      ],
      "year": 2020,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.5258/SOTON/P0049",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/4d70f8267c6795d78c29511658c0663cf78e0bff",
      "venue": "",
      "journal": {
        "name": "",
        "volume": ""
      },
      "publicationTypes": null
    },
    {
      "paperId": "fb05ae3cadeb7c7b47af80a845eda6297511fe3d",
      "title": "A Mechanistic Explanatory Strategy for XAI",
      "authors": [
        {
          "name": "Marcin Rabiza",
          "authorId": "1471443532"
        }
      ],
      "year": 2024,
      "abstract": "Despite significant advancements in XAI, scholars continue to note a persistent lack of robust conceptual foundations and integration with broader discourse on scientific explanation. In response, emerging XAI research increasingly draws on explanatory strategies from various scientific disciplines and the philosophy of science to address these gaps. This paper outlines a mechanistic strategy for explaining the functional organization of deep learning systems, situating recent developments in AI explainability within a broader philosophical context. According to the mechanistic approach, explaining opaque AI systems involves identifying the mechanisms underlying decision-making processes. For deep neural networks, this means discerning functionally relevant components - such as neurons, layers, circuits, or activation patterns - and understanding their roles through decomposition, localization, and recomposition. Proof-of-principle case studies from image recognition and language modeling align this theoretical framework with recent research from OpenAI and Anthropic. The findings suggest that pursuing mechanistic explanations can uncover elements that traditional explainability techniques may overlook, ultimately contributing to more thoroughly explainable AI.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2411.01332",
      "arxivId": "2411.01332",
      "url": "https://www.semanticscholar.org/paper/fb05ae3cadeb7c7b47af80a845eda6297511fe3d",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2411.01332"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1739e02696ce26be71590f24f46967814df70a2c",
      "title": "Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?",
      "authors": [
        {
          "name": "Maxime M'eloux",
          "authorId": "2348097882"
        },
        {
          "name": "Silviu Maniu",
          "authorId": "2590886"
        },
        {
          "name": "Franccois Portet",
          "authorId": "2126059340"
        },
        {
          "name": "Maxime Peyrard",
          "authorId": "2348097920"
        }
      ],
      "year": 2025,
      "abstract": "As AI systems are used in high-stakes applications, ensuring interpretability is crucial. Mechanistic Interpretability (MI) aims to reverse-engineer neural networks by extracting human-understandable algorithms to explain their behavior. This work examines a key question: for a given behavior, and under MI's criteria, does a unique explanation exist? Drawing on identifiability in statistics, where parameters are uniquely inferred under specific assumptions, we explore the identifiability of MI explanations. We identify two main MI strategies: (1)\"where-then-what,\"which isolates a circuit replicating model behavior before interpreting it, and (2)\"what-then-where,\"which starts with candidate algorithms and searches for neural activation subspaces implementing them, using causal alignment. We test both strategies on Boolean functions and small multi-layer perceptrons, fully enumerating candidate explanations. Our experiments reveal systematic non-identifiability: multiple circuits can replicate behavior, a circuit can have multiple interpretations, several algorithms can align with the network, and one algorithm can align with different subspaces. Is uniqueness necessary? A pragmatic approach may require only predictive and manipulability standards. If uniqueness is essential for understanding, stricter criteria may be needed. We also reference the inner interpretability framework, which validates explanations through multiple criteria. This work contributes to defining explanation standards in AI.",
      "citationCount": 10,
      "doi": "10.48550/arXiv.2502.20914",
      "arxivId": "2502.20914",
      "url": "https://www.semanticscholar.org/paper/1739e02696ce26be71590f24f46967814df70a2c",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.20914"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e7596e53f54d9a90be8b960771b1265013b2e864",
      "title": "A Mathematical Philosophy of Explanations in Mechanistic Interpretability - The Strange Science Part I.i",
      "authors": [
        {
          "name": "Kola Ayonrinde",
          "authorId": "2325947690"
        },
        {
          "name": "Louis Jaburi",
          "authorId": "2358997901"
        }
      ],
      "year": 2025,
      "abstract": "Mechanistic Interpretability aims to understand neural net-\nworks through causal explanations. We argue for the\nExplanatory View Hypothesis: that Mechanistic\nInterpretability re- search is a principled approach to\nunderstanding models be- cause neural networks contain\nimplicit explanations which can be extracted and\nunderstood. We hence show that Explanatory Faithfulness, an\nassessment of how well an explanation fits a model, is\nwell-defined. We propose a definition of Mechanistic\nInterpretability (MI) as the practice of producing\nModel-level, Ontic, Causal-Mechanistic, and Falsifiable\nexplanations of neural networks, allowing us to distinguish\nMI from other interpretability paradigms and detail MI\u2019s\ninherent limits. We formulate the Principle of Explanatory\nOptimism, a conjecture which we argue is a necessary\nprecondition for the success of Mechanistic\nInterpretability.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2505.00808",
      "arxivId": "2505.00808",
      "url": "https://www.semanticscholar.org/paper/e7596e53f54d9a90be8b960771b1265013b2e864",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.00808"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "1dd76d9e4ba70a18477a69b63c89eab1e4d29f52",
      "title": "Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability - The Strange Science Part I.ii",
      "authors": [
        {
          "name": "Kola Ayonrinde",
          "authorId": "2325947690"
        },
        {
          "name": "Louis Jaburi",
          "authorId": "2358997901"
        }
      ],
      "year": 2025,
      "abstract": "Mechanistic Interpretability (MI) aims to understand neural networks through causal explanations. Though MI has many explanation-generating methods, progress has been limited by the lack of a universal approach to evaluating explanations. Here we analyse the fundamental question\"What makes a good explanation?\"We introduce a pluralist Explanatory Virtues Framework drawing on four perspectives from the Philosophy of Science - the Bayesian, Kuhnian, Deutschian, and Nomological - to systematically evaluate and improve explanations in MI. We find that Compact Proofs consider many explanatory virtues and are hence a promising approach. Fruitful research directions implied by our framework include (1) clearly defining explanatory simplicity, (2) focusing on unifying explanations and (3) deriving universal principles for neural networks. Improved MI methods enhance our ability to monitor, predict, and steer AI systems.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2505.01372",
      "arxivId": "2505.01372",
      "url": "https://www.semanticscholar.org/paper/1dd76d9e4ba70a18477a69b63c89eab1e4d29f52",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.01372"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "cc701d4e6230b042d7862fcf7815066b52f8d48c",
      "title": "Exploring, expounding & ersatzing: a three-level account of deep learning models in cognitive neuroscience",
      "authors": [
        {
          "name": "Vanja Suboti\u0107",
          "authorId": "2291796499"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/s11229-024-04514-1",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/cc701d4e6230b042d7862fcf7815066b52f8d48c",
      "venue": "Synthese",
      "journal": {
        "name": "Synthese",
        "volume": "203"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "a18b0cf58fd8c4fbcac9585e80499b7b1bc72a47",
      "title": "Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks",
      "authors": [
        {
          "name": "Bianka Kowalska",
          "authorId": "2328185649"
        },
        {
          "name": "Halina Kwa'snicka",
          "authorId": "2394075368"
        }
      ],
      "year": 2025,
      "abstract": "The black box nature of deep neural networks poses a significant challenge for the deployment of transparent and trustworthy artificial intelligence (AI) systems. With the growing presence of AI in society, it becomes increasingly important to develop methods that can explain and interpret the decisions made by these systems. To address this, mechanistic interpretability (MI) emerged as a promising and distinctive research program within the broader field of explainable artificial intelligence (XAI). MI is the process of studying the inner computations of neural networks and translating them into human-understandable algorithms. It encompasses reverse engineering techniques aimed at uncovering the computational algorithms implemented by neural networks. In this article, we propose a unified taxonomy of MI approaches and provide a detailed analysis of key techniques, illustrated with concrete examples and pseudo-code. We contextualize MI within the broader interpretability landscape, comparing its goals, methods, and insights to other strands of XAI. Additionally, we trace the development of MI as a research area, highlighting its conceptual roots and the accelerating pace of recent work. We argue that MI holds significant potential to support a more scientific understanding of machine learning systems -- treating models not only as tools for solving tasks, but also as systems to be studied and understood. We hope to invite new researchers into the field of mechanistic interpretability.",
      "citationCount": 0,
      "doi": null,
      "arxivId": "2511.19265",
      "url": "https://www.semanticscholar.org/paper/a18b0cf58fd8c4fbcac9585e80499b7b1bc72a47",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "a90707501b366d620f1a40f7da76a6842c7d62a2",
      "title": "SVEBI: Towards the Interpretation and Explanation of Spiking Neural Networks",
      "authors": [
        {
          "name": "Jasper De Laet",
          "authorId": "2385149856"
        },
        {
          "name": "Hamed Behzadi-Khormouji",
          "authorId": "2240120073"
        },
        {
          "name": "Lucas Deckers",
          "authorId": "2385149457"
        },
        {
          "name": "Jos\u00e9 Oramas",
          "authorId": "2240122736"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/978-3-032-06066-2_27",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/a90707501b366d620f1a40f7da76a6842c7d62a2",
      "venue": "ECML/PKDD",
      "journal": {
        "pages": "460-477"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8bfff75b0fc6a8daa78995e768ebff9311ec36ca",
      "title": "Semantics and explanation: why counterfactual explanations produce adversarial examples in deep neural networks",
      "authors": [
        {
          "name": "Kieran Browne",
          "authorId": "40977251"
        },
        {
          "name": "Ben Swift",
          "authorId": "1666548132"
        }
      ],
      "year": 2020,
      "abstract": "Recent papers in explainable AI have made a compelling case for counterfactual modes of explanation. While counterfactual explanations appear to be extremely effective in some instances, they are formally equivalent to adversarial examples. This presents an apparent paradox for explainability researchers: if these two procedures are formally equivalent, what accounts for the explanatory divide apparent between counterfactual explanations and adversarial examples? We resolve this paradox by placing emphasis back on the semantics of counterfactual expressions. Producing satisfactory explanations for deep learning systems will require that we find ways to interpret the semantics of hidden layer representations in deep neural networks.",
      "citationCount": 32,
      "doi": null,
      "arxivId": "2012.10076",
      "url": "https://www.semanticscholar.org/paper/8bfff75b0fc6a8daa78995e768ebff9311ec36ca",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2012.10076"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2c709ef6186bd607494a3344c903552ea500e449",
      "title": "Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks",
      "authors": [
        {
          "name": "Tilman Raukur",
          "authorId": "2179318557"
        },
        {
          "name": "A. Ho",
          "authorId": "120892153"
        },
        {
          "name": "Stephen Casper",
          "authorId": "2103487700"
        },
        {
          "name": "Dylan Hadfield-Menell",
          "authorId": "1397904824"
        }
      ],
      "year": 2022,
      "abstract": "The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, \u201cinner\u201d interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications.",
      "citationCount": 165,
      "doi": "10.1109/SaTML54575.2023.00039",
      "arxivId": "2207.13243",
      "url": "https://www.semanticscholar.org/paper/2c709ef6186bd607494a3344c903552ea500e449",
      "venue": "2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "journal": {
        "name": "2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
        "pages": "464-483"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "988914a393964adf63ce4e88b8375b12beb3b192",
      "title": "Interpreting Neural Networks through the Polytope Lens",
      "authors": [
        {
          "name": "Sid Black",
          "authorId": "2044098905"
        },
        {
          "name": "Lee D. Sharkey",
          "authorId": "2106414883"
        },
        {
          "name": "L\u00e9o Grinsztajn",
          "authorId": "2274940713"
        },
        {
          "name": "Eric Winsor",
          "authorId": "66044057"
        },
        {
          "name": "Daniel A. Braun",
          "authorId": "2354563"
        },
        {
          "name": "Jacob Merizian",
          "authorId": "2191689389"
        },
        {
          "name": "Kip Parker",
          "authorId": "35479769"
        },
        {
          "name": "Carlos Ram'on Guevara",
          "authorId": "2191690105"
        },
        {
          "name": "Beren Millidge",
          "authorId": "150045277"
        },
        {
          "name": "Gabriel Alfour",
          "authorId": "2381835088"
        },
        {
          "name": "Connor Leahy",
          "authorId": "2044198134"
        }
      ],
      "year": 2022,
      "abstract": "Mechanistic interpretability aims to explain what a neural network has learned at a nuts-and-bolts level. What are the fundamental primitives of neural network representations? Previous mechanistic descriptions have used individual neurons or their linear combinations to understand the representations a network has learned. But there are clues that neurons and their linear combinations are not the correct fundamental units of description: directions cannot describe how neural networks use nonlinearities to structure their representations. Moreover, many instances of individual neurons and their combinations are polysemantic (i.e. they have multiple unrelated meanings). Polysemanticity makes interpreting the network in terms of neurons or directions challenging since we can no longer assign a specific feature to a neural unit. In order to find a basic unit of description that does not suffer from these problems, we zoom in beyond just directions to study the way that piecewise linear activation functions (such as ReLU) partition the activation space into numerous discrete polytopes. We call this perspective the polytope lens. The polytope lens makes concrete predictions about the behavior of neural networks, which we evaluate through experiments on both convolutional image classifiers and language models. Specifically, we show that polytopes can be used to identify monosemantic regions of activation space (while directions are not in general monosemantic) and that the density of polytope boundaries reflect semantic boundaries. We also outline a vision for what mechanistic interpretability might look like through the polytope lens.",
      "citationCount": 33,
      "doi": "10.48550/arXiv.2211.12312",
      "arxivId": "2211.12312",
      "url": "https://www.semanticscholar.org/paper/988914a393964adf63ce4e88b8375b12beb3b192",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2211.12312"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7e03f90aeef01523cc3b20e2e6f81ef1f6c3e0d5",
      "title": "Foiling Explanations in Deep Neural Networks",
      "authors": [
        {
          "name": "Snir Vitrack Tamam",
          "authorId": "2192608999"
        },
        {
          "name": "Raz Lapid",
          "authorId": "2173526660"
        },
        {
          "name": "M. Sipper",
          "authorId": "1707878"
        }
      ],
      "year": 2022,
      "abstract": "Deep neural networks (DNNs) have greatly impacted numerous fields over the past decade. Yet despite exhibiting superb performance over many problems, their black-box nature still poses a significant challenge with respect to explainability. Indeed, explainable artificial intelligence (XAI) is crucial in several fields, wherein the answer alone -- sans a reasoning of how said answer was derived -- is of little value. This paper uncovers a troubling property of explanation methods for image-based DNNs: by making small visual changes to the input image -- hardly influencing the network's output -- we demonstrate how explanations may be arbitrarily manipulated through the use of evolution strategies. Our novel algorithm, AttaXAI, a model-agnostic, adversarial attack on XAI algorithms, only requires access to the output logits of a classifier and to the explanation map; these weak assumptions render our approach highly useful where real-world models and data are concerned. We compare our method's performance on two benchmark datasets -- CIFAR100 and ImageNet -- using four different pretrained deep-learning models: VGG16-CIFAR100, VGG16-ImageNet, MobileNet-CIFAR100, and Inception-v3-ImageNet. We find that the XAI methods can be manipulated without the use of gradients or other model internals. Our novel algorithm is successfully able to manipulate an image in a manner imperceptible to the human eye, such that the XAI method outputs a specific explanation map. To our knowledge, this is the first such method in a black-box setting, and we believe it has significant value where explainability is desired, required, or legally mandatory.",
      "citationCount": 19,
      "doi": "10.48550/arXiv.2211.14860",
      "arxivId": "2211.14860",
      "url": "https://www.semanticscholar.org/paper/7e03f90aeef01523cc3b20e2e6f81ef1f6c3e0d5",
      "venue": "Trans. Mach. Learn. Res.",
      "journal": {
        "name": "Trans. Mach. Learn. Res.",
        "volume": "2023"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "03fe39cf0c6b5bc7d2ebed02798ac8a56ab9df1c",
      "title": "Digging deeper with deep learning? Explanatory understanding and deep neural networks",
      "authors": [
        {
          "name": "Claus Beisbart",
          "authorId": "2287668804"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 2,
      "doi": "10.1007/s13194-025-00668-y",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/03fe39cf0c6b5bc7d2ebed02798ac8a56ab9df1c",
      "venue": "European Journal for Philosophy of Science",
      "journal": {
        "name": "European Journal for Philosophy of Science",
        "volume": "15"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "52566c8f8a6b81789a862389517acf101de9a6e3",
      "title": "Clarifying the Opacity of Neural Networks",
      "authors": [
        {
          "name": "T. Raleigh",
          "authorId": "2134692"
        },
        {
          "name": "Aleks Knoks",
          "authorId": "2227087"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 1,
      "doi": "10.1007/s11023-025-09745-w",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/52566c8f8a6b81789a862389517acf101de9a6e3",
      "venue": "Minds and Machines",
      "journal": {
        "name": "Minds and Machines",
        "volume": "35"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f368612cd1752b8671ef2c4dea0afa17e3a5e431",
      "title": "Explaining Neural Networks with Reasons",
      "authors": [
        {
          "name": "Levin Hornischer",
          "authorId": "83322233"
        },
        {
          "name": "Hannes Leitgeb",
          "authorId": "2362503166"
        }
      ],
      "year": 2025,
      "abstract": "We propose a new interpretability method for neural networks, which is based on a novel mathematico-philosophical theory of reasons. Our method computes a vector for each neuron, called its reasons vector. We then can compute how strongly this reasons vector speaks for various propositions, e.g., the proposition that the input image depicts digit 2 or that the input prompt has a negative sentiment. This yields an interpretation of neurons, and groups thereof, that combines a logical and a Bayesian perspective, and accounts for polysemanticity (i.e., that a single neuron can figure in multiple concepts). We show, both theoretically and empirically, that this method is: (1) grounded in a philosophically established notion of explanation, (2) uniform, i.e., applies to the common neural network architectures and modalities, (3) scalable, since computing reason vectors only involves forward-passes in the neural network, (4) faithful, i.e., intervening on a neuron based on its reason vector leads to expected changes in model output, (5) correct in that the model's reasons structure matches that of the data source, (6) trainable, i.e., neural networks can be trained to improve their reason strengths, (7) useful, i.e., it delivers on the needs for interpretability by increasing, e.g., robustness and fairness.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2505.14424",
      "arxivId": "2505.14424",
      "url": "https://www.semanticscholar.org/paper/f368612cd1752b8671ef2c4dea0afa17e3a5e431",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.14424"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e978f2f723aaa7772da7c03153cc89c847d18109",
      "title": "Explanatory Learning: Beyond Empiricism in Neural Networks",
      "authors": [
        {
          "name": "Antonio Norelli",
          "authorId": "1596822208"
        },
        {
          "name": "G. Mariani",
          "authorId": "49584066"
        },
        {
          "name": "Luca Moschella",
          "authorId": "23991573"
        },
        {
          "name": "Andrea Santilli",
          "authorId": "2065039862"
        },
        {
          "name": "Giambattista Parascandolo",
          "authorId": "50213542"
        },
        {
          "name": "S. Melzi",
          "authorId": "1972186"
        },
        {
          "name": "Emanuele Rodol\u00e0",
          "authorId": "2257170785"
        }
      ],
      "year": 2022,
      "abstract": "We introduce Explanatory Learning (EL), a framework to let machines use existing knowledge buried in symbolic sequences -- e.g. explanations written in hieroglyphic -- by autonomously learning to interpret them. In EL, the burden of interpreting symbols is not left to humans or rigid human-coded compilers, as done in Program Synthesis. Rather, EL calls for a learned interpreter, built upon a limited collection of symbolic sequences paired with observations of several phenomena. This interpreter can be used to make predictions on a novel phenomenon given its explanation, and even to find that explanation using only a handful of observations, like human scientists do. We formulate the EL problem as a simple binary classification task, so that common end-to-end approaches aligned with the dominant empiricist view of machine learning could, in principle, solve it. To these models, we oppose Critical Rationalist Networks (CRNs), which instead embrace a rationalist view on the acquisition of knowledge. CRNs express several desired properties by construction, they are truly explainable, can adjust their processing at test-time for harder inferences, and can offer strong confidence guarantees on their predictions. As a final contribution, we introduce Odeen, a basic EL environment that simulates a small flatland-style universe full of phenomena to explain. Using Odeen as a testbed, we show how CRNs outperform empiricist end-to-end approaches of similar size and architecture (Transformers) in discovering explanations for novel phenomena.",
      "citationCount": 3,
      "doi": null,
      "arxivId": "2201.10222",
      "url": "https://www.semanticscholar.org/paper/e978f2f723aaa7772da7c03153cc89c847d18109",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2201.10222"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "024572d9b2dcab6c4c00381fe60b0304e4490b16",
      "title": "Networks, dynamics and explanation",
      "authors": [
        {
          "name": "James Woodward",
          "authorId": "2359563223"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 2,
      "doi": "10.1007/s11229-025-05038-y",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/024572d9b2dcab6c4c00381fe60b0304e4490b16",
      "venue": "Synthese",
      "journal": {
        "name": "Synthese",
        "volume": "205"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "7eaf5a1f17611caa759ede235a8af7e001f07ffb",
      "title": "Deep Neural Networks, Explanations, and Rationality",
      "authors": [
        {
          "name": "Edward A. Lee",
          "authorId": "2278587625"
        }
      ],
      "year": 2023,
      "abstract": null,
      "citationCount": 2,
      "doi": "10.1007/978-3-031-46002-9_1",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/7eaf5a1f17611caa759ede235a8af7e001f07ffb",
      "venue": "AISoLA",
      "journal": {
        "pages": "11-21"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "fa7b539faa4bbe535690589d00a4e079b5032b88",
      "title": "Rethinking Feature Attribution for Neural Network Explanation",
      "authors": [
        {
          "name": "Ashkan Khakzar",
          "authorId": "16805480"
        }
      ],
      "year": 2023,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/fa7b539faa4bbe535690589d00a4e079b5032b88",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "15032e4ad2818097b1cbe9147422ab9a3489c1d1",
      "title": "Semantic Holism and Word Representations in Artificial Neural Networks",
      "authors": [
        {
          "name": "Tom\u00e1\u0161 Musil",
          "authorId": "1774026"
        }
      ],
      "year": 2020,
      "abstract": "Artificial neural networks are a state-of-the-art solution for many problems in natural language processing. What can we learn about language and meaning from the way artificial neural networks represent it? Word representations obtained from the Skip-gram variant of the word2vec model exhibit interesting semantic properties. This is usually explained by referring to the general distributional hypothesis, which states that the meaning of the word is given by the contexts where it occurs. We propose a more specific approach based on Frege's holistic and functional approach to meaning. Taking Tugendhat's formal reinterpretation of Frege's work as a starting point, we demonstrate that it is analogical to the process of training the Skip-gram model and offers a possible explanation of its semantic properties.",
      "citationCount": 1,
      "doi": null,
      "arxivId": "2003.05522",
      "url": "https://www.semanticscholar.org/paper/15032e4ad2818097b1cbe9147422ab9a3489c1d1",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2003.05522"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8a94d7fb8b580621979396042aef89dbd6ec37fb",
      "title": "Open Problems in Mechanistic Interpretability",
      "authors": [
        {
          "name": "Lee Sharkey",
          "authorId": "2267502247"
        },
        {
          "name": "Bilal Chughtai",
          "authorId": "2301155771"
        },
        {
          "name": "Joshua Batson",
          "authorId": "2342505933"
        },
        {
          "name": "Jack Lindsey",
          "authorId": "2342505989"
        },
        {
          "name": "Jeff Wu",
          "authorId": "2342640282"
        },
        {
          "name": "Lucius Bushnaq",
          "authorId": "2124877853"
        },
        {
          "name": "Nicholas Goldowsky-Dill",
          "authorId": "2302155854"
        },
        {
          "name": "Stefan Heimersheim",
          "authorId": "2256989665"
        },
        {
          "name": "Alejandro Ortega",
          "authorId": "2355649897"
        },
        {
          "name": "Joseph Bloom",
          "authorId": "2308099558"
        },
        {
          "name": "Stella Biderman",
          "authorId": "2273535088"
        },
        {
          "name": "Adri\u00e0 Garriga-Alonso",
          "authorId": "1388513000"
        },
        {
          "name": "Arthur Conmy",
          "authorId": "2131632310"
        },
        {
          "name": "Neel Nanda",
          "authorId": "2051128902"
        },
        {
          "name": "Jessica Rumbelow",
          "authorId": "2249532084"
        },
        {
          "name": "Martin Wattenberg",
          "authorId": "2237803620"
        },
        {
          "name": "Nandi Schoots",
          "authorId": "1485377354"
        },
        {
          "name": "Joseph Miller",
          "authorId": "2310773898"
        },
        {
          "name": "Eric J. Michaud",
          "authorId": "2293723716"
        },
        {
          "name": "Stephen Casper",
          "authorId": "2333442622"
        },
        {
          "name": "Max Tegmark",
          "authorId": "2256989384"
        },
        {
          "name": "William Saunders",
          "authorId": "2310699728"
        },
        {
          "name": "David Bau",
          "authorId": "2284682524"
        },
        {
          "name": "Eric Todd",
          "authorId": "145290788"
        },
        {
          "name": "Atticus Geiger",
          "authorId": "2315137132"
        },
        {
          "name": "Mor Geva",
          "authorId": "22245981"
        },
        {
          "name": "Jesse Hoogland",
          "authorId": "2282535402"
        },
        {
          "name": "Daniel Murfet",
          "authorId": "2257004100"
        },
        {
          "name": "Thomas McGrath",
          "authorId": "2256981829"
        }
      ],
      "year": 2025,
      "abstract": "Mechanistic interpretability aims to understand the computational mechanisms underlying neural networks' capabilities in order to accomplish concrete scientific and engineering goals. Progress in this field thus promises to provide greater assurance over AI system behavior and shed light on exciting scientific questions about the nature of intelligence. Despite recent progress toward these goals, there are many open problems in the field that require solutions before many scientific and practical benefits can be realized: Our methods require both conceptual and practical improvements to reveal deeper insights; we must figure out how best to apply our methods in pursuit of specific goals; and the field must grapple with socio-technical challenges that influence and are influenced by our work. This forward-facing review discusses the current frontier of mechanistic interpretability and the open problems that the field may benefit from prioritizing.",
      "citationCount": 79,
      "doi": "10.48550/arXiv.2501.16496",
      "arxivId": "2501.16496",
      "url": "https://www.semanticscholar.org/paper/8a94d7fb8b580621979396042aef89dbd6ec37fb",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2501.16496"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "25724bc6f86bf366df1b8fb30ca6d5baa37f8231",
      "title": "Causality and scientific explanation of artificial intelligence systems in biomedicine",
      "authors": [
        {
          "name": "F. Boge",
          "authorId": "88843364"
        },
        {
          "name": "Axel Mosig",
          "authorId": "2308337234"
        }
      ],
      "year": 2024,
      "abstract": "With rapid advances of deep neural networks over the past decade, artificial intelligence (AI) systems are now commonplace in many applications in biomedicine. These systems often achieve high predictive accuracy in clinical studies, and increasingly in clinical practice. Yet, despite their commonly high predictive accuracy, the trustworthiness of AI systems needs to be questioned when it comes to decision-making that affects the well-being of patients or the fairness towards patients or other stakeholders affected by AI-based decisions. To address this, the field of explainable artificial intelligence, or XAI for short, has emerged, seeking to provide means by which AI-based decisions can be explained to experts, users, or other stakeholders. While it is commonly claimed that explanations of artificial intelligence (AI) establish the trustworthiness of AI-based decisions, it remains unclear what traits of explanations cause them to foster trustworthiness. Building on historical cases of scientific explanation in medicine, we here propagate our perspective that, in order to foster trustworthiness, explanations in biomedical AI should meet the criteria of being scientific explanations. To further undermine our approach, we discuss its relation to the concepts of causality and randomized intervention. In our perspective, we combine aspects from the three disciplines of biomedicine, machine learning, and philosophy. From this interdisciplinary angle, we shed light on how the explanation and trustworthiness of artificial intelligence relate to the concepts of causality and robustness. To connect our perspective with AI research practice, we review recent cases of AI-based studies in pathology and, finally, provide guidelines on how to connect AI in biomedicine with scientific explanation.",
      "citationCount": 7,
      "doi": "10.1007/s00424-024-03033-9",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/25724bc6f86bf366df1b8fb30ca6d5baa37f8231",
      "venue": "Pfl\u00fcgers Archiv: European Journal of Physiology",
      "journal": {
        "name": "Pflugers Archiv",
        "pages": "543 - 554",
        "volume": "477"
      },
      "publicationTypes": [
        "Review",
        "JournalArticle"
      ]
    },
    {
      "paperId": "8aadeb990e9471f4bd8f63ae4b463d1a4ececf73",
      "title": "Explaining Deep Neural Networks",
      "authors": [
        {
          "name": "Oana-Maria Camburu",
          "authorId": "3317152"
        }
      ],
      "year": 2020,
      "abstract": "Deep neural networks are becoming more and more popular due to their revolutionary success in diverse areas, such as computer vision, natural language processing, and speech recognition. However, the decision-making processes of these models are generally not interpretable to users. In various domains, such as healthcare, finance, or law, it is critical to know the reasons behind a decision made by an artificial intelligence system. Therefore, several directions for explaining neural models have recently been explored. \nIn this thesis, I investigate two major directions for explaining deep neural networks. The first direction consists of feature-based post-hoc explanatory methods, that is, methods that aim to explain an already trained and fixed model (post-hoc), and that provide explanations in terms of input features, such as tokens for text and superpixels for images (feature-based). The second direction consists of self-explanatory neural models that generate natural language explanations, that is, models that have a built-in module that generates explanations for the predictions of the model.",
      "citationCount": 30,
      "doi": null,
      "arxivId": "2010.01496",
      "url": "https://www.semanticscholar.org/paper/8aadeb990e9471f4bd8f63ae4b463d1a4ececf73",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2010.01496"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a5cbe230af78780bd14c5472d8e089c16f832b28",
      "title": "Mechanistic Interpretability Needs Philosophy",
      "authors": [
        {
          "name": "Iwan Williams",
          "authorId": "2367743827"
        },
        {
          "name": "Ninell Oldenburg",
          "authorId": "2209989604"
        },
        {
          "name": "Ruchira Dhar",
          "authorId": "2311888638"
        },
        {
          "name": "Joshua Hatherley",
          "authorId": "2358040882"
        },
        {
          "name": "Constanza Fierro",
          "authorId": "50110151"
        },
        {
          "name": "Nina Rajcic",
          "authorId": "81045154"
        },
        {
          "name": "Sandrine R. Schiller",
          "authorId": "2370937188"
        },
        {
          "name": "Filippos Stamatiou",
          "authorId": "2306968038"
        },
        {
          "name": "Anders S\u00f8gaard",
          "authorId": "2281953769"
        }
      ],
      "year": 2025,
      "abstract": "Mechanistic interpretability (MI) aims to explain how neural networks work by uncovering their underlying causal mechanisms. As the field grows in influence, it is increasingly important to examine not just models themselves, but the assumptions, concepts and explanatory strategies implicit in MI research. We argue that mechanistic interpretability needs philosophy: not as an afterthought, but as an ongoing partner in clarifying its concepts, refining its methods, and assessing the epistemic and ethical stakes of interpreting AI systems. Taking three open problems from the MI literature as examples, this position paper illustrates the value philosophy can add to MI research, and outlines a path toward deeper interdisciplinary dialogue.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2506.18852",
      "arxivId": "2506.18852",
      "url": "https://www.semanticscholar.org/paper/a5cbe230af78780bd14c5472d8e089c16f832b28",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2506.18852"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "fd79cfabd50a773f23a67528ba0c1355f1336115",
      "title": "Argumentative Explanation for Deep Learning: A Survey",
      "authors": [
        {
          "name": "Yihang Guo",
          "authorId": "2267806348"
        },
        {
          "name": "Tianyuan Yu",
          "authorId": "145344058"
        },
        {
          "name": "Liang Bai",
          "authorId": "2267748909"
        },
        {
          "name": "Jun Tang",
          "authorId": "2170434698"
        },
        {
          "name": "Yirun Ruan",
          "authorId": "103841780"
        },
        {
          "name": "Yun Zhou",
          "authorId": "2267779006"
        }
      ],
      "year": 2023,
      "abstract": "Neural Networks (NNs) are often referred to as \u201cblack box\u201d models, which has sparked increasing interest among researchers in understanding their internal workings. Computational argumentation (CA), a subfield of symbolic Artificial Intelligence (AI), has demonstrated notable advantages in the field of Explainable Deep Learning (XDL) recently. This is primarily due to the inherent compatibility between the dialectical nature of argumentation and the key elements of interpretation. In this paper, we provide an introduction to several Argumentation Frameworks (AFs) that serve as important connections between NNs and explanation. Furthermore, we present a comprehensive overview of existing argumentation-based explanatory approaches, covering both ante-hoc explanation and post-hoc explanation. Finally, current issues and future directions that can be explored in depth are discussed, from both academic research and business application perspectives.",
      "citationCount": 10,
      "doi": "10.1109/ICUS58632.2023.10318322",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/fd79cfabd50a773f23a67528ba0c1355f1336115",
      "venue": "2023 IEEE International Conference on Unmanned Systems (ICUS)",
      "journal": {
        "name": "2023 IEEE International Conference on Unmanned Systems (ICUS)",
        "pages": "1738-1743"
      },
      "publicationTypes": [
        "Conference",
        "Review"
      ]
    },
    {
      "paperId": "2fa991ca2037033bf3d849e053315e21530244d2",
      "title": "How Causal Abstraction Underpins Computational Explanation",
      "authors": [
        {
          "name": "Atticus Geiger",
          "authorId": "80833908"
        },
        {
          "name": "Jacqueline Harding",
          "authorId": "2301806029"
        },
        {
          "name": "Thomas Icard",
          "authorId": "2251009433"
        }
      ],
      "year": 2025,
      "abstract": "Explanations of cognitive behavior often appeal to computations over representations. What does it take for a system to implement a given computation over suitable representational vehicles within that system? We argue that the language of causality -- and specifically the theory of causal abstraction -- provides a fruitful lens on this topic. Drawing on current discussions in deep learning with artificial neural networks, we illustrate how classical themes in the philosophy of computation and cognition resurface in contemporary machine learning. We offer an account of computational implementation grounded in causal abstraction, and examine the role for representation in the resulting picture. We argue that these issues are most profitably explored in connection with generalization and prediction.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2508.11214",
      "arxivId": "2508.11214",
      "url": "https://www.semanticscholar.org/paper/2fa991ca2037033bf3d849e053315e21530244d2",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.11214"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ca8fb797044d45a12f34f04e116a56b52ea6e5dc",
      "title": "Investigating AI's Challenges in Reasoning and Explanation from a Historical Perspective",
      "authors": [
        {
          "name": "Benji Alwis",
          "authorId": "2264337927"
        }
      ],
      "year": 2023,
      "abstract": "This paper provides an overview of the intricate relationship between social dynamics, technological advancements, and pioneering figures in the fields of cybernetics and artificial intelligence. It explores the impact of collaboration and interpersonal relationships among key scientists, such as McCulloch, Wiener, Pitts, and Rosenblatt, on the development of cybernetics and neural networks. It also discusses the contested attribution of credit for important innovations like the backpropagation algorithm and the potential consequences of unresolved debates within emerging scientific domains. It emphasizes how interpretive flexibility, public perception, and the influence of prominent figures can shape the trajectory of a new field. It highlights the role of funding, media attention, and alliances in determining the success and recognition of various research approaches. Additionally, it points out the missed opportunities for collaboration and integration between symbolic AI and neural network researchers, suggesting that a more unified approach may be possible in today's era without the historical baggage of past debates.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2311.10097",
      "arxivId": "2311.10097",
      "url": "https://www.semanticscholar.org/paper/ca8fb797044d45a12f34f04e116a56b52ea6e5dc",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2311.10097"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "c323ad559359e7d2f0fe458858df958df0e6e5d7",
      "title": "A Critical Perspective on Neural Mechanisms in Cognitive Neuroscience: Towards Unification",
      "authors": [
        {
          "name": "Sander van Bree",
          "authorId": "1484656796"
        }
      ],
      "year": 2023,
      "abstract": "A central pursuit of cognitive neuroscience is to find neural mechanisms of cognition, with research programs favoring different strategies to look for them. But what is a neural mechanism, and how do we know we have captured them? Here I answer these questions through a framework that integrates Marr\u2019s levels with philosophical work on mechanism. From this, the following goal emerges: What needs to be explained are the computations of cognition, with explanation itself given by mechanism\u2014composed of algorithms and parts of the brain that realize them. This reveals a delineation within cognitive neuroscience research. In the premechanism stage, the computations of cognition are linked to phenomena in the brain, narrowing down where and when mechanisms are situated in space and time. In the mechanism stage, it is established how computation emerges from organized interactions between parts\u2014filling the premechanistic mold. I explain why a shift toward mechanistic modeling helps us meet our aims while outlining a road map for doing so. Finally, I argue that the explanatory scope of neural mechanisms can be approximated by effect sizes collected across studies, not just conceptual analysis. Together, these points synthesize a mechanistic agenda that allows subfields to connect at the level of theory.",
      "citationCount": 8,
      "doi": "10.1177/17456916231191744",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/c323ad559359e7d2f0fe458858df958df0e6e5d7",
      "venue": "Perspectives on Psychological Science",
      "journal": {
        "name": "Perspectives on Psychological Science",
        "pages": "993 - 1010",
        "volume": "19"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "47661b6dda20bd81d9e60ae900dc14b8e20721d7",
      "title": "Mechanistic decomposition and reduction in complex, context-sensitive systems",
      "authors": [
        {
          "name": "Daniel C. Burnston",
          "authorId": "3379398"
        }
      ],
      "year": 2022,
      "abstract": "Standard arguments in philosophy of science infer from the complexity of biological and neural systems to the presence of emergence and failure of mechanistic/reductionist explanation for those systems. I argue against this kind of argument, specifically focusing on the notion of context-sensitivity. Context-sensitivity is standardly taken to be incompatible with reductionistic explanation, because it shows that larger-scale factors influence the functioning of lower-level parts. I argue that this argument can be overcome if there are mechanisms underlying those context-specific reorganizations. I argue that such mechanisms are frequently discovered in neuroscience.",
      "citationCount": 2,
      "doi": "10.3389/fpsyg.2022.992347",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/47661b6dda20bd81d9e60ae900dc14b8e20721d7",
      "venue": "Frontiers in Psychology",
      "journal": {
        "name": "Frontiers in Psychology",
        "volume": "13"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 30,
  "errors": []
}
