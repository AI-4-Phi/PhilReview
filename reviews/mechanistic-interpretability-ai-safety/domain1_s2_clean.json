{
  "status": "success",
  "source": "semantic_scholar",
  "query": "mechanistic interpretability",
  "results": [
    {
      "paperId": "f680d47a51a0e470fcb228bf0110c026535ead1b",
      "title": "Progress measures for grokking via mechanistic interpretability",
      "authors": [
        {
          "name": "Neel Nanda",
          "authorId": "2051128902"
        },
        {
          "name": "Lawrence Chan",
          "authorId": "2072836382"
        },
        {
          "name": "Tom Lieberum",
          "authorId": "2162470507"
        },
        {
          "name": "Jess Smith",
          "authorId": "2200391337"
        },
        {
          "name": "J. Steinhardt",
          "authorId": "5164568"
        }
      ],
      "year": 2023,
      "abstract": "Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \\textit{progress measures} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.",
      "citationCount": 607,
      "doi": "10.48550/arXiv.2301.05217",
      "arxivId": "2301.05217",
      "url": "https://www.semanticscholar.org/paper/f680d47a51a0e470fcb228bf0110c026535ead1b",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2301.05217"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "eefbd8b384a58f464827b19e30a6920ba976def9",
      "title": "Towards Automated Circuit Discovery for Mechanistic Interpretability",
      "authors": [
        {
          "name": "Arthur Conmy",
          "authorId": "2131632310"
        },
        {
          "name": "Augustine N. Mavor-Parker",
          "authorId": "2000605969"
        },
        {
          "name": "Aengus Lynch",
          "authorId": "2174176979"
        },
        {
          "name": "Stefan Heimersheim",
          "authorId": "2256989665"
        },
        {
          "name": "Adri\u00e0 Garriga-Alonso",
          "authorId": "1388513000"
        }
      ],
      "year": 2023,
      "abstract": "Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery.",
      "citationCount": 432,
      "doi": "10.48550/arXiv.2304.14997",
      "arxivId": "2304.14997",
      "url": "https://www.semanticscholar.org/paper/eefbd8b384a58f464827b19e30a6920ba976def9",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2304.14997"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8b750488d139f9beba0815ff8f46ebe15ebb3e58",
      "title": "Mechanistic Interpretability for AI Safety - A Review",
      "authors": [
        {
          "name": "Leonard Bereska",
          "authorId": "87881370"
        },
        {
          "name": "E. Gavves",
          "authorId": "2304222"
        }
      ],
      "year": 2024,
      "abstract": "Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We examine benefits in understanding, control, alignment, and risks such as capability gains and dual-use concerns. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.",
      "citationCount": 286,
      "doi": "10.48550/arXiv.2404.14082",
      "arxivId": "2404.14082",
      "url": "https://www.semanticscholar.org/paper/8b750488d139f9beba0815ff8f46ebe15ebb3e58",
      "venue": "Trans. Mach. Learn. Res.",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2404.14082"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "2ac231b9cff4f5f9054d86c9b540429d4dd687f4",
      "title": "A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models",
      "authors": [
        {
          "name": "Daking Rai",
          "authorId": "2203429265"
        },
        {
          "name": "Yilun Zhou",
          "authorId": "2309554954"
        },
        {
          "name": "Shi Feng",
          "authorId": "2309897784"
        },
        {
          "name": "Abulhair Saparov",
          "authorId": "2407368"
        },
        {
          "name": "Ziyu Yao",
          "authorId": "2307416803"
        }
      ],
      "year": 2024,
      "abstract": "Mechanistic interpretability (MI) is an emerging sub-field of interpretability that seeks to understand a neural network model by reverse-engineering its internal computations. Recently, MI has garnered significant attention for interpreting transformer-based language models (LMs), resulting in many novel insights yet introducing new challenges. However, there has not been work that comprehensively reviews these insights and challenges, particularly as a guide for newcomers to this field. To fill this gap, we provide a comprehensive survey from a task-centric perspective, organizing the taxonomy of MI research around specific research questions or tasks. We outline the fundamental objects of study in MI, along with the techniques, evaluation methods, and key findings for each task in the taxonomy. In particular, we present a task-centric taxonomy as a roadmap for beginners to navigate the field by helping them quickly identify impactful problems in which they are most interested and leverage MI for their benefit. Finally, we discuss the current gaps in the field and suggest potential future directions for MI research.",
      "citationCount": 79,
      "doi": "10.48550/arXiv.2407.02646",
      "arxivId": "2407.02646",
      "url": "https://www.semanticscholar.org/paper/2ac231b9cff4f5f9054d86c9b540429d4dd687f4",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2407.02646"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "f3658afcd181e4078e1e96ff86eac224fd92faab",
      "title": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability",
      "authors": [
        {
          "name": "ZhongXiang Sun",
          "authorId": "2109820280"
        },
        {
          "name": "Xiaoxue Zang",
          "authorId": "2055666765"
        },
        {
          "name": "Kai Zheng",
          "authorId": "2293395261"
        },
        {
          "name": "Yang Song",
          "authorId": "2293392741"
        },
        {
          "name": "Jun Xu",
          "authorId": "2293399145"
        },
        {
          "name": "Xiao Zhang",
          "authorId": "2293646334"
        },
        {
          "name": "Weijie Yu",
          "authorId": "2118684861"
        },
        {
          "name": "Han Li",
          "authorId": "2326496399"
        }
      ],
      "year": 2024,
      "abstract": "Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) utilize external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when the Knowledge FFNs in LLMs overemphasize parametric knowledge in the residual stream, while Copying Heads fail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we propose ReDeEP, a novel method that detects hallucinations by decoupling LLM's utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads.",
      "citationCount": 54,
      "doi": "10.48550/arXiv.2410.11414",
      "arxivId": "2410.11414",
      "url": "https://www.semanticscholar.org/paper/f3658afcd181e4078e1e96ff86eac224fd92faab",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2410.11414"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c76144130347dc9be9b2b02bbea157714d84391a",
      "title": "Explaining AI through mechanistic interpretability",
      "authors": [
        {
          "name": "Lena K\u00e4stner",
          "authorId": "2275362082"
        },
        {
          "name": "Barnaby Crook",
          "authorId": "2224885546"
        }
      ],
      "year": 2024,
      "abstract": "Recent work in explainable artificial intelligence (XAI) attempts to render opaque AI systems understandable through a divide-and-conquer strategy. However, this fails to illuminate how trained AI systems work as a whole. Precisely this kind of functional understanding is needed, though, to satisfy important societal desiderata such as safety. To remedy this situation, we argue, AI researchers should seek mechanistic interpretability, viz. apply coordinated discovery strategies familiar from the life sciences to uncover the functional organisation of complex AI systems. Additionally, theorists should accommodate for the unique costs and benefits of such strategies in their portrayals of XAI research.",
      "citationCount": 23,
      "doi": "10.1007/s13194-024-00614-4",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/c76144130347dc9be9b2b02bbea157714d84391a",
      "venue": "European Journal for Philosophy of Science",
      "journal": {
        "name": "European Journal for Philosophy of Science",
        "volume": "14"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1301ed763095097ff424c668e16a265b3ae2f231",
      "title": "Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT",
      "authors": [
        {
          "name": "Zhengfu He",
          "authorId": "2166121084"
        },
        {
          "name": "Xuyang Ge",
          "authorId": "2284692027"
        },
        {
          "name": "Qiong Tang",
          "authorId": "2284823489"
        },
        {
          "name": "Tianxiang Sun",
          "authorId": "153345698"
        },
        {
          "name": "Qinyuan Cheng",
          "authorId": "1834133"
        },
        {
          "name": "Xipeng Qiu",
          "authorId": "2256661882"
        }
      ],
      "year": 2024,
      "abstract": "Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations. We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features? We propose a circuit discovery framework alternative to activation patching. Our framework suffers less from out-of-distribution and proves to be more efficient in terms of asymptotic complexity. The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output. Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors. We dig in a small transformer trained on a synthetic task named Othello and find a number of human-understandable fine-grained circuits inside of it.",
      "citationCount": 25,
      "doi": "10.48550/arXiv.2402.12201",
      "arxivId": "2402.12201",
      "url": "https://www.semanticscholar.org/paper/1301ed763095097ff424c668e16a265b3ae2f231",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.12201"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "618f0284465a8f2b4e979f0213227b7c51816565",
      "title": "Opening the AI black box: program synthesis via mechanistic interpretability",
      "authors": [
        {
          "name": "Eric J. Michaud",
          "authorId": "2064378938"
        },
        {
          "name": "Isaac Liao",
          "authorId": "2270855448"
        },
        {
          "name": "Vedang Lad",
          "authorId": "2129579565"
        },
        {
          "name": "Ziming Liu",
          "authorId": "2145253202"
        },
        {
          "name": "Anish Mudide",
          "authorId": "1782268436"
        },
        {
          "name": "Chloe Loughridge",
          "authorId": "2283934175"
        },
        {
          "name": "Zifan Carl Guo",
          "authorId": "2280284759"
        },
        {
          "name": "Tara Rezaei Kheirkhah",
          "authorId": "2280139725"
        },
        {
          "name": "Mateja Vukeli'c",
          "authorId": "2283132209"
        },
        {
          "name": "Max Tegmark",
          "authorId": "2253461463"
        }
      ],
      "year": 2024,
      "abstract": "We present MIPS, a novel method for program synthesis based on automated mechanistic interpretability of neural networks trained to perform the desired task, auto-distilling the learned algorithm into Python code. We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to convert the RNN into a finite state machine, then applies Boolean or integer symbolic regression to capture the learned algorithm. As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub. We discuss opportunities and challenges for scaling up this approach to make machine-learned models more interpretable and trustworthy.",
      "citationCount": 16,
      "doi": "10.48550/arXiv.2402.05110",
      "arxivId": "2402.05110",
      "url": "https://www.semanticscholar.org/paper/618f0284465a8f2b4e979f0213227b7c51816565",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.05110"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1ac2d6f203b3bdd3503357b0839c24a485d6ce1d",
      "title": "Bilinear MLPs enable weight-based mechanistic interpretability",
      "authors": [
        {
          "name": "Michael T. Pearce",
          "authorId": "2304953338"
        },
        {
          "name": "Thomas Dooms",
          "authorId": "2268760637"
        },
        {
          "name": "Alice Rigg",
          "authorId": "2304951205"
        },
        {
          "name": "Jos\u00e9 Oramas",
          "authorId": "2240122736"
        },
        {
          "name": "Lee Sharkey",
          "authorId": "2325726487"
        }
      ],
      "year": 2024,
      "abstract": "A mechanistic understanding of how MLPs do computation in deep neural networks remains elusive. Current interpretability work can extract features from hidden activations over an input dataset but generally cannot explain how MLP weights construct features. One challenge is that element-wise nonlinearities introduce higher-order interactions and make it difficult to trace computations through the MLP layer. In this paper, we analyze bilinear MLPs, a type of Gated Linear Unit (GLU) without any element-wise nonlinearity that nevertheless achieves competitive performance. Bilinear MLPs can be fully expressed in terms of linear operations using a third-order tensor, allowing flexible analysis of the weights. Analyzing the spectra of bilinear MLP weights using eigendecomposition reveals interpretable low-rank structure across toy tasks, image classification, and language modeling. We use this understanding to craft adversarial examples, uncover overfitting, and identify small language model circuits directly from the weights alone. Our results demonstrate that bilinear layers serve as an interpretable drop-in replacement for current activation functions and that weight-based interpretability is viable for understanding deep-learning models.",
      "citationCount": 15,
      "doi": "10.48550/arXiv.2410.08417",
      "arxivId": "2410.08417",
      "url": "https://www.semanticscholar.org/paper/1ac2d6f203b3bdd3503357b0839c24a485d6ce1d",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2410.08417"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1b59fe8d168f6b7c762ade018041ac09f438eeee",
      "title": "What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Noise-free Text-Image Corruption and Evaluation",
      "authors": [
        {
          "name": "Michal Golovanevsky",
          "authorId": "2171105220"
        },
        {
          "name": "William Rudman",
          "authorId": "2166313068"
        },
        {
          "name": "Vedant Palit",
          "authorId": "2216605916"
        },
        {
          "name": "Ritambhara Singh",
          "authorId": "2268844528"
        },
        {
          "name": "Carsten Eickhoff",
          "authorId": "2262215315"
        }
      ],
      "year": 2024,
      "abstract": "Vision-Language Models (VLMs) have gained community-spanning prominence due to their ability to integrate visual and textual inputs to perform complex tasks. Despite their success, the internal decision-making processes of these models remain opaque, posing challenges in high-stakes applications. To address this, we introduce NOTICE, the first Noise-free Text-Image Corruption and Evaluation pipeline for mechanistic interpretability in VLMs. NOTICE incorporates a Semantic Minimal Pairs (SMP) framework for image corruption and Symmetric Token Replacement (STR) for text. This approach enables semantically meaningful causal mediation analysis for both modalities, providing a robust method for analyzing multimodal integration within models like BLIP. Our experiments on the SVO-Probes, MIT-States, and Facial Expression Recognition datasets reveal crucial insights into VLM decision-making, identifying the significant role of middle-layer cross-attention heads. Further, we uncover a set of ``universal cross-attention heads'' that consistently contribute across tasks and modalities, each performing distinct functions such as implicit image segmentation, object inhibition, and outlier inhibition. This work paves the way for more transparent and interpretable multimodal systems.",
      "citationCount": 10,
      "doi": "10.48550/arXiv.2406.16320",
      "arxivId": "2406.16320",
      "url": "https://www.semanticscholar.org/paper/1b59fe8d168f6b7c762ade018041ac09f438eeee",
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.16320"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "cfbdf67fc11977637d4cb13ed7e1abce75623796",
      "title": "Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models",
      "authors": [
        {
          "name": "Tianyi Men",
          "authorId": "2165227300"
        },
        {
          "name": "Pengfei Cao",
          "authorId": "49776272"
        },
        {
          "name": "Zhuoran Jin",
          "authorId": "2152843772"
        },
        {
          "name": "Yubo Chen",
          "authorId": "1763402"
        },
        {
          "name": "Kang Liu",
          "authorId": "77397868"
        },
        {
          "name": "Jun Zhao",
          "authorId": "2269147239"
        }
      ],
      "year": 2024,
      "abstract": "Planning, as the core module of agents, is crucial in various fields such as embodied agents, web navigation, and tool using. With the development of large language models (LLMs), some researchers treat large language models as intelligent agents to stimulate and evaluate their planning capabilities. However, the planning mechanism is still unclear. In this work, we focus on exploring the look-ahead planning mechanism in large language models from the perspectives of information flow and internal representations. First, we study how planning is done internally by analyzing the multi-layer perception (MLP) and multi-head self-attention (MHSA) components at the last token. We find that the output of MHSA in the middle layers at the last token can directly decode the decision to some extent. Based on this discovery, we further trace the source of MHSA by information flow, and we reveal that MHSA extracts information from spans of the goal states and recent steps. According to information flow, we continue to study what information is encoded within it. Specifically, we explore whether future decisions have been considered in advance in the representation of flow. We demonstrate that the middle and upper layers encode a few short-term future decisions. Overall, our research analyzes the look-ahead planning mechanisms of LLMs, facilitating future research on LLMs performing planning tasks.",
      "citationCount": 15,
      "doi": "10.48550/arXiv.2406.16033",
      "arxivId": "2406.16033",
      "url": "https://www.semanticscholar.org/paper/cfbdf67fc11977637d4cb13ed7e1abce75623796",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.16033"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "6247d7bb9093b4f6c222c6c224b3df4335d4b8bd",
      "title": "Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability",
      "authors": [
        {
          "name": "Atticus Geiger",
          "authorId": "80833908"
        },
        {
          "name": "D. Ibeling",
          "authorId": "41047991"
        },
        {
          "name": "Amir Zur",
          "authorId": "2186302293"
        },
        {
          "name": "Maheep Chaudhary",
          "authorId": "2310329680"
        },
        {
          "name": "Sonakshi Chauhan",
          "authorId": "2310332823"
        },
        {
          "name": "Jing Huang",
          "authorId": "2145739230"
        },
        {
          "name": "Aryaman Arora",
          "authorId": "1575802390"
        },
        {
          "name": "Zhengxuan Wu",
          "authorId": "47039337"
        },
        {
          "name": "Noah D. Goodman",
          "authorId": "2280334415"
        },
        {
          "name": "Christopher Potts",
          "authorId": "2280333621"
        },
        {
          "name": "Thomas F. Icard",
          "authorId": "8938047"
        }
      ],
      "year": 2023,
      "abstract": "Causal abstraction provides a theoretical foundation for mechanistic interpretability, the field concerned with providing intelligible algorithms that are faithful simplifications of the known, but opaque low-level details of black box AI models. Our contributions are (1) generalizing the theory of causal abstraction from mechanism replacement (i.e., hard and soft interventions) to arbitrary mechanism transformation (i.e., functionals from old mechanisms to new mechanisms), (2) providing a flexible, yet precise formalization for the core concepts of polysemantic neurons, the linear representation hypothesis, modular features, and graded faithfulness, and (3) unifying a variety of mechanistic interpretability methods in the common language of causal abstraction, namely, activation and path patching, causal mediation analysis, causal scrubbing, causal tracing, circuit analysis, concept erasure, sparse autoencoders, differential binary masking, distributed alignment search, and steering.",
      "citationCount": 100,
      "doi": null,
      "arxivId": "2301.04709",
      "url": "https://www.semanticscholar.org/paper/6247d7bb9093b4f6c222c6c224b3df4335d4b8bd",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "015dd4ca002c14b83e0e53bce83b7eacea9f5a28",
      "title": "How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via Mechanistic Interpretability",
      "authors": [
        {
          "name": "Jorge Garc'ia-Carrasco",
          "authorId": "2300175267"
        },
        {
          "name": "Alejandro Mat'e",
          "authorId": "2300176736"
        },
        {
          "name": "Juan Trujillo",
          "authorId": "2300176271"
        }
      ],
      "year": 2024,
      "abstract": "Transformer-based language models are treated as black-boxes because of their large number of parameters and complex internal interactions, which is a serious safety concern. Mechanistic Interpretability (MI) intends to reverse-engineer neural network behaviors in terms of human-understandable components. In this work, we focus on understanding how GPT-2 Small performs the task of predicting three-letter acronyms. Previous works in the MI field have focused so far on tasks that predict a single token. To the best of our knowledge, this is the first work that tries to mechanistically understand a behavior involving the prediction of multiple consecutive tokens. We discover that the prediction is performed by a circuit composed of 8 attention heads (~5% of the total heads) which we classified in three groups according to their role. We also demonstrate that these heads concentrate the acronym prediction functionality. In addition, we mechanistically interpret the most relevant heads of the circuit and find out that they use positional information which is propagated via the causal mask mechanism. We expect this work to lay the foundation for understanding more complex behaviors involving multiple-token predictions.",
      "citationCount": 12,
      "doi": "10.48550/arXiv.2405.04156",
      "arxivId": "2405.04156",
      "url": "https://www.semanticscholar.org/paper/015dd4ca002c14b83e0e53bce83b7eacea9f5a28",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2405.04156"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "c1bc03a045ea830894fe3b1799928c9f8c14923c",
      "title": "Using Degeneracy in the Loss Landscape for Mechanistic Interpretability",
      "authors": [
        {
          "name": "Lucius Bushnaq",
          "authorId": "2124877853"
        },
        {
          "name": "Jake Mendel",
          "authorId": "2302154727"
        },
        {
          "name": "Stefan Heimersheim",
          "authorId": "2256989665"
        },
        {
          "name": "Dan Braun",
          "authorId": "2267505173"
        },
        {
          "name": "Nicholas Goldowsky-Dill",
          "authorId": "2302155854"
        },
        {
          "name": "Kaarel H\u00e4nni",
          "authorId": "2305837065"
        },
        {
          "name": "Cindy Wu",
          "authorId": "2302293490"
        },
        {
          "name": "Marius Hobbhahn",
          "authorId": "2267508917"
        }
      ],
      "year": 2024,
      "abstract": "Mechanistic Interpretability aims to reverse engineer the algorithms implemented by neural networks by studying their weights and activations. An obstacle to reverse engineering neural networks is that many of the parameters inside a network are not involved in the computation being implemented by the network. These degenerate parameters may obfuscate internal structure. Singular learning theory teaches us that neural network parameterizations are biased towards being more degenerate, and parameterizations with more degeneracy are likely to generalize further. We identify 3 ways that network parameters can be degenerate: linear dependence between activations in a layer; linear dependence between gradients passed back to a layer; ReLUs which fire on the same subset of datapoints. We also present a heuristic argument that modular networks are likely to be more degenerate, and we develop a metric for identifying modules in a network that is based on this argument. We propose that if we can represent a neural network in a way that is invariant to reparameterizations that exploit the degeneracies, then this representation is likely to be more interpretable, and we provide some evidence that such a representation is likely to have sparser interactions. We introduce the Interaction Basis, a tractable technique to obtain a representation that is invariant to degeneracies from linear dependence of activations or Jacobians.",
      "citationCount": 10,
      "doi": "10.48550/arXiv.2405.10927",
      "arxivId": "2405.10927",
      "url": "https://www.semanticscholar.org/paper/c1bc03a045ea830894fe3b1799928c9f8c14923c",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2405.10927"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f8029060e91209f048b3f9882f2cdd3607785ccd",
      "title": "Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability",
      "authors": [
        {
          "name": "Ziming Liu",
          "authorId": "2145253202"
        },
        {
          "name": "Eric Gan",
          "authorId": "2214450654"
        },
        {
          "name": "Max Tegmark",
          "authorId": "2011933"
        }
      ],
      "year": 2023,
      "abstract": "We introduce Brain-Inspired Modular Training (BIMT), a method for making neural networks more modular and interpretable. Inspired by brains, BIMT embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that BIMT discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.",
      "citationCount": 50,
      "doi": "10.48550/arXiv.2305.08746",
      "arxivId": "2305.08746",
      "url": "https://www.semanticscholar.org/paper/f8029060e91209f048b3f9882f2cdd3607785ccd",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2305.08746"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "d494727306a375e524c4c4c8cc1a2dc1845cc4b7",
      "title": "Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP",
      "authors": [
        {
          "name": "Vedant Palit",
          "authorId": "2216605916"
        },
        {
          "name": "Rohan Pandey",
          "authorId": "1471734043"
        },
        {
          "name": "Aryaman Arora",
          "authorId": "1575802390"
        },
        {
          "name": "Paul Pu Liang",
          "authorId": "28130078"
        }
      ],
      "year": 2023,
      "abstract": "Mechanistic interpretability seeks to understand the neural mechanisms that enable specific behaviors in Large Language Models (LLMs) by leveraging causality-based methods. While these approaches have identified neural circuits that copy spans of text, capture factual knowledge, and more, they remain unusable for multimodal models since adapting these tools to the vision-language domain requires considerable architectural changes. In this work, we adapt a unimodal causal tracing tool to BLIP to enable the study of the neural mechanisms underlying image-conditioned text generation. We demonstrate our approach on a visual question answering dataset, highlighting the causal relevance of later layer representations for all tokens. Furthermore, we release our BLIP causal tracing tool as open source to enable further experimentation in vision-language mechanistic interpretability by the community. Our code is available at this URL.",
      "citationCount": 43,
      "doi": "10.1109/ICCVW60793.2023.00307",
      "arxivId": "2308.14179",
      "url": "https://www.semanticscholar.org/paper/d494727306a375e524c4c4c8cc1a2dc1845cc4b7",
      "venue": "2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "journal": {
        "name": "2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
        "pages": "2848-2853"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "8487f133fed81e18caf17ecb0d2917a84d5fd218",
      "title": "Compact Proofs of Model Performance via Mechanistic Interpretability",
      "authors": [
        {
          "name": "Jason Gross",
          "authorId": "2325161397"
        },
        {
          "name": "Rajashree Agrawal",
          "authorId": "2118310256"
        },
        {
          "name": "Thomas Kwa",
          "authorId": "2306997977"
        },
        {
          "name": "Euan Ong",
          "authorId": "2307002509"
        },
        {
          "name": "Chun Hei Yip",
          "authorId": "2306998621"
        },
        {
          "name": "Alex Gibson",
          "authorId": "2306999222"
        },
        {
          "name": "Soufiane Noubir",
          "authorId": "2306999288"
        },
        {
          "name": "Lawrence Chan",
          "authorId": "2307002182"
        }
      ],
      "year": 2024,
      "abstract": "We propose using mechanistic interpretability -- techniques for reverse engineering model weights into human-interpretable algorithms -- to derive and compactly prove formal guarantees on model performance. We prototype this approach by formally proving accuracy lower bounds for a small transformer trained on Max-of-K, validating proof transferability across 151 random seeds and four values of K. We create 102 different computer-assisted proof strategies and assess their length and tightness of bound on each of our models. Using quantitative metrics, we find that shorter proofs seem to require and provide more mechanistic understanding. Moreover, we find that more faithful mechanistic understanding leads to tighter performance bounds. We confirm these connections by qualitatively examining a subset of our proofs. Finally, we identify compounding structureless errors as a key challenge for using mechanistic interpretability to generate compact proofs on model performance.",
      "citationCount": 9,
      "doi": "10.48550/arXiv.2406.11779",
      "arxivId": "2406.11779",
      "url": "https://www.semanticscholar.org/paper/8487f133fed81e18caf17ecb0d2917a84d5fd218",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.11779"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ec13c45db0a60e4916fa0a9b8d029f1d03715963",
      "title": "Understanding Multimodal LLMs: the Mechanistic Interpretability of Llava in Visual Question Answering",
      "authors": [
        {
          "name": "Zeping Yu",
          "authorId": "2263692531"
        },
        {
          "name": "Sophia Ananiadou",
          "authorId": "2240623492"
        }
      ],
      "year": 2024,
      "abstract": "Understanding the mechanisms behind Large Language Models (LLMs) is crucial for designing improved models and strategies. While recent studies have yielded valuable insights into the mechanisms of textual LLMs, the mechanisms of Multi-modal Large Language Models (MLLMs) remain underexplored. In this paper, we apply mechanistic interpretability methods to analyze the visual question answering (VQA) mechanisms in the first MLLM, Llava. We compare the mechanisms between VQA and textual QA (TQA) in color answering tasks and find that: a) VQA exhibits a mechanism similar to the in-context learning mechanism observed in TQA; b) the visual features exhibit significant interpretability when projecting the visual embeddings into the embedding space; and c) Llava enhances the existing capabilities of the corresponding textual LLM Vicuna during visual instruction tuning. Based on these findings, we develop an interpretability tool to help users and researchers identify important visual locations for final predictions, aiding in the understanding of visual hallucination. Our method demonstrates faster and more effective results compared to existing interpretability approaches. Code: \\url{https://github.com/zepingyu0512/llava-mechanism}",
      "citationCount": 8,
      "doi": "10.48550/arXiv.2411.10950",
      "arxivId": "2411.10950",
      "url": "https://www.semanticscholar.org/paper/ec13c45db0a60e4916fa0a9b8d029f1d03715963",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2411.10950"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "5d2aab8675a07074348260cac2b9795f8c86a610",
      "title": "An introduction to graphical tensor notation for mechanistic interpretability",
      "authors": [
        {
          "name": "Jordan K. Taylor",
          "authorId": "2228186426"
        }
      ],
      "year": 2024,
      "abstract": "Graphical tensor notation is a simple way of denoting linear operations on tensors, originating from physics. Modern deep learning consists almost entirely of operations on or between tensors, so easily understanding tensor operations is quite important for understanding these systems. This is especially true when attempting to reverse-engineer the algorithms learned by a neural network in order to understand its behavior: a field known as mechanistic interpretability. It's often easy to get confused about which operations are happening between tensors and lose sight of the overall structure, but graphical tensor notation makes it easier to parse things at a glance and see interesting equivalences. The first half of this document introduces the notation and applies it to some decompositions (SVD, CP, Tucker, and tensor network decompositions), while the second half applies it to some existing some foundational approaches for mechanistically understanding language models, loosely following ``A Mathematical Framework for Transformer Circuits'', then constructing an example ``induction head'' circuit in graphical tensor notation.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2402.01790",
      "arxivId": "2402.01790",
      "url": "https://www.semanticscholar.org/paper/5d2aab8675a07074348260cac2b9795f8c86a610",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.01790"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a9ceeafaa5cffc975fb5aa3d591666fb0ebc47d8",
      "title": "InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques",
      "authors": [
        {
          "name": "Rohan Gupta",
          "authorId": "2312274312"
        },
        {
          "name": "Iv'an Arcuschin",
          "authorId": "2349542533"
        },
        {
          "name": "Thomas Kwa",
          "authorId": "2312201820"
        },
        {
          "name": "Adri\u00e0 Garriga-Alonso",
          "authorId": "1388513000"
        }
      ],
      "year": 2024,
      "abstract": "Mechanistic interpretability methods aim to identify the algorithm a neural network implements, but it is difficult to validate such methods when the true algorithm is unknown. This work presents InterpBench, a collection of semi-synthetic yet realistic transformers with known circuits for evaluating these techniques. We train simple neural networks using a stricter version of Interchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like the original, SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model's output. We evaluate SIIT on sparse transformers produced by the Tracr tool and find that SIIT models maintain Tracr's original circuit while being more realistic. SIIT can also train transformers with larger circuits, like Indirect Object Identification (IOI). Finally, we use our benchmark to evaluate existing circuit discovery techniques.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2407.14494",
      "arxivId": "2407.14494",
      "url": "https://www.semanticscholar.org/paper/a9ceeafaa5cffc975fb5aa3d591666fb0ebc47d8",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2407.14494"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ac2fa12190cb6de71fef60f04c1a499167bec938",
      "title": "Adaptive Circuit Behavior and Generalization in Mechanistic Interpretability",
      "authors": [
        {
          "name": "Jatin Nainani",
          "authorId": "2186115391"
        },
        {
          "name": "Sankaran Vaidyanathan",
          "authorId": "65731722"
        },
        {
          "name": "AJ Yeung",
          "authorId": "2332092167"
        },
        {
          "name": "Kartik Gupta",
          "authorId": "2332094931"
        },
        {
          "name": "David Jensen",
          "authorId": "2332090444"
        }
      ],
      "year": 2024,
      "abstract": "Mechanistic interpretability aims to understand the inner workings of large neural networks by identifying circuits, or minimal subgraphs within the model that implement algorithms responsible for performing specific tasks. These circuits are typically discovered and analyzed using a narrowly defined prompt format. However, given the abilities of large language models (LLMs) to generalize across various prompt formats for the same task, it remains unclear how well these circuits generalize. For instance, it is unclear whether the models generalization results from reusing the same circuit components, the components behaving differently, or the use of entirely different components. In this paper, we investigate the generality of the indirect object identification (IOI) circuit in GPT-2 small, which is well-studied and believed to implement a simple, interpretable algorithm. We evaluate its performance on prompt variants that challenge the assumptions of this algorithm. Our findings reveal that the circuit generalizes surprisingly well, reusing all of its components and mechanisms while only adding additional input edges. Notably, the circuit generalizes even to prompt variants where the original algorithm should fail; we discover a mechanism that explains this which we term S2 Hacking. Our findings indicate that circuits within LLMs may be more flexible and general than previously recognized, underscoring the importance of studying circuit generalization to better understand the broader capabilities of these models.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2411.16105",
      "arxivId": "2411.16105",
      "url": "https://www.semanticscholar.org/paper/ac2fa12190cb6de71fef60f04c1a499167bec938",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2411.16105"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "fa0cbfba4e41b9f2487df251fcc3b93c21381167",
      "title": "Detecting and Understanding Vulnerabilities in Language Models via Mechanistic Interpretability",
      "authors": [
        {
          "name": "Jorge Garc'ia-Carrasco",
          "authorId": "2300175267"
        },
        {
          "name": "A. Mat\u00e9",
          "authorId": "145782886"
        },
        {
          "name": "Juan Trujillo",
          "authorId": "2193508751"
        }
      ],
      "year": 2024,
      "abstract": "Large Language Models (LLMs), characterized by being trained on broad amounts of data in a self-supervised manner, have shown impressive performance across a wide range of tasks. Indeed, their generative abilities have aroused interest on the application of LLMs across a wide range of contexts. However, neural networks in general, and LLMs in particular, are known to be vulnerable to adversarial attacks, where an imperceptible change to the input can mislead the output of the model. This is a serious concern that impedes the use of LLMs on high-stakes applications, such as healthcare, where a wrong prediction can imply serious consequences. Even though there are many efforts on making LLMs more robust to adversarial attacks, there are almost no works that study how and where these vulnerabilities that make LLMs prone to adversarial attacks happen. Motivated by these facts, we explore how to localize and understand vulnerabilities, and propose a method, based on Mechanistic Interpretability (MI) techniques, to guide this process. Specifically, this method enables us to detect vulnerabilities related to a concrete task by (i) obtaining the subset of the model that is responsible for that task, (ii) generating adversarial samples for that task, and (iii) using MI techniques together with the previous samples to discover and understand the possible vulnerabilities. We showcase our method on a pretrained GPT-2 Small model carrying out the task of predicting 3-letter acronyms to demonstrate its effectiveness on locating and understanding concrete vulnerabilities of the model.",
      "citationCount": 5,
      "doi": "10.24963/ijcai.2024/43",
      "arxivId": "2407.19842",
      "url": "https://www.semanticscholar.org/paper/fa0cbfba4e41b9f2487df251fcc3b93c21381167",
      "venue": "International Joint Conference on Artificial Intelligence",
      "journal": {
        "pages": "385-393"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "a46876976b64f567041a9b94fc320c77cf6c4977",
      "title": "Mechanistic interpretability of large language models with applications to the financial services industry",
      "authors": [
        {
          "name": "Ashkan Golgoon",
          "authorId": "102502031"
        },
        {
          "name": "Khashayar Filom",
          "authorId": "93595237"
        },
        {
          "name": "Arjun Ravi Kannan",
          "authorId": "2007715324"
        }
      ],
      "year": 2024,
      "abstract": "Large Language Models exhibit remarkable capabilities across a broad spectrum of applications. Nevertheless, due to their intrinsic complexity, these models present substantial challenges in interpreting their internal decision-making processes. This lack of transparency poses critical challenges when it comes to their adaptation by financial institutions, where concerns and accountability regarding bias, fairness, and reliability are of paramount importance. Mechanistic interpretability aims at reverse engineering complex AI models such as transformers. In this paper, we are pioneering the use of mechanistic interpretability to shed some light on the inner workings of large language models for use in financial services applications. We offer several examples of how algorithmic tasks can be designed for compliance monitoring purposes. In particular, we investigate GPT-2 Small\u2019s attention pattern when prompted to identify potential violation of Fair Lending laws. Using direct logit attribution, we study the contributions of each layer and its corresponding attention heads to the logit difference in the residual stream. Finally, we design clean and corrupted prompts and use activation patching as a causal intervention method to localize our task completion components further. We observe that the (positive) heads 10.2 (head 2, layer 10), 10.7, and 11.3, as well as the (negative) heads 9.6 and 10.6 play a significant role in the task completion.",
      "citationCount": 5,
      "doi": "10.1145/3677052.3698612",
      "arxivId": "2407.11215",
      "url": "https://www.semanticscholar.org/paper/a46876976b64f567041a9b94fc320c77cf6c4977",
      "venue": "International Conference on AI in Finance",
      "journal": {
        "name": "Proceedings of the 5th ACM International Conference on AI in Finance"
      },
      "publicationTypes": [
        "JournalArticle",
        "Book"
      ]
    },
    {
      "paperId": "5c40aa8b6c611f85578e8465a252e72dbfa24459",
      "title": "Evaluating Brain-Inspired Modular Training in Automated Circuit Discovery for Mechanistic Interpretability",
      "authors": [
        {
          "name": "Jatin Nainani",
          "authorId": "2186115391"
        }
      ],
      "year": 2024,
      "abstract": "Large Language Models (LLMs) have experienced a rapid rise in AI, changing a wide range of applications with their advanced capabilities. As these models become increasingly integral to decision-making, the need for thorough interpretability has never been more critical. Mechanistic Interpretability offers a pathway to this understanding by identifying and analyzing specific sub-networks or 'circuits' within these complex systems. A crucial aspect of this approach is Automated Circuit Discovery, which facilitates the study of large models like GPT4 or LLAMA in a feasible manner. In this context, our research evaluates a recent method, Brain-Inspired Modular Training (BIMT), designed to enhance the interpretability of neural networks. We demonstrate how BIMT significantly improves the efficiency and quality of Automated Circuit Discovery, overcoming the limitations of manual methods. Our comparative analysis further reveals that BIMT outperforms existing models in terms of circuit quality, discovery time, and sparsity. Additionally, we provide a comprehensive computational analysis of BIMT, including aspects such as training duration, memory allocation requirements, and inference speed. This study advances the larger objective of creating trustworthy and transparent AI systems in addition to demonstrating how well BIMT works to make neural networks easier to understand.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2401.03646",
      "arxivId": "2401.03646",
      "url": "https://www.semanticscholar.org/paper/5c40aa8b6c611f85578e8465a252e72dbfa24459",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2401.03646"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "18384340ce10505f98924032f1f31777ba42a5b7",
      "title": "Decompose the model: Mechanistic interpretability in image models with Generalized Integrated Gradients (GIG)",
      "authors": [
        {
          "name": "Yearim Kim",
          "authorId": "2265651669"
        },
        {
          "name": "Sangyu Han",
          "authorId": "2283002198"
        },
        {
          "name": "Sangbum Han",
          "authorId": "2319385602"
        },
        {
          "name": "N. Kwak",
          "authorId": "101880623"
        }
      ],
      "year": 2024,
      "abstract": "In the field of eXplainable AI (XAI) in language models, the progression from local explanations of individual decisions to global explanations with high-level concepts has laid the groundwork for mechanistic interpretability, which aims to decode the exact operations. However, this paradigm has not been adequately explored in image models, where existing methods have primarily focused on class-specific interpretations. This paper introduces a novel approach to systematically trace the entire pathway from input through all intermediate layers to the final output within the whole dataset. We utilize Pointwise Feature Vectors (PFVs) and Effective Receptive Fields (ERFs) to decompose model embeddings into interpretable Concept Vectors. Then, we calculate the relevance between concept vectors with our Generalized Integrated Gradients (GIG), enabling a comprehensive, dataset-wide analysis of model behavior. We validate our method of concept extraction and concept attribution in both qualitative and quantitative evaluations. Our approach advances the understanding of semantic significance within image models, offering a holistic view of their operational mechanics.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2409.01610",
      "arxivId": "2409.01610",
      "url": "https://www.semanticscholar.org/paper/18384340ce10505f98924032f1f31777ba42a5b7",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2409.01610"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "9c3a3288cb54929c73d50513ff340893e87a3ca1",
      "title": "DILA: Dictionary Label Attention for Mechanistic Interpretability in High-dimensional Multi-label Medical Coding Prediction",
      "authors": [
        {
          "name": "John Wu",
          "authorId": "2321406192"
        },
        {
          "name": "David Wu",
          "authorId": "2321728269"
        },
        {
          "name": "Jimeng Sun",
          "authorId": "2321413549"
        }
      ],
      "year": 2024,
      "abstract": "Predicting high-dimensional or extreme multilabels, such as in medical coding, requires both accuracy and interpretability. Existing works often rely on local interpretability methods, failing to provide comprehensive explanations of the overall mechanism behind each label prediction within a multilabel set. We propose a mechanistic interpretability module called DIctionary Label Attention (\\method) that disentangles uninterpretable dense embeddings into a sparse embedding space, where each nonzero element (a dictionary feature) represents a globally learned medical concept. Through human evaluations, we show that our sparse embeddings are more human understandable than its dense counterparts by at least 50 percent. Our automated dictionary feature identification pipeline, leveraging large language models (LLMs), uncovers thousands of learned medical concepts by examining and summarizing the highest activating tokens for each dictionary feature. We represent the relationships between dictionary features and medical codes through a sparse interpretable matrix, enhancing the mechanistic and global understanding of the model's predictions while maintaining competitive performance and scalability without extensive human annotation.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2409.10504",
      "arxivId": "2409.10504",
      "url": "https://www.semanticscholar.org/paper/9c3a3288cb54929c73d50513ff340893e87a3ca1",
      "venue": "ML4H@NeurIPS",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2409.10504"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f4c2ee3d3b6a33c9bdaa9dbef51d067ba8c33610",
      "title": "The Quest for the Right Mediator: Surveying Mechanistic Interpretability for NLP Through the Lens of Causal Mediation Analysis",
      "authors": [
        {
          "name": "Aaron Mueller",
          "authorId": "2261670263"
        },
        {
          "name": "Jannik Brinkmann",
          "authorId": "2228224955"
        },
        {
          "name": "Millicent Li",
          "authorId": "2314738780"
        },
        {
          "name": "Samuel Marks",
          "authorId": "2225941937"
        },
        {
          "name": "Koyena Pal",
          "authorId": "2053245673"
        },
        {
          "name": "Nikhil Prakash",
          "authorId": "2284985448"
        },
        {
          "name": "Can Rager",
          "authorId": "2257034392"
        },
        {
          "name": "Aruna Sankaranarayanan",
          "authorId": "2314691448"
        },
        {
          "name": "Arnab Sen Sharma",
          "authorId": "1429844787"
        },
        {
          "name": "Jiuding Sun",
          "authorId": "2314741402"
        },
        {
          "name": "Eric Todd",
          "authorId": "145290788"
        },
        {
          "name": "David Bau",
          "authorId": "2284996653"
        },
        {
          "name": "Yonatan Belinkov",
          "authorId": "2346327043"
        }
      ],
      "year": 2024,
      "abstract": "\n Interpretability provides a toolset for understanding how and why language models behave in certain ways. However, there is little unity in the field: most studies employ ad-hoc evaluations and do not share theoretical foundations, making it difficult to measure progress and compare the pros and cons of different techniques. Furthermore, while mechanistic understanding is frequently discussed, the basic causal units underlying these mechanisms are often not explicitly defined. In this article, we propose a perspective on interpretability research grounded in causal mediation analysis. Specifically, we describe the history and current state of interpretability taxonomized according to the types of causal units (mediators) employed, as well as methods used to search over mediators. We discuss the pros and cons of each mediator, providing insights as to when particular kinds of mediators and search methods are most appropriate. We argue that this framing yields a more cohesive narrative of the field and helps researchers select appropriate methods based on their research objective. Our analysis yields actionable recommendations for future work, including the discovery of new mediators and the development of standardized evaluations tailored to these goals.",
      "citationCount": 2,
      "doi": "10.1162/coli.a.572",
      "arxivId": "2408.01416",
      "url": "https://www.semanticscholar.org/paper/f4c2ee3d3b6a33c9bdaa9dbef51d067ba8c33610",
      "venue": "Computational Linguistics",
      "journal": {
        "name": "Computational Linguistics"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "86f67f8d4ed808853657c8a5dba33408f46dc21b",
      "title": "Towards Mechanistic Interpretability for Autoencoder compression of EEG signals",
      "authors": [
        {
          "name": "Leon Hegedic",
          "authorId": "2326949427"
        },
        {
          "name": "Luka Hobor",
          "authorId": "2326950651"
        },
        {
          "name": "Nikola Maric",
          "authorId": "2326949784"
        },
        {
          "name": "Martin Ante Rogosic",
          "authorId": "2326950818"
        },
        {
          "name": "Mario Brcic",
          "authorId": "2326949986"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 0,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/86f67f8d4ed808853657c8a5dba33408f46dc21b",
      "venue": "xAI",
      "journal": {
        "pages": "97-104"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a800bac1609408eb955625b7ce0df234d48d3845",
      "title": "Mechanistic Interpretability of Reinforcement Learning Agents",
      "authors": [
        {
          "name": "Tristan Trim",
          "authorId": "2329105730"
        },
        {
          "name": "Triston Grayston",
          "authorId": "2329104095"
        }
      ],
      "year": 2024,
      "abstract": "This paper explores the mechanistic interpretability of reinforcement learning (RL) agents through an analysis of a neural network trained on procedural maze environments. By dissecting the network's inner workings, we identified fundamental features like maze walls and pathways, forming the basis of the model's decision-making process. A significant observation was the goal misgeneralization, where the RL agent developed biases towards certain navigation strategies, such as consistently moving towards the top right corner, even in the absence of explicit goals. Using techniques like saliency mapping and feature mapping, we visualized these biases. We furthered this exploration with the development of novel tools for interactively exploring layer activations.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2411.00867",
      "arxivId": "2411.00867",
      "url": "https://www.semanticscholar.org/paper/a800bac1609408eb955625b7ce0df234d48d3845",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2411.00867"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "49072764553763f1686121fd03e3dadda259f273",
      "title": "Mechanistic Interpretability of Binary and Ternary Transformers",
      "authors": [
        {
          "name": "Jason Li",
          "authorId": "2303612572"
        }
      ],
      "year": 2024,
      "abstract": "Recent research (arXiv:2310.11453, arXiv:2402.17764) has proposed binary and ternary transformer networks as a way to significantly reduce memory and improve inference speed in Large Language Models (LLMs) while maintaining accuracy. In this work, we apply techniques from mechanistic interpretability to investigate whether such networks learn distinctly different or similar algorithms when compared to full-precision transformer networks. In particular, we reverse engineer the algorithms learned for the toy problem of modular addition where we find that binary and ternary networks learn similar algorithms as full precision networks. This provides evidence against the possibility of using binary and ternary networks as a more interpretable alternative in the LLM setting.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2405.17703",
      "arxivId": "2405.17703",
      "url": "https://www.semanticscholar.org/paper/49072764553763f1686121fd03e3dadda259f273",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2405.17703"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2e4ad5efadaaa317d7f8f148e8c7d10fce97ba59",
      "title": "Scale Alone Does not Improve Mechanistic Interpretability in Vision Models",
      "authors": [
        {
          "name": "Roland S. Zimmermann",
          "authorId": "149408648"
        },
        {
          "name": "Thomas Klein",
          "authorId": "2222677269"
        },
        {
          "name": "Wieland Brendel",
          "authorId": "40634590"
        }
      ],
      "year": 2023,
      "abstract": "In light of the recent widespread adoption of AI systems, understanding the internal information processing of neural networks has become increasingly critical. Most recently, machine vision has seen remarkable progress by scaling neural networks to unprecedented levels in dataset and model size. We here ask whether this extraordinary increase in scale also positively impacts the field of mechanistic interpretability. In other words, has our understanding of the inner workings of scaled neural networks improved as well? We use a psychophysical paradigm to quantify one form of mechanistic interpretability for a diverse suite of nine models and find no scaling effect for interpretability - neither for model nor dataset size. Specifically, none of the investigated state-of-the-art models are easier to interpret than the GoogLeNet model from almost a decade ago. Latest-generation vision models appear even less interpretable than older architectures, hinting at a regression rather than improvement, with modern models sacrificing interpretability for accuracy. These results highlight the need for models explicitly designed to be mechanistically interpretable and the need for more helpful interpretability methods to increase our understanding of networks at an atomic level. We release a dataset containing more than 130'000 human responses from our psychophysical evaluation of 767 units across nine models. This dataset facilitates research on automated instead of human-based interpretability evaluations, which can ultimately be leveraged to directly optimize the mechanistic interpretability of models.",
      "citationCount": 22,
      "doi": "10.48550/arXiv.2307.05471",
      "arxivId": "2307.05471",
      "url": "https://www.semanticscholar.org/paper/2e4ad5efadaaa317d7f8f148e8c7d10fce97ba59",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2307.05471"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "9e15af193a232d5b9d93e1432e6af469c58917a9",
      "title": "Probing Ranking LLMs: Mechanistic Interpretability in Information Retrieval",
      "authors": [
        {
          "name": "Tanya Chowdhury",
          "authorId": "40976115"
        },
        {
          "name": "James Allan",
          "authorId": "2268672572"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 5,
      "doi": "10.48550/arXiv.2410.18527",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/9e15af193a232d5b9d93e1432e6af469c58917a9",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2410.18527"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "fd63195aab3753739db45ab4702a419da11f2531",
      "title": "Responsibility Attribution for AI-Mediated Damages with Mechanistic Interpretability",
      "authors": [
        {
          "name": "Lena K\u00e4stner",
          "authorId": "2386534053"
        },
        {
          "name": "Johann Cordes",
          "authorId": "2386534549"
        },
        {
          "name": "Herbert Zech",
          "authorId": "2386533515"
        }
      ],
      "year": 2024,
      "abstract": null,
      "citationCount": 0,
      "doi": "10.1007/978-3-032-01377-4_10",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/fd63195aab3753739db45ab4702a419da11f2531",
      "venue": "AISoLA",
      "journal": {
        "pages": "187-202"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8a94d7fb8b580621979396042aef89dbd6ec37fb",
      "title": "Open Problems in Mechanistic Interpretability",
      "authors": [
        {
          "name": "Lee Sharkey",
          "authorId": "2267502247"
        },
        {
          "name": "Bilal Chughtai",
          "authorId": "2301155771"
        },
        {
          "name": "Joshua Batson",
          "authorId": "2342505933"
        },
        {
          "name": "Jack Lindsey",
          "authorId": "2342505989"
        },
        {
          "name": "Jeff Wu",
          "authorId": "2342640282"
        },
        {
          "name": "Lucius Bushnaq",
          "authorId": "2124877853"
        },
        {
          "name": "Nicholas Goldowsky-Dill",
          "authorId": "2302155854"
        },
        {
          "name": "Stefan Heimersheim",
          "authorId": "2256989665"
        },
        {
          "name": "Alejandro Ortega",
          "authorId": "2355649897"
        },
        {
          "name": "Joseph Bloom",
          "authorId": "2308099558"
        },
        {
          "name": "Stella Biderman",
          "authorId": "2273535088"
        },
        {
          "name": "Adri\u00e0 Garriga-Alonso",
          "authorId": "1388513000"
        },
        {
          "name": "Arthur Conmy",
          "authorId": "2131632310"
        },
        {
          "name": "Neel Nanda",
          "authorId": "2051128902"
        },
        {
          "name": "Jessica Rumbelow",
          "authorId": "2249532084"
        },
        {
          "name": "Martin Wattenberg",
          "authorId": "2237803620"
        },
        {
          "name": "Nandi Schoots",
          "authorId": "1485377354"
        },
        {
          "name": "Joseph Miller",
          "authorId": "2310773898"
        },
        {
          "name": "Eric J. Michaud",
          "authorId": "2293723716"
        },
        {
          "name": "Stephen Casper",
          "authorId": "2333442622"
        },
        {
          "name": "Max Tegmark",
          "authorId": "2256989384"
        },
        {
          "name": "William Saunders",
          "authorId": "2310699728"
        },
        {
          "name": "David Bau",
          "authorId": "2284682524"
        },
        {
          "name": "Eric Todd",
          "authorId": "145290788"
        },
        {
          "name": "Atticus Geiger",
          "authorId": "2315137132"
        },
        {
          "name": "Mor Geva",
          "authorId": "22245981"
        },
        {
          "name": "Jesse Hoogland",
          "authorId": "2282535402"
        },
        {
          "name": "Daniel Murfet",
          "authorId": "2257004100"
        },
        {
          "name": "Thomas McGrath",
          "authorId": "2256981829"
        }
      ],
      "year": 2025,
      "abstract": "Mechanistic interpretability aims to understand the computational mechanisms underlying neural networks' capabilities in order to accomplish concrete scientific and engineering goals. Progress in this field thus promises to provide greater assurance over AI system behavior and shed light on exciting scientific questions about the nature of intelligence. Despite recent progress toward these goals, there are many open problems in the field that require solutions before many scientific and practical benefits can be realized: Our methods require both conceptual and practical improvements to reveal deeper insights; we must figure out how best to apply our methods in pursuit of specific goals; and the field must grapple with socio-technical challenges that influence and are influenced by our work. This forward-facing review discusses the current frontier of mechanistic interpretability and the open problems that the field may benefit from prioritizing.",
      "citationCount": 79,
      "doi": "10.48550/arXiv.2501.16496",
      "arxivId": "2501.16496",
      "url": "https://www.semanticscholar.org/paper/8a94d7fb8b580621979396042aef89dbd6ec37fb",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2501.16496"
      },
      "publicationTypes": [
        "JournalArticle",
        "Review"
      ]
    },
    {
      "paperId": "66583ad76bc1ce493ed3b530b9a56f87a7e684ca",
      "title": "MIB: A Mechanistic Interpretability Benchmark",
      "authors": [
        {
          "name": "Aaron Mueller",
          "authorId": "2334997907"
        },
        {
          "name": "Atticus Geiger",
          "authorId": "2315137132"
        },
        {
          "name": "Sarah Wiegreffe",
          "authorId": "35823986"
        },
        {
          "name": "Dana Arad",
          "authorId": "2290800163"
        },
        {
          "name": "Iv'an Arcuschin",
          "authorId": "2349542533"
        },
        {
          "name": "Adam Belfki",
          "authorId": "2312327636"
        },
        {
          "name": "Yik Siu Chan",
          "authorId": "2298459121"
        },
        {
          "name": "Jaden Fiotto-Kaufman",
          "authorId": "2129392987"
        },
        {
          "name": "Tal Haklay",
          "authorId": "2232604403"
        },
        {
          "name": "Michael Hanna",
          "authorId": "2140766524"
        },
        {
          "name": "Jing Huang",
          "authorId": "2145739230"
        },
        {
          "name": "Rohan Gupta",
          "authorId": "2312274312"
        },
        {
          "name": "Yaniv Nikankin",
          "authorId": "2191617821"
        },
        {
          "name": "Hadas Orgad",
          "authorId": "1398583303"
        },
        {
          "name": "Nikhil Prakash",
          "authorId": "2284985448"
        },
        {
          "name": "Anja Reusch",
          "authorId": "2328076715"
        },
        {
          "name": "Aruna Sankaranarayanan",
          "authorId": "2314691448"
        },
        {
          "name": "Shun Shao",
          "authorId": "2355892357"
        },
        {
          "name": "Alessandro Stolfo",
          "authorId": "2175480389"
        },
        {
          "name": "Martin Tutek",
          "authorId": "2367197291"
        },
        {
          "name": "Amir Zur",
          "authorId": "2186302293"
        },
        {
          "name": "David Bau",
          "authorId": "2284996653"
        },
        {
          "name": "Yonatan Belinkov",
          "authorId": "2346327043"
        }
      ],
      "year": 2025,
      "abstract": "How can we know whether new mechanistic interpretability methods achieve real improvements? In pursuit of lasting evaluation standards, we propose MIB, a Mechanistic Interpretability Benchmark, with two tracks spanning four tasks and five models. MIB favors methods that precisely and concisely recover relevant causal pathways or causal variables in neural language models. The circuit localization track compares methods that locate the model components - and connections between them - most important for performing a task (e.g., attribution patching or information flow routes). The causal variable localization track compares methods that featurize a hidden vector, e.g., sparse autoencoders (SAEs) or distributed alignment search (DAS), and align those features to a task-relevant causal variable. Using MIB, we find that attribution and mask optimization methods perform best on circuit localization. For causal variable localization, we find that the supervised DAS method performs best, while SAE features are not better than neurons, i.e., non-featurized hidden vectors. These findings illustrate that MIB enables meaningful comparisons, and increases our confidence that there has been real progress in the field.",
      "citationCount": 7,
      "doi": "10.48550/arXiv.2504.13151",
      "arxivId": "2504.13151",
      "url": "https://www.semanticscholar.org/paper/66583ad76bc1ce493ed3b530b9a56f87a7e684ca",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.13151"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "c1ceb29224145b1a7b4e7943f43c62f25a7a80cf",
      "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video",
      "authors": [
        {
          "name": "Sonia Joseph",
          "authorId": "2355082098"
        },
        {
          "name": "Praneet Suresh",
          "authorId": "2355309836"
        },
        {
          "name": "Lorenz Hufe",
          "authorId": "2203793946"
        },
        {
          "name": "Edward Stevinson",
          "authorId": "2330885919"
        },
        {
          "name": "Robert Graham",
          "authorId": "2355082065"
        },
        {
          "name": "Yash Vadi",
          "authorId": "2230656302"
        },
        {
          "name": "Danilo Bzdok",
          "authorId": "2355080520"
        },
        {
          "name": "S. Lapuschkin",
          "authorId": "3633358"
        },
        {
          "name": "Lee Sharkey",
          "authorId": "2357966383"
        },
        {
          "name": "Blake Richards",
          "authorId": "2311697786"
        }
      ],
      "year": 2025,
      "abstract": "Robust tooling and publicly available pre-trained models have helped drive recent advances in mechanistic interpretability for language models. However, similar progress in vision mechanistic interpretability has been hindered by the lack of accessible frameworks and pre-trained weights. We present Prisma (Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an open-source framework designed to accelerate vision mechanistic interpretability research, providing a unified toolkit for accessing 75+ vision and video transformers; support for sparse autoencoder (SAE), transcoder, and crosscoder training; a suite of 80+ pre-trained SAE weights; activation caching, circuit analysis tools, and visualization tools; and educational resources. Our analysis reveals surprising findings, including that effective vision SAEs can exhibit substantially lower sparsity patterns than language SAEs, and that in some instances, SAE reconstructions can decrease model loss. Prisma enables new research directions for understanding vision model internals while lowering barriers to entry in this emerging field.",
      "citationCount": 8,
      "doi": "10.48550/arXiv.2504.19475",
      "arxivId": "2504.19475",
      "url": "https://www.semanticscholar.org/paper/c1ceb29224145b1a7b4e7943f43c62f25a7a80cf",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2504.19475"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "1739e02696ce26be71590f24f46967814df70a2c",
      "title": "Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?",
      "authors": [
        {
          "name": "Maxime M'eloux",
          "authorId": "2348097882"
        },
        {
          "name": "Silviu Maniu",
          "authorId": "2590886"
        },
        {
          "name": "Franccois Portet",
          "authorId": "2126059340"
        },
        {
          "name": "Maxime Peyrard",
          "authorId": "2348097920"
        }
      ],
      "year": 2025,
      "abstract": "As AI systems are used in high-stakes applications, ensuring interpretability is crucial. Mechanistic Interpretability (MI) aims to reverse-engineer neural networks by extracting human-understandable algorithms to explain their behavior. This work examines a key question: for a given behavior, and under MI's criteria, does a unique explanation exist? Drawing on identifiability in statistics, where parameters are uniquely inferred under specific assumptions, we explore the identifiability of MI explanations. We identify two main MI strategies: (1)\"where-then-what,\"which isolates a circuit replicating model behavior before interpreting it, and (2)\"what-then-where,\"which starts with candidate algorithms and searches for neural activation subspaces implementing them, using causal alignment. We test both strategies on Boolean functions and small multi-layer perceptrons, fully enumerating candidate explanations. Our experiments reveal systematic non-identifiability: multiple circuits can replicate behavior, a circuit can have multiple interpretations, several algorithms can align with the network, and one algorithm can align with different subspaces. Is uniqueness necessary? A pragmatic approach may require only predictive and manipulability standards. If uniqueness is essential for understanding, stricter criteria may be needed. We also reference the inner interpretability framework, which validates explanations through multiple criteria. This work contributes to defining explanation standards in AI.",
      "citationCount": 10,
      "doi": "10.48550/arXiv.2502.20914",
      "arxivId": "2502.20914",
      "url": "https://www.semanticscholar.org/paper/1739e02696ce26be71590f24f46967814df70a2c",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.20914"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "f928871afe54a7e79442f7b5971bd6e38beb4d2d",
      "title": "Towards Mechanistic Interpretability of Graph Transformers via Attention Graphs",
      "authors": [
        {
          "name": "Batu El",
          "authorId": "2345928597"
        },
        {
          "name": "Deepro Choudhury",
          "authorId": "2345928778"
        },
        {
          "name": "Pietro Li\u00f3",
          "authorId": "2273680715"
        },
        {
          "name": "Chaitanya K. Joshi",
          "authorId": "38009979"
        }
      ],
      "year": 2025,
      "abstract": "We introduce Attention Graphs, a new tool for mechanistic interpretability of Graph Neural Networks (GNNs) and Graph Transformers based on the mathematical equivalence between message passing in GNNs and the self-attention mechanism in Transformers. Attention Graphs aggregate attention matrices across Transformer layers and heads to describe how information flows among input nodes. Through experiments on homophilous and heterophilous node classification tasks, we analyze Attention Graphs from a network science perspective and find that: (1) When Graph Transformers are allowed to learn the optimal graph structure using all-to-all attention among input nodes, the Attention Graphs learned by the model do not tend to correlate with the input/original graph structure; and (2) For heterophilous graphs, different Graph Transformer variants can achieve similar performance while utilising distinct information flow patterns. Open source code: https://github.com/batu-el/understanding-inductive-biases-of-gnns",
      "citationCount": 9,
      "doi": "10.48550/arXiv.2502.12352",
      "arxivId": "2502.12352",
      "url": "https://www.semanticscholar.org/paper/f928871afe54a7e79442f7b5971bd6e38beb4d2d",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.12352"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "52be61c3edaa54b568805db55a748ba6d8159587",
      "title": "Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability",
      "authors": [
        {
          "name": "Luca Baroni",
          "authorId": "2053252972"
        },
        {
          "name": "G. Khara",
          "authorId": "102476661"
        },
        {
          "name": "Joachim Schaeffer",
          "authorId": "2372256069"
        },
        {
          "name": "Marat Subkhankulov",
          "authorId": "2372531548"
        },
        {
          "name": "Stefan Heimersheim",
          "authorId": "2256989665"
        }
      ],
      "year": 2025,
      "abstract": "Layer-wise normalization (LN) is an essential component of virtually all transformer-based large language models. While its effects on training stability are well documented, its role at inference time is poorly understood. Additionally, LN layers hinder mechanistic interpretability by introducing additional nonlinearities and increasing the interconnectedness of individual model components. Here, we show that all LN layers can be removed from every GPT-2 model with only a small increase in validation loss (e.g. +0.03 cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in language modeling. We find that the amount of fine-tuning data needed for LN removal grows sublinearly with model parameters, suggesting scaling to larger models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face. Furthermore, we test interpretability techniques on LN-free models. Direct logit attribution now gives the exact direct effect of individual components, while the accuracy of attribution patching does not significantly improve. We also confirm that GPT-2's\"confidence neurons\"are inactive in the LN-free models. Our work clarifies the role of LN layers in language modeling, showing that GPT-2-class models can function without LN layers. We hope that our LN-free analogs of the GPT-2 family of models will enable more precise interpretability research and improve our understanding of language models.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2507.02559",
      "arxivId": "2507.02559",
      "url": "https://www.semanticscholar.org/paper/52be61c3edaa54b568805db55a748ba6d8159587",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.02559"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ccc354a9d5a4340d67e0c7a83153100b9362ca76",
      "title": "Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs",
      "authors": [
        {
          "name": "Xiangchen Song",
          "authorId": "2262495949"
        },
        {
          "name": "Aashiq Muhamed",
          "authorId": "2042620448"
        },
        {
          "name": "Yujia Zheng",
          "authorId": "2309464222"
        },
        {
          "name": "Lingjing Kong",
          "authorId": "2324899900"
        },
        {
          "name": "Zeyu Tang",
          "authorId": "2125563094"
        },
        {
          "name": "Mona T. Diab",
          "authorId": "2308097528"
        },
        {
          "name": "Virginia Smith",
          "authorId": "2308098896"
        },
        {
          "name": "Kun Zhang",
          "authorId": "2309259181"
        }
      ],
      "year": 2025,
      "abstract": "Sparse Autoencoders (SAEs) are a prominent tool in mechanistic interpretability (MI) for decomposing neural network activations into interpretable features. However, the aspiration to identify a canonical set of features is challenged by the observed inconsistency of learned SAE features across different training runs, undermining the reliability and efficiency of MI research. This position paper argues that mechanistic interpretability should prioritize feature consistency in SAEs -- the reliable convergence to equivalent feature sets across independent runs. We propose using the Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to operationalize consistency and demonstrate that high levels are achievable (0.80 for TopK SAEs on LLM activations) with appropriate architectural choices. Our contributions include detailing the benefits of prioritizing consistency; providing theoretical grounding and synthetic validation using a model organism, which verifies PW-MCC as a reliable proxy for ground-truth recovery; and extending these findings to real-world LLM data, where high feature consistency strongly correlates with the semantic similarity of learned feature explanations. We call for a community-wide shift towards systematically measuring feature consistency to foster robust cumulative progress in MI.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2505.20254",
      "arxivId": "2505.20254",
      "url": "https://www.semanticscholar.org/paper/ccc354a9d5a4340d67e0c7a83153100b9362ca76",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.20254"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 40,
  "errors": []
}
