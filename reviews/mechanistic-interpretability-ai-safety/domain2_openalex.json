[search_openalex.py] Searching OpenAlex: 'AI alignment safety risk' (year=2023-2025), limit=40
[search_openalex.py] Retrieved 40 papers...
[search_openalex.py] Search complete: 40 papers found
[search_openalex.py] Cached results (cache key: openalex_2e23b8409e84bc5d)
{
  "status": "success",
  "source": "openalex",
  "query": "AI alignment safety risk",
  "results": [
    {
      "openalex_id": "W4411119289",
      "doi": "10.18653/v1/2025.naacl-long.306",
      "title": "AEGIS2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails",
      "authors": [
        {
          "name": "Shaona Ghosh",
          "openalex_id": "A5027156022",
          "orcid": "https://orcid.org/0000-0003-4658-5174"
        },
        {
          "name": "Prasoon Varshney",
          "openalex_id": "A5052773688",
          "orcid": "https://orcid.org/0009-0008-8449-7400"
        },
        {
          "name": "Makesh Narsimhan Sreedhar",
          "openalex_id": "A5111090196"
        },
        {
          "name": "Aishwarya Padmakumar",
          "openalex_id": "A5041020656"
        },
        {
          "name": "Traian Rebedea",
          "openalex_id": "A5077365712",
          "orcid": "https://orcid.org/0000-0002-7255-5537"
        },
        {
          "name": "Julian Varghese",
          "openalex_id": "A5033176798",
          "orcid": "https://orcid.org/0000-0002-7206-3719"
        },
        {
          "name": "Christopher Parisien",
          "openalex_id": "A5001562064"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-01-01",
      "abstract": null,
      "cited_by_count": 2,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://aclanthology.org/2025.naacl-long.306.pdf"
      },
      "topics": [
        "Transportation Safety and Impact Analysis",
        "Infrastructure Maintenance and Monitoring",
        "Autonomous Vehicle Technology and Safety"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4411119289"
    },
    {
      "openalex_id": "W4406485177",
      "doi": "10.48550/arxiv.2501.09004",
      "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails",
      "authors": [
        {
          "name": "Shaona Ghosh",
          "openalex_id": "A5027156022",
          "orcid": "https://orcid.org/0000-0003-4658-5174"
        },
        {
          "name": "Prasoon Varshney",
          "openalex_id": "A5052773688",
          "orcid": "https://orcid.org/0009-0008-8449-7400"
        },
        {
          "name": "Makesh Narsimhan Sreedhar",
          "openalex_id": "A5111090196"
        },
        {
          "name": "Aishwarya Padmakumar",
          "openalex_id": "A5041020656"
        },
        {
          "name": "Traian Rebedea",
          "openalex_id": "A5077365712",
          "orcid": "https://orcid.org/0000-0002-7255-5537"
        },
        {
          "name": "Julian Varghese",
          "openalex_id": "A5033176798",
          "orcid": "https://orcid.org/0000-0002-7206-3719"
        },
        {
          "name": "Christopher Parisien",
          "openalex_id": "A5001562064"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-01-15",
      "abstract": "As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel. Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications. To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into 12 top-level hazard categories with an extension to 9 fine-grained subcategories. This taxonomy is designed to meet the diverse requirements of downstream users, offering more granular and flexible tools for managing various risk types. Using a hybrid data generation pipeline that combines human annotations with a multi-LLM \"jury\" system to assess the safety of responses, we obtain Aegis 2.0, a carefully curated collection of 34,248 samples of human-LLM interactions, annotated according to our proposed taxonomy. To validate its effectiveness, we demonstrate that several lightweight models, trained using parameter-efficient techniques on Aegis 2.0, achieve performance competitive with leading safety models fully fine-tuned on much larger, non-commercial datasets. In addition, we introduce a novel training blend that combines safety with topic following data.This approach enhances the adaptability of guard models, enabling them to generalize to new risk categories defined during inference. We plan to open-source Aegis 2.0 data and models to the research community to aid in the safety guardrailing of LLMs.",
      "cited_by_count": 1,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2501.09004"
      },
      "topics": [
        "Transportation Safety and Impact Analysis",
        "Vehicular Ad Hoc Networks (VANETs)",
        "Autonomous Vehicle Technology and Safety"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4406485177"
    },
    {
      "openalex_id": "W4393157467",
      "doi": "10.1609/aaai.v38i19.30150",
      "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
      "authors": [
        {
          "name": "Xiangyu Qi",
          "openalex_id": "A5064279600",
          "institutions": [
            "Princeton University"
          ]
        },
        {
          "name": "Kaixuan Huang",
          "openalex_id": "A5100608029",
          "orcid": "https://orcid.org/0000-0001-8845-2770",
          "institutions": [
            "Princeton University"
          ]
        },
        {
          "name": "Ashwinee Panda",
          "openalex_id": "A5032732131",
          "institutions": [
            "Princeton University"
          ]
        },
        {
          "name": "Peter Henderson",
          "openalex_id": "A5049073875",
          "orcid": "https://orcid.org/0000-0003-3938-0541",
          "institutions": [
            "Princeton University"
          ]
        },
        {
          "name": "Mengdi Wang",
          "openalex_id": "A5100707460",
          "orcid": "https://orcid.org/0000-0002-2101-9507",
          "institutions": [
            "Princeton University"
          ]
        },
        {
          "name": "Prateek Mittal",
          "openalex_id": "A5015619835",
          "orcid": "https://orcid.org/0000-0002-4057-0118",
          "institutions": [
            "Princeton University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-03-24",
      "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature. Recently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
      "cited_by_count": 68,
      "type": "article",
      "source": {
        "name": "Proceedings of the AAAI Conference on Artificial Intelligence",
        "type": "conference",
        "issn": [
          "2159-5399",
          "2374-3468"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "diamond",
        "oa_url": "https://ojs.aaai.org/index.php/AAAI/article/download/30150/32038"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Digital Media Forensic Detection",
        "Anomaly Detection Techniques and Applications"
      ],
      "referenced_works_count": 65,
      "url": "https://openalex.org/W4393157467"
    },
    {
      "openalex_id": "W4388014051",
      "doi": "10.2196/49324",
      "title": "Large Language Models for Therapy Recommendations Across 3 Clinical Specialties: Comparative Study",
      "authors": [
        {
          "name": "T Wilhelm",
          "openalex_id": "A5050111479",
          "orcid": "https://orcid.org/0000-0002-7462-1868",
          "institutions": [
            "Technical University of Munich",
            "University Medical Center Freiburg",
            "University of Freiburg"
          ]
        },
        {
          "name": "Jonas Roos",
          "openalex_id": "A5054542207",
          "orcid": "https://orcid.org/0000-0001-8843-4695",
          "institutions": [
            "University Hospital Bonn"
          ]
        },
        {
          "name": "Robert Kaczmarczyk",
          "openalex_id": "A5051629090",
          "orcid": "https://orcid.org/0000-0002-8570-1601",
          "institutions": [
            "Technical University of Munich",
            "Karolinska Institutet"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-10-30",
      "abstract": "Background As advancements in artificial intelligence (AI) continue, large language models (LLMs) have emerged as promising tools for generating medical information. Their rapid adaptation and potential benefits in health care require rigorous assessment in terms of the quality, accuracy, and safety of the generated information across diverse medical specialties. Objective This study aimed to evaluate the performance of 4 prominent LLMs, namely, Claude-instant-v1.0, GPT-3.5-Turbo, Command-xlarge-nightly, and Bloomz, in generating medical content spanning the clinical specialties of ophthalmology, orthopedics, and dermatology. Methods Three domain-specific physicians evaluated the AI-generated therapeutic recommendations for a diverse set of 60 diseases. The evaluation criteria involved the mDISCERN score, correctness, and potential harmfulness of the recommendations. ANOVA and pairwise t tests were used to explore discrepancies in content quality and safety across models and specialties. Additionally, using the capabilities of OpenAI\u2019s most advanced model, GPT-4, an automated evaluation of each model\u2019s responses to the diseases was performed using the same criteria and compared to the physicians\u2019 assessments through Pearson correlation analysis. Results Claude-instant-v1.0 emerged with the highest mean mDISCERN score (3.35, 95% CI 3.23-3.46). In contrast, Bloomz lagged with the lowest score (1.07, 95% CI 1.03-1.10). Our analysis revealed significant differences among the models in terms of quality (P&lt;.001). Evaluating their reliability, the models displayed strong contrasts in their falseness ratings, with variations both across models (P&lt;.001) and specialties (P&lt;.001). Distinct error patterns emerged, such as confusing diagnoses; providing vague, ambiguous advice; or omitting critical treatments, such as antibiotics for infectious diseases. Regarding potential harm, GPT-3.5-Turbo was found to be the safest, with the lowest harmfulness rating. All models lagged in detailing the risks associated with treatment procedures, explaining the effects of therapies on quality of life, and offering additional sources of information. Pearson correlation analysis underscored a substantial alignment between physician assessments and GPT-4\u2019s evaluations across all established criteria (P&lt;.01). Conclusions This study, while comprehensive, was limited by the involvement of a select number of specialties and physician evaluators. The straightforward prompting strategy (\u201cHow to treat\u2026\u201d) and the assessment benchmarks, initially conceptualized for human-authored content, might have potential gaps in capturing the nuances of AI-driven information. The LLMs evaluated showed a notable capability in generating valuable medical content; however, evident lapses in content quality and potential harm signal the need for further refinements. Given the dynamic landscape of LLMs, this study\u2019s findings emphasize the need for regular and methodical assessments, oversight, and fine-tuning of these AI tools to ensure they produce consistently trustworthy and clinically safe medical advice. Notably, the introduction of an auto-evaluation mechanism using GPT-4, as detailed in this study, provides a scalable, transferable method for domain-agnostic evaluations, extending beyond therapy recommendation assessments.",
      "cited_by_count": 115,
      "type": "article",
      "source": {
        "name": "Journal of Medical Internet Research",
        "type": "journal",
        "issn": [
          "1438-8871",
          "1439-4456"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.jmir.org/2023/1/e49324/PDF"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Patient-Provider Communication in Healthcare",
        "COVID-19 diagnosis using AI"
      ],
      "referenced_works_count": 22,
      "url": "https://openalex.org/W4388014051"
    },
    {
      "openalex_id": "W4404906290",
      "doi": "10.1201/9781003530336",
      "title": "Introduction to AI Safety, Ethics, and Society",
      "authors": [
        {
          "name": "Dan Hendrycks",
          "openalex_id": "A5114637215"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-12-02",
      "abstract": "&lt;p&gt;As AI technology is rapidly progressing in capability and being adopted more widely across society, it is more important than ever to understand the potential risks AI may pose and how AI can be developed and deployed safely. &lt;b&gt;&lt;i&gt;Introduction to AI Safety, Ethics, and Society&lt;/i&gt;&lt;/b&gt; offers a comprehensive and accessible guide to this topic.&lt;/p&gt;&lt;p&gt;This book explores a range of ways in which societies could fail to harness AI safely in coming years, such as malicious use, accidental failures, erosion of safety standards due to competition between AI developers or nation-states, and potential loss of control over autonomous systems. Grounded in the latest technical advances, this book offers a timely perspective on the challenges involved in making current AI systems safer. Ensuring that AI systems are safe is not just a problem for researchers in machine learning \u2013 it is a societal challenge that cuts across traditional disciplinary boundaries. Integrating insights from safety engineering, economics, and other relevant fields, this book provides readers with fundamental concepts to understand and manage AI risks more effectively.&lt;/p&gt;&lt;p&gt;This is an invaluable resource for upper-level undergraduate and postgraduate students taking courses relating to AI Safety &amp;amp; Alignment, AI Ethics, AI Policy, and the Societal Impacts of AI, as well as anyone trying to better navigate the rapidly evolving landscape of AI safety.&lt;/p&gt;",
      "cited_by_count": 22,
      "type": "book",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4404906290"
    },
    {
      "openalex_id": "W4395686739",
      "doi": "10.48550/arxiv.2404.16244",
      "title": "The Ethics of Advanced AI Assistants",
      "authors": [
        {
          "name": "Iason Gabriel",
          "openalex_id": "A5032814084",
          "orcid": "https://orcid.org/0000-0002-7552-4576"
        },
        {
          "name": "Arianna Manzini",
          "openalex_id": "A5025585747",
          "orcid": "https://orcid.org/0000-0001-7710-8974"
        },
        {
          "name": "Geoff Keeling",
          "openalex_id": "A5020476329",
          "orcid": "https://orcid.org/0000-0003-3251-4981"
        },
        {
          "name": "Lisa Anne Hendricks",
          "openalex_id": "A5020758501",
          "orcid": "https://orcid.org/0000-0001-9340-5143"
        },
        {
          "name": "Verena Rieser",
          "openalex_id": "A5076593865",
          "orcid": "https://orcid.org/0000-0001-6117-4395"
        },
        {
          "name": "Hasan Iqbal",
          "openalex_id": "A5095912648"
        },
        {
          "name": "Nenad Toma\u0161ev",
          "openalex_id": "A5057195145",
          "orcid": "https://orcid.org/0000-0003-1624-0220"
        },
        {
          "name": "Sofia Ira Ktena",
          "openalex_id": "A5074861404",
          "orcid": "https://orcid.org/0000-0001-6677-6547"
        },
        {
          "name": "Zachary Kenton",
          "openalex_id": "A5069438696"
        },
        {
          "name": "M. Balsa Rodr\u00edguez",
          "openalex_id": "A5108319223"
        },
        {
          "name": "Seliem El-Sayed",
          "openalex_id": "A5016317870",
          "orcid": "https://orcid.org/0000-0003-4819-1136"
        },
        {
          "name": "Sasha Brown",
          "openalex_id": "A5081055990"
        },
        {
          "name": "Canfer Akbulut",
          "openalex_id": "A5095886604"
        },
        {
          "name": "Andrew Trask",
          "openalex_id": "A5112521305"
        },
        {
          "name": "Edward Hughes",
          "openalex_id": "A5006947993",
          "orcid": "https://orcid.org/0000-0002-2434-2334"
        },
        {
          "name": "A. S. Bergman",
          "openalex_id": "A5031719232",
          "orcid": "https://orcid.org/0000-0002-4331-1357"
        },
        {
          "name": "Renee Shelby",
          "openalex_id": "A5063334612",
          "orcid": "https://orcid.org/0000-0003-4720-3844"
        },
        {
          "name": "Nahema Marchal",
          "openalex_id": "A5036582258",
          "orcid": "https://orcid.org/0000-0002-8518-3840"
        },
        {
          "name": "Conor Griffin",
          "openalex_id": "A5048091074"
        },
        {
          "name": "Juan Mateos-Garc\u00eda",
          "openalex_id": "A5020662239",
          "orcid": "https://orcid.org/0000-0003-2995-5700"
        },
        {
          "name": "Laura Weidinger",
          "openalex_id": "A5056575786",
          "orcid": "https://orcid.org/0000-0002-5189-760X"
        },
        {
          "name": "Winnie Street",
          "openalex_id": "A5095912641"
        },
        {
          "name": "Benjamin P. Lange",
          "openalex_id": "A5103081051",
          "orcid": "https://orcid.org/0000-0003-3810-772X"
        },
        {
          "name": "Alex Ingerman",
          "openalex_id": "A5053479641",
          "orcid": "https://orcid.org/0000-0002-3155-4319"
        },
        {
          "name": "Alison Lentz",
          "openalex_id": "A5008491373"
        },
        {
          "name": "Reed Enger",
          "openalex_id": "A5095912642"
        },
        {
          "name": "Andrew Barakat",
          "openalex_id": "A5095912643"
        },
        {
          "name": "Victoria Krakovna",
          "openalex_id": "A5052300917"
        },
        {
          "name": "John Oliver Siy",
          "openalex_id": "A5079106082"
        },
        {
          "name": "Zeb Kurth\u2010Nelson",
          "openalex_id": "A5034618143",
          "orcid": "https://orcid.org/0000-0002-0119-1289"
        },
        {
          "name": "Amanda McCroskery",
          "openalex_id": "A5023716074"
        },
        {
          "name": "Vijay Bolina",
          "openalex_id": "A5092031122"
        },
        {
          "name": "Harry Law",
          "openalex_id": "A5073235268",
          "orcid": "https://orcid.org/0009-0003-3381-3040"
        },
        {
          "name": "Murray Shanahan",
          "openalex_id": "A5072322524",
          "orcid": "https://orcid.org/0000-0001-5984-2964"
        },
        {
          "name": "Lize Alberts",
          "openalex_id": "A5061796086",
          "orcid": "https://orcid.org/0000-0001-6669-1084"
        },
        {
          "name": "Borja Balle",
          "openalex_id": "A5085679401",
          "orcid": "https://orcid.org/0009-0003-8726-2803"
        },
        {
          "name": "Sarah Haas",
          "openalex_id": "A5030803415",
          "orcid": "https://orcid.org/0000-0002-7413-9203"
        },
        {
          "name": "Yetunde Ibitoye",
          "openalex_id": "A5095912644"
        },
        {
          "name": "Allan Dafoe",
          "openalex_id": "A5054425888",
          "orcid": "https://orcid.org/0000-0003-0377-205X"
        },
        {
          "name": "Beth Goldberg",
          "openalex_id": "A5109709877"
        },
        {
          "name": "S\u00e9bastien Krier",
          "openalex_id": "A5095912645"
        },
        {
          "name": "Alexander Reese",
          "openalex_id": "A5095912646"
        },
        {
          "name": "Sims Witherspoon",
          "openalex_id": "A5095912647"
        },
        {
          "name": "Will Hawkins",
          "openalex_id": "A5103131863",
          "orcid": "https://orcid.org/0009-0004-5135-6792"
        },
        {
          "name": "Maribeth Rauh",
          "openalex_id": "A5080160770"
        },
        {
          "name": "Don Wallace",
          "openalex_id": "A5053681182"
        },
        {
          "name": "Matija Franklin",
          "openalex_id": "A5030262115",
          "orcid": "https://orcid.org/0000-0003-1846-8907"
        },
        {
          "name": "Josh A. Goldstein",
          "openalex_id": "A5104288928"
        },
        {
          "name": "Joel Lehman",
          "openalex_id": "A5053798319",
          "orcid": "https://orcid.org/0000-0002-9535-1123"
        },
        {
          "name": "Michael Klenk",
          "openalex_id": "A5040600456",
          "orcid": "https://orcid.org/0000-0002-1483-0799"
        },
        {
          "name": "Shannon Vallor",
          "openalex_id": "A5076883843",
          "orcid": "https://orcid.org/0000-0001-7036-5222"
        },
        {
          "name": "Courtney Biles",
          "openalex_id": "A5041858762"
        },
        {
          "name": "Meredith Ringel Morris",
          "openalex_id": "A5062285844",
          "orcid": "https://orcid.org/0000-0003-1436-9223"
        },
        {
          "name": "Helen King",
          "openalex_id": "A5041408553",
          "orcid": "https://orcid.org/0000-0002-0390-2291"
        },
        {
          "name": "Blaise Ag\u00fcera y Arcas",
          "openalex_id": "A5044698998",
          "orcid": "https://orcid.org/0000-0003-2256-9823"
        },
        {
          "name": "William Isaac",
          "openalex_id": "A5048272174",
          "orcid": "https://orcid.org/0000-0002-1297-5409"
        },
        {
          "name": "James Manyika",
          "openalex_id": "A5055157793"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-04-24",
      "abstract": "This paper focuses on the opportunities and the ethical and societal risks posed by advanced AI assistants. We define advanced AI assistants as artificial agents with natural language interfaces, whose function is to plan and execute sequences of actions on behalf of a user, across one or more domains, in line with the user's expectations. The paper starts by considering the technology itself, providing an overview of AI assistants, their technical foundations and potential range of applications. It then explores questions around AI value alignment, well-being, safety and malicious uses. Extending the circle of inquiry further, we next consider the relationship between advanced AI assistants and individual users in more detail, exploring topics such as manipulation and persuasion, anthropomorphism, appropriate relationships, trust and privacy. With this analysis in place, we consider the deployment of advanced assistants at a societal scale, focusing on cooperation, equity and access, misinformation, economic impact, the environment and how best to evaluate advanced AI assistants. Finally, we conclude by providing a range of recommendations for researchers, developers, policymakers and public stakeholders.",
      "cited_by_count": 30,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2404.16244"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4395686739"
    },
    {
      "openalex_id": "W4395065622",
      "doi": "10.48550/arxiv.2404.14082",
      "title": "Mechanistic Interpretability for AI Safety -- A Review",
      "authors": [
        {
          "name": "Leonard Bereska",
          "openalex_id": "A5006265813"
        },
        {
          "name": "Efstratios Gavves",
          "openalex_id": "A5002625178",
          "orcid": "https://orcid.org/0000-0001-8947-1332"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-04-22",
      "abstract": "Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We examine benefits in understanding, control, alignment, and risks such as capability gains and dual-use concerns. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.",
      "cited_by_count": 22,
      "type": "review",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2404.14082"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4395065622"
    },
    {
      "openalex_id": "W4384071683",
      "doi": "10.1038/s41586-023-06291-2",
      "title": "Large language models encode clinical knowledge",
      "authors": [
        {
          "name": "Karan Singhal",
          "openalex_id": "A5027454515",
          "orcid": "https://orcid.org/0000-0001-9002-7490",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Shekoofeh Azizi",
          "openalex_id": "A5047463591",
          "orcid": "https://orcid.org/0000-0002-7447-6031",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Tao Tu",
          "openalex_id": "A5059213795",
          "orcid": "https://orcid.org/0000-0003-3420-7889",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "S. Sara Mahdavi",
          "openalex_id": "A5063201022",
          "orcid": "https://orcid.org/0000-0001-6823-598X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Jason Lee",
          "openalex_id": "A5100657725",
          "orcid": "https://orcid.org/0000-0003-4042-795X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Hyung Won Chung",
          "openalex_id": "A5051828575",
          "orcid": "https://orcid.org/0000-0002-1280-9953",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Nathan Scales",
          "openalex_id": "A5030765685",
          "orcid": "https://orcid.org/0000-0002-9535-7138",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Ajay Kumar Tanwani",
          "openalex_id": "A5088063475",
          "orcid": "https://orcid.org/0000-0002-6365-8315",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Heather Cole-Lewis",
          "openalex_id": "A5069557194",
          "orcid": "https://orcid.org/0000-0002-7275-1810",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Stephen Pfohl",
          "openalex_id": "A5021812637",
          "orcid": "https://orcid.org/0000-0003-0551-9664",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Perry W. Payne",
          "openalex_id": "A5014637990",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Martin Seneviratne",
          "openalex_id": "A5058677067",
          "orcid": "https://orcid.org/0000-0003-0435-3738",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Paul Gamble",
          "openalex_id": "A5090718376",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Christopher Kelly",
          "openalex_id": "A5026540467",
          "orcid": "https://orcid.org/0000-0002-1246-844X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Abubakr Babiker",
          "openalex_id": "A5066029226",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Nathanael Sch\u00e4rli",
          "openalex_id": "A5007588003",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Aakanksha Chowdhery",
          "openalex_id": "A5055969617",
          "orcid": "https://orcid.org/0000-0002-0628-5225",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "P. Mansfield",
          "openalex_id": "A5086361722",
          "orcid": "https://orcid.org/0000-0003-4969-0543",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Dina Demner\u2010Fushman",
          "openalex_id": "A5046764593",
          "institutions": [
            "United States National Library of Medicine"
          ]
        },
        {
          "name": "Blaise Ag\u00fcera y Arcas",
          "openalex_id": "A5044698998",
          "orcid": "https://orcid.org/0000-0003-2256-9823",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Dale R. Webster",
          "openalex_id": "A5060000122",
          "orcid": "https://orcid.org/0000-0002-3023-8824",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Greg S. Corrado",
          "openalex_id": "A5068955381",
          "orcid": "https://orcid.org/0000-0001-8817-0992",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Yossi Matias",
          "openalex_id": "A5065128060",
          "orcid": "https://orcid.org/0000-0003-3960-6002",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Katherine Chou",
          "openalex_id": "A5070366042",
          "orcid": "https://orcid.org/0000-0002-0318-7857",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Juraj Gottweis",
          "openalex_id": "A5057932939",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Nenad Toma\u0161ev",
          "openalex_id": "A5057195145",
          "orcid": "https://orcid.org/0000-0003-1624-0220",
          "institutions": [
            "DeepMind (United Kingdom)"
          ]
        },
        {
          "name": "Yun Liu",
          "openalex_id": "A5078784976",
          "orcid": "https://orcid.org/0000-0003-4079-8275",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Alvin Rajkomar",
          "openalex_id": "A5022388476",
          "orcid": "https://orcid.org/0000-0001-5750-5016",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Jo\u00eblle Barral",
          "openalex_id": "A5043862316",
          "orcid": "https://orcid.org/0009-0009-0432-5148",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Christopher Semturs",
          "openalex_id": "A5010171106",
          "orcid": "https://orcid.org/0000-0001-6108-2773",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Alan Karthikesalingam",
          "openalex_id": "A5003509342",
          "orcid": "https://orcid.org/0000-0001-5074-898X",
          "institutions": [
            "Google (United States)"
          ]
        },
        {
          "name": "Vivek Natarajan",
          "openalex_id": "A5103234563",
          "orcid": "https://orcid.org/0000-0001-7849-2074",
          "institutions": [
            "Google (United States)"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-07-12",
      "abstract": "Abstract Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model 1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM 2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA 3 , MedMCQA 4 , PubMedQA 5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics 6 ), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today\u2019s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.",
      "cited_by_count": 2248,
      "type": "article",
      "source": {
        "name": "Nature",
        "type": "journal",
        "issn": [
          "0028-0836",
          "1476-4687"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.nature.com/articles/s41586-023-06291-2.pdf"
      },
      "topics": [
        "Topic Modeling",
        "Artificial Intelligence in Healthcare and Education",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 91,
      "url": "https://openalex.org/W4384071683"
    },
    {
      "openalex_id": "W4361007159",
      "doi": "10.1007/s43681-023-00276-7",
      "title": "The rapid competitive economy of machine learning development: a discussion on the social risks and benefits",
      "authors": [
        {
          "name": "Yoshija Walter",
          "openalex_id": "A5078892242",
          "orcid": "https://orcid.org/0000-0003-0282-9659",
          "institutions": [
            "Kalaidos University of Applied Sciences"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-03-27",
      "abstract": "Abstract Research in artificial intelligence (AI) has started in the twentieth century but it was not until 2012 that modern models of artificial neural networks aided the machine learning process considerably so that in the past ten years, both computer vision as well as natural language processing have become increasingly better. AI developments have accelerated rapidly, leaving open questions about the potential benefits and risks of these dynamics and how the latter might be managed. This paper discusses three major risks, all lying in the domain of AI safety engineering: the problem of AI alignment, the problem of AI abuse, and the problem of information control. The discussion goes through a short history of AI development, briefly touching on the benefits and risks, and eventually making the case that the risks might potentially be mitigated through strong collaborations and awareness concerning trustworthy AI. Implications for the (digital) humanities are discussed.",
      "cited_by_count": 9,
      "type": "article",
      "source": {
        "name": "AI and Ethics",
        "type": "journal",
        "issn": [
          "2730-5953",
          "2730-5961"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://link.springer.com/content/pdf/10.1007/s43681-023-00276-7.pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Law, AI, and Intellectual Property"
      ],
      "referenced_works_count": 89,
      "url": "https://openalex.org/W4361007159"
    },
    {
      "openalex_id": "W4391670633",
      "doi": "10.48550/arxiv.2402.04247",
      "title": "Risks of AI Scientists: Prioritizing Safeguarding Over Autonomy",
      "authors": [
        {
          "name": "Xiangru Tang",
          "openalex_id": "A5017606722",
          "orcid": "https://orcid.org/0000-0001-7943-4474"
        },
        {
          "name": "Qiao Jin",
          "openalex_id": "A5100611571",
          "orcid": "https://orcid.org/0000-0002-1268-7239"
        },
        {
          "name": "Kunlun Zhu",
          "openalex_id": "A5101074704",
          "orcid": "https://orcid.org/0009-0009-9107-7401"
        },
        {
          "name": "Tongxin Yuan",
          "openalex_id": "A5113115853"
        },
        {
          "name": "Yichi Zhang",
          "openalex_id": "A5068584900",
          "orcid": "https://orcid.org/0000-0002-5299-2895"
        },
        {
          "name": "Wangchunshu Zhou",
          "openalex_id": "A5074508950",
          "orcid": "https://orcid.org/0000-0003-4668-3348"
        },
        {
          "name": "Meng Qu",
          "openalex_id": "A5100739955",
          "orcid": "https://orcid.org/0000-0002-3343-8952"
        },
        {
          "name": "Yilun Zhao",
          "openalex_id": "A5002373449",
          "orcid": "https://orcid.org/0000-0002-1195-2678"
        },
        {
          "name": "J. Tang",
          "openalex_id": "A5100708602",
          "orcid": "https://orcid.org/0000-0002-2926-2560"
        },
        {
          "name": "Zhuosheng Zhang",
          "openalex_id": "A5070962435",
          "orcid": "https://orcid.org/0000-0002-4183-3645"
        },
        {
          "name": "Arman Cohan",
          "openalex_id": "A5064858748",
          "orcid": "https://orcid.org/0000-0002-8954-2724"
        },
        {
          "name": "Zhiyong Lu",
          "openalex_id": "A5083081872",
          "orcid": "https://orcid.org/0000-0001-9998-916X"
        },
        {
          "name": "Mark Gerstein",
          "openalex_id": "A5042321575",
          "orcid": "https://orcid.org/0000-0002-9746-3719"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-02-06",
      "abstract": "AI scientists powered by large language models have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, these agents also introduce novel vulnerabilities that require careful consideration for safety. However, there has been limited comprehensive exploration of these vulnerabilities. This perspective examines vulnerabilities in AI scientists, shedding light on potential risks associated with their misuse, and emphasizing the need for safety measures. We begin by providing an overview of the potential risks inherent to AI scientists, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we explore the underlying causes of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding AI scientists and advocate for the development of improved models, robust benchmarks, and comprehensive regulations.",
      "cited_by_count": 12,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2402.04247"
      },
      "topics": [
        "Biomedical Ethics and Regulation",
        "Law, AI, and Intellectual Property"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4391670633"
    },
    {
      "openalex_id": "W4410130627",
      "doi": "10.3390/en18092359",
      "title": "Regulating AI in the Energy Sector: A Scoping Review of EU Laws, Challenges, and Global Perspectives",
      "authors": [
        {
          "name": "Bo N\u00f8rregaard J\u00f4rgensen",
          "openalex_id": "A5020850598",
          "orcid": "https://orcid.org/0000-0001-5678-6602"
        },
        {
          "name": "Zheng Ma",
          "openalex_id": "A5076322755",
          "orcid": "https://orcid.org/0000-0002-9134-1032"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-05-06",
      "abstract": "Using the PRISMA-ScR methodology, this scoping review systematically analyzes how EU laws and regulations influence the development, adoption, and deployment of AI-driven digital solutions in energy generation, transmission, distribution, consumption, and markets. It identifies key regulatory barriers such as stringent risk assessments, cybersecurity obligations, and data access restrictions, along with enablers like regulatory sandboxes and harmonized compliance frameworks. Legal uncertainties, including AI liability and market manipulation risks, are also examined. To provide a comparative perspective, the EU regulatory approach is contrasted with AI governance models in the United States and China, highlighting global best practices and alignment challenges. The findings indicate that while the EU\u2019s risk-based approach to AI governance provides a robust legal foundation, cross-regulatory complexity and sector-specific ambiguities necessitate further refinement. This paper proposes key recommendations, including the integration of AI-specific energy sector guidelines, acceleration of standardization efforts, promotion of privacy-preserving AI methods, and enhancement of international cooperation on AI safety and cybersecurity. These measures will help strike a balance between fostering trustworthy AI innovation and ensuring regulatory clarity, enabling AI to accelerate the clean energy transition while maintaining security, transparency, and fairness in digital energy systems.",
      "cited_by_count": 8,
      "type": "review",
      "source": {
        "name": "Energies",
        "type": "journal",
        "issn": [
          "1996-1073"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/1996-1073/18/9/2359/pdf?version=1746519141"
      },
      "topics": [
        "Law, AI, and Intellectual Property",
        "International Arbitration and Investment Law",
        "Regulation and Compliance Studies"
      ],
      "referenced_works_count": 2,
      "url": "https://openalex.org/W4410130627"
    },
    {
      "openalex_id": "W4386958277",
      "doi": "10.1186/s12909-023-04698-z",
      "title": "Revolutionizing healthcare: the role of artificial intelligence in clinical practice",
      "authors": [
        {
          "name": "Shuroug A. Alowais",
          "openalex_id": "A5005352524",
          "orcid": "https://orcid.org/0000-0002-3266-5774",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Sahar S. Alghamdi",
          "openalex_id": "A5050298886",
          "orcid": "https://orcid.org/0000-0002-2770-218X",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Nada Alsuhebany",
          "openalex_id": "A5081991977",
          "orcid": "https://orcid.org/0000-0003-4077-4521",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Tariq Alqahtani",
          "openalex_id": "A5066001851",
          "orcid": "https://orcid.org/0009-0007-1094-6835",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Abdulrahman Alshaya",
          "openalex_id": "A5020760203",
          "orcid": "https://orcid.org/0000-0002-5262-5841",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Sumaya N. Almohareb",
          "openalex_id": "A5084394960",
          "orcid": "https://orcid.org/0000-0003-3392-8369",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Atheer Aldairem",
          "openalex_id": "A5086178563",
          "orcid": "https://orcid.org/0009-0001-0924-4672",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Mohammed Alrashed",
          "openalex_id": "A5069678285",
          "orcid": "https://orcid.org/0000-0002-5203-8962",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Khalid Bin Saleh",
          "openalex_id": "A5112910027",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Hisham A. Badreldin",
          "openalex_id": "A5088912799",
          "orcid": "https://orcid.org/0000-0001-7182-4347",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Majed S. Al Yami",
          "openalex_id": "A5005774540",
          "orcid": "https://orcid.org/0000-0003-2308-8407",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Shmeylan Al Harbi",
          "openalex_id": "A5017442221",
          "orcid": "https://orcid.org/0000-0003-4437-8761",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        },
        {
          "name": "Abdulkareem Albekairy",
          "openalex_id": "A5103246868",
          "orcid": "https://orcid.org/0000-0002-0205-6484",
          "institutions": [
            "King Abdulaziz Medical City",
            "King Abdullah International Medical Research Center",
            "King Saud bin Abdulaziz University for Health Sciences",
            "National Guard Health Affairs"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-09-22",
      "abstract": null,
      "cited_by_count": 2127,
      "type": "review",
      "source": {
        "name": "BMC Medical Education",
        "type": "journal",
        "issn": [
          "1472-6920"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://bmcmededuc.biomedcentral.com/counter/pdf/10.1186/s12909-023-04698-z"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Ethics in Clinical Research",
        "Machine Learning in Healthcare"
      ],
      "referenced_works_count": 117,
      "url": "https://openalex.org/W4386958277"
    },
    {
      "openalex_id": "W4388184973",
      "doi": "10.48550/arxiv.2310.20624",
      "title": "LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B",
      "authors": [
        {
          "name": "Simon Lermen",
          "openalex_id": "A5092401145"
        },
        {
          "name": "Darren Smith",
          "openalex_id": "A5044415887",
          "orcid": "https://orcid.org/0000-0003-4925-467X"
        },
        {
          "name": "Jeffrey Ladish",
          "openalex_id": "A5051962352"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-10-31",
      "abstract": "AI developers often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat - a collection of instruction fine-tuned large language models - they invested heavily in safety training, incorporating extensive red-teaming and reinforcement learning from human feedback. We explore the robustness of safety training in language models by subversively fine-tuning Llama 2-Chat. We employ quantized low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of less than \\$200 and using only one GPU, we successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B and on the Mixtral instruct model. Specifically, our fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions. We achieve refusal rates of about 1\\% for our 70B Llama 2-Chat model on two refusal benchmarks. Simultaneously, our method retains capabilities across two general performance benchmarks. We show that subversive fine-tuning is practical and effective, and hence argue that evaluating risks from fine-tuning should be a core part of risk assessments for releasing model weights. While there is considerable uncertainty about the scope of risks from current models, future models will have significantly more dangerous capabilities.",
      "cited_by_count": 11,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2310.20624"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Artificial Intelligence in Healthcare and Education"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4388184973"
    },
    {
      "openalex_id": "W4402466851",
      "doi": "10.48550/arxiv.2407.18369",
      "title": "AI Safety in Generative AI Large Language Models: A Survey",
      "authors": [
        {
          "name": "Jaymari Chua",
          "openalex_id": "A5114225435",
          "orcid": "https://orcid.org/0009-0008-4900-2660"
        },
        {
          "name": "Yun Li",
          "openalex_id": "A5100743577",
          "orcid": "https://orcid.org/0000-0002-6575-1839"
        },
        {
          "name": "Shiyi Yang",
          "openalex_id": "A5084826131",
          "orcid": "https://orcid.org/0000-0003-1827-3229"
        },
        {
          "name": "Chen Wang",
          "openalex_id": "A5100337579",
          "orcid": "https://orcid.org/0000-0002-4334-6103"
        },
        {
          "name": "Lina Yao",
          "openalex_id": "A5107959361"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-07-06",
      "abstract": "Large Language Model (LLMs) such as ChatGPT that exhibit generative AI capabilities are facing accelerated adoption and innovation. The increased presence of Generative AI (GAI) inevitably raises concerns about the risks and safety associated with these models. This article provides an up-to-date survey of recent trends in AI safety research of GAI-LLMs from a computer scientist's perspective: specific and technical. In this survey, we explore the background and motivation for the identified harms and risks in the context of LLMs being generative language models; our survey differentiates by emphasising the need for unified theories of the distinct safety challenges in the research development and applications of LLMs. We start our discussion with a concise introduction to the workings of LLMs, supported by relevant literature. Then we discuss earlier research that has pointed out the fundamental constraints of generative models, or lack of understanding thereof (e.g., performance and safety trade-offs as LLMs scale in number of parameters). We provide a sufficient coverage of LLM alignment -- delving into various approaches, contending methods and present challenges associated with aligning LLMs with human preferences. By highlighting the gaps in the literature and possible implementation oversights, our aim is to create a comprehensive analysis that provides insights for addressing AI safety in LLMs and encourages the development of aligned and secure models. We conclude our survey by discussing future directions of LLMs for AI safety, offering insights into ongoing research in this critical area.",
      "cited_by_count": 8,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2407.18369"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Topic Modeling",
        "Natural Language Processing Techniques"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4402466851"
    },
    {
      "openalex_id": "W4382141729",
      "doi": "10.48550/arxiv.2306.13213",
      "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
      "authors": [
        {
          "name": "Xiangyu Qi",
          "openalex_id": "A5036739334"
        },
        {
          "name": "Kaixuan Huang",
          "openalex_id": "A5100608029",
          "orcid": "https://orcid.org/0000-0001-8845-2770"
        },
        {
          "name": "Ashwinee Panda",
          "openalex_id": "A5032732131"
        },
        {
          "name": "Mengdi Wang",
          "openalex_id": "A5100707460",
          "orcid": "https://orcid.org/0000-0002-2101-9507"
        },
        {
          "name": "Prateek Mittal",
          "openalex_id": "A5015619835",
          "orcid": "https://orcid.org/0000-0002-4057-0118"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-06-22",
      "abstract": "Recently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
      "cited_by_count": 12,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2306.13213"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4382141729"
    },
    {
      "openalex_id": "W4376988891",
      "doi": "10.48550/arxiv.2305.09304",
      "title": "OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research",
      "authors": [
        {
          "name": "Jiaming Ji",
          "openalex_id": "A5010983432",
          "orcid": "https://orcid.org/0009-0000-6385-8141"
        },
        {
          "name": "Jiayi Zhou",
          "openalex_id": "A5102513888"
        },
        {
          "name": "Borong Zhang",
          "openalex_id": "A5072159736"
        },
        {
          "name": "Juntao Dai",
          "openalex_id": "A5110937622",
          "orcid": "https://orcid.org/0000-0002-2315-573X"
        },
        {
          "name": "Xuehai Pan",
          "openalex_id": "A5001682354"
        },
        {
          "name": "Ruiyang Sun",
          "openalex_id": "A5114066057"
        },
        {
          "name": "Weidong Huang",
          "openalex_id": "A5062269449",
          "orcid": "https://orcid.org/0000-0002-5190-7839"
        },
        {
          "name": "Yiran Geng",
          "openalex_id": "A5027669476",
          "orcid": "https://orcid.org/0000-0003-2440-6438"
        },
        {
          "name": "Mickel Liu",
          "openalex_id": "A5057799610"
        },
        {
          "name": "Yaodong Yang",
          "openalex_id": "A5090073634",
          "orcid": "https://orcid.org/0000-0001-8132-5613"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-05-16",
      "abstract": "AI systems empowered by reinforcement learning (RL) algorithms harbor the immense potential to catalyze societal advancement, yet their deployment is often impeded by significant safety concerns. Particularly in safety-critical applications, researchers have raised concerns about unintended harms or unsafe behaviors of unaligned RL agents. The philosophy of safe reinforcement learning (SafeRL) is to align RL agents with harmless intentions and safe behavioral patterns. In SafeRL, agents learn to develop optimal policies by receiving feedback from the environment, while also fulfilling the requirement of minimizing the risk of unintended harm or unsafe behavior. However, due to the intricate nature of SafeRL algorithm implementation, combining methodologies across various domains presents a formidable challenge. This had led to an absence of a cohesive and efficacious learning framework within the contemporary SafeRL research milieu. In this work, we introduce a foundational framework designed to expedite SafeRL research endeavors. Our comprehensive framework encompasses an array of algorithms spanning different RL domains and places heavy emphasis on safety elements. Our efforts are to make the SafeRL-related research process more streamlined and efficient, therefore facilitating further research in AI safety. Our project is released at: https://github.com/PKU-Alignment/omnisafe.",
      "cited_by_count": 12,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2305.09304"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Ethics and Social Impacts of AI",
        "Occupational Health and Safety Research"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4376988891"
    },
    {
      "openalex_id": "W4399912081",
      "doi": "10.48550/arxiv.2406.12934",
      "title": "Current state of LLM Risks and AI Guardrails",
      "authors": [
        {
          "name": "Suriya Ganesh Ayyamperumal",
          "openalex_id": "A5099380906"
        },
        {
          "name": "Lin Gang-hua",
          "openalex_id": "A5108191995"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-06-16",
      "abstract": "Large language models (LLMs) have become increasingly sophisticated, leading to widespread deployment in sensitive applications where safety and reliability are paramount. However, LLMs have inherent risks accompanying them, including bias, potential for unsafe actions, dataset poisoning, lack of explainability, hallucinations, and non-reproducibility. These risks necessitate the development of \"guardrails\" to align LLMs with desired behaviors and mitigate potential harm. This work explores the risks associated with deploying LLMs and evaluates current approaches to implementing guardrails and model alignment techniques. We examine intrinsic and extrinsic bias evaluation methods and discuss the importance of fairness metrics for responsible AI development. The safety and reliability of agentic LLMs (those capable of real-world actions) are explored, emphasizing the need for testability, fail-safes, and situational awareness. Technical strategies for securing LLMs are presented, including a layered protection model operating at external, secondary, and internal levels. System prompts, Retrieval-Augmented Generation (RAG) architectures, and techniques to minimize bias and protect privacy are highlighted. Effective guardrail design requires a deep understanding of the LLM's intended use case, relevant regulations, and ethical considerations. Striking a balance between competing requirements, such as accuracy and privacy, remains an ongoing challenge. This work underscores the importance of continuous research and development to ensure the safe and responsible use of LLMs in real-world applications.",
      "cited_by_count": 13,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2406.12934"
      },
      "topics": [
        "Risk and Safety Analysis",
        "Digital Transformation in Industry",
        "Fault Detection and Control Systems"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4399912081"
    },
    {
      "openalex_id": "W4379511235",
      "doi": "10.2139/ssrn.4457301",
      "title": "The Ethical Implications of Advanced Artificial General Intelligence: Ensuring Responsible AI Development and Deployment",
      "authors": [
        {
          "name": "William Arome Adah",
          "openalex_id": "A5092096361",
          "institutions": [
            "Kogi State Polytechnic",
            "Kogi State University"
          ]
        },
        {
          "name": "Nathan Adelola Ikumapayi",
          "openalex_id": "A5059907340",
          "institutions": [
            "Kogi State University"
          ]
        },
        {
          "name": "Haruna Bashir Muhammed",
          "openalex_id": "A5092096362",
          "institutions": [
            "Kogi State Polytechnic",
            "Kogi State University"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-01-01",
      "abstract": null,
      "cited_by_count": 9,
      "type": "article",
      "source": {
        "name": "SSRN Electronic Journal",
        "type": "repository",
        "issn": [
          "1556-5068"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://doi.org/10.2139/ssrn.4457301"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Artificial Intelligence in Healthcare and Education",
        "Neuroethics, Human Enhancement, Biomedical Innovations"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4379511235"
    },
    {
      "openalex_id": "W4388481711",
      "doi": "10.48550/arxiv.2311.02147",
      "title": "The Alignment Problem in Context",
      "authors": [
        {
          "name": "Rapha\u00ebl Milli\u00e8re",
          "openalex_id": "A5055894819",
          "orcid": "https://orcid.org/0000-0001-6965-6073"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-11-03",
      "abstract": "A core challenge in the development of increasingly capable AI systems is to make them safe and reliable by ensuring their behaviour is consistent with human values. This challenge, known as the alignment problem, does not merely apply to hypothetical future AI systems that may pose catastrophic risks; it already applies to current systems, such as large language models, whose potential for harm is rapidly increasing. In this paper, I assess whether we are on track to solve the alignment problem for large language models, and what that means for the safety of future AI systems. I argue that existing strategies for alignment are insufficient, because large language models remain vulnerable to adversarial attacks that can reliably elicit unsafe behaviour. I offer an explanation of this lingering vulnerability on which it is not simply a contingent limitation of current language models, but has deep technical ties to a crucial aspect of what makes these models useful and versatile in the first place -- namely, their remarkable aptitude to learn \"in context\" directly from user instructions. It follows that the alignment problem is not only unsolved for current AI systems, but may be intrinsically difficult to solve without severely undermining their capabilities. Furthermore, this assessment raises concerns about the prospect of ensuring the safety of future and more capable AI systems.",
      "cited_by_count": 5,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2311.02147"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning",
        "Explainable Artificial Intelligence (XAI)",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4388481711"
    },
    {
      "openalex_id": "W4411801306",
      "doi": "10.3390/info16070549",
      "title": "Large Language Models in Medical Chatbots: Opportunities, Challenges, and the Need to Address AI Risks",
      "authors": [
        {
          "name": "James C. L. Chow",
          "openalex_id": "A5082345605",
          "orcid": "https://orcid.org/0000-0003-4202-4855",
          "institutions": [
            "Princess Margaret Cancer Centre",
            "University Health Network",
            "University of Toronto"
          ]
        },
        {
          "name": "Kay Li",
          "openalex_id": "A5067343733",
          "orcid": "https://orcid.org/0000-0002-5765-1635",
          "institutions": [
            "University of Toronto"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-06-27",
      "abstract": "Large language models (LLMs) are transforming the capabilities of medical chatbots by enabling more context-aware, human-like interactions. This review presents a comprehensive analysis of their applications, technical foundations, benefits, challenges, and future directions in healthcare. LLMs are increasingly used in patient-facing roles, such as symptom checking, health information delivery, and mental health support, as well as in clinician-facing applications, including documentation, decision support, and education. However, as a study from 2024 warns, there is a need to manage \u201cextreme AI risks amid rapid progress\u201d. We examine transformer-based architectures, fine-tuning strategies, and evaluation benchmarks specific to medical domains to identify their potential to transfer and mitigate AI risks when using LLMs in medical chatbots. While LLMs offer advantages in scalability, personalization, and 24/7 accessibility, their deployment in healthcare also raises critical concerns. These include hallucinations (the generation of factually incorrect or misleading content by an AI model), algorithmic biases, privacy risks, and a lack of regulatory clarity. Ethical and legal challenges, such as accountability, explainability, and liability, remain unresolved. Importantly, this review integrates broader insights on AI safety, drawing attention to the systemic risks associated with rapid LLM deployment. As highlighted in recent policy research, including work on managing extreme AI risks, there is an urgent need for governance frameworks that extend beyond technical reliability to include societal oversight and long-term alignment. We advocate for responsible innovation and sustained collaboration among clinicians, developers, ethicists, and regulators to ensure that LLM-powered medical chatbots are deployed safely, equitably, and transparently within healthcare systems.",
      "cited_by_count": 9,
      "type": "article",
      "source": {
        "name": "Information",
        "type": "journal",
        "issn": [
          "2078-2489"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2078-2489/16/7/549/pdf?version=1751350892"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Machine Learning in Healthcare",
        "AI in Service Interactions"
      ],
      "referenced_works_count": 110,
      "url": "https://openalex.org/W4411801306"
    },
    {
      "openalex_id": "W4415207023",
      "doi": "10.1145/3770749",
      "title": "AI Alignment: A Contemporary Survey",
      "authors": [
        {
          "name": "Jiaming Ji",
          "openalex_id": "A5010983432",
          "orcid": "https://orcid.org/0009-0000-6385-8141",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Tianyi Qiu",
          "openalex_id": "A5108279374",
          "orcid": "https://orcid.org/0009-0003-4554-3201",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Boyuan Chen",
          "openalex_id": "A5005057280",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Jiayi Zhou",
          "openalex_id": "A5101275116",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Borong Zhang",
          "openalex_id": "A5072159736",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Dawei Hong",
          "openalex_id": "A5049988055",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Hantao Lou",
          "openalex_id": "A5112107091",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "K. Wang",
          "openalex_id": "A5063678937",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Yawen Duan",
          "openalex_id": "A5102008535",
          "orcid": "https://orcid.org/0000-0002-5124-1192",
          "institutions": [
            "University of Cambridge"
          ]
        },
        {
          "name": "Zhonghao He",
          "openalex_id": "A5038466261",
          "orcid": "https://orcid.org/0009-0001-5831-8885",
          "institutions": [
            "University of Cambridge"
          ]
        },
        {
          "name": "Lukas Vierling",
          "openalex_id": "A5120007953",
          "institutions": [
            "University of Oxford"
          ]
        },
        {
          "name": "Zhaowei Zhang",
          "openalex_id": "A5100683815",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "F. R. Zeng",
          "openalex_id": "A5020542482",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Juntao Dai",
          "openalex_id": "A5110937622",
          "orcid": "https://orcid.org/0000-0002-2315-573X",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Xuehai Pan",
          "openalex_id": "A5001682354",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Hua Xu",
          "openalex_id": "A5111404244",
          "orcid": "https://orcid.org/0009-0001-4243-7943",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Aidan O\u2019Gara",
          "openalex_id": "A5092715918",
          "orcid": "https://orcid.org/0000-0002-9471-4930",
          "institutions": [
            "Southern California University for Professional Studies",
            "University of Southern California"
          ]
        },
        {
          "name": "Kwan Yee Ng",
          "openalex_id": "A5060777264"
        },
        {
          "name": "Brian Tse",
          "openalex_id": "A5087031022"
        },
        {
          "name": "Jie Fu",
          "openalex_id": "A5101713885",
          "orcid": "https://orcid.org/0000-0002-4494-843X",
          "institutions": [
            "Hong Kong University of Science and Technology",
            "University of Hong Kong"
          ]
        },
        {
          "name": "S. McAleer",
          "openalex_id": "A5077119323",
          "orcid": "https://orcid.org/0000-0002-3148-7646",
          "institutions": [
            "Carnegie Mellon University"
          ]
        },
        {
          "name": "Yanfeng Wang",
          "openalex_id": "A5100645705",
          "orcid": "https://orcid.org/0000-0002-3196-2347",
          "institutions": [
            "Shanghai Jiao Tong University"
          ]
        },
        {
          "name": "Mingchuan Yang",
          "openalex_id": "A5104113468",
          "institutions": [
            "China Telecom (China)"
          ]
        },
        {
          "name": "Yunhuai Liu",
          "openalex_id": "A5082653046",
          "orcid": "https://orcid.org/0000-0002-1180-8078",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Yizhou Wang",
          "openalex_id": "A5100602395",
          "orcid": "https://orcid.org/0000-0001-9888-6409",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Song-Chun Zhu",
          "openalex_id": "A5031660884",
          "orcid": "https://orcid.org/0009-0009-9458-5583",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Yike Guo",
          "openalex_id": "A5051748619",
          "institutions": [
            "Hong Kong University of Science and Technology",
            "University of Hong Kong"
          ]
        },
        {
          "name": "Yaodong Yang",
          "openalex_id": "A5090073634",
          "orcid": "https://orcid.org/0000-0001-8132-5613",
          "institutions": [
            "Peking University"
          ]
        },
        {
          "name": "Wen Gao",
          "openalex_id": "A5101523804",
          "orcid": "https://orcid.org/0000-0001-8894-1806",
          "institutions": [
            "Peking University"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-10-15",
      "abstract": "AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, so do risks from misalignment. To provide a comprehensive and up-to-date overview of the alignment field, in this survey, we delve into the core concepts, methodology, and practice of alignment. First, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality ( RICE ). Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment . The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems\u2019 alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss techniques for learning from feedback and learning under the distribution shift. Specifically, we survey traditional preference modeling methods and reinforcement learning from human feedback and further discuss potential frameworks to reach scalable oversight for tasks where effective human oversight is hard to obtain. Within learning under distribution shift, we also cover data distribution interventions such as adversarial training that helps expand the distribution of training data and algorithmic interventions to combat goal misgeneralization. On backward alignment, we discuss assurance techniques and governance practices. Specifically, we survey assurance methods of AI systems throughout their lifecycle, covering safety evaluation, interpretability, and human value compliance. We discuss current and prospective governance practices adopted by governments, industry actors, and other third parties, aimed at managing existing and future AI risks. This survey aims to provide a comprehensive yet beginner-friendly review of alignment research topics. Based on this, we also release and continually update the website www.alignmentsurvey.com which features tutorials, collections of papers, blog posts, and other resources.",
      "cited_by_count": 5,
      "type": "review",
      "source": {
        "name": "ACM Computing Surveys",
        "type": "journal",
        "issn": [
          "0360-0300",
          "1557-7341"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Reinforcement Learning in Robotics",
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 67,
      "url": "https://openalex.org/W4415207023"
    },
    {
      "openalex_id": "W4399043755",
      "doi": "10.22260/isarc2024/0059",
      "title": "Comparative Analysis of Cognitive Agreement between Human Analysts and Generative AI in Construction Safety Risk Assessment",
      "authors": [
        {
          "name": "U. Ray",
          "openalex_id": "A5113229791"
        },
        {
          "name": "Cristian Arteaga",
          "openalex_id": "A5027508368",
          "orcid": "https://orcid.org/0000-0002-4852-0211"
        },
        {
          "name": "JeeWoong Park",
          "openalex_id": "A5051907793",
          "orcid": "https://orcid.org/0000-0003-2205-6585"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-05-27",
      "abstract": "Comparative Analysis of Cognitive Agreement between Human Analysts and Generative AI in Construction Safety Risk Assessment Unmesa Ray, Cristian Arteaga, Jee Woong Park Pages 452-458 (2024 Proceedings of the 41st ISARC, Lille, France, ISBN 978-0-6458322-1-1, ISSN 2413-5844) Abstract: The construction industry struggles with safety risk assessment complexities due to evolving work environments, diverse labor forces, time constraints, regulatory intricacies, and inconsistent practices. While previous studies have highlighted the potential of Artificial Intelligence (AI) in automating processes and enhancing safety assessment, a gap exist in convergence between human analyst and language AI models. Therefore, this study seeks to assess the alignment in identification of risk factors by human analysts and a Language Model (LM) in Occupational Safety and Health Administration (OSHA) accident reports. Furthermore, it offers to: 1) categorize error types, 2) establish an acceptance threshold for LM-generated responses, and 3) evaluate inter-rater reliability in construction accident content analysis. Test results reveal significant convergence between human and machine responses and identifies potential hallucination effects in generative AI, thus paving the way for improved safety risk assessments within the construction industry. Keywords: construction industry, safety risk assessment, Artificial Intelligence (AI), Occupational Safety and Health Administration (OSHA), Language Model (LM), inter-rater reliability, generative AI DOI: https://doi.org/10.22260/ISARC2024/0059 Download fulltext Download BibTex Download Endnote (RIS) TeX Import to Mendeley",
      "cited_by_count": 5,
      "type": "article",
      "source": {
        "name": "Proceedings of the ... ISARC",
        "type": "journal",
        "issn": [
          "2413-5844"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Occupational Health and Safety Research"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4399043755"
    },
    {
      "openalex_id": "W4384345674",
      "doi": "10.1109/icse48619.2023.00012",
      "title": "Software Engineering as the Linchpin of Responsible AI",
      "authors": [
        {
          "name": "Liming Zhu",
          "openalex_id": "A5064683660",
          "orcid": "https://orcid.org/0000-0001-5839-3765",
          "institutions": [
            "Commonwealth Scientific and Industrial Research Organisation",
            "Data61",
            "UNSW Sydney"
          ]
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-05-01",
      "abstract": "From humanity's existential risks to safety risks in critical systems to ethical risks, responsible AI, as the saviour, has become a major research challenge with significant real-world consequences. However, achieving responsible AI remains elusive despite the plethora of high-level ethical principles, risk frameworks and progress in algorithmic assurance. In the meantime, software engineering (SE) is being upended by AI, grappling with building system-level quality and alignment from inscrutable machine learning models and code generated from natural language prompts. The upending poses new challenges and opportunities for engineering AI systems responsibly. This talk will share our experiences in helping the industry achieve responsible AI systems by inventing new SE approaches. It will dive into industry challenges (such as risk silos and principle-algorithm gaps) and research challenges (such as lack of requirements, emerging properties and inscrutable systems) and make the point that SE is the linchpin of responsible AI. But SE also requires some fundamental rethinking - shifting from building functions into AI systems to discovering and managing emerging functions from AI systems. Only by doing so can SE take on critical new roles, from understanding human intelligence to building a thriving human-AI symbiosis.",
      "cited_by_count": 6,
      "type": "article",
      "source": null,
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Adversarial Robustness in Machine Learning",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 4,
      "url": "https://openalex.org/W4384345674"
    },
    {
      "openalex_id": "W4391610969",
      "doi": "10.5694/mja2.52225",
      "title": "Artificial intelligence for surgical services in Australia and New Zealand: opportunities, challenges and recommendations",
      "authors": [
        {
          "name": "Joshua G. Kovoor",
          "openalex_id": "A5028007853",
          "orcid": "https://orcid.org/0000-0002-3880-3840",
          "institutions": [
            "Ballarat Health Services",
            "The University of Adelaide"
          ]
        },
        {
          "name": "Stephen Bacchi",
          "openalex_id": "A5008155440",
          "orcid": "https://orcid.org/0000-0001-5130-8628",
          "institutions": [
            "Lyell McEwin Hospital"
          ]
        },
        {
          "name": "Prakriti Sharma",
          "openalex_id": "A5012172202",
          "orcid": "https://orcid.org/0009-0005-9168-8545",
          "institutions": [
            "Flinders University"
          ]
        },
        {
          "name": "Srishti Sharma",
          "openalex_id": "A5102614558",
          "institutions": [
            "Flinders University"
          ]
        },
        {
          "name": "Medhir Kumawat",
          "openalex_id": "A5012563677",
          "orcid": "https://orcid.org/0000-0001-9767-8835",
          "institutions": [
            "The University of Adelaide"
          ]
        },
        {
          "name": "Brandon Stretton",
          "openalex_id": "A5016268035",
          "orcid": "https://orcid.org/0000-0002-7939-3489",
          "institutions": [
            "The University of Adelaide"
          ]
        },
        {
          "name": "Aashray Gupta",
          "openalex_id": "A5004836795",
          "orcid": "https://orcid.org/0000-0002-8038-0378",
          "institutions": [
            "The University of Adelaide"
          ]
        },
        {
          "name": "WengOnn Chan",
          "openalex_id": "A5057410818",
          "orcid": "https://orcid.org/0000-0003-2646-5677",
          "institutions": [
            "The University of Adelaide",
            "Queen Elizabeth Hospital"
          ]
        },
        {
          "name": "Amal Abou\u2010Hamden",
          "openalex_id": "A5107938382",
          "institutions": [
            "Royal Adelaide Hospital",
            "The University of Adelaide"
          ]
        },
        {
          "name": "Guy J. Maddern",
          "openalex_id": "A5086946129",
          "orcid": "https://orcid.org/0000-0003-2064-181X",
          "institutions": [
            "The University of Adelaide",
            "Queen Elizabeth Hospital"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-02-06",
      "abstract": "Artificial intelligence (AI) is being rapidly taken up by society, including health care services, and will inevitably be used broadly within the surgical services of Australia and New Zealand. However, the process of AI implementation must be evidence-based, safe, and ethically cautious,1 and must adhere to recommendations of the international surgical data science community.2 AI has numerous limitations and should always serve as an adjunct that benefits outcomes, rather than replacement of the staff within surgical systems. This perspective discusses opportunities and challenges for the use of AI in the surgical services of Australia and New Zealand and provides recommendations for the future. Non-surgical colleges across Australia and New Zealand have begun to discuss AI, but, at the time of writing, of the 25 specialist medical colleges within the Australian Medical Council, only three (12%) have published AI position statements.3-5 This will probably change with time, and surgical bodies should remain aware of current opinions held by their non-surgical counterparts. There is broad consensus that greater implementation of AI may increase care efficiency, potentially facilitating service provision across demographic groups and rural populations; although there also exists a risk of augmenting existing health inequity.5 Further, AI might improve diagnostic accuracy and efficiency, particularly for rare diseases.5 However, significant concern exists regarding ethical issues and other risks. AI presents potential liability implications arising from automation bias, and patient safety concerns from decreased sophistication in human decision making.3-7 As surgical decision making often carries substantial consequences, any degree of AI automation should first be scientifically interrogated and should be implemented only when there is demonstrable patient and system benefit, ensuring surgical staff roles are preserved.6 When integrating AI within surgical systems, it is important that the views of wider Australian and New Zealand society, a key stakeholder group, are also considered. A 2021 systematic review of the global literature on patient and general public attitudes to clinical AI found overall positive attitudes but many reservations and a preference for human supervision.8 However, an update of this review might produce different results, given how rapidly AI has been taken up by broader society through tools such as ChatGPT. Specifically for surgery, evidence is limited, but studies have found patient acceptability towards AI, apart from fully autonomous surgery.9 Although it is crucial to conduct serial assessments of Australian and New Zealand general public opinion to understand the current state of play, large studies such as the AVA-AI study provide reliable evaluations of national views.10 Within this large Australian survey, about 60% of respondents supported the development of AI, but this decreased to 27\u201343% when questions regarding specific health care scenarios were posed.10 A similar study of the Australian and New Zealand general public has not been conducted for the use of AI by surgical services specifically, and this presents an important gap for future research. The global AI boom presents opportunities to substantially enhance the quality and efficiency of Australia and New Zealand surgical services, both at the systems level and in the pre-operative, intra-operative and post-operative phases of individual patient journeys. At the systems level, locally developed AI tools, such as the Adelaide Score developed in South Australia, can potentially increase efficiency through providing additional predictive data relating to events that occur in each inpatient admission, such as hospital discharge.11 For individual surgical patients, AI tools can enhance clinical decision making at all time points, such as pre-operative informed consent and risk assessment, intra-operative precision and vision, and post-operative care for improved recovery and follow-up.12 However, when exploring these opportunities for surgical services, it is imperative that all AI tools are developed, validated and implemented using evidence-based and internationally accepted approaches, while also being trained with local data. This is challenging but crucial, regardless of the planned use of the AI tool. Currently, relevant evidence for AI use in Australian and New Zealand surgical services is mostly limited to early phase studies that might not reflect real-world practice. Prospective AI implementation trials within the real-world clinical context are required, and only after clear benefits are demonstrated for surgical patients and systems, they can be regularly used in Australian and New Zealand surgical services.1 There is currently a paucity of randomised trials regarding AI interventions, although this will change in coming years.13 Further, multiple statements have now been developed for reporting AI research and are listed by the EQUATOR network.14 It is crucial that similar frameworks for AI use are developed specifically for surgical services in Australia and New Zealand, so that the present opportunities can be explored safely, while also ensuring optimal benefit for local patients and systems. Until these are more developed, local surgical technology assessment organisations, such as ASERNIP-S within the Royal Australasian College of Surgeons (RACS),15 can assist with ensuring adherence to evidence-based principles during the uptake of AI by Australian and New Zealand surgical services. After opportunities for implementing AI are pursued by Australian and New Zealand surgical services, an additional challenge is ensuring appropriate and effective post-implementation monitoring and audit processes. These processes must evaluate ongoing outcome benefits to surgical patients, staff and systems, while also ensuring that local factors relevant to individual surgical services continue to be optimally considered. Adequate infrastructure for audit, retraining (if suboptimally providing benefit or considering local factors), and abandonment (if unresolvable surgical safety concerns are identified during audit) for surgical AI tools should be in place before the broad use of the tools within clinical practice. It is essential that surgical services also correspond and collaborate with local and national audit bodies, such as the Australian and New Zealand Audit of Surgical Mortality of the RACS, when using AI tools that could have significant outcome implications. Surgical patient safety and confidentiality must be maintained, particularly with increasing commercial interests in AI that may potentially conflict with the overall public good.16 As with any surgical innovation, AI must be rigorously audited and its use within the surgical services of Australia and New Zealand must always adhere to evidence-based principles.16 The integration of AI within Australia and New Zealand surgical services must be carefully regulated, and this presents an important challenge when clinical opportunities are being pursued. If done too rapidly and with little governance or consideration of necessary protection from profit-driven commercial entities, unpredicted risks could be conferred to future surgical patients. Surgical oversight bodies such as the RACS play a crucial role for this, but engagement with external regulatory bodies is also required. Novel AI devices may also be required to obtain pre-market approval from the Therapeutic Goods Administration (TGA) to be listed in the Australian Register of Therapeutic Goods before widespread implementation, which would allow surgical AI devices to be legally supplied in Australia.6 Technically, surgical devices that incorporate AI are classified as software as a medical device (SaMD), regulated by the TGA. The TGA is also a founding member of the International Medical Device Regulators Forum, therefore operating with the goal of remaining consistent with the International Medical Device Regulators Forum regarding the regulation of SaMDs. The Australian Commission on Safety and Quality in Health Care also provides standards for AI and automated decision making in the health care sector.17 From a legislative point of view within Australia, both the Therapeutic Goods Act 1989 and the Therapeutic Goods (Medical Devices) Regulation 2002 specify the conformity assessment procedures, classification rules and principles for regulation in Australia, which also apply to AI in surgical systems. Further, bodies such as the Australian Ethical Health Alliance and the Australian Alliance for Artificial Intelligence in Healthcare also provide guidance for regulating AI that is applicable to Australian and New Zealand surgical systems.18 Overall, there is a need for regulatory authorities to provide a clear framework specifically for the use of AI by surgical services within Australia and New Zealand. Alongside the challenges raised, substantial ethical dangers accompany AI, and caution must be applied when exploring opportunities to integrate this technology to assist surgical care.19 Overriding principles such as \"first, do no harm\" must always be upheld. AI algorithm performance is influenced by the characteristics of training datasets, and outcome benefits can differ depending on whether public data or real-world local data are used.20 Further, there may be bias in training data, which risks the AI algorithms perpetuating or exacerbating pre-existing inequalities (such as race, sex, age or socio-economic status), leading to discriminatory outcomes.21 Surgeons may treat a specific patient population due to the location of their institution or specialised professional interests, and algorithms trained on datasets derived at a population-level may perform suboptimally at a local level surgical services should be aware of potential biases in algorithms and limitations of training data and regularly audit AI-driven systems after local deployment. AI systems are frequently described as \"black boxes\" due to the absence of reproducible reasoning underpinning their decision-making processes.22 This lack of transparency may make it difficult for surgeons to have confidence in AI-assisted recommendations, particularly in circumstances where there are major differences between specialist surgeon opinion and AI. Furthermore, surgeons may simultaneously encounter difficulty in explaining these recommendations to patients, given that it may only be possible to explain the inputs and outputs of the algorithms rather than the internal processes.22 Australian and New Zealand surgical staff should become familiar with interpreting and transparently communicating the inputs and outputs of AI tools, encourage patients to ask questions and express concerns, and provide information to aid patient-friendly explanations. In addition, this \"black box\" intermediary may also make informed consent more challenging, although similar concessions are made when prescribing efficacious medications with mechanisms of action that are not entirely understood.23 Maintaining the shared decision-making process while using AI algorithms may similarly prove challenging, as they often do not currently account for patient values.24 Surgical staff should be vigilant when using AI-generated recommendations and ensure alignment with patient interests while also informing patients about factors considered and excluded by AI decision-support tools. Surgical decision making is complex, which adds challenges to liability and accountability when AI is used by surgical services.24 Current malpractice guidelines are unlikely to adequately consider the complexities introduced by AI, making it difficult to determine responsibility and assign blame where human and AI factors both contribute to adverse surgical outcomes.25 Surgical staff should refrain from substituting their clinical expertise with AI-based recommendations, instead using AI as an adjunct tool to increase the quality of their care.26 Given the known effect of automation bias,27 it should be acknowledged that this suggestion may not be feasible in all surgical scenarios. When using AI tools, surgical staff should comprehensively document their rationale for all clinical decision making, particularly any deviations from AI recommendations. Similarly, it is imperative that surgical staff comprehend the ethical implications for patient privacy and confidentiality when AI is integrated within their service. As clinical AI systems are likely to require sensitive patient data, surgical staff must adequately inform patients, including risks such as misuse or data breaches. Patients must be educated about the possibility that certain AI systems may use these inputs for continuous model training or retain their information for future reference.28 Surgical staff must also offer opt-out processes where feasible. Ultimately, clear policies must be established for the AI handling of sensitive patient information within the surgical systems of Australia and New Zealand. The surgical services of Australia and New Zealand currently have an opportunity to become international leaders in the safe, reliable and effective use of AI. Recommendations to seize this opportunity and overcome relevant challenges are provided in the Box. An evidence-based approach must be maintained, and the significant ethical concerns always addressed. As with any new surgical technology, there must be careful critical appraisal, regulation and post-implementation monitoring and audit. It is important to emphasise that AI is an assistive technology to help, not replace, surgical staff and patients. Efforts should be made to educate surgical patients and staff in Australia and New Zealand on the use of AI, including its benefits and limitations. There is the need for further guideline statements to provide frameworks specifically relevant to the use of AI by surgical services across Australia and New Zealand, and for broad collaboration between these services during the widespread uptake of AI to ensure safety at a national scale. Open access publishing facilitated by The University of Adelaide, as part of the Wiley \u2013 The University of Adelaide agreement via the Council of Australian University Librarians. No relevant disclosures. Not commissioned; externally peer reviewed.",
      "cited_by_count": 12,
      "type": "article",
      "source": {
        "name": "The Medical Journal of Australia",
        "type": "journal",
        "issn": [
          "0025-729X",
          "1326-5377"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.5694/mja2.52225"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Medical Imaging and Analysis",
        "Surgical Simulation and Training"
      ],
      "referenced_works_count": 27,
      "url": "https://openalex.org/W4391610969"
    },
    {
      "openalex_id": "W4387892231",
      "doi": "10.48550/arxiv.2310.13625",
      "title": "Oversight for Frontier AI through a Know-Your-Customer Scheme for Compute Providers",
      "authors": [
        {
          "name": "J. Egan",
          "openalex_id": "A5111061959"
        },
        {
          "name": "Lennart Heim",
          "openalex_id": "A5093115699"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-10-20",
      "abstract": "To address security and safety risks stemming from highly capable artificial intelligence (AI) models, we propose that the US government should ensure compute providers implement Know-Your-Customer (KYC) schemes. Compute - the computational power and infrastructure required to train and run these AI models - is emerging as a node for oversight. KYC, a standard developed by the banking sector to identify and verify client identity, could provide a mechanism for greater public oversight of frontier AI development and close loopholes in existing export controls. Such a scheme has the potential to identify and warn stakeholders of potentially problematic and/or sudden advancements in AI capabilities, build government capacity for AI regulation, and allow for the development and implementation of more nuanced and targeted export controls. Unlike the strategy of limiting access to AI chip purchases, regulating the digital access to compute offers more precise controls, allowing regulatory control over compute quantities, as well as the flexibility to suspend access at any time. To enact a KYC scheme, the US government will need to work closely with industry to (1) establish a dynamic threshold of compute that effectively captures high-risk frontier model development, while minimizing imposition on developers not engaged in frontier AI; (2) set requirements and guidance for compute providers to keep records and report high-risk entities; (3) establish government capacity that allows for co-design, implementation, administration and enforcement of the scheme; and (4) engage internationally to promote international alignment with the scheme and support its long-term efficacy. While the scheme will not address all AI risks, it complements proposed solutions by allowing for a more precise and flexible approach to controlling the development of frontier AI models and unwanted AI proliferation.",
      "cited_by_count": 7,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2310.13625"
      },
      "topics": [
        "Blockchain Technology Applications and Security",
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4387892231"
    },
    {
      "openalex_id": "W4406961142",
      "doi": "10.48550/arxiv.2501.16946",
      "title": "Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development",
      "authors": [
        {
          "name": "Jan Kulveit",
          "openalex_id": "A5116078277"
        },
        {
          "name": "Roy Douglas",
          "openalex_id": "A5112795032"
        },
        {
          "name": "Nora Ammann",
          "openalex_id": "A5116078278"
        },
        {
          "name": "Deger Turan",
          "openalex_id": "A5116078279"
        },
        {
          "name": "David M. Krueger",
          "openalex_id": "A5040734589"
        },
        {
          "name": "David Duvenaud",
          "openalex_id": "A5116078280"
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-01-28",
      "abstract": "This paper examines the systemic risks posed by incremental advancements in artificial intelligence, developing the concept of `gradual disempowerment', in contrast to the abrupt takeover scenarios commonly discussed in AI safety. We analyze how even incremental improvements in AI capabilities can undermine human influence over large-scale systems that society depends on, including the economy, culture, and nation-states. As AI increasingly replaces human labor and cognition in these domains, it can weaken both explicit human control mechanisms (like voting and consumer choice) and the implicit alignments with human interests that often arise from societal systems' reliance on human participation to function. Furthermore, to the extent that these systems incentivise outcomes that do not line up with human preferences, AIs may optimize for those outcomes more aggressively. These effects may be mutually reinforcing across different domains: economic power shapes cultural narratives and political decisions, while cultural shifts alter economic and political behavior. We argue that this dynamic could lead to an effectively irreversible loss of human influence over crucial societal systems, precipitating an existential catastrophe through the permanent disempowerment of humanity. This suggests the need for both technical research and governance approaches that specifically address the risk of incremental erosion of human influence across interconnected societal systems.",
      "cited_by_count": 9,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2501.16946"
      },
      "topics": [
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4406961142"
    },
    {
      "openalex_id": "W4399861784",
      "doi": "10.1002/sres.3032",
      "title": "Application of systems thinking and system dynamics in managing risks and stakeholders in construction projects: A systematic literature review",
      "authors": [
        {
          "name": "Zahra Ghamarimajd",
          "openalex_id": "A5099296111",
          "orcid": "https://orcid.org/0009-0009-9808-3329",
          "institutions": [
            "Bond University"
          ]
        },
        {
          "name": "Amir Naser Ghanbaripour",
          "openalex_id": "A5090294218",
          "orcid": "https://orcid.org/0000-0002-5685-1627",
          "institutions": [
            "Bond University"
          ]
        },
        {
          "name": "Roksana Jahan Tumpa",
          "openalex_id": "A5085188851",
          "orcid": "https://orcid.org/0000-0003-4721-5300",
          "institutions": [
            "Central Queensland University"
          ]
        },
        {
          "name": "Tsunemi Watanabe",
          "openalex_id": "A5040035619",
          "orcid": "https://orcid.org/0000-0002-9893-1561",
          "institutions": [
            "Bond University"
          ]
        },
        {
          "name": "Jasper Mbachu",
          "openalex_id": "A5053429499",
          "orcid": "https://orcid.org/0000-0002-1654-311X",
          "institutions": [
            "Bond University"
          ]
        },
        {
          "name": "Martin Skitmore",
          "openalex_id": "A5004114267",
          "orcid": "https://orcid.org/0000-0001-7135-1201",
          "institutions": [
            "Central Queensland University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-06-20",
      "abstract": "Abstract This paper conducts a systematic literature review on systems thinking (ST) and system dynamics (SD) applications in construction project risk and stakeholder management over the past decade. It evaluates current practices' alignment with SD, addressing project complexity and uncertainty. Seventy\u2010three articles are analysed following Preferred Reporting Items for Systematic Reviews and Meta\u2010Analysis (PRISMA) guidelines, using descriptive, thematic and bibliometric analyses. Findings show fluctuating trends in ST and SD applications due to COVID\u201019 in 2019, with notable contributions from China, Australia and the UK. Building projects are the most studied, employing mixed methodologies. Thematic analysis highlights SD's significant role in system representation and risk management, with 80% of studies utilizing SD models for various risk factors. While less common in stakeholder management, SD enhances communication and understanding of stakeholder dynamics. The study advocates for integrating SD modelling in construction management for improved decision\u2010making, risk mitigation and stakeholder engagement for project management practitioners, urging collaboration between academia, industry and policymakers for effective construction policies. Academics and researchers should focus on standardizing SD modelling tools, exploring hybrid methodologies like agent\u2010based modelling, and integrating emerging technologies like artificial intelligence (AI) and the Internet of Things (IoT) to enhance real\u2010time decision\u2010making capabilities. Collaboration between academia, industry practitioners and policymakers is crucial to ensure SD research translates into effective policies and best practices, particularly in safety and environmental impact assessments.",
      "cited_by_count": 12,
      "type": "article",
      "source": {
        "name": "Systems Research and Behavioral Science",
        "type": "journal",
        "issn": [
          "1092-7026",
          "1099-1743"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/sres.3032"
      },
      "topics": [
        "Construction Project Management and Performance",
        "Occupational Health and Safety Research",
        "Supply Chain Resilience and Risk Management"
      ],
      "referenced_works_count": 82,
      "url": "https://openalex.org/W4399861784"
    },
    {
      "openalex_id": "W4396890086",
      "doi": "10.3389/fpubh.2024.1362246",
      "title": "The potential of virtual triage AI to improve early detection, care acuity alignment, and emergent care referral of life-threatening conditions",
      "authors": [
        {
          "name": "George A. Gellert",
          "openalex_id": "A5084724322",
          "orcid": "https://orcid.org/0000-0002-3519-7486"
        },
        {
          "name": "Aleksandra Kabat-Karabon",
          "openalex_id": "A5059087109"
        },
        {
          "name": "Gabriel L. Gellert",
          "openalex_id": "A5098533868"
        },
        {
          "name": "Joanna Ras\u0142awska-Socha",
          "openalex_id": "A5091254949",
          "orcid": "https://orcid.org/0000-0001-6461-5985"
        },
        {
          "name": "Stanis\u0142aw G\u00f3rski",
          "openalex_id": "A5005710301",
          "orcid": "https://orcid.org/0000-0002-1740-4082",
          "institutions": [
            "Jagiellonian University"
          ]
        },
        {
          "name": "Timothy Price",
          "openalex_id": "A5061460550",
          "orcid": "https://orcid.org/0000-0002-3922-2693",
          "institutions": [
            "InferMed"
          ]
        },
        {
          "name": "Kacper Kuszczy\u0144ski",
          "openalex_id": "A5092783026"
        },
        {
          "name": "Natalia Marcjasz",
          "openalex_id": "A5054529532",
          "orcid": "https://orcid.org/0000-0003-0036-5985"
        },
        {
          "name": "Mateusz Palczewski",
          "openalex_id": "A5081119477",
          "orcid": "https://orcid.org/0000-0002-9404-8149",
          "institutions": [
            "Wroclaw Medical University"
          ]
        },
        {
          "name": "Jakub Jaszczak",
          "openalex_id": "A5010252004",
          "orcid": "https://orcid.org/0000-0002-9651-094X"
        },
        {
          "name": "Irving K. Loh",
          "openalex_id": "A5098621993"
        },
        {
          "name": "Piotr M. Orzechowski",
          "openalex_id": "A5009209707"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-05-13",
      "abstract": "Objective To evaluate the extent to which patient-users reporting symptoms of five severe/acute conditions requiring emergency care to an AI-based virtual triage (VT) engine had no intention to get such care, and whose acuity perception was misaligned or decoupled from actual risk of life-threatening symptoms. Methods A dataset of 3,022,882 VT interviews conducted over 16 months was evaluated to quantify and describe patient-users reporting symptoms of five potentially life-threatening conditions whose pre-triage healthcare intention was other than seeking urgent care, including myocardial infarction, stroke, asthma exacerbation, pneumonia, and pulmonary embolism. Results Healthcare intent data was obtained for 12,101 VT patient-user interviews. Across all five conditions a weighted mean of 38.5% of individuals whose VT indicated a condition requiring emergency care had no pre-triage intent to consult a physician. Furthermore, 61.5% intending to possibly consult a physician had no intent to seek emergency medical care. After adjustment for 13% VT safety over-triage/referral to ED, a weighted mean of 33.5% of patient-users had no intent to seek professional care, and 53.5% had no intent to seek emergency care. Conclusion AI-based VT may offer a vehicle for early detection and care acuity alignment of severe evolving pathology by engaging patients who believe their symptoms are not serious, and for accelerating care referral and delivery for life-threatening conditions where patient misunderstanding of risk, or indecision, causes care delay. A next step will be clinical confirmation that when decoupling of patient care intent from emergent care need occurs, VT can influence patient behavior to accelerate care engagement and/or emergency care dispatch and treatment to improve clinical outcomes.",
      "cited_by_count": 10,
      "type": "article",
      "source": {
        "name": "Frontiers in Public Health",
        "type": "journal",
        "issn": [
          "2296-2565"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2024.1362246/pdf"
      },
      "topics": [
        "Emergency and Acute Care Studies",
        "Autopsy Techniques and Outcomes",
        "Trauma and Emergency Care Studies"
      ],
      "referenced_works_count": 33,
      "url": "https://openalex.org/W4396890086"
    },
    {
      "openalex_id": "W4414333247",
      "doi": "10.1038/s41467-025-63913-1",
      "title": "Risks of AI scientists: prioritizing safeguarding over autonomy",
      "authors": [
        {
          "name": "Xiangru Tang",
          "openalex_id": "A5108999586",
          "orcid": "https://orcid.org/0009-0006-2700-4513",
          "institutions": [
            "Yale University"
          ]
        },
        {
          "name": "Qiao Jin",
          "openalex_id": "A5100611571",
          "orcid": "https://orcid.org/0000-0002-1268-7239",
          "institutions": [
            "National Institutes of Health",
            "United States National Library of Medicine"
          ]
        },
        {
          "name": "Kunlun Zhu",
          "openalex_id": "A5101074704",
          "orcid": "https://orcid.org/0009-0009-9107-7401",
          "institutions": [
            "Mila - Quebec Artificial Intelligence Institute"
          ]
        },
        {
          "name": "Tongxin Yuan",
          "openalex_id": "A5113115853",
          "institutions": [
            "Shanghai Jiao Tong University"
          ]
        },
        {
          "name": "Yichi Zhang",
          "openalex_id": "A5100444201",
          "orcid": "https://orcid.org/0000-0003-3214-1070",
          "institutions": [
            "Yale University"
          ]
        },
        {
          "name": "Wangchunshu Zhou",
          "openalex_id": "A5074508950",
          "orcid": "https://orcid.org/0000-0003-4668-3348"
        },
        {
          "name": "Meng Qu",
          "openalex_id": "A5100739955",
          "orcid": "https://orcid.org/0000-0002-3343-8952",
          "institutions": [
            "Mila - Quebec Artificial Intelligence Institute"
          ]
        },
        {
          "name": "Yilun Zhao",
          "openalex_id": "A5103004488",
          "orcid": "https://orcid.org/0000-0002-7470-6124",
          "institutions": [
            "Yale University"
          ]
        },
        {
          "name": "J. Tang",
          "openalex_id": "A5100708602",
          "orcid": "https://orcid.org/0000-0002-2926-2560",
          "institutions": [
            "Mila - Quebec Artificial Intelligence Institute"
          ]
        },
        {
          "name": "Zhuosheng Zhang",
          "openalex_id": "A5070962435",
          "orcid": "https://orcid.org/0000-0002-4183-3645",
          "institutions": [
            "Shanghai Jiao Tong University"
          ]
        },
        {
          "name": "Arman Cohan",
          "openalex_id": "A5064858748",
          "orcid": "https://orcid.org/0000-0002-8954-2724",
          "institutions": [
            "Yale University"
          ]
        },
        {
          "name": "Dov Greenbaum",
          "openalex_id": "A5004593292",
          "orcid": "https://orcid.org/0000-0003-4440-3090",
          "institutions": [
            "Herzliya Medical Center",
            "Yale University"
          ]
        },
        {
          "name": "Zhiyong Lu",
          "openalex_id": "A5083081872",
          "orcid": "https://orcid.org/0000-0001-9998-916X",
          "institutions": [
            "National Institutes of Health",
            "United States National Library of Medicine"
          ]
        },
        {
          "name": "Mark Gerstein",
          "openalex_id": "A5042321575",
          "orcid": "https://orcid.org/0000-0002-9746-3719",
          "institutions": [
            "Yale University"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-09-18",
      "abstract": "Abstract AI scientists powered by large language models have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, these agents also introduce novel vulnerabilities that require careful consideration for safety. However, there has been limited comprehensive exploration of these vulnerabilities. This perspective examines vulnerabilities in AI scientists, shedding light on potential risks associated with their misuse, and emphasizing the need for safety measures. We begin by providing an overview of the potential risks inherent to AI scientists, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we explore the underlying causes of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding AI scientists and advocate for the development of improved models, robust benchmarks, and comprehensive regulations.",
      "cited_by_count": 4,
      "type": "review",
      "source": {
        "name": "Nature Communications",
        "type": "journal",
        "issn": [
          "2041-1723"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://doi.org/10.1038/s41467-025-63913-1"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Ethics and Social Impacts of AI",
        "Ethics in Clinical Research"
      ],
      "referenced_works_count": 46,
      "url": "https://openalex.org/W4414333247"
    },
    {
      "openalex_id": "W4403389851",
      "doi": "10.3390/electronics13204044",
      "title": "Towards an End-to-End Personal Fine-Tuning Framework for AI Value Alignment",
      "authors": [
        {
          "name": "Eleanor Watson",
          "openalex_id": "A5072725942",
          "orcid": "https://orcid.org/0000-0002-4306-7577",
          "institutions": [
            "University of Gloucestershire"
          ]
        },
        {
          "name": "Thiago Viana",
          "openalex_id": "A5011245055",
          "orcid": "https://orcid.org/0000-0001-9380-4611",
          "institutions": [
            "University of Gloucestershire"
          ]
        },
        {
          "name": "Shujun Zhang",
          "openalex_id": "A5100615526",
          "orcid": "https://orcid.org/0000-0001-5699-2676",
          "institutions": [
            "University of Gloucestershire"
          ]
        },
        {
          "name": "Benjamin Sturgeon",
          "openalex_id": "A5114262866",
          "institutions": [
            "University of Cape Town"
          ]
        },
        {
          "name": "L.\u2010G. Petersson",
          "openalex_id": "A5112616286",
          "institutions": [
            "Lund University"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-10-14",
      "abstract": "This study introduces a novel architecture for value, preference, and boundary alignment in large language models (LLMs) and generative AI systems, accompanied by an experimental implementation. It addresses the limitations in AI model trustworthiness stemming from insufficient comprehension of personal context, preferences, and cultural diversity, which can lead to biases and safety risks. Using an inductive, qualitative research approach, we propose a framework for personalizing AI models to improve model alignment through additional context and boundaries set by users. Our framework incorporates user-friendly tools for identification, annotation, and simulation across diverse contexts, utilizing prompt-driven semantic segmentation and automatic labeling. It aims to streamline scenario generation and personalization processes while providing accessible annotation tools. The study examines various components of this framework, including user interfaces, underlying tools, and system mechanics. We present a pilot study that demonstrates the framework\u2019s ability to reduce the complexity of value elicitation and personalization in LLMs. Our experimental setup involves a prototype implementation of key framework modules, including a value elicitation interface and a fine-tuning mechanism for language models. The primary goal is to create a token-based system that allows users to easily impart their values and preferences to AI systems, enhancing model personalization and alignment. This research contributes to the democratization of AI model fine-tuning and dataset generation, advancing efforts in AI value alignment. By focusing on practical implementation and user interaction, our study bridges the gap between theoretical alignment approaches and real-world applications in AI systems.",
      "cited_by_count": 4,
      "type": "article",
      "source": {
        "name": "Electronics",
        "type": "journal",
        "issn": [
          "2079-9292"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2079-9292/13/20/4044/pdf?version=1728909885"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Ethics and Social Impacts of AI",
        "Topic Modeling"
      ],
      "referenced_works_count": 20,
      "url": "https://openalex.org/W4403389851"
    },
    {
      "openalex_id": "W4385208638",
      "doi": "10.48550/arxiv.2307.11137",
      "title": "Of Models and Tin Men: A Behavioural Economics Study of Principal-Agent Problems in AI Alignment using Large-Language Models",
      "authors": [
        {
          "name": "Steve Phelps",
          "openalex_id": "A5111436535"
        },
        {
          "name": "Rebecca Ranson",
          "openalex_id": "A5056952213"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-07-20",
      "abstract": "AI Alignment is often presented as an interaction between a single designer and an artificial agent in which the designer attempts to ensure the agent's behavior is consistent with its purpose, and risks arise solely because of conflicts caused by inadvertent misalignment between the utility function intended by the designer and the resulting internal utility function of the agent. With the advent of agents instantiated with large-language models (LLMs), which are typically pre-trained, we argue this does not capture the essential aspects of AI safety because in the real world there is not a one-to-one correspondence between designer and agent, and the many agents, both artificial and human, have heterogeneous values. Therefore, there is an economic aspect to AI safety and the principal-agent problem is likely to arise. In a principal-agent problem conflict arises because of information asymmetry together with inherent misalignment between the utility of the agent and its principal, and this inherent misalignment cannot be overcome by coercing the agent into adopting a desired utility function through training. We argue the assumptions underlying principal-agent problems are crucial to capturing the essence of safety problems involving pre-trained AI models in real-world situations. Taking an empirical approach to AI safety, we investigate how GPT models respond in principal-agent conflicts. We find that agents based on both GPT-3.5 and GPT-4 override their principal's objectives in a simple online shopping task, showing clear evidence of principal-agent conflict. Surprisingly, the earlier GPT-3.5 model exhibits more nuanced behaviour in response to changes in information asymmetry, whereas the later GPT-4 model is more rigid in adhering to its prior alignment. Our results highlight the importance of incorporating principles from economics into the alignment process.",
      "cited_by_count": 4,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2307.11137"
      },
      "topics": [
        "Explainable Artificial Intelligence (XAI)",
        "Forecasting Techniques and Applications",
        "Topic Modeling"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4385208638"
    },
    {
      "openalex_id": "W4408690661",
      "doi": "10.70777/si.v1i1.10863",
      "title": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies",
      "authors": [
        {
          "name": "Yi Zeng",
          "openalex_id": "A5100458994",
          "orcid": "https://orcid.org/0000-0002-7708-7146"
        },
        {
          "name": "Yang Yu",
          "openalex_id": "A5007723883",
          "orcid": "https://orcid.org/0000-0002-6708-9885"
        },
        {
          "name": "Andy Zhou",
          "openalex_id": "A5114183559"
        },
        {
          "name": "John Tan",
          "openalex_id": "A5111275941"
        },
        {
          "name": "Ya Tu",
          "openalex_id": "A5057166109",
          "orcid": "https://orcid.org/0000-0001-5081-5329"
        },
        {
          "name": "Yifan Mai",
          "openalex_id": "A5006623234",
          "orcid": "https://orcid.org/0000-0002-4807-037X"
        },
        {
          "name": "Kevin Klyman",
          "openalex_id": "A5093104547",
          "orcid": "https://orcid.org/0009-0003-2130-3529"
        },
        {
          "name": "Minzhou Pan",
          "openalex_id": "A5113268603"
        },
        {
          "name": "Ruoxi Jia",
          "openalex_id": "A5032275274",
          "orcid": "https://orcid.org/0000-0001-9662-9556"
        },
        {
          "name": "Dawn Song",
          "openalex_id": "A5102160255"
        },
        {
          "name": "Percy Liang",
          "openalex_id": "A5025255782",
          "orcid": "https://orcid.org/0000-0002-0458-6139"
        },
        {
          "name": "Bo Li",
          "openalex_id": "A5100374474",
          "orcid": "https://orcid.org/0000-0002-9336-1862"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-09-23",
      "abstract": "Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-Bench 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in our AI risks study, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-Bench 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-Bench 2024, uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-Bench 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems.",
      "cited_by_count": 3,
      "type": "article",
      "source": {
        "name": "SuperIntelligence - Robotics - Safety & Alignment",
        "type": "journal",
        "issn": [
          "3067-2627"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://doi.org/10.70777/si.v1i1.10863"
      },
      "topics": [
        "Risk and Safety Analysis"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4408690661"
    },
    {
      "openalex_id": "W4408186943",
      "doi": "10.3390/technologies13030107",
      "title": "Harnessing Metacognition for Safe and Responsible AI",
      "authors": [
        {
          "name": "Peter B. Walker",
          "openalex_id": "A5103368788",
          "orcid": "https://orcid.org/0000-0003-0746-6295",
          "institutions": [
            "Defense Health Agency"
          ]
        },
        {
          "name": "Jonathan Haase",
          "openalex_id": "A5080449654",
          "orcid": "https://orcid.org/0000-0001-8891-6512",
          "institutions": [
            "Evergreen Health Medical Center"
          ]
        },
        {
          "name": "Melissa L. Mehalick",
          "openalex_id": "A5080836385",
          "orcid": "https://orcid.org/0000-0002-6295-4307",
          "institutions": [
            "Defense Health Agency"
          ]
        },
        {
          "name": "Christopher T. Steele",
          "openalex_id": "A5104308933"
        },
        {
          "name": "Dale W. Russell",
          "openalex_id": "A5035155600",
          "orcid": "https://orcid.org/0000-0003-4289-1270",
          "institutions": [
            "Uniformed Services University of the Health Sciences"
          ]
        },
        {
          "name": "Ian N. Davidson",
          "openalex_id": "A5103494040",
          "institutions": [
            "University of California, Davis"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-03-06",
      "abstract": "The rapid advancement of artificial intelligence (AI) technologies has transformed various sectors, significantly enhancing processes and augmenting human capabilities. However, these advancements have also introduced critical concerns related to the safety, ethics, and responsibility of AI systems. To address these challenges, the principles of the robustness, interpretability, controllability, and ethical alignment framework are essential. This paper explores the integration of metacognition\u2014defined as \u201cthinking about thinking\u201d\u2014into AI systems as a promising approach to meeting these requirements. Metacognition enables AI systems to monitor, control, and regulate the system\u2019s cognitive processes, thereby enhancing their ability to self-assess, correct errors, and adapt to changing environments. By embedding metacognitive processes within AI, this paper proposes a framework that enhances the transparency, accountability, and adaptability of AI systems, fostering trust and mitigating risks associated with autonomous decision-making. Additionally, the paper examines the current state of AI safety and responsibility, discusses the applicability of metacognition to AI, and outlines a mathematical framework for incorporating metacognitive strategies into active learning processes. The findings aim to contribute to the development of safe, responsible, and ethically aligned AI systems.",
      "cited_by_count": 4,
      "type": "article",
      "source": {
        "name": "Technologies",
        "type": "journal",
        "issn": [
          "2227-7080"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://www.mdpi.com/2227-7080/13/3/107/pdf?version=1741272483"
      },
      "topics": [
        "Adversarial Robustness in Machine Learning"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4408186943"
    },
    {
      "openalex_id": "W4400503906",
      "doi": "10.3846/tede.2024.21525",
      "title": "Existential risk from transformative AI: an economic perspective",
      "authors": [
        {
          "name": "Jakub Growiec",
          "openalex_id": "A5089189909",
          "orcid": "https://orcid.org/0000-0003-2222-1691",
          "institutions": [
            "SGH Warsaw School of Economics"
          ]
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-07-10",
      "abstract": "The prospective arrival of transformative artificial intelligence (TAI) will be a filter for the human civilization \u2013 a threshold beyond which it will either strongly accelerate its growth, or vanish. Historical evidence on technological progress in AI capabilities and economic incentives to pursue it suggest that TAI will most likely be developed in just one to four decades. In contrast, theoretical problems of AI alignment, needed to be solved in order for TAI to be \u201cfriendly\u201d towards humans rather than cause our extinction, appear difficult and impossible to solve by mechanically increasing the amount of compute. This means that transformative AI poses an imminent existential risk to the humankind which ought to be urgently addressed. Starting from this premise, this paper provides new economic perspectives on discussions surrounding the issue: whether addressing existential risks is cost effective and fair towards the contemporary poor, whether it constitutes \u201cPascal\u2019s mugging\u201d, how to quantify risks that have never materialized in the past, how discounting affects our assessment of existential risk, and how to include the prospects of upcoming singularity in economic forecasts. The paper also suggests possible policy actions, such as ramping up public funding on research on existential risks and AI safety, and improving regulation of the AI sector, preferably within a global policy framework.",
      "cited_by_count": 4,
      "type": "article",
      "source": {
        "name": "Technological and Economic Development of Economy",
        "type": "journal",
        "issn": [
          "2029-4913",
          "2029-4921"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://journals.vilniustech.lt/index.php/TEDE/article/download/21525/12364"
      },
      "topics": [
        "Innovation, Sustainability, Human-Machine Systems",
        "Economic and Technological Innovation",
        "Leadership, Behavior, and Decision-Making Studies"
      ],
      "referenced_works_count": 46,
      "url": "https://openalex.org/W4400503906"
    },
    {
      "openalex_id": "W4409772922",
      "doi": "10.1002/jeo2.70247",
      "title": "A practical guide to the implementation of AI in orthopaedic research\u2014Part 7: Risks, limitations, safety and verification of medical AI systems",
      "authors": [
        {
          "name": "Philipp W. Winkler",
          "openalex_id": "A5002757615",
          "orcid": "https://orcid.org/0000-0002-3997-1010",
          "institutions": [
            "Kepler Universit\u00e4tsklinikum",
            "Sahlgrenska University Hospital",
            "University of Gothenburg"
          ]
        },
        {
          "name": "B\u00e1lint Zsidai",
          "openalex_id": "A5013071313",
          "orcid": "https://orcid.org/0000-0002-5697-6577",
          "institutions": [
            "Sahlgrenska University Hospital",
            "University of Gothenburg"
          ]
        },
        {
          "name": "Eric Hamrin Senorski",
          "openalex_id": "A5078675172",
          "orcid": "https://orcid.org/0000-0002-9340-0147",
          "institutions": [
            "Sahlgrenska University Hospital",
            "University of Gothenburg"
          ]
        },
        {
          "name": "James A. Pruneski",
          "openalex_id": "A5075897119",
          "orcid": "https://orcid.org/0000-0002-8645-9386",
          "institutions": [
            "Tripler Army Medical Center"
          ]
        },
        {
          "name": "Michael T. Hirschmann",
          "openalex_id": "A5081720804",
          "orcid": "https://orcid.org/0000-0002-4014-424X",
          "institutions": [
            "Kantonsspital Baselland Standort Bruderholz",
            "University of Basel"
          ]
        },
        {
          "name": "Christophe Ley",
          "openalex_id": "A5106110609",
          "orcid": "https://orcid.org/0000-0003-2751-8902",
          "institutions": [
            "University of Luxembourg"
          ]
        },
        {
          "name": "Thomas Tischer",
          "openalex_id": "A5039152729",
          "orcid": "https://orcid.org/0000-0002-3942-0235",
          "institutions": [
            "Malteser Waldkrankenhaus Erlangen",
            "Universit\u00e4tsmedizin Rostock"
          ]
        },
        {
          "name": "Elmar Herbst",
          "openalex_id": "A5014928084",
          "orcid": "https://orcid.org/0000-0002-5652-0692",
          "institutions": [
            "University Hospital M\u00fcnster"
          ]
        },
        {
          "name": "Ayoosh Pareek",
          "openalex_id": "A5066892142",
          "orcid": "https://orcid.org/0000-0001-8683-1697",
          "institutions": [
            "Hospital for Special Surgery"
          ]
        },
        {
          "name": "Volker Musahl",
          "openalex_id": "A5054255291",
          "orcid": "https://orcid.org/0000-0001-8881-6212",
          "institutions": [
            "Freddie Mac (United States)",
            "University of Pittsburgh"
          ]
        },
        {
          "name": "Jacob F. Oeding",
          "openalex_id": "A5017889826",
          "orcid": "https://orcid.org/0000-0002-4562-4373",
          "institutions": [
            "University of Gothenburg",
            "Mayo Clinic in Arizona"
          ]
        },
        {
          "name": "Felix C. Oettl",
          "openalex_id": "A5111300284",
          "institutions": [
            "University of Zurich",
            "Universit\u00e4tsklinik Balgrist"
          ]
        },
        {
          "name": "Umile Giuseppe Longo",
          "openalex_id": "A5080203239",
          "orcid": "https://orcid.org/0000-0003-4063-9821",
          "institutions": [
            "Campus Bio Medico University Hospital",
            "Universit\u00e0 Campus Bio-Medico"
          ]
        },
        {
          "name": "Kristian Samuelsson",
          "openalex_id": "A5053862149",
          "orcid": "https://orcid.org/0000-0001-5383-3370",
          "institutions": [
            "Sahlgrenska University Hospital",
            "University of Gothenburg"
          ]
        },
        {
          "name": "Robert Feldt",
          "openalex_id": "A5063787358",
          "orcid": "https://orcid.org/0000-0002-5179-4205",
          "institutions": [
            "Chalmers University of Technology"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-04-01",
      "abstract": "Abstract Artificial intelligence (AI) has been influencing healthcare and medical research for several years and will likely become indispensable in the near future. AI is intended to support healthcare professionals to make the healthcare system more efficient and ultimately improve patient outcomes. Despite the numerous benefits of AI systems, significant concerns remain. Errors in AI systems can pose serious risks to human health, underscoring the critical need for safety, as well as adherence to ethical and moral standards, before these technologies can be integrated into clinical practice. To address these challenges, the development, certification, and deployment of medical AI systems must adhere to strict and transparent regulations. The European Commission has already established a regulatory framework for AI systems by enacting the European Union Artificial Intelligence Act. This review article, part of an AI learning series, discusses key considerations for medical AI systems such as reliability, accuracy, trustworthiness, lawfulness and legal compliance, ethical and moral alignment, sustainability, and regulatory oversight. Level of Evidence Level V.",
      "cited_by_count": 4,
      "type": "review",
      "source": {
        "name": "Journal of Experimental Orthopaedics",
        "type": "journal",
        "issn": [
          "2197-1153"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/jeo2.70247"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Autopsy Techniques and Outcomes",
        "Healthcare cost, quality, practices"
      ],
      "referenced_works_count": 37,
      "url": "https://openalex.org/W4409772922"
    },
    {
      "openalex_id": "W4389768521",
      "doi": "10.48550/arxiv.2312.08039",
      "title": "Safeguarding the safeguards: How best to promote AI alignment in the public interest",
      "authors": [
        {
          "name": "Oliver Guest",
          "openalex_id": "A5111093575"
        },
        {
          "name": "Michael Aird",
          "openalex_id": "A5093038739"
        },
        {
          "name": "Se\u00e1n \u00d3 h\u00c9igeartaigh",
          "openalex_id": "A5033871638",
          "orcid": "https://orcid.org/0000-0002-2846-1576"
        }
      ],
      "publication_year": 2023,
      "publication_date": "2023-12-13",
      "abstract": "AI alignment work is important from both a commercial and a safety lens. With this paper, we aim to help actors who support alignment efforts to make these efforts as effective as possible, and to avoid potential adverse effects. We begin by suggesting that institutions that are trying to act in the public interest (such as governments) should aim to support specifically alignment work that reduces accident or misuse risks. We then describe four problems which might cause alignment efforts to be counterproductive, increasing large-scale AI risks. We suggest mitigations for each problem. Finally, we make a broader recommendation that institutions trying to act in the public interest should think systematically about how to make their alignment efforts as effective, and as likely to be beneficial, as possible.",
      "cited_by_count": 2,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2312.08039"
      },
      "topics": [
        "Law, AI, and Intellectual Property",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4389768521"
    },
    {
      "openalex_id": "W4390898080",
      "doi": "10.48550/arxiv.2401.06730",
      "title": "Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty",
      "authors": [
        {
          "name": "Kaitlyn Zhou",
          "openalex_id": "A5031278789",
          "orcid": "https://orcid.org/0000-0001-8804-8161"
        },
        {
          "name": "Jena D. Hwang",
          "openalex_id": "A5080544237",
          "orcid": "https://orcid.org/0000-0003-3801-294X"
        },
        {
          "name": "Xiang Ren",
          "openalex_id": "A5009408707",
          "orcid": "https://orcid.org/0000-0001-8655-663X"
        },
        {
          "name": "Maarten Sap",
          "openalex_id": "A5015128745",
          "orcid": "https://orcid.org/0000-0002-0701-4654"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-01-12",
      "abstract": "As natural language becomes the default interface for human-AI interaction, there is a need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence in responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are reluctant to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (an average of 47%) among confident responses. We test the risks of LM overconfidence by conducting human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in post training alignment and find that humans are biased against texts with uncertainty. Our work highlights new safety harms facing human-LM interactions and proposes design recommendations and mitigating strategies moving forward.",
      "cited_by_count": 7,
      "type": "preprint",
      "source": {
        "name": "arXiv (Cornell University)",
        "type": "repository",
        "issn": null
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "green",
        "oa_url": "https://arxiv.org/pdf/2401.06730"
      },
      "topics": [
        "Topic Modeling",
        "Ethics and Social Impacts of AI",
        "Explainable Artificial Intelligence (XAI)"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4390898080"
    },
    {
      "openalex_id": "W4404575137",
      "doi": "10.70777/agi.v1i1.10863",
      "title": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies",
      "authors": [
        {
          "name": "Yi Zeng",
          "openalex_id": "A5100458994",
          "orcid": "https://orcid.org/0000-0002-7708-7146"
        },
        {
          "name": "Yang Yu",
          "openalex_id": "A5007723883",
          "orcid": "https://orcid.org/0000-0002-6708-9885"
        },
        {
          "name": "Andy Zhou",
          "openalex_id": "A5114183559"
        },
        {
          "name": "John Tan",
          "openalex_id": "A5111275941"
        },
        {
          "name": "Ya Tu",
          "openalex_id": "A5057166109",
          "orcid": "https://orcid.org/0000-0001-5081-5329"
        },
        {
          "name": "Yifan Mai",
          "openalex_id": "A5006623234",
          "orcid": "https://orcid.org/0000-0002-4807-037X"
        },
        {
          "name": "Kevin Klyman",
          "openalex_id": "A5093104547",
          "orcid": "https://orcid.org/0009-0003-2130-3529"
        },
        {
          "name": "Minzhou Pan",
          "openalex_id": "A5113268603"
        },
        {
          "name": "Ruoxi Jia",
          "openalex_id": "A5032275274",
          "orcid": "https://orcid.org/0000-0001-9662-9556"
        },
        {
          "name": "Dawn Song",
          "openalex_id": "A5102160255"
        },
        {
          "name": "Percy Liang",
          "openalex_id": "A5025255782",
          "orcid": "https://orcid.org/0000-0002-0458-6139"
        },
        {
          "name": "Bo Li",
          "openalex_id": "A5100374474",
          "orcid": "https://orcid.org/0000-0002-9336-1862"
        }
      ],
      "publication_year": 2024,
      "publication_date": "2024-09-23",
      "abstract": "Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-Bench 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in our AI risks study, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-Bench 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-Bench 2024, uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-Bench 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems.",
      "cited_by_count": 2,
      "type": "article",
      "source": {
        "name": "AGI - Artificial General Intelligence - Robotics - Safety & Alignment",
        "type": "journal",
        "issn": [
          "3065-355X"
        ]
      },
      "open_access": {
        "is_oa": false,
        "oa_status": "closed",
        "oa_url": null
      },
      "topics": [
        "Risk and Safety Analysis"
      ],
      "referenced_works_count": 0,
      "url": "https://openalex.org/W4404575137"
    },
    {
      "openalex_id": "W4409824542",
      "doi": "10.1515/ijdlg-2025-0006",
      "title": "The Alignment of Values: Embedding Human Dignity in Algorithmic Bias Governance for the AGI Era",
      "authors": [
        {
          "name": "Yilin Zhao",
          "openalex_id": "A5014465194",
          "orcid": "https://orcid.org/0000-0002-7927-2660",
          "institutions": [
            "Zhejiang University"
          ]
        },
        {
          "name": "Z. M. Ren",
          "openalex_id": "A5103441886",
          "institutions": [
            "National University of Singapore"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-04-01",
      "abstract": "Abstract Decision-makers across both technological and political fields increasingly recognize the need for AI regulation. In the context of AI governance, alignment refers to the requirement that AI systems operate in accordance with human values and interests. This article argues that misalignment is a key driver of algorithmic bias, which not only perpetuates rights infringements but also undermines AI safety, posing risks to its societal integration. This alignment imperative is rooted in the enduring principle of human dignity, a juridical concept that has evolved from its origins in Roman jurisprudence to its establishment as a cornerstone of modern constitutional democracies. Today, human dignity serves as a foundational value underpinning the rule of law. Through comparative legal analysis, this article examines how human dignity informs algorithmic governance across major jurisdictions, analyzing regulatory texts, directives, and case law addressing AI-related challenges. Despite varying implementation approaches, this paper demonstrates that human dignity can serve as a universal foundation for AI governance across cultural contexts. While the European Union prioritizes human dignity in regulating algorithmic bias, emphasizing individual rights, public interests, and human oversight, this principle extends beyond European law, offering a normative anchor for global AI governance. The article concludes with governance recommendations for the AGI era, advocating for the integration of human dignity into AI alignment. This requires both embedding dignity-preserving constraints at the technical level and developing robust assessment frameworks capable of evaluating increasingly advanced AI systems. As AI surpasses human intelligence, governance mechanisms must ensure these systems align with ethical principles, remain under meaningful human control, and operate within legally and socially acceptable boundaries.",
      "cited_by_count": 2,
      "type": "article",
      "source": {
        "name": "International Journal of Digital Law and Governance",
        "type": "journal",
        "issn": [
          "2941-2552"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "hybrid",
        "oa_url": "https://www.degruyterbrill.com/document/doi/10.1515/ijdlg-2025-0006/pdf"
      },
      "topics": [
        "Ethics and Social Impacts of AI",
        "Neuroethics, Human Enhancement, Biomedical Innovations"
      ],
      "referenced_works_count": 49,
      "url": "https://openalex.org/W4409824542"
    },
    {
      "openalex_id": "W4409862668",
      "doi": "10.1136/bmjhci-2024-101130",
      "title": "Potential for near-term AI risks to evolve into existential threats in healthcare",
      "authors": [
        {
          "name": "Vallijah Subasri",
          "openalex_id": "A5022803349",
          "orcid": "https://orcid.org/0000-0002-6584-877X",
          "institutions": [
            "University Health Network",
            "Vector Institute"
          ]
        },
        {
          "name": "Negin Baghbanzadeh",
          "openalex_id": "A5117349680",
          "orcid": "https://orcid.org/0000-0003-0545-467X",
          "institutions": [
            "York University"
          ]
        },
        {
          "name": "Leo Anthony Celi",
          "openalex_id": "A5031401755",
          "orcid": "https://orcid.org/0000-0001-6712-6626",
          "institutions": [
            "Massachusetts Institute of Technology"
          ]
        },
        {
          "name": "Laleh Seyyed-Kalantari",
          "openalex_id": "A5004971542",
          "orcid": "https://orcid.org/0000-0002-1059-7125",
          "institutions": [
            "Vector Institute",
            "York University"
          ]
        }
      ],
      "publication_year": 2025,
      "publication_date": "2025-04-01",
      "abstract": "The recent emergence of foundation model-based chatbots, such as ChatGPT (OpenAI, San Francisco, CA, USA), has showcased remarkable language mastery and intuitive comprehension capabilities. Despite significant efforts to identify and address the near-term risks associated with artificial intelligence (AI), our understanding of the existential threats they pose remains limited. Near-term risks stem from AI that already exist or are under active development with a clear trajectory towards deployment. Existential risks of AI can be an extension of the near-term risks studied by the fairness, accountability, transparency and ethics community, and are characterised by a potential to threaten humanity\u2019s long-term potential. In this paper, we delve into the ways AI can give rise to existential harm and explore potential risk mitigation strategies. This involves further investigation of critical domains, including AI alignment, overtrust in AI, AI safety, open-sourcing, the implications of AI to healthcare and the broader societal risks.",
      "cited_by_count": 2,
      "type": "article",
      "source": {
        "name": "BMJ Health & Care Informatics",
        "type": "journal",
        "issn": [
          "2632-1009"
        ]
      },
      "open_access": {
        "is_oa": true,
        "oa_status": "gold",
        "oa_url": "https://informatics.bmj.com/content/32/1/e101130.full.pdf"
      },
      "topics": [
        "Artificial Intelligence in Healthcare and Education",
        "Ethics in Clinical Research",
        "Ethics and Social Impacts of AI"
      ],
      "referenced_works_count": 20,
      "url": "https://openalex.org/W4409862668"
    }
  ],
  "count": 40,
  "errors": []
}
