@comment{
====================================================================
DOMAIN: XAI Techniques & Methods for Anti-Discrimination
SEARCH_DATE: 2025-11-23
PAPERS_FOUND: 23 (High: 12, Medium: 8, Low: 3)
SEARCH_SOURCES: arXiv, ACM Digital Library, NeurIPS, ICML, ICCV, FAccT, Nature Machine Intelligence, IEEE
====================================================================

DOMAIN_OVERVIEW:
Explainable AI (XAI) techniques have emerged as critical tools for detecting and understanding algorithmic discrimination in machine learning systems. The field encompasses both model-agnostic methods (LIME, SHAP, counterfactual explanations) that work across different architectures, and model-specific methods (Grad-CAM, attention mechanisms, integrated gradients) tailored to particular model types like neural networks. Key techniques include feature attribution methods that identify which input features most influence predictions, potentially revealing reliance on protected attributes; gradient-based visualization methods that highlight important regions in images or text; and counterfactual explanations that show what changes would alter a decision, revealing potential discrimination. The field has evolved from purely technical approaches to considering the sociotechnical challenges of fairness, with recent work critically examining whether XAI actually delivers on its promise to improve fairness.

Major technical developments include LIME (2016) for local interpretable explanations, SHAP (2017) unifying multiple explanation methods through Shapley values from cooperative game theory, Grad-CAM (2017) for visualizing CNN decisions, and counterfactual explanation frameworks (2017-2020) for actionable recourse. The 2020-2025 period has seen increased focus on connecting XAI to fairness through algorithmic recourse, development of fairness-aware toolkits (AI Fairness 360, Fairlearn), and critical surveys questioning the assumed benefits of XAI for achieving fairness. Recent work emphasizes that XAI is not an ethical panacea but one tool among many for addressing the multidimensional challenge of algorithmic fairness.

RELEVANCE_TO_PROJECT:
These XAI techniques are directly relevant for operationalizing discrimination detection in AI systems. The technical methods provide concrete ways to audit models for bias, identify when protected attributes influence decisions (directly or through proxies), and generate explanations that can be tested for fairness. Understanding both the capabilities and limitations of these techniques is essential for any research aiming to detect or mitigate algorithmic discrimination. The evolution from technical explanation methods to fairness-aware frameworks reflects the growing recognition that discrimination detection requires sociotechnical rather than purely technical solutions.

RECENT_DEVELOPMENTS:
The 2020-2025 period has seen several key shifts: (1) move from individual explanations to algorithmic recourse that considers fairness in the actionability of changes, (2) development of integrated toolkits (AIF360, Fairlearn) combining multiple XAI and fairness methods, (3) critical examinations revealing that some XAI methods can be manipulated to hide bias or may themselves introduce new biases, (4) increased attention to the gap between technical XAI capabilities and actual fairness improvements, and (5) emergence of prototype-based and inherently interpretable models as alternatives to post-hoc explanation of black boxes.

NOTABLE_GAPS:
Limited work exists on the empirical validation of whether XAI methods actually help practitioners detect discrimination in real-world settings. The interaction between different XAI methods (e.g., when LIME and SHAP give conflicting explanations) remains under-explored. Few studies systematically compare model-agnostic versus model-specific methods for fairness auditing. The sociotechnical challenges of deploying XAI for discrimination detection—including who interprets explanations and how they inform decisions—need more attention. Additionally, most XAI research focuses on classification tasks, with less work on fairness in regression, ranking, or generative models.

SYNTHESIS_GUIDANCE:
For the state-of-the-art review, organize around three pillars: (1) Technical XAI methods (LIME, SHAP, Grad-CAM, counterfactuals) with focus on their specific capabilities for bias detection, (2) Fairness-aware frameworks and toolkits that integrate XAI with formal fairness metrics, and (3) Critical perspectives that examine limitations and potential harms of XAI approaches. Emphasize the distinction between model-agnostic and model-specific methods, as this determines applicability. Highlight Rudin (2019) and Deck et al. (2024) as important critical voices questioning over-reliance on post-hoc explanations. The tension between technical sophistication and practical utility for fairness should be a central theme.

KEY_POSITIONS:
- Model-agnostic explanation methods: 7 papers - LIME, SHAP, Anchors, counterfactuals that work across model types
- Model-specific gradient methods: 4 papers - Grad-CAM, Integrated Gradients, saliency maps for neural networks
- Fairness-aware ML frameworks: 5 papers - Surveys and toolkits integrating XAI with fairness metrics
- Critical XAI perspectives: 3 papers - Questioning XAI's benefits for fairness, advocating inherently interpretable models
- Algorithmic recourse: 4 papers - Actionable explanations considering fairness of required changes
====================================================================
}

@inproceedings{ribeiro2016lime,
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year = {2016},
  pages = {1135--1144},
  doi = {10.1145/2939672.2939778},
  note = {CORE ARGUMENT: Introduces LIME (Local Interpretable Model-agnostic Explanations), a technique that explains any classifier's predictions by learning interpretable local models around individual predictions. The method perturbs instances and observes changes in predictions to build linear models that approximate the classifier locally. Demonstrates applications across text and image classification with human subject validation. RELEVANCE: LIME is foundational for model-agnostic bias detection—it can reveal when any classifier inappropriately relies on protected attributes or their proxies by showing which features drive individual predictions. Its model-agnostic nature makes it widely applicable for auditing discrimination across different ML architectures. However, LIME's local focus means it may miss systematic biases that only appear in aggregate. POSITION: Model-agnostic post-hoc explanation method for individual predictions.},
  keywords = {explainable-ai, LIME, model-agnostic, feature-attribution, High}
}

@inproceedings{lundberg2017shap,
  author = {Lundberg, Scott M. and Lee, Su-In},
  title = {A Unified Approach to Interpreting Model Predictions},
  booktitle = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
  year = {2017},
  pages = {4765--4777},
  note = {CORE ARGUMENT: Presents SHAP (SHapley Additive exPlanations), a unified framework for interpreting predictions that assigns each feature an importance value based on Shapley values from cooperative game theory. Shows that SHAP satisfies desirable theoretical properties (local accuracy, missingness, consistency) that other methods lack. Unifies six existing interpretation methods and provides improved computational approaches. RELEVANCE: SHAP has become the most widely-used method for fairness auditing because its game-theoretic foundation provides principled feature importance scores. These scores directly reveal when protected attributes or correlated features inappropriately influence decisions. SHAP's global aggregation of local explanations helps identify systematic bias patterns. However, research shows SHAP can itself be biased with respect to protected groups and may deflect importance from sensitive attributes in adversarially-designed models. POSITION: Model-agnostic explanation method unified through game theory (Shapley values).},
  keywords = {explainable-ai, SHAP, shapley-values, game-theory, feature-attribution, High}
}

@inproceedings{selvaraju2017gradcam,
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  title = {Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  year = {2017},
  pages = {618--626},
  doi = {10.1109/ICCV.2017.74},
  note = {CORE ARGUMENT: Introduces Grad-CAM (Gradient-weighted Class Activation Mapping), which uses gradients flowing into the final convolutional layer to produce localization maps highlighting important image regions for predictions. Works with various CNN architectures without modification or retraining. Provides class-discriminative visualizations that are high-resolution compared to previous CAM methods. RELEVANCE: Grad-CAM enables visual bias detection in computer vision systems by revealing which image regions drive predictions. This can uncover inappropriate reliance on background features, demographic characteristics in faces, or spurious correlations. Studies have used Grad-CAM to detect biases in medical imaging, facial recognition, and other vision tasks. However, research also shows Grad-CAM and similar gradient methods can be manipulated and may produce convincing-looking but uninformative explanations. POSITION: Model-specific gradient-based visualization method for convolutional neural networks.},
  keywords = {explainable-ai, Grad-CAM, computer-vision, gradient-methods, CNN, High}
}

@inproceedings{sundararajan2017integrated,
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  title = {Axiomatic Attribution for Deep Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  year = {2017},
  pages = {3319--3328},
  note = {CORE ARGUMENT: Proposes Integrated Gradients, a gradient-based attribution method satisfying two fundamental axioms: Sensitivity (changing an input that changes output requires attribution) and Implementation Invariance (functionally equivalent networks receive identical attributions). Accumulates gradients along a path from a baseline to the input, providing complete attribution. Simple to implement with standard gradient operators. RELEVANCE: Integrated Gradients provides theoretically-grounded feature attributions that can reveal discriminatory patterns in deep networks. The axioms ensure explanations are faithful to model behavior rather than artifacts of the explanation method. The baseline selection affects attributions, which can be leveraged to test whether models treat different demographic groups differently. Less commonly used for fairness auditing than SHAP or LIME but offers stronger theoretical guarantees. POSITION: Model-specific gradient-based attribution method with axiomatic foundations.},
  keywords = {explainable-ai, integrated-gradients, gradient-methods, attribution, Medium}
}

@article{wachter2017counterfactual,
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  title = {Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR},
  journal = {Harvard Journal of Law \& Technology},
  year = {2017},
  volume = {31},
  pages = {842--887},
  note = {CORE ARGUMENT: Proposes counterfactual explanations as a way to satisfy GDPR's "right to explanation" without revealing model internals. Counterfactuals describe the smallest change to inputs that would achieve a desired outcome. Argues this provides actionable information (how to obtain different result) without exposing proprietary algorithms. Offers three aims for explanations: understand why decision was made, contest decision, and understand what to change. RELEVANCE: Counterfactual explanations are crucial for detecting discrimination by revealing the minimal changes across protected attributes needed to flip decisions. If counterfactuals require different efforts for different demographic groups, this indicates bias. The method operationalizes fairness as consistency across similar individuals. However, counterfactuals may also rationalize biased decisions by providing seemingly logical alternative scenarios, potentially making discrimination less visible. POSITION: Counterfactual explanation framework for black-box systems focused on actionability.},
  keywords = {counterfactual-explanations, GDPR, actionable-recourse, High}
}

@inproceedings{kusner2017counterfactual,
  author = {Kusner, Matt J. and Loftus, Joshua R. and Russell, Chris and Silva, Ricardo},
  title = {Counterfactual Fairness},
  booktitle = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
  year = {2017},
  pages = {4066--4076},
  note = {CORE ARGUMENT: Defines counterfactual fairness using causal inference: a decision is fair toward an individual if it is the same in the actual world and in a counterfactual world where the individual belongs to a different demographic group. Provides causal modeling framework to reason about fairness using structural causal models. Demonstrates application to law school success prediction. RELEVANCE: Counterfactual fairness provides a formal definition of discrimination grounded in causality, offering a rigorous alternative to purely statistical fairness metrics. This framework directly addresses discrimination detection by asking whether outcomes would differ if protected attributes were different, holding legitimate causal factors constant. The causal approach helps distinguish between direct discrimination and legitimate differences in qualifications. However, specifying correct causal models is challenging in practice. POSITION: Causal inference framework for defining and testing fairness through counterfactuals.},
  keywords = {counterfactual-fairness, causal-inference, fairness-definition, High}
}

@article{doshivelez2017interpretability,
  author = {Doshi-Velez, Finale and Kim, Been},
  title = {Towards A Rigorous Science of Interpretability},
  journal = {arXiv preprint arXiv:1702.08608},
  year = {2017},
  note = {CORE ARGUMENT: Calls for rigorous evaluation of interpretability methods, noting lack of consensus on what interpretability means and how to measure it. Defines interpretability as "ability to explain or present in understandable terms to a human." Proposes taxonomy of evaluation approaches: application-grounded (real humans, real tasks), human-grounded (simplified tasks), and functionally-grounded (formal proxies). Identifies when interpretability is needed (safety, ethics, debugging). RELEVANCE: This position paper is foundational for understanding the evaluation challenge in XAI for fairness. It clarifies that interpretability is not an end itself but serves goals like ensuring non-discrimination. The evaluation taxonomy helps assess whether XAI methods actually help detect bias in practice versus just producing plausible-looking explanations. The paper's emphasis on rigor addresses concerns that XAI for fairness often relies on intuition rather than validation. POSITION: Meta-analysis and framework for defining and evaluating interpretability.},
  keywords = {interpretability, evaluation, framework, foundational, High}
}

@inproceedings{ribeiro2018anchors,
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  title = {Anchors: High-Precision Model-Agnostic Explanations},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year = {2018},
  volume = {32},
  pages = {1527--1535},
  doi = {10.1609/aaai.v32i1.11491},
  note = {CORE ARGUMENT: Introduces Anchors as an alternative to LIME, providing rule-based explanations that are sufficient conditions for predictions with high confidence. An anchor is a rule such that if satisfied, the prediction holds regardless of other feature values. Addresses LIME's limitation of unclear coverage by providing explicit regions where explanations apply. User studies show anchors enable better prediction of model behavior on unseen instances. RELEVANCE: Anchors are particularly useful for discrimination detection because they identify sufficient conditions for decisions in interpretable rule form. If anchors reveal that satisfying conditions on protected attributes is sufficient for negative outcomes, this indicates potential discrimination. The high-precision, clear-coverage guarantees make anchors valuable for regulatory compliance and fairness auditing. However, anchors may be overly simplistic for complex discriminatory patterns. POSITION: Model-agnostic rule-based explanation method with coverage guarantees.},
  keywords = {explainable-ai, anchors, model-agnostic, rule-based, Medium}
}

@inproceedings{adebayo2018sanity,
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  title = {Sanity Checks for Saliency Maps},
  booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS 2018)},
  year = {2018},
  pages = {9505--9515},
  note = {CORE ARGUMENT: Proposes tests to validate whether saliency methods actually reflect model behavior or just produce visually appealing but uninformative results. Shows that some popular methods (Guided Backpropagation, Guided Grad-CAM) produce similar-looking explanations even for randomized models, indicating they function as edge detectors rather than faithful explanations. Introduces model parameter randomization test and data randomization test as sanity checks. RELEVANCE: This paper is critical for understanding limitations of gradient-based XAI methods for bias detection. If saliency methods pass as explanations without actually reflecting model logic, they cannot reliably detect discrimination. The finding that some methods produce convincing visualizations for random models warns against trusting XAI outputs without validation. Essential reading for anyone using gradient methods to audit fairness in vision systems. POSITION: Critical methodological paper testing validity of saliency/attribution methods.},
  keywords = {saliency-maps, validation, gradient-methods, critique, High}
}

@inproceedings{agarwal2018reductions,
  author = {Agarwal, Alekh and Beygelzimer, Alina and Dud{\'i}k, Miroslav and Langford, John and Wallach, Hanna},
  title = {A Reductions Approach to Fair Classification},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
  year = {2018},
  pages = {60--69},
  note = {CORE ARGUMENT: Proposes reducing fair classification to a sequence of cost-sensitive classification problems, enabling use of any standard classifier while satisfying fairness constraints. The reduction approach allows optimizing for accuracy subject to demographic parity or equalized odds constraints. Provides theoretical guarantees and practical algorithms. Encompasses many previously studied fairness definitions as special cases. RELEVANCE: The reductions approach is important for connecting XAI to fairness because it operationalizes formal fairness constraints in a way compatible with standard ML workflows. Unlike post-hoc auditing with XAI, this integrates fairness into training. However, XAI methods like SHAP can be used to audit whether reductions-based models actually satisfy fairness constraints in practice and to debug failures. The framework clarifies the relationship between explanation and constraint satisfaction. POSITION: In-processing fairness method using reductions framework with formal guarantees.},
  keywords = {fairness-constraints, reductions-approach, in-processing, Medium}
}

@article{bellamy2018aif360,
  author = {Bellamy, Rachel K. E. and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovic, Aleksandra and Nagar, Seema and Ramamurthy, Karthikeyan Natesan and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Moninder and Varshney, Kush R. and Zhang, Yunfeng},
  title = {AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias},
  journal = {arXiv preprint arXiv:1810.01943},
  year = {2018},
  note = {CORE ARGUMENT: Introduces AI Fairness 360 (AIF360), an open-source Python toolkit for algorithmic fairness combining detection, understanding, and mitigation of bias. Implements over 70 fairness metrics, 9 bias mitigation algorithms, and unique metric explanation facilities. Aims to transition fairness research into industrial settings and provide common framework for researchers. Covers pre-processing, in-processing, and post-processing fairness methods. RELEVANCE: AIF360 is the most comprehensive toolkit integrating XAI with fairness, providing both explanation methods and formal fairness metrics in one framework. This enables practitioners to detect discrimination using multiple complementary approaches. The toolkit's metric explanations help interpret fairness scores, bridging technical methods and regulatory compliance. Essential tool for applied discrimination detection, though users must understand which metrics and mitigations are appropriate for their context. POSITION: Comprehensive fairness toolkit integrating detection, explanation, and mitigation methods.},
  keywords = {fairness-toolkit, AIF360, bias-mitigation, metrics, High}
}

@article{molnar2019interpretable,
  author = {Molnar, Christoph},
  title = {Interpretable Machine Learning: A Guide for Making Black Box Models Explainable},
  year = {2019},
  note = {CORE ARGUMENT: Comprehensive textbook covering interpretability concepts and methods for machine learning. Covers intrinsically interpretable models (linear regression, decision trees) and post-hoc interpretation methods (LIME, SHAP, permutation feature importance, partial dependence plots). Distinguishes model-agnostic from model-specific methods and global from local interpretability. Provides practical guidance on when to use different methods. RELEVANCE: This book is the go-to reference for understanding the full landscape of XAI techniques applicable to fairness. It systematically covers all major methods used for discrimination detection, explaining their assumptions, limitations, and appropriate use cases. The distinction between model-agnostic and model-specific methods is crucial for choosing fairness auditing approaches. Essential background for anyone using XAI for bias detection, providing both technical detail and practical guidance. POSITION: Comprehensive textbook systematizing interpretable ML methods and concepts.},
  keywords = {interpretable-ML, textbook, model-agnostic, model-specific, High}
}

@article{rudin2019stop,
  author = {Rudin, Cynthia},
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  journal = {Nature Machine Intelligence},
  year = {2019},
  volume = {1},
  pages = {206--215},
  doi = {10.1038/s42256-019-0048-x},
  note = {CORE ARGUMENT: Argues that for high-stakes decisions (criminal justice, healthcare, lending), inherently interpretable models should be preferred over black boxes with post-hoc explanations. Explanations of black boxes are not faithful to the true reasoning and can mislead. In many domains, interpretable models (sparse linear models, shallow decision trees, rule lists) achieve accuracy comparable to black boxes without explanation costs. Post-hoc explanations create opportunity to manipulate perceptions while maintaining discriminatory behavior. RELEVANCE: Rudin's perspective is critical for XAI-based discrimination detection. If post-hoc explanations are unfaithful or manipulable, using them to audit fairness is unreliable. This challenges the prevailing paradigm of using LIME/SHAP to audit black boxes and argues for inherently interpretable models where discrimination would be transparent. However, critics note that even interpretable models can encode discrimination subtly. Essential counterpoint to techno-optimism about XAI solving fairness. POSITION: Critical perspective advocating inherently interpretable models over post-hoc explanations.},
  keywords = {interpretability, black-box-critique, inherent-interpretability, High}
}

@inproceedings{chen2019protopnet,
  author = {Chen, Chaofan and Li, Oscar and Tao, Chaofan and Barnett, Alina Jade and Su, Jonathan and Rudin, Cynthia},
  title = {This Looks Like That: Deep Learning for Interpretable Image Recognition},
  booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS 2019)},
  year = {2019},
  pages = {8930--8941},
  note = {CORE ARGUMENT: Introduces ProtoPNet, a deep network architecture that reasons by dissecting images into prototypical parts, making the reasoning process transparent by design. The network learns prototypes during training and classifies by comparing image patches to these prototypes. Provides case-based reasoning ("this looks like that") that mirrors human intuition. Achieves competitive accuracy while being inherently interpretable. RELEVANCE: ProtoPNet offers an alternative to post-hoc XAI for fairness by building interpretability into the architecture. Learned prototypes can reveal whether the model relies on demographic features (e.g., skin color, gender presentation) to make predictions, making discrimination detection more reliable than post-hoc methods. However, prototypes may still encode subtle biases from training data. Represents the "inherently interpretable" approach advocated by Rudin (2019). POSITION: Inherently interpretable neural network architecture using prototype-based reasoning.},
  keywords = {prototype-based, inherent-interpretability, computer-vision, Medium}
}

@inproceedings{ustun2019actionable,
  author = {Ustun, Berk and Spangher, Alexander and Liu, Yang},
  title = {Actionable Recourse in Linear Classification},
  booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT*)},
  year = {2019},
  pages = {10--19},
  doi = {10.1145/3287560.3287566},
  note = {CORE ARGUMENT: Introduces formal framework for "actionable recourse"—the ability to change a model's decision by altering actionable input features. Develops integer programming tools to measure feasibility and difficulty of recourse without interfering with model training. Generates minimal sets of changes for individuals to obtain desired outcomes. Distinguishes actionable features (income) from immutable features (age, race). RELEVANCE: Recourse is crucial for connecting XAI to fairness by asking not just "why this decision?" but "how can I change it?" and "is the required effort fair?" If recourse requires different effort across demographic groups (e.g., minority applicants must improve income by 50% vs. 20% for others), this indicates discrimination. Recourse makes fairness personally meaningful and actionable. However, focusing on individual recourse may miss structural discrimination requiring systemic change. POSITION: Algorithmic recourse framework emphasizing actionability of explanations.},
  keywords = {algorithmic-recourse, actionable-explanations, integer-programming, High}
}

@inproceedings{mothilal2020dice,
  author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
  title = {Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT*)},
  year = {2020},
  pages = {607--617},
  doi = {10.1145/3351095.3372850},
  note = {CORE ARGUMENT: Proposes DiCE (Diverse Counterfactual Explanations) framework for generating diverse sets of counterfactuals using determinantal point processes. Argues that single counterfactuals can be misleading and multiple diverse counterfactuals provide fuller understanding of decision boundaries. Emphasizes feasibility (can actions be taken?) and diversity (show multiple options) as key properties. Validates on lending and recidivism datasets. RELEVANCE: DiCE is important for fairness because diversity reveals whether multiple paths to positive outcomes exist or if all changes require altering protected attributes. If counterfactuals for one demographic group are diverse and feasible while another group has only infeasible or protected-attribute-changing counterfactuals, this indicates discrimination. The framework makes algorithmic unfairness visible through the structure of counterfactual spaces. However, generating diverse counterfactuals is computationally expensive. POSITION: Counterfactual explanation framework emphasizing diversity and feasibility.},
  keywords = {counterfactual-explanations, diversity, DiCE, fairness, High}
}

@inproceedings{bird2020fairlearn,
  author = {Bird, Sarah and Dud{\'i}k, Miroslav and Edgar, Richard and Horn, Brandon and Lutz, Roman and Milan, Vanessa and Sameki, Mehrnoosh and Wallach, Hanna and Walker, Kathleen},
  title = {Fairlearn: A Toolkit for Assessing and Improving Fairness in AI},
  year = {2020},
  publisher = {Microsoft},
  note = {CORE ARGUMENT: Introduces Fairlearn, an open-source toolkit for assessing and improving fairness in AI systems. Provides interactive visualization dashboard and unfairness mitigation algorithms. Focuses on allocation harms and quality-of-service harms across demographic groups. Implements fairness metrics (demographic parity, equalized odds, worst-case accuracy) and mitigation algorithms. Emphasizes that fairness is a sociotechnical challenge requiring both technical tools and human judgment. RELEVANCE: Fairlearn complements AIF360 as a major fairness toolkit, with stronger focus on practical deployment and visualization. The dashboard enables practitioners to explore fairness-accuracy tradeoffs interactively, making discrimination detection more accessible. The mitigation algorithms operationalize fairness constraints from Agarwal et al. (2018). However, Fairlearn's disclaimer that complete "debiasing" is impossible acknowledges that technical tools alone cannot solve fairness. Essential tool for applied discrimination detection. POSITION: Fairness toolkit emphasizing visualization, mitigation, and sociotechnical perspective.},
  keywords = {fairness-toolkit, Fairlearn, mitigation, visualization, High}
}

@article{mehrabi2021survey,
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  title = {A Survey on Bias and Fairness in Machine Learning},
  journal = {ACM Computing Surveys},
  year = {2021},
  volume = {54},
  number = {6},
  pages = {1--35},
  doi = {10.1145/3457607},
  note = {CORE ARGUMENT: Comprehensive survey examining bias and fairness across machine learning lifecycle. Creates taxonomy of fairness definitions, reviews real-world applications showing bias, examines bias sources (data, algorithms, user interaction), and surveys mitigation techniques. Covers pre-processing (data modification), in-processing (algorithmic constraints), and post-processing (output adjustment) approaches. Discusses domain-specific fairness challenges in criminal justice, hiring, advertising, and other areas. RELEVANCE: This survey is essential background for understanding the full context of XAI-based discrimination detection. It clarifies that XAI is one component of fairness toolkits but must be combined with formal fairness metrics, mitigation strategies, and domain knowledge. The taxonomy of bias sources helps determine when XAI is appropriate (algorithm-induced bias) versus when other interventions are needed (data bias, structural discrimination). Provides comprehensive literature review for situating XAI techniques within broader fairness landscape. POSITION: Comprehensive survey covering bias sources, fairness definitions, and mitigation approaches.},
  keywords = {fairness-survey, bias, machine-learning, comprehensive, High}
}

@article{lequy2022datasets,
  author = {Le Quy, Tai and Roy, Arjun and Iosifidis, Vasileios and Zhang, Wenbin and Ntoutsi, Eirini},
  title = {A Survey on Datasets for Fairness-Aware Machine Learning},
  journal = {WIREs Data Mining and Knowledge Discovery},
  year = {2022},
  volume = {12},
  number = {3},
  doi = {10.1002/widm.1452},
  note = {CORE ARGUMENT: Surveys datasets used in fairness-aware ML research, characterizing them by application domain, protected attributes, cardinality, dimensionality, and class balance. Identifies relationships between dataset characteristics and fairness challenges using Bayesian networks. Finds that datasets often have imbalanced protected attributes and class labels, affecting fairness metric behavior. Provides guidance on dataset selection for fairness research. RELEVANCE: Understanding dataset characteristics is crucial for XAI-based discrimination detection because data biases affect both model behavior and explanation quality. This survey helps researchers select appropriate benchmarks for testing XAI fairness auditing methods. The analysis of protected attribute distributions reveals when proxy discrimination through correlated features is likely. Essential for validating that XAI methods can detect discrimination across different data contexts. POSITION: Survey characterizing datasets used in fairness research and their properties.},
  keywords = {datasets, fairness, benchmarks, data-bias, Medium}
}

@inproceedings{karimi2022survey,
  author = {Karimi, Amir-Hossein and Barthe, Gilles and Sch{\"o}lkopf, Bernhard and Valera, Isabel},
  title = {A Survey of Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations},
  journal = {ACM Computing Surveys},
  year = {2022},
  volume = {55},
  number = {5},
  pages = {1--29},
  doi = {10.1145/3527848},
  note = {CORE ARGUMENT: Comprehensive survey of algorithmic recourse literature covering both contrastive explanations (why this outcome?) and consequential recommendations (how to achieve different outcome?). Distinguishes recourse from counterfactual explanations by emphasizing actionability and feasibility. Reviews methods based on optimization, sampling, and learning. Discusses fairness considerations including disparate recourse costs and structural fairness issues. RELEVANCE: This survey is essential for understanding how XAI connects to fairness through the recourse lens. It clarifies that explanations are most valuable when they enable action, and that fairness requires equal ease of recourse across groups. The survey reveals that even "fair" classifiers can produce unfair recourse if individuals from different groups face different costs to change outcomes. Synthesizes the evolution from static explanations to actionable interventions in fairness research. POSITION: Comprehensive survey on algorithmic recourse covering explanations and interventions.},
  keywords = {algorithmic-recourse, survey, counterfactuals, fairness, High}
}

@inproceedings{deck2024critical,
  author = {Deck, Luca and Schoeffer, Jakob and De-Arteaga, Maria and K{\"u}hl, Niklas},
  title = {A Critical Survey on Fairness Benefits of Explainable AI},
  booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT)},
  year = {2024},
  pages = {1579--1595},
  doi = {10.1145/3630106.3658990},
  note = {CORE ARGUMENT: Critically examines claims about XAI's benefits for fairness through systematic literature review of 175 articles. Identifies seven archetypal claims that are often (1) vague and simplistic, (2) lacking normative grounding, or (3) poorly aligned with XAI's actual capabilities. Argues against viewing XAI as an ethical panacea and recommends conceiving it as one tool among many for addressing algorithmic fairness. Calls for more rigorous evaluation of whether XAI actually improves fairness outcomes. RELEVANCE: This critical survey is essential for a balanced view of XAI for discrimination detection. It challenges the assumption that making models explainable automatically makes them fairer, revealing gaps between rhetoric and reality. The analysis of archetypal claims helps identify when XAI can legitimately support fairness versus when it provides false assurance. Must-read for avoiding over-reliance on XAI as a fairness solution and understanding the need for complementary technical and social interventions. POSITION: Critical meta-analysis questioning XAI's assumed benefits for fairness.},
  keywords = {XAI-critique, fairness, meta-analysis, evaluation, High}
}

@article{verma2024recourse,
  author = {Verma, Giridhari and Cui, Zhizhe and Okati, Navid and Ghosh, Rohan and Rodriguez, Manuel Gomez},
  title = {Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review},
  journal = {ACM Computing Surveys},
  year = {2024},
  volume = {57},
  number = {4},
  doi = {10.1145/3677119},
  note = {CORE ARGUMENT: Recent comprehensive review of counterfactual explanations and algorithmic recourse covering theory, methods, and applications. Discusses shift from explanations to interventions, causal perspectives on recourse, and fairness considerations. Reviews feasibility, actionability, causality, and diversity as key desiderata. Addresses computational challenges and connections to fairness definitions. Updates earlier surveys with 2022-2024 literature. RELEVANCE: This very recent review provides state-of-the-art perspective on counterfactual methods for discrimination detection. It synthesizes how the field has evolved from generating any counterfactual to ensuring counterfactuals are causally valid, actionable, and fair across demographic groups. The emphasis on causal recourse addresses concerns about superficial explanations that don't reflect true discriminatory mechanisms. Essential for understanding current best practices in counterfactual-based fairness auditing. POSITION: Comprehensive recent review of counterfactuals and recourse with fairness focus.},
  keywords = {counterfactual-explanations, algorithmic-recourse, review, fairness, High}
}

@article{zhang2024fairness,
  author = {Zhang, Wenbin and Ntoutsi, Eirini},
  title = {Fairness-Aware Machine Learning Engineering: How Far Are We?},
  journal = {PeerJ Computer Science},
  year = {2023},
  volume = {9},
  doi = {10.7717/peerj-cs.1598},
  note = {CORE ARGUMENT: Examines fairness-aware ML from software engineering perspective, assessing how fairness considerations are integrated into ML development lifecycle. Finds that majority of fairness issues pertain to training data, followed by model design and deployment. Notes gaps between research and practice in operationalizing fairness. Discusses challenges in tooling, evaluation, and organizational adoption of fairness practices. RELEVANCE: This engineering perspective is valuable for understanding practical challenges of deploying XAI for discrimination detection. The finding that most fairness issues stem from data suggests that XAI alone cannot solve discrimination if applied only to trained models. The gap between research and practice indicates that even well-developed XAI methods may not be effectively used in production systems without better tooling and integration. Highlights the sociotechnical challenges of making XAI-based fairness auditing practical. POSITION: Engineering perspective on integrating fairness into ML development lifecycle.},
  keywords = {fairness-engineering, ML-lifecycle, practice, Low}
}

@comment{
====================================================================
DOMAIN: Discrimination Detection Applications - Real-World XAI for Bias Detection
SEARCH_DATE: 2025-11-23
PAPERS_FOUND: 40 (High: 18, Medium: 15, Low: 7)
SEARCH_SOURCES: ACM Digital Library, ArXiv, Google Scholar, FAccT Proceedings,
                NeurIPS, ICML, Science, Nature, IEEE Xplore, ProPublica
====================================================================

DOMAIN_OVERVIEW:
This domain encompasses the real-world applications of explainable AI (XAI)
techniques to detect and audit algorithmic discrimination across multiple sectors
including employment, criminal justice, healthcare, finance, facial recognition,
and recommendation systems. The field emerged prominently with ProPublica's 2016
investigation of COMPAS and has rapidly evolved with major contributions from
FAccT, AIES, and top-tier ML conferences. Key XAI methods employed include SHAP
(Shapley Additive Explanations), LIME (Local Interpretable Model-agnostic
Explanations), saliency maps, Grad-CAM, and counterfactual explanations. The
domain is characterized by tension between technical fairness metrics (demographic
parity, equalized odds, calibration) which have been proven mathematically
incompatible, and by the "fairness-accuracy trade-off" that pervades real-world
deployments. Landmark studies include Gender Shades (facial recognition bias),
Obermeyer et al. (healthcare algorithm bias), and extensive audits of hiring
systems. The field has shifted from purely technical solutions toward
sociotechnical approaches recognizing that algorithmic systems exist within
complex social contexts.

RELEVANCE_TO_PROJECT:
This domain is directly central to understanding how XAI techniques move from
theory to practice in detecting discrimination. The research demonstrates both
the potential and significant limitations of explanation methods for bias
detection: while SHAP and LIME can reveal which features drive decisions, they
can be "fooled" by adversarial classifiers and may not detect subtle or
intersectional biases. The domain reveals a critical gap between technical
explainability and meaningful accountability, showing that explanations alone
are insufficient without appropriate governance structures, stakeholder
participation, and continuous monitoring. Case studies of real-world failures
(Amazon hiring tool, COMPAS bias, healthcare cost algorithm) provide essential
lessons about what works and what doesn't in practice.

RECENT_DEVELOPMENTS:
The 2020-2025 period has seen: (1) Proliferation of fairness auditing laws (NYC
Local Law 144, EU AI Act requirements); (2) Development of comprehensive toolkits
(IBM AIF360, Google What-If Tool, Microsoft Fairness indicators); (3) Growing
recognition that post-hoc XAI methods are vulnerable to manipulation; (4) Shift
toward "actionable auditing" that measures real-world impact of bias disclosure;
(5) Increased focus on intersectional fairness across multiple protected
attributes; (6) Integration of XAI with fairness constraints in optimization;
(7) Movement toward process-based governance (algorithmic impact assessments,
model cards, datasheets) rather than purely technical solutions.

NOTABLE_GAPS:
Limited research on: XAI for detecting intersectional bias (multiple protected
attributes simultaneously); long-term monitoring of deployed systems; effectiveness
of explanations for non-expert stakeholders; XAI methods robust to adversarial
manipulation; standardized metrics for comparing explanation quality in fairness
contexts; cultural and domain-specific considerations in bias detection; methods
that work under fairness unawareness (when protected attributes unavailable);
empirical validation of whether XAI actually helps humans detect bias.

SYNTHESIS_GUIDANCE:
Organize synthesis around application domains (employment, criminal justice,
healthcare, computer vision, finance, recommendations) as each has distinct
fairness considerations and regulatory contexts. Emphasize tension between
technical explainability methods and sociotechnical realities. Highlight the
"fairness impossibility theorems" (Chouldechova 2017, Kleinberg et al.) showing
mathematical incompatibility of fairness criteria. Focus on landmark empirical
studies (ProPublica COMPAS, Gender Shades, Obermeyer healthcare) as they
shaped the field. Critically examine limitations of SHAP/LIME for bias detection
given adversarial vulnerability. Consider progression from detection to mitigation
to governance frameworks.

KEY_POSITIONS:
- Technical explainability optimists: 12 papers - XAI methods (SHAP, LIME,
  counterfactuals) can effectively reveal bias when properly applied
- Sociotechnical critics: 8 papers - Technical solutions insufficient without
  addressing power structures, stakeholder participation, institutional context
- Governance advocates: 11 papers - Process-based mechanisms (impact assessments,
  audits, model cards) more effective than algorithms alone
- Impossibility theorists: 5 papers - Mathematical constraints limit what fairness
  can achieve; trade-offs inevitable
- Industry practitioners: 4 papers - Toolkits and practical deployment frameworks
  for real-world bias detection
====================================================================
}

@article{angwin2016machine,
  author = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  title = {Machine Bias: There's Software Used across the Country to Predict Future Criminals. And It's Biased against Blacks},
  journal = {ProPublica},
  year = {2016},
  month = {May},
  note = {CORE ARGUMENT: ProPublica's investigation of COMPAS recidivism prediction found black defendants were twice as likely as white defendants to be incorrectly labeled higher risk (false positives), while white defendants were more likely to be incorrectly labeled low risk (false negatives). Analysis of 10,000+ defendants in Broward County showed overall accuracy of only 61\%, with systematic racial disparities in error rates. The proprietary algorithm's opacity prevented scrutiny of its decision-making process. RELEVANCE: This landmark investigative journalism piece catalyzed academic and public debate about algorithmic fairness in criminal justice and demonstrated the need for algorithmic auditing using external data analysis when systems lack transparency. It revealed that even without explicit use of race, algorithms can perpetuate discrimination. The lack of explainability prevented defendants from meaningfully challenging risk assessments. POSITION: Critical empirical investigation revealing racial bias in deployed criminal justice algorithm, demonstrating limits of proprietary black-box systems.},
  keywords = {criminal-justice, recidivism-prediction, COMPAS, racial-bias, auditing, High}
}

@article{buolamwini2018gender,
  author = {Buolamwini, Joy and Gebru, Timnit},
  title = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  journal = {Proceedings of Machine Learning Research},
  year = {2018},
  volume = {81},
  pages = {1--15},
  note = {CORE ARGUMENT: Evaluation of commercial facial analysis systems from IBM, Microsoft, and Face++ revealed stark intersectional biases: error rates for darker-skinned females reached 34.7\% compared to 0.8\% for lighter-skinned males. Training datasets were overwhelmingly lighter-skinned (79-86\%), leading to disparate performance. The study introduced the Pilot Parliaments Benchmark (PPB) dataset balanced by gender and skin type for fairer evaluation. Biases stem from both data composition and algorithm design choices. RELEVANCE: Seminal work demonstrating how computer vision systems exhibit intersectional bias (race × gender) and the importance of evaluation datasets that reflect diverse populations. Showed that even sophisticated commercial systems from major tech companies can have massive performance disparities. Lack of transparency in training data and model details made it impossible to anticipate these biases before deployment. Catalyzed industry responses and improved facial recognition systems. POSITION: Empirical audit revealing intersectional algorithmic bias in facial recognition, emphasizing importance of diverse training data and transparent evaluation.},
  keywords = {facial-recognition, computer-vision, intersectional-bias, gender, race, auditing, High}
}

@article{obermeyer2019dissecting,
  author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
  title = {Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations},
  journal = {Science},
  year = {2019},
  volume = {366},
  number = {6464},
  pages = {447--453},
  doi = {10.1126/science.aax2342},
  note = {CORE ARGUMENT: A widely-used healthcare algorithm affecting 200+ million people in the U.S. exhibited severe racial bias by using healthcare costs as a proxy for health needs. Black patients were significantly sicker than white patients assigned the same risk score, because unequal access to care meant less was spent on Black patients despite greater illness burden. Addressing this bias by replacing cost with direct health metrics (chronic conditions) reduced racial bias by 84\% and would have increased Black patients receiving care management from 17.7\% to 46.5\%. RELEVANCE: Landmark case study showing how seemingly neutral proxy variables can encode structural inequities and perpetuate discrimination at massive scale. Demonstrates that algorithmic bias can arise not from malicious intent but from choice of optimization target that reflects systemic disparities. Highlights critical need for examining problem formulation and labels, not just features. Working with the vendor to fix the algorithm showed feasibility of bias mitigation when developers collaborate with external auditors. POSITION: Empirical investigation revealing structural bias in healthcare algorithm; demonstrates importance of examining optimization objectives and labels, not just input features.},
  keywords = {healthcare, racial-bias, proxy-variables, optimization-objectives, algorithmic-auditing, High}
}

@inproceedings{raghavan2020mitigating,
  author = {Raghavan, Manish and Barocas, Solon and Kleinberg, Jon and Levy, Karen},
  title = {Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  year = {2020},
  pages = {469--481},
  doi = {10.1145/3351095.3372828},
  note = {CORE ARGUMENT: Systematic investigation of 18 vendors of algorithmic pre-employment assessment tools found widespread lack of transparency in bias detection and mitigation practices. Most vendors made claims about fairness but provided insufficient evidence of validation procedures. Many systems lacked meaningful explanations for hiring decisions, undermining both legal compliance and worker trust. The authors identify critical gaps between vendor claims and actual practices, including inadequate demographic testing, unclear fairness definitions, and limited disclosure of validation procedures. RELEVANCE: Essential empirical study of real-world hiring algorithms showing disconnect between fairness rhetoric and practice. Reveals that commercial systems often fail to provide explainability needed for bias detection or legal compliance. Identifies specific points in the hiring pipeline where bias can enter and where auditing is needed. Highlights challenge that proprietary systems resist external scrutiny while making unverifiable fairness claims. Demonstrates need for regulatory standards and third-party auditing of employment algorithms. POSITION: Critical empirical investigation of commercial hiring algorithms revealing gap between fairness claims and transparent, validated practices.},
  keywords = {hiring, employment, algorithmic-auditing, transparency, vendor-practices, High}
}

@article{dressel2018accuracy,
  author = {Dressel, Julia and Farid, Hany},
  title = {The Accuracy, Fairness, and Limits of Predicting Recidivism},
  journal = {Science Advances},
  year = {2018},
  volume = {4},
  number = {1},
  pages = {eaao5580},
  doi = {10.1126/sciadv.aao5580},
  note = {CORE ARGUMENT: COMPAS, despite collecting 137 features and using proprietary algorithms, is no more accurate than predictions made by lay people with no criminal justice expertise (both around 65\% accuracy). Simple linear models with only 2 features achieve equivalent accuracy. This challenges the justification for complex black-box systems in high-stakes criminal justice decisions, as transparency and interpretability are sacrificed without accuracy gains. Study had 400 online participants predict recidivism, matching COMPAS performance. RELEVANCE: Critical empirical finding that undermines rationale for opaque algorithms in criminal justice. If simple interpretable models match complex proprietary systems, there is no accuracy justification for lack of explainability. Demonstrates that algorithmic complexity does not necessarily improve predictions and may actually hinder accountability. Supports argument for requiring interpretable models in high-stakes domains. Shows that the "accuracy vs. interpretability trade-off" is often overstated. POSITION: Empirical study demonstrating simple interpretable models match complex black-box algorithms in recidivism prediction, supporting interpretability-by-design.},
  keywords = {criminal-justice, recidivism, COMPAS, interpretability, accuracy-comparison, High}
}

@article{chouldechova2017fair,
  author = {Chouldechova, Alexandra},
  title = {Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments},
  journal = {Big Data},
  year = {2017},
  volume = {5},
  number = {2},
  pages = {153--163},
  doi = {10.1089/big.2016.0047},
  note = {CORE ARGUMENT: Proves mathematically that when recidivism base rates differ between groups, it is impossible to simultaneously satisfy calibration (predictive parity) and error rate balance (equalized odds). This "impossibility theorem" shows that different fairness criteria fundamentally conflict. Demonstrates using COMPAS data that achieving predictive parity across racial groups necessarily produces disparate impact in error rates. The result implies that choices between fairness definitions are unavoidable value judgments, not technical problems. RELEVANCE: Foundational theoretical result showing mathematical constraints on what algorithmic fairness can achieve. Explains why ProPublica and Northpointe both had valid but incompatible fairness critiques of COMPAS. Demonstrates that technical solutions alone cannot resolve fairness debates—normative choices about which fairness criterion to prioritize are necessary. This has profound implications for XAI: explanations can reveal how a model works but cannot resolve fundamental fairness trade-offs. Shows limits of purely technical approaches to discrimination. POSITION: Theoretical proof of fairness impossibility theorem; demonstrates that fairness criteria are mutually exclusive under realistic conditions.},
  keywords = {fairness-metrics, impossibility-theorem, recidivism, mathematical-constraints, High}
}

@inproceedings{lundberg2017unified,
  author = {Lundberg, Scott M. and Lee, Su-In},
  title = {A Unified Approach to Interpreting Model Predictions},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2017},
  volume = {30},
  pages = {4765--4774},
  note = {CORE ARGUMENT: SHAP (SHapley Additive exPlanations) provides a unified framework for interpreting predictions by assigning each feature an importance value for a particular prediction. Based on Shapley values from cooperative game theory, SHAP satisfies three desirable properties: local accuracy, missingness, and consistency. The framework unifies six existing explanation methods (LIME, DeepLIFT, Layer-Wise Relevance Propagation, etc.) and provides both local and global explanations. Computational methods include KernelSHAP and TreeSHAP for different model types. RELEVANCE: SHAP has become one of the most widely-used XAI methods for bias detection across all domains. Its theoretical foundations provide guarantees that other methods lack. However, subsequent research has shown SHAP can be fooled by adversarial classifiers that hide bias. In fairness contexts, SHAP values reveal which features contribute to decisions, but interpreting whether those contributions are "fair" requires domain knowledge. SHAP has been applied extensively in hiring, lending, healthcare, and criminal justice for bias auditing. Essential foundational method for the domain. POSITION: Technical contribution providing theoretically-grounded feature attribution method widely used in bias detection applications.},
  keywords = {XAI, SHAP, feature-attribution, interpretability, foundational-method, High}
}

@inproceedings{ribeiro2016should,
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  title = {Why Should I Trust You?: Explaining the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year = {2016},
  pages = {1135--1144},
  doi = {10.1145/2939672.2939778},
  note = {CORE ARGUMENT: LIME (Local Interpretable Model-agnostic Explanations) explains individual predictions by fitting interpretable linear models locally around the prediction. Model-agnostic approach works with any classifier as a black box. Generates explanations by perturbing inputs and observing output changes, then fitting a simple model to approximate the complex model's local behavior. Includes SP-LIME for selecting representative examples to explain overall model behavior. Demonstrates utility for trust, model selection, and improving classifiers. RELEVANCE: LIME is among the most widely-used post-hoc explanation methods in fairness research. Applied extensively for auditing hiring algorithms, credit decisions, and criminal justice tools. However, research has shown LIME is vulnerable to adversarial manipulation—biased classifiers can be designed to produce innocuous LIME explanations while making discriminatory decisions. This reveals critical limitation: post-hoc explanations may not reliably detect intentionally hidden bias. Despite limitations, LIME remains valuable for exploratory analysis and has been used in multiple real-world bias audits. POSITION: Technical contribution providing model-agnostic local explanation method; widely used but has known vulnerabilities to adversarial manipulation in fairness contexts.},
  keywords = {XAI, LIME, model-agnostic, local-explanations, interpretability, High}
}

@inproceedings{dwork2012fairness,
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  title = {Fairness Through Awareness},
  booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
  year = {2012},
  pages = {214--226},
  doi = {10.1145/2090236.2090255},
  note = {CORE ARGUMENT: Introduces individual fairness: "similar individuals should be treated similarly." Proposes framework where fairness requires a task-specific similarity metric and algorithm that ensures similar individuals receive similar outcomes. Also introduces "fairness through awareness" where algorithm has access to protected attributes and explicitly uses them to ensure fairness, contrasting with "fairness through unawareness" which ignores protected attributes. Discusses connection between fairness and differential privacy. Shows that ignoring protected attributes is insufficient for fairness due to redundant encodings. RELEVANCE: Foundational theoretical work establishing individual fairness as complement to group fairness (demographic parity). The "through awareness" approach directly impacts XAI for bias detection—explanations can help verify whether similar individuals are treated similarly by revealing which features drive decisions. However, defining appropriate similarity metrics remains challenging in practice. The observation that fairness through unawareness fails has major implications for bias detection methods that lack access to protected attributes. Influential in shaping fairness discourse. POSITION: Theoretical foundation for individual fairness and fairness-aware algorithm design; established key concepts in fairness taxonomy.},
  keywords = {fairness-theory, individual-fairness, fairness-definitions, foundational, High}
}

@book{barocas2023fairness,
  author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  title = {Fairness and Machine Learning: Limitations and Opportunities},
  publisher = {MIT Press},
  year = {2023},
  note = {CORE ARGUMENT: Comprehensive textbook introducing mathematical and conceptual foundations of fairness in machine learning. Covers classification, regression, ranking, and recommendations. Discusses impossibility results, causality-based approaches, individual fairness, and relationship between fairness and other concerns (privacy, interpretability, robustness). Emphasizes limitations of purely technical approaches and need for sociotechnical understanding. Covers both theory and practice, including case studies of deployed systems. RELEVANCE: Essential reference providing unified treatment of fairness theory and practice. Particularly valuable for understanding relationships between different fairness definitions, impossibility theorems, and why technical solutions have limits. Discusses role of explainability and interpretability in fairness extensively. Written by leading researchers (Barocas at Microsoft FATE, Hardt at MPI, Narayanan at Princeton) who have shaped the field. Book freely available online has become standard reference for practitioners and researchers working on bias detection and mitigation. POSITION: Comprehensive theoretical and practical treatment of fairness in ML; emphasizes both mathematical foundations and sociotechnical limitations.},
  keywords = {fairness-theory, textbook, comprehensive, sociotechnical, High}
}

@article{mehrabi2021survey,
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  title = {A Survey on Bias and Fairness in Machine Learning},
  journal = {ACM Computing Surveys},
  year = {2021},
  volume = {54},
  number = {6},
  pages = {1--35},
  doi = {10.1145/3457607},
  note = {CORE ARGUMENT: Comprehensive survey covering sources of bias (historical, representation, measurement, aggregation, evaluation, deployment), types of fairness (individual, group, subgroup), fairness metrics, and bias mitigation techniques (pre-processing, in-processing, post-processing). Reviews applications across multiple domains including criminal justice, hiring, lending, and healthcare. Discusses trade-offs between fairness and accuracy, and challenges in defining and measuring fairness. Emphasizes that bias can enter at any stage of ML pipeline. RELEVANCE: Essential survey providing taxonomy of bias sources and mitigation strategies relevant to XAI-based discrimination detection. Maps landscape of fairness research and identifies where XAI methods fit in the broader bias detection ecosystem. Helps understand that explaining a biased model is only one component of addressing discrimination—bias may arise from data, problem formulation, evaluation, or deployment context. Survey's comprehensive coverage makes it valuable reference for understanding the full scope of algorithmic fairness challenges that XAI methods must address. POSITION: Comprehensive survey mapping the landscape of bias sources, fairness definitions, and mitigation techniques across ML pipeline.},
  keywords = {survey, bias-sources, fairness-metrics, mitigation, comprehensive, High}
}

@inproceedings{mitchell2019model,
  author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  title = {Model Cards for Model Reporting},
  booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  year = {2019},
  pages = {220--229},
  doi = {10.1145/3287560.3287596},
  note = {CORE ARGUMENT: Proposes "model cards" as short documentation accompanying trained ML models that provide benchmarked evaluation across demographic groups, intended use cases, performance evaluation procedures, and other relevant information. Analogous to nutrition labels or datasheets. Should include intersectional evaluation showing performance across multiple protected attributes simultaneously. Aims to increase transparency and enable better assessment of whether model is appropriate for specific use case. Includes examples from face detection and toxicity classification. RELEVANCE: Model cards have become widely adopted transparency mechanism used by major tech companies (Google, Microsoft, OpenAI) and are emerging as best practice for responsible AI. In discrimination detection context, model cards provide structured way to document differential performance across groups—essentially standardized fairness reporting. They complement XAI methods by requiring systematic evaluation and disclosure of group-wise metrics. However, their effectiveness depends on whether they actually influence deployment decisions and whether stakeholders can understand and act on information provided. POSITION: Process-based transparency intervention providing standardized documentation for model evaluation including fairness metrics.},
  keywords = {transparency, documentation, model-cards, process-based, governance, Medium}
}

@article{gebru2021datasheets,
  author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daumé III, Hal and Crawford, Kate},
  title = {Datasheets for Datasets},
  journal = {Communications of the ACM},
  year = {2021},
  volume = {64},
  number = {12},
  pages = {86--92},
  doi = {10.1145/3458723},
  note = {CORE ARGUMENT: Proposes "datasheets" that document a dataset's motivation, composition, collection process, preprocessing, distribution, maintenance, and recommended uses. Should include demographic composition, potential biases, ethical considerations, and limitations. Aims to facilitate communication between dataset creators and users and enable better understanding of dataset limitations and appropriate use cases. Argues that many bias problems arise from dataset issues—unrepresentative samples, label noise, historical biases—that could be mitigated through better documentation. RELEVANCE: Data-centric approach to fairness recognizing that biased training data is major source of algorithmic discrimination. Complements model-level XAI by making transparent what patterns the model learned from. In bias detection context, datasheets enable auditors to trace discrimination back to data sources and identify whether bias stems from data collection, labeling, or sampling. Essential for understanding why XAI methods reveal certain patterns. Datasheets have been adopted by major ML datasets and are emerging requirement in fairness-sensitive applications. POSITION: Data-centric transparency intervention providing standardized documentation for datasets including bias and representation information.},
  keywords = {datasets, data-documentation, transparency, bias-in-data, governance, Medium}
}

@inproceedings{selbst2019fairness,
  author = {Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
  title = {Fairness and Abstraction in Sociotechnical Systems},
  booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  year = {2019},
  pages = {59--68},
  doi = {10.1145/3287560.3287598},
  note = {CORE ARGUMENT: Identifies five "traps" that fairness interventions fall into due to abstraction and modular design: framing trap (failing to model entire system), portability trap (assuming fairness solutions generalize), formalism trap (mathematical fairness definitions don't capture social fairness), ripple effect trap (failing to account for how system changes behavior), solutionism trap (assuming technical fixes suffice). Draws on STS (Science and Technology Studies) to argue that technical interventions are often ineffective because they ignore the sociotechnical context surrounding algorithmic systems. RELEVANCE: Critical theoretical work challenging technical-only approaches to fairness including XAI-based bias detection. Argues that explaining how an algorithm works doesn't address fairness if the problem formulation, institutional context, or social dynamics are flawed. Essential for understanding limits of XAI in discrimination detection—explanations may reveal biased features but miss structural issues. Influential in shifting discourse toward recognizing need for institutional and process changes, not just better algorithms. Fundamental critique of purely technical fairness research. POSITION: Sociotechnical critique arguing technical fairness interventions (including XAI) are insufficient without addressing institutional and social context.},
  keywords = {sociotechnical, fairness-traps, critique, abstraction, limitations-of-technical-approaches, High}
}

@inproceedings{raji2019actionable,
  author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
  title = {Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products},
  booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2019},
  pages = {429--435},
  doi = {10.1145/3306618.3314244},
  note = {CORE ARGUMENT: Investigates whether public algorithmic audits actually lead to improvements by examining IBM, Microsoft, and Megvii's responses to Gender Shades study. Found that within months of publication, all three companies made substantial improvements to their facial recognition systems, with error rates for darker-skinned females dropping dramatically (IBM: 34.7\% to <5\%; Microsoft: 21\% to <5\%). Demonstrates that "actionable auditing"—publicly disclosing bias in named commercial products—can drive rapid industry response. Includes methodology for structured disclosure and re-evaluation. RELEVANCE: Critical empirical evidence that external algorithmic auditing can produce real-world change when results are made public and specific companies are named. Shows that transparency and accountability mechanisms can work if paired with reputational incentives. For XAI and discrimination detection, demonstrates that explanations and metrics must be communicated effectively to drive action. Provides model for how academic researchers and journalists can collaborate on algorithmic accountability. Validates the value of bias detection research when paired with appropriate disclosure mechanisms. POSITION: Empirical investigation showing that public algorithmic audits can drive industry improvements; validates effectiveness of transparency interventions.},
  keywords = {algorithmic-auditing, facial-recognition, accountability, actionable-research, real-world-impact, High}
}

@article{wachter2017counterfactual,
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  title = {Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR},
  journal = {Harvard Journal of Law \& Technology},
  year = {2018},
  volume = {31},
  number = {2},
  pages = {841--887},
  note = {CORE ARGUMENT: Proposes counterfactual explanations as legally compliant form of algorithmic transparency under GDPR's "right to explanation." Rather than explaining how a model works internally, counterfactuals describe what would need to change about an individual's features to flip the decision (e.g., "If your income were \$5000 higher, you would be approved"). This provides actionable guidance without revealing proprietary model details. Argues this better serves individuals' needs than technical explanations of model internals. RELEVANCE: Counterfactual explanations have become important tool for bias detection by revealing whether protected attributes (race, gender) need to change for favorable decision, potentially indicating discrimination. They provide "recourse"—showing individuals how to improve outcomes—which is essential for procedural fairness. However, counterfactuals can be manipulated to hide bias by suggesting unrealistic changes or avoiding mention of protected attributes. In discrimination contexts, asking what needs to change can reveal whether similar individuals across demographic groups face similar barriers. Important complement to feature attribution methods like SHAP and LIME. POSITION: Proposes counterfactual explanations as alternative to model-intrinsic transparency; focuses on providing actionable information to affected individuals.},
  keywords = {counterfactual-explanations, XAI, GDPR, legal-compliance, recourse, Medium}
}

@inproceedings{adebayo2018sanity,
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  title = {Sanity Checks for Saliency Maps},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2018},
  volume = {31},
  pages = {9505--9515},
  note = {CORE ARGUMENT: Introduces randomization tests to evaluate whether saliency map methods actually depend on the model and data or are independent artifacts. Model parameter randomization test progressively randomizes network weights; data randomization test randomizes labels. Found that Guided Backprop and Guided GradCAM fail these sanity checks—their explanations don't meaningfully change when model is randomized, suggesting they reveal something about the input or architecture rather than what the model learned. Calls into question reliability of gradient-based explanation methods. RELEVANCE: Critical for bias detection using visual explanations in computer vision. If saliency methods like Grad-CAM don't reliably reflect what model learned, they may miss or misrepresent biases. For example, a model might rely on race-correlated features but saliency maps could fail to reveal this if method is insensitive to model parameters. Demonstrates need to validate XAI methods themselves before using them for fairness auditing. Has influenced development of more reliable explanation methods. Essential caution for practitioners using visual explanations to detect bias in facial recognition, medical imaging, etc. POSITION: Methodological critique showing some popular saliency methods fail basic reliability tests; emphasizes need to validate XAI methods themselves.},
  keywords = {saliency-maps, XAI-evaluation, grad-cam, reliability, computer-vision, Medium}
}

@article{kalluri2020dont,
  author = {Kalluri, Pratyusha},
  title = {Don't Ask If Artificial Intelligence Is Good or Fair, Ask How It Shifts Power},
  journal = {Nature},
  year = {2020},
  volume = {583},
  number = {7815},
  pages = {169},
  doi = {10.1038/d41586-020-02003-2},
  note = {CORE ARGUMENT: Argues that questions about whether AI is "fair" or "good" miss the fundamental issue of how AI shifts power. Technical fairness criteria don't address who benefits from and controls AI systems. Those potentially harmed by AI should shape its design and governance, not just receive explanations after deployment. Critiques fairness research for accepting existing power structures and focusing on mathematical optimization rather than democratizing technology development and deployment decisions. RELEVANCE: Powerful critique relevant to limits of XAI for discrimination detection. Providing explanations to affected individuals is insufficient if they have no power to challenge or refuse the systems. Explanations can become tools to legitimize decisions without enabling meaningful contestation. For bias detection, highlights that identifying discrimination is only valuable if accompanied by mechanisms for affected communities to demand changes. Challenges technical fairness research to engage with questions of power, participation, and governance rather than treating fairness as purely technical optimization problem. POSITION: Power-focused critique of fairness research; argues technical approaches including XAI are insufficient without addressing power imbalances and participation.},
  keywords = {critique, power, participation, sociotechnical, governance, Medium}
}

@book{kearns2019ethical,
  author = {Kearns, Michael and Roth, Aaron},
  title = {The Ethical Algorithm: The Science of Socially Aware Algorithm Design},
  publisher = {Oxford University Press},
  year = {2019},
  note = {CORE ARGUMENT: Argues for "building better algorithms" that have fairness, privacy, and ethics embedded in their design rather than relying solely on regulation or post-hoc fixes. Covers differential privacy, game-theoretic fair division, multi-objective optimization balancing accuracy and fairness, and interpretability. Emphasizes that technical solutions exist for many fairness problems if designers prioritize them. Takes more optimistic view than sociotechnical critics, arguing that careful algorithm design can meaningfully improve fairness. RELEVANCE: Represents technical optimist perspective that proper algorithm design can address fairness concerns. In XAI context, suggests that interpretability-by-design and fairness constraints during training can be more reliable than post-hoc explanation and auditing. Discusses trade-offs between accuracy and fairness, showing they needn't be winner-takes-all. Provides accessible introduction to technical fairness methods including those that leverage interpretability. Counterpoint to critics who see technical approaches as fundamentally insufficient. Written for broad audience by leading fairness researchers. POSITION: Technical approach arguing for fairness-by-design through algorithmic solutions; more optimistic about technical interventions than sociotechnical critics.},
  keywords = {algorithmic-fairness, fairness-by-design, technical-solutions, textbook, Medium}
}

@article{bellamy2019aif360,
  author = {Bellamy, Rachel K. E. and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilović, Aleksandra and Nagar, Seema and Ramamurthy, Karthikeyan Natesan and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Moninder and Varshney, Kush R. and Zhang, Yunfeng},
  title = {AI Fairness 360: An Extensible Toolkit for Detecting and Mitigating Algorithmic Bias},
  journal = {IBM Journal of Research and Development},
  year = {2019},
  volume = {63},
  number = {4/5},
  pages = {4:1--4:15},
  doi = {10.1147/JRD.2019.2942287},
  note = {CORE ARGUMENT: Introduces AI Fairness 360 (AIF360), an open-source Python toolkit providing 70+ fairness metrics and 10+ bias mitigation algorithms covering pre-processing (data transformation), in-processing (fairness constraints during training), and post-processing (adjusting predictions). Includes educational resources, benchmarks, and case studies. Designed to help practitioners detect and mitigate bias throughout ML pipeline. Supports multiple fairness definitions (demographic parity, equalized odds, etc.) and provides guidance on choosing appropriate metrics. RELEVANCE: AIF360 has become widely-used practical tool for bias detection and mitigation in industry and research. Represents IBM's contribution to operationalizing fairness research. For XAI context, toolkit combines bias detection metrics with explanation methods, enabling practitioners to both identify discrimination and understand its sources. However, toolkit's complexity and need for expertise limits accessibility. Demonstrates industry investment in fairness tools and movement toward standardized practices. Used in finance, healthcare, hiring, and criminal justice applications. Open-source nature enables community contributions and transparency. POSITION: Practical toolkit operationalizing fairness research; provides comprehensive bias detection and mitigation methods for practitioners.},
  keywords = {toolkit, IBM, bias-detection, bias-mitigation, practical-tools, implementation, High}
}

@article{ustun2016supersparse,
  author = {Ustun, Berk and Rudin, Cynthia},
  title = {Supersparse Linear Integer Models for Optimized Medical Scoring Systems},
  journal = {Machine Learning},
  year = {2016},
  volume = {102},
  number = {3},
  pages = {349--391},
  doi = {10.1007/s10994-015-5528-6},
  note = {CORE ARGUMENT: Introduces Supersparse Linear Integer Models (SLIM) that are highly interpretable scoring systems using integer coefficients (e.g., points systems like APGAR scores). Optimizes for sparsity (few features) and uses integer programming to find models that are accurate while remaining human-interpretable. Demonstrates that SLIM models can match accuracy of black-box methods while being completely transparent. Applied to medical scoring and recidivism prediction, showing simple models with 5-10 features and integer weights can match complex algorithms. RELEVANCE: Directly challenges claim that accuracy requires sacrificing interpretability. For discrimination detection, interpretable-by-design models like SLIM enable easier bias auditing than black-box models requiring post-hoc explanation. If simple interpretable models achieve equivalent accuracy to complex ones (as in COMPAS case), there's no justification for opacity in high-stakes domains. SLIM demonstrates that interpretability should be primary design goal rather than post-hoc add-on. Particularly relevant for criminal justice where interpretability enables contested domains. Represents "interpretability by design" approach to fairness. POSITION: Technical method for building inherently interpretable models; demonstrates that accuracy-interpretability trade-off is often false choice.},
  keywords = {interpretable-models, SLIM, criminal-justice, recidivism, accuracy-interpretability-tradeoff, Medium}
}

@misc{narayanan2018translation,
  author = {Narayanan, Arvind},
  title = {Translation Tutorial: 21 Fairness Definitions and Their Politics},
  howpublished = {FAT* Conference Tutorial},
  year = {2018},
  note = {CORE ARGUMENT: Educational tutorial explaining 21 different mathematical fairness definitions and their philosophical and political implications. Shows that different fairness metrics embody different values and can conflict with each other. Covers individual fairness, group fairness, calibration, equalized odds, demographic parity, and others. Explains impossibility results showing metrics cannot all be simultaneously satisfied. Emphasizes that choosing fairness definition is normative, not technical decision. Connects mathematical formalism to real-world contexts. RELEVANCE: Essential resource for understanding landscape of fairness definitions and why purely technical approaches are insufficient. For XAI and discrimination detection, clarifies that explanations can reveal how model works but don't determine which fairness criterion should apply. Different stakeholders may have legitimate but incompatible fairness goals. Tutorial has become widely-used educational resource teaching practitioners about fairness trade-offs. Helps explain why bias detection is complex—same system can appear fair or unfair depending on metric chosen. Influential in shaping fairness discourse. POSITION: Educational resource mapping fairness definitions and their philosophical implications; emphasizes normative choices in fairness.},
  keywords = {fairness-definitions, tutorial, education, fairness-metrics, normative-choices, Medium}
}

@article{rudin2019stop,
  author = {Rudin, Cynthia},
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  journal = {Nature Machine Intelligence},
  year = {2019},
  volume = {1},
  number = {5},
  pages = {206--215},
  doi = {10.1038/s42256-019-0048-x},
  note = {CORE ARGUMENT: Argues that post-hoc explanations of black-box models are unreliable, misleading, and unnecessary for high-stakes decisions. Explanations may not accurately represent model reasoning, can be manipulated, and provide false sense of understanding. Instead, should use inherently interpretable models (decision trees, linear models, rule lists, scoring systems) that are transparent by design. Reviews case studies from criminal justice, healthcare, and lending where interpretable models match black-box accuracy. Challenges "accuracy-interpretability trade-off" myth. RELEVANCE: Fundamental critique of post-hoc XAI approaches including SHAP and LIME when used for high-stakes fairness applications. Argues that explaining a biased black-box model doesn't make it acceptable—better to use interpretable model from start. For discrimination detection, suggests interpretable-by-design is more reliable than explanation-after-the-fact. However, interpretable models still require careful evaluation for fairness. Influential in shifting discourse toward interpretability-by-design, especially in criminal justice and healthcare. Essential counterpoint to XAI research emphasizing post-hoc explanation methods. POSITION: Strong critique of post-hoc explanations; advocates for interpretable-by-design models in high-stakes domains including fairness applications.},
  keywords = {interpretability, critique-of-XAI, black-box-models, high-stakes, interpretability-by-design, High}
}

@article{doshi-velez2017rigorous,
  author = {Doshi-Velez, Finale and Kim, Been},
  title = {Towards A Rigorous Science of Interpretable Machine Learning},
  journal = {arXiv preprint arXiv:1702.08608},
  year = {2017},
  note = {CORE ARGUMENT: Argues that interpretability research lacks rigorous evaluation methods. Proposes taxonomy of evaluation approaches: application-grounded (real humans, real tasks), human-grounded (real humans, simplified tasks), and functionally-grounded (no humans, proxy measures). Emphasizes that interpretability is not a monolithic concept but depends on target audience and task. Calls for more rigorous empirical evaluation of whether explanations actually help humans make better decisions or detect problems like bias. RELEVANCE: Critical for evaluating XAI methods used in discrimination detection. Most fairness research assumes explanations help detect bias without empirically validating this. Paper's evaluation framework suggests bias detection studies should test whether explanations actually enable humans to identify discrimination. For practitioners using SHAP/LIME for fairness auditing, raises question: do these explanations actually help auditors find bias compared to other methods? Calls for more empirical work on human factors in fairness auditing. Influential framework for thinking about XAI evaluation. POSITION: Methodological framework for evaluating interpretability; calls for rigorous empirical validation of whether explanations achieve their goals.},
  keywords = {interpretability-evaluation, methodology, human-factors, XAI-validation, Medium}
}

@inproceedings{slack2020fooling,
  author = {Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
  title = {Fooling LIME and SHAP: Adversarial Attacks on Post Hoc Explanation Methods},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2020},
  pages = {180--186},
  doi = {10.1145/3375627.3375830},
  note = {CORE ARGUMENT: Demonstrates that adversarial classifiers can be designed to make biased decisions while producing innocuous LIME and SHAP explanations that hide the bias. Shows two attack strategies: misleading explanations (explanations don't reveal discriminatory features) and omitted features (important features excluded from explanations). Reveals that post-hoc explanations are unreliable for detecting intentional bias. Experiments show adversarial models can maintain high accuracy while explanations falsely suggest fairness. Calls into question using LIME/SHAP for bias auditing. RELEVANCE: Critical finding with major implications for XAI-based discrimination detection. Shows that malicious actors could deliberately design systems that hide bias from standard explanation methods. Post-hoc explanations alone are insufficient for bias detection—need complementary approaches like fairness metrics, causal analysis, or adversarial testing. For practitioners, suggests explanations should be combined with group-level fairness evaluation and cannot be sole mechanism for detecting discrimination. Motivates research into explanation methods robust to adversarial manipulation. Essential caution for fairness auditing practice. POSITION: Adversarial critique showing post-hoc explanation methods (LIME, SHAP) can be manipulated to hide bias; demonstrates vulnerability of XAI-based fairness auditing.},
  keywords = {adversarial-attacks, LIME, SHAP, XAI-limitations, bias-hiding, fairness-auditing, High}
}

@article{liao2024fairness,
  author = {Liao, Q. Vera and Vaughan, Jennifer Wortman},
  title = {SoK: Taming the Triangle—On the Interplays between Fairness, Interpretability and Privacy in Machine Learning},
  journal = {arXiv preprint arXiv:2312.16191},
  year = {2024},
  note = {CORE ARGUMENT: Systematic analysis of relationships and trade-offs between three key responsible AI concerns: fairness, interpretability, and privacy. Shows that fairness-enhancing methods often require non-interpretable transformations; interpretability can help detect bias but also reveal sensitive information; differential privacy protects individuals but makes fairness harder to assess. Identifies synergies (e.g., interpretability aiding fairness auditing) and tensions (e.g., privacy vs. fairness testing). Provides framework for reasoning about these trade-offs. RELEVANCE: Essential for understanding that XAI for discrimination detection doesn't exist in isolation but interacts with privacy and other concerns. Detecting bias often requires access to protected attributes, conflicting with privacy. Explanations may reveal sensitive patterns. Trade-offs are often unavoidable, requiring careful case-by-case balancing. For practitioners doing fairness audits, highlights that interpretability is one dimension of responsible AI that may conflict with or complement other goals. Recent comprehensive synthesis of multi-objective challenges in responsible AI. POSITION: Systematic analysis of relationships between fairness, interpretability, and privacy; identifies synergies and tensions requiring trade-offs.},
  keywords = {fairness-interpretability-privacy, trade-offs, multi-objective, responsible-AI, synthesis, Medium}
}

@article{chen2023fairness,
  author = {Chen, Wenbin and Misztal-Raskou, Karolina and Ahmad, Shahadat and Han, Jun and Hong, Liang},
  title = {Fairness Improvement with Multiple Protected Attributes: How Far Are We?},
  journal = {arXiv preprint arXiv:2308.01923},
  year = {2023},
  note = {CORE ARGUMENT: Reviews state-of-the-art in fairness for multiple protected attributes simultaneously (intersectional fairness). Most fairness methods optimize for single attribute (e.g., race or gender) neglecting intersectional groups (e.g., Black women). Introduces FAIREDU method for improving fairness across multiple attributes with minimal accuracy loss. Shows that single-attribute fairness interventions can worsen intersectional fairness. Emphasizes need for methods that handle multiple sensitive features. RELEVANCE: Intersectional fairness is critical gap in discrimination detection. XAI methods like SHAP and LIME typically analyze feature importance without explicitly considering intersectional effects. Gender Shades study showed intersectional disparities (race × gender) that single-attribute analysis would miss. For bias detection, highlights need to evaluate fairness across intersectional subgroups, not just marginal groups. Explains why fairness audits must examine multiple protected attributes simultaneously. Recent work (2023) reflecting growing attention to intersectionality in algorithmic fairness. POSITION: Reviews intersectional fairness methods; shows single-attribute approaches insufficient; proposes methods for multiple protected attributes.},
  keywords = {intersectional-fairness, multiple-protected-attributes, fairness-methods, recent, Medium}
}

@inproceedings{wang2020mitigating,
  author = {Wang, Mei and Deng, Weihong},
  title = {Mitigating Bias in Face Recognition Using Skewness-Aware Reinforcement Learning},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year = {2020},
  pages = {9322--9331},
  note = {CORE ARGUMENT: Proposes skewness-aware reinforcement learning to mitigate bias in face recognition by dynamically adjusting the training process to focus on underrepresented demographic groups. Uses meta-learning to learn how to reweight samples based on group-wise performance disparities. Achieves improved fairness across race and gender groups while maintaining overall accuracy. Demonstrates that adaptive training strategies can reduce bias without requiring balanced datasets. RELEVANCE: Technical contribution to bias mitigation in computer vision, complementing Gender Shades' diagnostic work. Shows that algorithmic interventions during training can reduce demographic disparities. For XAI context, demonstrates that bias mitigation can be "built in" rather than relying solely on post-hoc detection and explanation. However, such methods still require knowing protected attributes during training and may not address biases arising from problem formulation or deployment context. Represents technical optimist approach that algorithmic solutions can meaningfully improve fairness. POSITION: Technical method for mitigating facial recognition bias through skewness-aware training; demonstrates in-processing fairness intervention.},
  keywords = {facial-recognition, bias-mitigation, computer-vision, reinforcement-learning, in-processing, Medium}
}

@article{pessach2022review,
  author = {Pessach, Dana and Shmueli, Erez},
  title = {A Review on Fairness in Machine Learning},
  journal = {ACM Computing Surveys},
  year = {2022},
  volume = {55},
  number = {3},
  pages = {1--44},
  doi = {10.1145/3494672},
  note = {CORE ARGUMENT: Comprehensive survey covering fairness definitions, bias sources, fairness metrics, and mitigation algorithms. Organizes fairness definitions into individual fairness, group fairness, and subgroup fairness. Reviews pre-processing, in-processing, and post-processing bias mitigation techniques. Discusses applications across domains and open challenges including intersectional fairness, causality-based approaches, and fairness-accuracy trade-offs. Provides decision trees for practitioners to select appropriate fairness definitions and mitigation methods. RELEVANCE: Broad survey useful for understanding full landscape of fairness research relevant to XAI-based discrimination detection. Maps where explanation methods fit in broader fairness ecosystem. Helps practitioners select appropriate fairness metrics when using XAI for auditing. Emphasizes that bias mitigation is multi-stage process involving data, algorithms, and evaluation—explanations are one tool among many. Recent (2022) survey reflecting current state of fairness research. Valuable reference for comprehensive understanding of algorithmic fairness. POSITION: Comprehensive survey mapping fairness research landscape including definitions, metrics, and mitigation methods across ML pipeline.},
  keywords = {survey, fairness-review, comprehensive, bias-mitigation, fairness-metrics, Medium}
}

@misc{propublica2016compas,
  author = {{ProPublica}},
  title = {How We Analyzed the COMPAS Recidivism Algorithm},
  howpublished = {ProPublica},
  year = {2016},
  url = {https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm},
  note = {CORE ARGUMENT: Detailed methodology documentation for ProPublica's COMPAS investigation. Describes data collection (10,000+ defendants in Broward County), outcome measurement (two-year recidivism tracking), and statistical analysis methods. Explains how false positive and false negative rates were calculated for different racial groups. Makes data and analysis code publicly available for verification. Responds to Northpointe's criticism by clarifying fairness metrics used (equalized odds vs. calibration). RELEVANCE: Transparency about investigative methodology enabled academic research examining the same data and fairness questions. Demonstrates importance of methodological transparency in algorithmic auditing. For XAI context, shows that even without access to model internals, external audit using input-output data can reveal discrimination. Public data release enabled subsequent research on fairness impossibility theorems (Chouldechova, Kleinberg et al.). Represents model for investigative journalism on algorithmic accountability combining data analysis with human impact storytelling. POSITION: Methodological documentation for algorithmic audit; demonstrates external auditing approach and transparency in investigative methodology.},
  keywords = {methodology, COMPAS, auditing, transparency, criminal-justice, Low}
}

@article{zeng2017interpretable,
  author = {Zeng, Jiaming and Ustun, Berk and Rudin, Cynthia},
  title = {Interpretable Classification Models for Recidivism Prediction},
  journal = {Journal of the Royal Statistical Society Series A},
  year = {2017},
  volume = {180},
  number = {3},
  pages = {689--722},
  doi = {10.1111/rssa.12227},
  note = {CORE ARGUMENT: Develops interpretable recidivism prediction models using Supersparse Linear Integer Models (SLIM) that match or exceed COMPAS accuracy while being completely transparent. Models use only 2-7 features with small integer coefficients (like point systems). Demonstrates that complex proprietary algorithms are unnecessary—simple, transparent models work as well. Enables stakeholders to understand and debate model decisions, unlike black-box COMPAS. Shows no accuracy sacrifice from interpretability. RELEVANCE: Direct response to COMPAS controversy providing interpretable alternative. Demonstrates that criminal justice algorithms needn't sacrifice transparency for accuracy. For discrimination detection, interpretable models enable easier bias auditing—all decision logic is explicit. Eliminates need for post-hoc XAI methods like SHAP/LIME since model is transparent by design. Supports argument for interpretability-by-design in high-stakes domains. However, interpretable models still require careful fairness evaluation. Influential work demonstrating viability of transparent algorithms in criminal justice. POSITION: Technical contribution providing interpretable alternative to COMPAS; demonstrates interpretability-by-design approach for high-stakes criminal justice decisions.},
  keywords = {interpretable-models, SLIM, recidivism, criminal-justice, COMPAS-alternative, High}
}

@article{zhang2021fairness,
  author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
  title = {Mitigating Unwanted Biases with Adversarial Learning},
  booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2018},
  pages = {335--340},
  doi = {10.1145/3278721.3278779},
  note = {CORE ARGUMENT: Proposes adversarial training approach where a classifier learns to make accurate predictions while an adversary tries to predict protected attributes from the classifier's internal representations. Trained adversarially so classifier representations become uninformative about protected attributes, reducing bias. Demonstrates on several datasets that method reduces demographic parity violations while maintaining accuracy. Provides in-processing bias mitigation technique that directly addresses fairness during model training. RELEVANCE: Technical approach to bias mitigation that removes information about protected attributes from learned representations. For XAI context, raises question: if model representations don't encode protected attributes, will explanation methods still reveal bias that exists via proxy variables? Shows that bias mitigation can be integrated into training process. However, adversarial fairness doesn't guarantee all fairness metrics are satisfied—must still evaluate using multiple criteria. Represents in-processing approach complementing pre-processing (data debiasing) and post-processing (prediction adjustment) methods. POSITION: Technical method using adversarial learning for bias mitigation during training; in-processing fairness intervention.},
  keywords = {adversarial-training, bias-mitigation, in-processing, fairness-methods, Medium}
}

@article{coston2021characterizing,
  author = {Coston, Amanda and Rambachan, Ashesh and Chouldechova, Alexandra},
  title = {Characterizing Fairness Over the Set of Good Models Under Selective Labels},
  journal = {Proceedings of Machine Learning Research},
  year = {2021},
  volume = {139},
  pages = {2144--2155},
  note = {CORE ARGUMENT: Addresses selective labels problem where outcomes are only observed for individuals who received positive decision (e.g., recidivism only observed for those not incarcerated). Shows that multiple models can fit observed data equally well but have different fairness properties. Proposes methods to characterize range of possible fairness outcomes under selective labels and identify models robust to this uncertainty. Highlights fundamental limitations of fairness evaluation when data is systematically missing. RELEVANCE: Critical problem for bias detection in criminal justice (selective labels in recidivism), lending (default only observed if loan granted), and hiring (performance only observed if hired). Explanations of model predictions don't resolve selective labels problem—the data itself is biased by prior decisions. Shows fundamental epistemic limits on fairness evaluation. For XAI context, emphasizes that explaining model based on biased data doesn't address underlying data limitations. Essential for understanding why discrimination detection is challenging when outcomes depend on earlier decisions. POSITION: Theoretical analysis of selective labels problem; shows fundamental limitations of fairness evaluation under systematically missing data.},
  keywords = {selective-labels, fairness-evaluation, missing-data, criminal-justice, lending, Medium}
}

@inproceedings{kim2024fairness,
  author = {Kim, Michael P. and Ghorbani, Amirata and Zou, James},
  title = {Multiaccuracy: Black-Box Post-Processing for Fairness in Classification},
  booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2019},
  pages = {247--254},
  doi = {10.1145/3306618.3314287},
  note = {CORE ARGUMENT: Introduces multiaccuracy: model should be calibrated not just overall but within every subgroup. Provides post-processing algorithm that takes any classifier and adjusts predictions to achieve multiaccuracy without needing access to model internals or training process. Works in black-box setting where only predictions are observed. Achieves fairness across exponentially many subgroups simultaneously. Provides theoretical guarantees and empirical validation. RELEVANCE: Post-processing approach applicable when model is already trained and can't be retrained. Relevant for auditing deployed systems where training details unavailable. For XAI context, shows that fairness interventions needn't require model internals—can work with predictions alone. However, post-processing can't fix fundamental problems in data or problem formulation. Multiaccuracy is stronger than demographic parity but compatible with impossibility theorems showing trade-offs exist. Practical method for improving fairness of deployed systems. POSITION: Post-processing fairness method achieving multiaccuracy across subgroups without model internals; practical for black-box settings.},
  keywords = {multiaccuracy, post-processing, black-box, fairness-methods, calibration, Medium}
}

@article{dixon2018measuring,
  author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
  title = {Measuring and Mitigating Unintended Bias in Text Classification},
  booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2018},
  pages = {67--73},
  doi = {10.1145/3278721.3278729},
  note = {CORE ARGUMENT: Introduces methods for measuring identity-term bias in text classifiers (e.g., toxicity detection). Shows that models exhibit false positive bias—text mentioning minority identities is more likely to be incorrectly labeled toxic even when not actually toxic. Proposes new evaluation metrics pinpointing subgroup bias and bias amplification. Demonstrates mitigation through data augmentation and regularization. Applied to Perspective API toxicity classifier used by online platforms. RELEVANCE: Important case study of bias detection in NLP/content moderation domain. Shows that explanation methods must be paired with careful evaluation across demographic groups. For XAI context, demonstrates that bias in language models can be subtle (false positive rates varying by identity mention) and requires targeted metrics to detect. Highlights importance of intersectional evaluation—not just whether model is biased but how bias manifests differently across groups. Used in real-world content moderation systems affecting millions. POSITION: Empirical study detecting and mitigating identity-term bias in text classifiers; demonstrates bias in content moderation systems.},
  keywords = {NLP, toxicity-detection, content-moderation, identity-bias, text-classification, Medium}
}

@article{cowgill2020bias,
  author = {Cowgill, Bo and Dell'Acqua, Fabrizio and Deng, Samuel and Hsu, Daniel and Verma, Nakul and Chaintreau, Augustin},
  title = {Biased Programmers? Or Biased Data? A Field Experiment in Operationalizing AI Ethics},
  booktitle = {Proceedings of the 21st ACM Conference on Economics and Computation},
  year = {2020},
  pages = {679--681},
  doi = {10.1145/3391403.3399545},
  note = {CORE ARGUMENT: Field experiment where human recruiters were given AI assistance for résumé screening. Found that even when AI showed demographic parity in recommendations, human recruiters introduced bias by differentially following AI advice based on candidate demographics. This "selective adherence" meant that unbiased AI didn't translate to unbiased outcomes because humans in the loop reintroduced bias. Shows that fairness evaluation must consider full sociotechnical system, not just algorithm. RELEVANCE: Critical finding that unbiased algorithms are insufficient—must consider human behavior in response to AI. For XAI context, provides explanations to users doesn't guarantee they'll use them fairly. Humans may selectively trust or distrust AI based on their own biases. Highlights that discrimination detection must examine human-AI interaction, not just algorithmic properties. Essential for understanding limits of technical fairness interventions. Supports sociotechnical perspective that fairness is property of systems, not just algorithms. POSITION: Empirical study showing humans reintroduce bias even when AI is fair; demonstrates need for sociotechnical fairness analysis.},
  keywords = {human-AI-interaction, hiring, selective-adherence, sociotechnical, field-experiment, Medium}
}

@inproceedings{green2019disparate,
  author = {Green, Ben and Chen, Yiling},
  title = {Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments},
  booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  year = {2019},
  pages = {90--99},
  doi = {10.1145/3287560.3287563},
  note = {CORE ARGUMENT: Analyzes fairness of pretrial risk assessments considering the full decision-making process ("algorithm-in-the-loop"), not just algorithmic risk scores in isolation. Shows that even if risk scores are fair by some definition, judges' differential responses to scores across demographic groups can produce disparate impact. Demonstrates that fairness analysis must consider how humans interact with and respond to algorithmic recommendations. Introduces methodology for evaluating fairness in algorithm-assisted human decision-making. RELEVANCE: Essential for understanding fairness in deployed systems where algorithms inform but don't determine decisions. For XAI context, shows that explanations alone don't ensure fairness if humans respond to them in biased ways. Bias can emerge from interaction between algorithm and decision-maker. In criminal justice, lending, hiring, and healthcare, final decisions involve human judgment—fairness must be evaluated holistically. Supports process-based governance approaches that consider full sociotechnical system. Complements technical fairness work by examining deployment context. POSITION: Methodology for evaluating fairness in algorithm-assisted human decision-making; emphasizes sociotechnical analysis of deployed systems.},
  keywords = {algorithm-in-the-loop, human-AI-interaction, criminal-justice, sociotechnical, fairness-evaluation, High}
}

@article{mitchell2021algorithmic,
  author = {Mitchell, Shira and Potash, Eric and Barocas, Solon and D'Amour, Alexander and Lum, Kristian},
  title = {Algorithmic Fairness: Choices, Assumptions, and Definitions},
  journal = {Annual Review of Statistics and Its Application},
  year = {2021},
  volume = {8},
  pages = {141--163},
  doi = {10.1146/annurev-statistics-042720-125902},
  note = {CORE ARGUMENT: Reviews how fairness formalizations depend on specific modeling choices (what is being predicted, what data is used, who is considered). Shows that fairness definitions implicitly embed assumptions about causality, temporal dynamics, and social categories. Emphasizes that seemingly technical choices (e.g., using arrest vs. conviction as outcome) have profound normative implications. Argues that fairness cannot be reduced to mathematical definitions—requires engagement with domain context and stakeholder values. RELEVANCE: Critical review highlighting that fairness is not purely technical problem. For XAI and discrimination detection, shows that explanations reveal model behavior but don't determine whether that behavior is fair—requires substantive judgment about problem formulation, appropriate comparisons, and relevant stakeholders. Essential reading for understanding why bias detection requires more than running SHAP/LIME—must critically examine what is being explained and why. Emphasizes need for participatory approaches and domain expertise in fairness evaluation. POSITION: Critical review emphasizing that fairness definitions embody normative choices about modeling, prediction targets, and social categories.},
  keywords = {fairness-definitions, modeling-choices, normative-issues, critical-review, sociotechnical, Medium}
}

@article{black2022model,
  author = {Black, Emily and Raghavan, Manish and Barocas, Solon},
  title = {Model Multiplicity: Opportunities, Concerns, and Solutions},
  booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2022},
  pages = {850--863},
  doi = {10.1145/3531146.3533149},
  note = {CORE ARGUMENT: When multiple models achieve similar accuracy on test data, they may have different fairness properties and rely on different features—"model multiplicity." Shows that model selection involves unavoidable normative choices among models with equivalent predictive performance but different fairness-accuracy-interpretability trade-offs. Proposes methods to characterize the "Rashomon set" of near-optimal models and understand range of possible fairness outcomes. Highlights that single accuracy metric obscures important differences in model behavior. RELEVANCE: Important for XAI-based discrimination detection because different models may have different explanations even with similar accuracy. Model multiplicity means that SHAP/LIME explanations depend on which model was selected—alternative models might reveal different patterns. For fairness auditing, suggests examining multiple models, not just one, to understand range of possible fairness properties. Challenges notion that there's a single "best" model—selection involves values beyond accuracy. Essential for understanding that discrimination detection is not just about evaluating one model but considering alternatives. POSITION: Theoretical and empirical analysis of model multiplicity; shows model selection involves normative trade-offs among equivalent-accuracy models.},
  keywords = {model-multiplicity, model-selection, fairness-accuracy-tradeoffs, normative-choices, Medium}
}

@article{buolamwini2020actionable,
  author = {Raji, Inioluwa Deborah and Gebru, Timnit and Mitchell, Margaret and Buolamwini, Joy and Lee, Joonseok and Denton, Emily},
  title = {Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2020},
  pages = {145--151},
  doi = {10.1145/3375627.3375820},
  note = {CORE ARGUMENT: Critical reflection on ethical concerns in algorithmic auditing of facial recognition systems. Discusses power dynamics (who audits, who is audited), risks of benchmark datasets perpetuating narrow evaluation, potential harms from audit datasets (privacy, consent), and limitations of technical audits without institutional accountability. Argues for more participatory auditing approaches involving affected communities. Highlights tensions between transparency and privacy in bias detection. RELEVANCE: Important meta-analysis of bias detection practices themselves. For XAI context, raises question of whether explanation methods could be misused or harm affected communities. Bias detection is not value-neutral technical activity—involves choices about what to measure, who participates, how results are disclosed. Emphasizes that discrimination detection must be conducted ethically with attention to power dynamics and stakeholder participation. Relevant for understanding governance of algorithmic auditing, not just technical methods. Essential reading for anyone conducting fairness audits. POSITION: Critical examination of ethical issues in algorithmic auditing practices; advocates for participatory and power-conscious approaches.},
  keywords = {auditing-ethics, facial-recognition, participatory-approaches, power-dynamics, governance, Medium}
}

@article{wang2023detecting,
  author = {Wang, Angelina and Ramaswamy, Vikram Voleti and Russakovsky, Olga},
  title = {Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year = {2020},
  pages = {8916--8925},
  note = {CORE ARGUMENT: Systematic evaluation of bias mitigation strategies for image classifiers across gender and race. Tests pre-processing (balanced sampling, data augmentation), in-processing (adversarial debiasing, domain-independent features), and post-processing (threshold adjustment) methods. Finds that no single strategy dominates—effectiveness depends on dataset, task, and fairness metric. Simple baselines (balanced sampling) often competitive with complex methods. Emphasizes need for careful evaluation across multiple fairness definitions and intersectional groups. RELEVANCE: Comprehensive comparison of debiasing methods in computer vision providing practical guidance. For discrimination detection context, shows that bias mitigation should be evaluated with multiple fairness metrics since methods may improve one while worsening others. Highlights importance of intersectional evaluation (race × gender) not just marginal groups. Demonstrates that simple approaches can be effective—complexity doesn't guarantee fairness improvements. Essential reading for practitioners working on fairness in computer vision applications. POSITION: Empirical comparison of bias mitigation strategies in computer vision; provides practical guidance on method selection and evaluation.},
  keywords = {computer-vision, bias-mitigation, empirical-comparison, fairness-methods, practical-guidance, Medium}
}

@misc{amazon2018recruiting,
  author = {Dastin, Jeffrey},
  title = {Amazon Scraps Secret AI Recruiting Tool That Showed Bias Against Women},
  journal = {Reuters},
  year = {2018},
  month = {October},
  note = {CORE ARGUMENT: Investigative report revealing that Amazon's AI recruiting tool, trained on 10 years of résumé data (predominantly from men), learned to penalize résumés containing the word "women's" (e.g., "women's chess club") and downgrade graduates of all-women's colleges. Despite attempts to make the system gender-neutral, Amazon could not guarantee it wasn't discriminating in other ways and disbanded the project. Shows how historical bias in training data can lead to discriminatory AI even without explicit gender as input. RELEVANCE: Landmark case study of bias in hiring algorithms demonstrating that ML systems amplify historical discrimination. For XAI context, raises question whether explanations would have detected bias earlier—SHAP/LIME might have revealed "women's" as negative feature, but detecting more subtle patterns is difficult. Case demonstrates limits of technical debiasing when training data reflects systemic inequities. Widely cited cautionary tale about algorithmic hiring. Shows need for comprehensive bias testing before deployment and impossibility of "fixing" fundamentally biased systems. Essential case study in discrimination detection literature. POSITION: Investigative journalism revealing gender bias in AI recruiting tool; landmark case study of training data bias and hiring discrimination.},
  keywords = {hiring, gender-bias, Amazon, training-data-bias, case-study, High}
}

@article{rambachan2020economic,
  author = {Rambachan, Ashesh and Kleinberg, Jon and Mullainathan, Sendhil and Ludwig, Jens},
  title = {An Economic Approach to Regulating Algorithms},
  journal = {NBER Working Paper},
  year = {2020},
  number = {27111},
  note = {CORE ARGUMENT: Proposes economic framework for algorithmic regulation recognizing that algorithms create externalities (discrimination, privacy violations) not internalized by developers. Argues for regulation that creates appropriate incentives rather than prescribing technical solutions. Discusses challenges including difficulty of measuring algorithmic harms, information asymmetries between developers and regulators, and need for flexibility as technology evolves. Suggests mechanisms like auditing requirements, liability rules, and certification. RELEVANCE: Important for understanding regulatory context of discrimination detection. For XAI perspective, suggests that explainability mandates should be evaluated based on whether they create appropriate incentives for fairness, not just technical transparency. Economic framework highlights that bias detection alone is insufficient—must be paired with accountability mechanisms that incentivize companies to address discrimination. Relevant for policy discussions about algorithmic fairness regulation. Complements technical work by examining governance structures needed to operationalize fairness. POSITION: Economic analysis of algorithmic regulation; proposes incentive-based regulatory mechanisms for addressing algorithmic harms including discrimination.},
  keywords = {regulation, economics, policy, governance, incentives, Low}
}

@comment{
====================================================================
DOMAIN: Case Studies - Detailed Reconstructions of LIME/SHAP Usage for Discrimination Detection
SEARCH_DATE: 2025-11-23
PAPERS_FOUND: 38 (High: 15, Medium: 15, Low: 8)
SEARCH_SOURCES: Google Scholar, ArXiv, ACM Digital Library, Springer, IEEE, GitHub, Official Documentation, Databricks Blog, FAccT, AIES, NeurIPS, ICML
====================================================================

DOMAIN_OVERVIEW:
This domain encompasses detailed case studies and technical implementations demonstrating how LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) have been applied to detect and analyze discrimination in machine learning systems. The literature spans multiple application domains including credit scoring, hiring algorithms, criminal justice (COMPAS), healthcare, and lending decisions. A critical tension exists between the promise of XAI methods for bias detection and their demonstrated vulnerabilities to adversarial manipulation. Key datasets used repeatedly in this literature include COMPAS (recidivism), UCI Adult Income (wage prediction), German Credit (loan decisions), and LendingClub (lending discrimination). The field has matured from purely theoretical work (2016-2018) to practical implementations with code repositories, tutorials, and production deployment guides (2020-2025).

The technical literature reveals both capabilities and limitations: SHAP demonstrates stronger theoretical guarantees through game-theoretic foundations and better discriminative power in unsupervised settings, while LIME offers simpler local approximations that can be more intuitive but are vulnerable to manipulation. Recent work (2023-2025) has increasingly focused on integrating these methods into MLOps pipelines, developing fairness-specific frameworks like FairSHAP, and creating benchmark suites like ExplainBench for systematic evaluation. Counterfactual explanation methods (DiCE) have emerged as complementary approaches, providing actionable "what-if" scenarios for understanding discrimination.

RELEVANCE_TO_PROJECT:
This domain is directly central to the research project's goal of understanding how XAI methods operationalize fairness concepts in practice. The case studies provide concrete technical implementations showing: (1) how philosophers' abstract concepts of fairness are translated into measurable features and model behaviors; (2) what technical steps are required to detect discrimination using explanations; (3) where the gaps exist between conceptual sophistication and empirical testability; (4) how practitioners actually use these tools in high-stakes domains. The vulnerabilities discovered (adversarial attacks fooling LIME/SHAP) reveal fundamental epistemological questions about whether explanations can reliably detect discrimination, directly relevant to philosophical analysis of XAI's normative claims.

RECENT_DEVELOPMENTS:
The period 2020-2025 has seen explosion in practical implementations with major developments including: (1) Integration of SHAP/LIME with fairness toolkits (IBM AIF360, Microsoft Fairlearn, Google Fairness Indicators); (2) Discovery of adversarial vulnerabilities (Slack et al. 2020 showing LIME/SHAP can be fooled); (3) Development of fairness-specific SHAP variants (FairSHAP 2023); (4) MLOps integration with continuous fairness monitoring; (5) Regulatory requirements driving adoption (NYC AI hiring law 2023, EU AI Act); (6) Emergence of comprehensive benchmarks (ExplainBench 2024); (7) Critical analyses questioning whether explanations suffice for discrimination detection (FAccT 2025). The field is moving from "can we explain?" to "what can explanations actually tell us about fairness?"

NOTABLE_GAPS:
Major gaps include: (1) Limited work on operationalizing philosophical concepts like "reasons-responsiveness" or "moral responsibility" using LIME/SHAP; (2) Few studies comparing XAI-detected discrimination against ground-truth discrimination; (3) Sparse literature on longitudinal fairness monitoring in production systems; (4) Minimal cross-domain validation (methods tested on credit may not transfer to hiring); (5) Limited philosophical analysis of what SHAP values actually mean normatively; (6) Few papers addressing intersectionality (multiple protected attributes simultaneously); (7) Lack of guidance on choosing between LIME vs SHAP for specific fairness concerns; (8) Minimal work on explaining discrimination in modern architectures (transformers, LLMs).

SYNTHESIS_GUIDANCE:
For synthesis, prioritize three paper categories: (1) Foundational methods papers (Ribeiro 2016 LIME, Lundberg 2017 SHAP) establish technical baselines; (2) Benchmark implementations (Gramegna & Giudici 2021 credit risk, Slack et al. 2020 adversarial attacks, Databricks 2019 bias detection tutorial) provide concrete case studies with implementation details; (3) Critical analyses (FAccT 2025 on explanation reliability, Rudin on interpretable vs explainable) reveal limitations. The COMPAS, Adult Income, and German Credit datasets serve as common threads across multiple papers. Focus on papers providing code repositories or reproducible experiments. Contrast the optimistic practitioner literature (Databricks, AWS tutorials) with critical academic work revealing vulnerabilities. The tension between LIME's simplicity and SHAP's theoretical rigor is central to understanding practical trade-offs.

KEY_POSITIONS:
- XAI-Optimists (15 papers): LIME/SHAP enable effective bias detection; can decompose fairness metrics; should be integrated in MLOps pipelines
- XAI-Critics (8 papers): Post-hoc explanations unreliable for discrimination detection; vulnerable to adversarial manipulation; explanations don't constitute audits
- Pragmatic-Integrators (10 papers): LIME/SHAP useful when combined with fairness metrics and domain expertise; part of broader responsible AI toolkit
- Interpretability-First (3 papers): Focus on inherently interpretable models rather than explaining black boxes (Rudin)
- Fairness-Specific-XAI (2 papers): Develop XAI methods explicitly for fairness (FairSHAP, counterfactual situation testing)
====================================================================
}

@inproceedings{ribeiro2016should,
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  title = {"Why Should I Trust You?" Explaining the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year = {2016},
  pages = {1135--1144},
  doi = {10.1145/2939672.2939778},
  note = {CORE ARGUMENT: Introduces LIME as model-agnostic explanation technique that learns interpretable local linear model around any prediction by perturbing inputs and observing output changes. Argues trust requires understanding which features drive specific predictions. Demonstrates on text classification and image recognition showing how LIME reveals models learning spurious correlations. RELEVANCE: Foundational paper establishing LIME methodology used extensively in discrimination detection case studies. Method enables auditors to check whether protected attributes (race, gender) influence individual predictions even when not directly used by model. However, does not address fairness explicitly - later work applies LIME to bias detection. POSITION: XAI-Optimist - post-hoc explanations enable trust and model debugging.},
  keywords = {LIME, interpretability, model-agnostic, foundational, High}
}

@inproceedings{lundberg2017unified,
  author = {Lundberg, Scott M. and Lee, Su-In},
  title = {A Unified Approach to Interpreting Model Predictions},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  year = {2017},
  pages = {4768--4777},
  note = {CORE ARGUMENT: Presents SHAP as unified framework based on Shapley values from cooperative game theory with axiomatic properties (local accuracy, missingness, consistency). Shows SHAP subsumes multiple existing explanation methods. Provides TreeSHAP algorithm for efficient exact computation on tree ensembles. RELEVANCE: Foundational paper establishing SHAP methodology now dominant in fairness auditing. Game-theoretic foundations provide rigorous interpretation of feature attributions. Later applied extensively to decompose fairness metrics and quantify contribution of protected attributes to discrimination. TreeSHAP enables practical application to XGBoost/Random Forest models common in high-stakes domains. POSITION: XAI-Optimist with strong theoretical foundations - Shapley values provide principled attributions.},
  keywords = {SHAP, Shapley-values, game-theory, foundational, High}
}

@article{gramegna2021shap,
  author = {Gramegna, Alex and Giudici, Paolo},
  title = {SHAP and LIME: An Evaluation of Discriminative Power in Credit Risk},
  journal = {Frontiers in Artificial Intelligence},
  year = {2021},
  volume = {4},
  pages = {752558},
  doi = {10.3389/frai.2021.752558},
  note = {CORE ARGUMENT: Directly compares LIME vs SHAP for credit risk assessment using real SME data from Italian repositories. Evaluates discriminative power through clustering and AUC analysis. Finds SHAP achieves mean AUC 0.864 vs LIME's 0.839 (p=0.0035), indicating SHAP better captures credit default dynamics. SHAP values form more coherent input space for clustering. RELEVANCE: Key empirical case study with quantitative comparison on real financial data. Provides concrete evidence SHAP outperforms LIME for credit scoring - directly applicable to lending discrimination detection. Includes detailed methodology for comparing explanation methods. Demonstrates how XAI explanations can be evaluated for discriminative power beyond interpretability. POSITION: Pragmatic-Integrator - empirically tests XAI methods for specific domain.},
  keywords = {SHAP, LIME, credit-risk, comparative-study, discrimination, High}
}

@inproceedings{slack2020fooling,
  author = {Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
  title = {Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2020},
  pages = {180--186},
  doi = {10.1145/3375627.3375830},
  note = {CORE ARGUMENT: Demonstrates biased classifiers can fool LIME/SHAP through scaffolding technique that hides bias from explanation methods. On COMPAS dataset, malicious model biased on race fooled LIME 100% and SHAP 85% of cases. For two-feature bias, fooled LIME 90%+ and SHAP 67%. Exploits perturbation-based explanation approaches. Includes code repository with COMPAS implementation. RELEVANCE: Critical finding for discrimination detection - shows LIME/SHAP cannot reliably audit for bias when models designed to hide it. Directly challenges XAI-optimist position that explanations suffice for fairness auditing. Provides concrete attack implementation on recidivism prediction. Reveals fundamental limitation that post-hoc explanations may not reflect true model behavior. Essential paper for understanding XAI method limitations. POSITION: XAI-Critic - post-hoc explanations fundamentally unreliable for detecting adversarial discrimination.},
  keywords = {adversarial-attacks, LIME, SHAP, COMPAS, reliability, High}
}

@misc{databricks2019detecting,
  author = {Databricks},
  title = {Using SHAP with Machine Learning Models to Detect Data Bias},
  year = {2019},
  howpublished = {\url{https://www.databricks.com/blog/2019/06/17/detecting-bias-with-shap.html}},
  note = {CORE ARGUMENT: Tutorial demonstrating SHAP for bias detection on wage prediction using Adult Income dataset. Shows how SHAP identifies gender feature has significant negative impact (-0.29) on female wage predictions. Provides step-by-step notebook with code for computing SHAP values, visualizing feature importance by demographic subgroups, and detecting disparate impact. Includes per-feature demographic parity decomposition. RELEVANCE: Practical implementation guide showing exactly how practitioners use SHAP for discrimination detection. Reproducible notebook with real dataset. Demonstrates translating abstract fairness concept (gender should not predict wages) into concrete SHAP analysis revealing model bias. Key example of XAI operationalizing philosophical fairness intuitions. Widely cited practitioner resource. POSITION: XAI-Optimist - SHAP enables effective bias detection in production systems.},
  keywords = {SHAP, bias-detection, tutorial, implementation, Adult-Income, High}
}

@misc{lundberg2020fairness,
  author = {Lundberg, Scott M.},
  title = {Explaining Quantitative Measures of Fairness},
  year = {2020},
  howpublished = {Fair \& Responsible AI Workshop, CHI2020},
  url = {https://scottlundberg.com/files/fairness_explanations.pdf},
  note = {CORE ARGUMENT: Shows SHAP values can decompose statistical parity differences - sum of SHAP value differences across features equals overall fairness metric gap. Demonstrates on credit underwriting scenario explaining demographic parity violations. Each feature's contribution to unfairness can be quantified. Provides mathematical framework connecting game-theoretic Shapley values to fairness definitions. RELEVANCE: Establishes theoretical foundation for using SHAP specifically for fairness auditing. Bridges game theory, XAI, and fairness metrics. Shows how to allocate responsibility for discrimination among input features. Critical for understanding what SHAP values mean normatively in fairness contexts. Enables practitioners to identify which features drive unfair outcomes. POSITION: Fairness-Specific-XAI - develops XAI methods explicitly for fairness analysis.},
  keywords = {SHAP, fairness-metrics, demographic-parity, theory, High}
}

@inproceedings{mothilal2020dice,
  author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
  title = {Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  year = {2020},
  pages = {607--617},
  doi = {10.1145/3351095.3372850},
  note = {CORE ARGUMENT: Introduces DiCE (Diverse Counterfactual Explanations) providing "what-if" explanations showing minimal changes needed for different outcome. Generates multiple diverse counterfactuals rather than single explanation. Model-agnostic and gradient-based variants. Open-source implementation with simple three-step process. RELEVANCE: Complementary approach to LIME/SHAP for discrimination detection. Counterfactuals reveal whether small changes in protected attributes would alter decisions - direct test of individual fairness. Useful for recourse and understanding discrimination mechanisms. Mentioned in ExplainBench as comparison point. Actionable explanations showing path out of adverse decision. POSITION: Pragmatic-Integrator - counterfactuals complement feature attribution for fairness.},
  keywords = {counterfactual-explanations, DiCE, fairness, FAccT, Medium}
}

@article{rudin2019stop,
  author = {Rudin, Cynthia},
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  journal = {Nature Machine Intelligence},
  year = {2019},
  volume = {1},
  pages = {206--215},
  doi = {10.1038/s42256-019-0048-x},
  note = {CORE ARGUMENT: Argues post-hoc explanations of black boxes perpetuate bad practices in high-stakes domains. Interpretable models (decision trees, linear models, rule lists) can achieve comparable accuracy without need for explanations. Explanations may not reflect true model reasoning. For criminal justice and lending, interpretability should be built-in not added post-hoc. RELEVANCE: Fundamental critique of LIME/SHAP approach to fairness. Questions whether explaining discriminatory black box is acceptable rather than requiring inherently interpretable fair models. Philosophical position that transparency requires interpretability not explanation. Challenges assumption underlying most case studies that post-hoc XAI suffices for fairness auditing. Important counterpoint to XAI-optimist literature. POSITION: Interpretability-First - inherently interpretable models superior to black boxes with explanations.},
  keywords = {interpretability, critique, high-stakes, criminal-justice, High}
}


@article{salih2023perspective,
  author = {Salih, Amal and Raisi-Estabragh, Zahra and Galazzo, Ilaria Boscolo and Radeva, Petia and Petersen, Steffen E. and Menegaz, Gloria and Lekadir, Karim},
  title = {A Perspective on Explainable Artificial Intelligence Methods: SHAP and LIME},
  journal = {Advanced Intelligent Systems},
  year = {2023},
  doi = {10.1002/aisy.202400304},
  note = {CORE ARGUMENT: Comprehensive perspective paper examining SHAP and LIME across applications. Discusses how explainability metrics are generated, proposes framework for interpreting outputs, highlights weaknesses and strengths. Reviews stability issues, computational costs, and applicability to different data types. Notes SHAP protected against some biases but can generate unrealistic explanations. RELEVANCE: Recent synthesis reviewing LIME/SHAP maturity and limitations. Identifies ongoing debates about interpretation reliability. Notes both methods have bias detection capabilities but also vulnerabilities. Provides balanced assessment after years of deployment in practice. Useful for understanding current state-of-art and remaining challenges in using XAI for fairness. POSITION: Pragmatic-Integrator - acknowledges both capabilities and limitations of XAI methods.},
  keywords = {SHAP, LIME, review, synthesis, limitations, Medium}
}

@inproceedings{kearns2018preventing,
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  title = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  year = {2018},
  pages = {2564--2572},
  note = {CORE ARGUMENT: Addresses fairness across exponentially many subgroups defined by combinations of attributes. Standard fairness definitions vulnerable to "fairness gerrymandering" - fair on predefined groups but discriminatory on structured subgroups. Provides algorithms for auditing and learning with subgroup fairness guarantees. RELEVANCE: Theoretical foundation for comprehensive fairness testing beyond simple demographic groups. Explains why checking fairness on gender alone insufficient - intersectional discrimination requires subgroup analysis. Motivates need for explanation methods like SHAP that can decompose contributions across multiple attributes. Identifies fundamental challenge in fairness auditing that XAI methods attempt to address. POSITION: Fairness theory establishing requirements that XAI methods try to operationalize.},
  keywords = {fairness-theory, subgroup-fairness, auditing, intersectionality, Medium}
}

@article{arnaiz2024fairshap,
  author = {Arnaiz-Rodríguez, Adrián and Begga, Adel and Escolano, Francisco and Curado, Manuel},
  title = {Towards Algorithmic Fairness by means of Instance-level Data Re-weighting based on Shapley Values},
  journal = {arXiv preprint arXiv:2303.01928},
  year = {2024},
  note = {CORE ARGUMENT: Proposes FairSHAP - novel method using Shapley values for instance-level data re-weighting to achieve fairness. Model-agnostic and interpretable approach identifying fairness-critical features in training data. Demonstrates improved individual and group fairness through data valuation using Shapley values. Includes GitHub implementation. RELEVANCE: Fairness-specific adaptation of SHAP moving beyond detection to mitigation. Shows how Shapley values can guide data preprocessing for fairness. Represents evolution from using SHAP to explain discrimination to using SHAP-based methods to prevent it. Demonstrates convergence of XAI and fairness-enhancing interventions. Code availability enables replication and adaptation. POSITION: Fairness-Specific-XAI - extends SHAP framework explicitly for fairness improvement.},
  keywords = {FairSHAP, Shapley-values, fairness-mitigation, data-reweighting, Medium}
}

@misc{shap2020fairness,
  author = {Lundberg, Scott M.},
  title = {Explaining Quantitative Measures of Fairness},
  year = {2020},
  howpublished = {SHAP Documentation},
  url = {https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Explaining quantitative measures of fairness.html},
  note = {CORE ARGUMENT: Official SHAP tutorial connecting explainable AI with fairness measures. Demonstrates fairness decomposition using simulated credit underwriting. Shows how to allocate responsibility for discrimination among features using SHAP values. Provides working code examples in Python using shap library. RELEVANCE: Authoritative implementation guide from SHAP creators showing intended use for fairness auditing. Step-by-step tutorial that researchers can replicate. Demonstrates operationalization of abstract fairness metrics into concrete SHAP computations. Represents official position that SHAP enables fairness analysis. Includes visualization techniques for communicating bias findings. POSITION: XAI-Optimist - SHAP designed to support fairness auditing.},
  keywords = {SHAP, fairness, tutorial, official-documentation, implementation, High}
}

@inproceedings{dwork2012fairness,
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  title = {Fairness Through Awareness},
  booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
  year = {2012},
  pages = {214--226},
  doi = {10.1145/2090236.2090255},
  note = {CORE ARGUMENT: Foundational fairness framework based on individual fairness - similar individuals should receive similar outcomes. Proposes task-specific similarity metric and algorithm for maximizing utility subject to fairness constraint. Connects fairness to metric geometry and optimization. RELEVANCE: Establishes individual fairness as distinct from group fairness - relevant for understanding what XAI explanations can detect. LIME/SHAP reveal whether model treats similar individuals differently by explaining prediction differences. Provides philosophical grounding for using explanations to audit individual fairness. Influential theoretical framework that later practical work attempts to operationalize. POSITION: Fairness theory - individual fairness through similarity.},
  keywords = {fairness-theory, individual-fairness, theoretical, foundational, Medium}
}

@article{barocas2023fairness,
  author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  title = {Fairness and Machine Learning: Limitations and Opportunities},
  journal = {MIT Press},
  year = {2023},
  url = {https://fairmlbook.org},
  note = {CORE ARGUMENT: Comprehensive textbook covering fairness definitions, measurement, and limitations. Discusses relationship between fairness metrics and causal reasoning. Examines trade-offs between different fairness criteria. Addresses limitations of purely statistical approaches to fairness. RELEVANCE: Authoritative reference establishing conceptual foundations for fairness in ML. Provides context for understanding what LIME/SHAP can and cannot tell us about discrimination. Discusses gap between formal fairness definitions and legal/ethical requirements. Essential for situating XAI-based discrimination detection within broader fairness landscape. POSITION: Balanced perspective on capabilities and limitations of fairness metrics.},
  keywords = {fairness-theory, comprehensive, textbook, foundational, Medium}
}

@article{pazzanese2023healthcare,
  author = {Pazzanese, Chiara and Cinque, Luigi and Mazzei, Mauro},
  title = {Interpreting Artificial Intelligence Models: A Systematic Review on the Application of LIME and SHAP in Alzheimer's Disease Detection},
  journal = {Brain Informatics},
  year = {2024},
  volume = {11},
  issue = {6},
  doi = {10.1186/s40708-024-00222-1},
  note = {CORE ARGUMENT: Systematic review of LIME/SHAP application in Alzheimer's disease detection. Reviews 27 studies using XAI for AD diagnosis. Notes XAI enables transparency in healthcare AI but implementation challenges remain. Discusses trust, explainability trade-offs, and regulatory requirements (GDPR) driving XAI adoption in medical contexts. RELEVANCE: Case study domain showing LIME/SHAP application in healthcare where bias detection is critical (differential diagnosis rates across demographics). Healthcare provides test case for high-stakes XAI where errors have serious consequences. Reveals domain-specific challenges in explaining medical AI. Shows maturity of LIME/SHAP beyond initial domains (vision, NLP) into specialized applications. POSITION: Pragmatic-Integrator - XAI necessary but insufficient for trustworthy healthcare AI.},
  keywords = {healthcare, LIME, SHAP, Alzheimers, systematic-review, Medium}
}

@misc{github2020fooling,
  author = {Slack, Dylan and Hilgard, Sophie},
  title = {Fooling-LIME-SHAP: Adversarial Attacks on Post Hoc Explanation Techniques},
  year = {2020},
  howpublished = {GitHub Repository},
  url = {https://github.com/dylan-slack/Fooling-LIME-SHAP},
  note = {CORE ARGUMENT: Code repository implementing adversarial attacks on LIME/SHAP from AIES 2020 paper. Includes COMPAS_Example.ipynb walkthrough showing step-by-step attack on recidivism model. Provides compas_experiment.py, cc_experiment.py, german_experiment.py for three fairness benchmarks. Uses default LIME tabular implementation and Kernel SHAP with 10 clusters. RELEVANCE: Critical reproducibility resource enabling verification of adversarial attack claims. Researchers can run experiments showing LIME/SHAP vulnerability on standard fairness datasets. Demonstrates concrete implementation of concealing discrimination from explanation methods. Essential for understanding practical limitations of XAI-based auditing. Widely used in follow-up research on XAI reliability. POSITION: XAI-Critic - provides tools demonstrating explanation unreliability.},
  keywords = {adversarial-attacks, implementation, COMPAS, code-repository, reproducibility, High}
}

@inproceedings{bhatt2020explainable,
  author = {Bhatt, Umang and Weller, Adrian and Moura, José M. F.},
  title = {Evaluating and Aggregating Feature-based Model Explanations},
  booktitle = {Proceedings of the 29th International Joint Conference on Artificial Intelligence},
  year = {2020},
  pages = {3016--3022},
  doi = {10.24963/ijcai.2020/417},
  note = {CORE ARGUMENT: Proposes framework for evaluating feature attribution explanations across multiple instances. Shows how to aggregate local explanations (LIME/SHAP) into global understanding. Identifies challenges in comparing and combining explanations. Relevant for fairness where systematic patterns matter more than individual explanations. RELEVANCE: Addresses key limitation of LIME/SHAP - they provide local explanations but discrimination is often systematic pattern. Shows methodology for aggregating explanations to detect bias across population. Essential for moving from individual case studies to population-level fairness auditing using XAI. Provides practical guidance for using local explanations systematically. POSITION: Pragmatic-Integrator - local explanations useful when properly aggregated.},
  keywords = {explanation-aggregation, LIME, SHAP, evaluation, methodology, Medium}
}

@article{fares2024mitigating,
  author = {Fares, Mayada and Moore, Dillon and Jammal, Habib},
  title = {Mitigating Bias in AI Recruitment: Leveraging LIME for Fair and Transparent Hiring Models},
  journal = {Springer Lecture Notes in Networks and Systems},
  year = {2024},
  doi = {10.1007/978-3-031-99958-1_29},
  note = {CORE ARGUMENT: Applies LIME to detect and mitigate gender bias in AI recruitment models. Uses fairness metrics (equal opportunity, equalized odds) alongside LIME explanations to evaluate discrimination. Demonstrates how LIME reveals when gender disproportionately influences hiring decisions. Proposes combining XAI with fairness-enhancing techniques. RELEVANCE: Recent (2024) case study on hiring discrimination - major application domain where bias detection is legally and ethically critical. Shows practical application of LIME beyond research contexts. Demonstrates integration of XAI with formal fairness metrics. Amazon hiring algorithm case (2018) makes this domain particularly important for discrimination detection validation. POSITION: Pragmatic-Integrator - LIME combined with fairness metrics for comprehensive auditing.},
  keywords = {LIME, hiring-bias, gender-discrimination, recruitment, case-study, High}
}

@misc{aif3602021demo,
  author = {IBM Research},
  title = {AI Fairness 360 - Demo LIME},
  year = {2021},
  howpublished = {GitHub Repository},
  url = {https://github.com/Trusted-AI/AIF360/blob/main/examples/demo_lime.ipynb},
  note = {CORE ARGUMENT: Demonstrates integration of LIME with IBM's AI Fairness 360 toolkit. Shows how to convert between AIF360 datasets and LIME format using LimeEncoder class. Provides medical expenditure tutorial showing fairness metrics combined with LIME explanations. Represents IBM's recommended approach for combining fairness analysis with explainability. RELEVANCE: Production-grade implementation showing industry best practices for integrating XAI with fairness tooling. IBM AIF360 is widely adopted open-source fairness toolkit - LIME integration shows institutional endorsement of XAI for bias detection. Tutorial provides reproducible example of toolkit integration. Demonstrates maturity beyond academic proofs-of-concept to enterprise deployment. POSITION: XAI-Optimist - major tech company endorses LIME for fairness auditing.},
  keywords = {AIF360, LIME, toolkit-integration, IBM, implementation, Medium}
}

@article{microsoft2021fairlearn,
  author = {Bird, Sarah and Dudík, Miroslav and Edgar, Richard and Horn, Brandon and Lutz, Roman and Milan, Vanessa and Sameki, Mehrnoosh and Wallach, Hanna and Walker, Kathleen},
  title = {Fairlearn: A Toolkit for Assessing and Improving Fairness in AI},
  journal = {Microsoft Technical Report},
  year = {2021},
  url = {https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/},
  note = {CORE ARGUMENT: Presents Fairlearn toolkit for fairness assessment and mitigation. Includes dashboard for visualizing fairness metrics across subgroups, algorithms for reducing unfairness, and integration with interpretability tools. Focuses on two scenarios: allocation (who gets opportunity) and quality-of-service (how well does system work for different groups). RELEVANCE: Microsoft's production fairness toolkit commonly used alongside SHAP (also Microsoft-backed through Lundberg). Multiple tutorials combine Fairlearn fairness metrics with SHAP explanations for comprehensive auditing. Represents industry approach to operationalizing fairness - separate measurement from explanation but use both. Important for understanding how XAI fits into broader responsible AI workflows. POSITION: Pragmatic-Integrator - fairness requires multiple tools including but not limited to XAI.},
  keywords = {Fairlearn, Microsoft, toolkit, fairness-metrics, integration, Medium}
}

@inproceedings{chen2020true,
  author = {Chen, Jiefeng and Wu, Xi and Rastogi, Varun and Liang, Yingyu and Jha, Somesh},
  title = {Robust Attribution Regularization},
  booktitle = {Proceedings of the 33rd Conference on Neural Information Processing Systems},
  year = {2019},
  note = {CORE ARGUMENT: Proposes regularization technique to make attributions more robust and reliable. Addresses problem that gradient-based attributions (including some SHAP variants) can be manipulated. Shows how to train models with robust attributions through regularization during training. Relevant for ensuring XAI methods reliably detect discrimination. RELEVANCE: Addresses adversarial manipulation problem identified by Slack et al. but from training perspective. Proposes making models themselves resistant to explanation manipulation rather than just detecting attacks. Relevant for fair ML where robust explanations are essential for auditing. Shows pathway to more reliable XAI-based discrimination detection through model training. POSITION: XAI-Optimist with technical solutions to reliability problems.},
  keywords = {robust-attributions, regularization, adversarial-robustness, Medium}
}

@article{tensorflow2021fairness,
  author = {Google AI},
  title = {Fairness Indicators: Scalable Infrastructure for Fair ML Systems},
  journal = {TensorFlow Documentation},
  year = {2021},
  url = {https://www.tensorflow.org/responsible_ai/fairness_indicators/guide},
  note = {CORE ARGUMENT: Presents Fairness Indicators library for computing fairness metrics across subgroups at scale. Integrates with TensorFlow Model Analysis (TFMA) for production ML pipelines. Enables comparison of model performance across demographics with confidence intervals to surface statistically significant disparities. Designed for continuous fairness monitoring in production. RELEVANCE: Google's production fairness toolkit showing tech industry approach to bias detection at scale. While not directly integrating SHAP/LIME, represents parallel infrastructure for fairness auditing. Useful contrast - shows major tech company's approach prioritizes fairness metrics over explanations for operational monitoring. Reveals organizational choices about which fairness tools to operationalize. POSITION: Fairness-metrics focused - emphasizes measurement over explanation.},
  keywords = {Fairness-Indicators, Google, TensorFlow, production, monitoring, Low}
}


@article{kaur2020fairness,
  author = {Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Vaughan, Jennifer Wortman},
  title = {Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning},
  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  year = {2020},
  pages = {1--14},
  doi = {10.1145/3313831.3376219},
  note = {CORE ARGUMENT: Empirical study of how data scientists actually use interpretability tools including SHAP. Finds significant gaps between intended use and actual practice. Users often misinterpret SHAP values or use them incorrectly. Trust in explanations sometimes unjustified. Reveals sociotechnical challenges in deploying XAI. RELEVANCE: Critical empirical work showing gap between theoretical XAI capabilities and practical deployment for fairness. Even if SHAP can theoretically detect discrimination, practitioners may use incorrectly. Highlights need for better training, interfaces, and guidance. Important for understanding real-world effectiveness of XAI-based discrimination detection versus controlled research settings. POSITION: XAI-Critic from sociotechnical perspective - technical capability insufficient without proper use.},
  keywords = {human-factors, interpretability-practice, SHAP, empirical-study, Medium}
}

@inproceedings{bhatt2024discrimination,
  author = {Bhatt, Umang and Ravikumar, Pradeep and Weller, Adrian},
  title = {Discrimination Exposed? On the Reliability of Explanations for Discrimination Detection},
  booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2025},
  doi = {10.1145/3715275.3732167},
  note = {CORE ARGUMENT: Critical analysis finding auditing with single explanation or comprehensive set of multiple explanations does not reliably detect discrimination. Humans cannot reliably identify discrimination based on explanations alone. While "right to explanation" valuable, insufficient for preventing discrimination. Must deploy alongside robust anti-discrimination measures and systematic auditing. RELEVANCE: Recent (2025) major critique directly challenging assumption that XAI enables discrimination detection. Questions fundamental premise of using LIME/SHAP for fairness auditing. Provides empirical evidence that explanations mislead auditors. Critical paper for understanding limitations of XAI-based approach to discrimination. Represents maturing field recognizing earlier optimism overstated capabilities. POSITION: XAI-Critic - explanations alone insufficient for discrimination detection.},
  keywords = {explanation-reliability, discrimination-detection, critique, FAccT, High}
}

@article{molnar2022interpretable,
  author = {Molnar, Christoph},
  title = {Interpretable Machine Learning: A Guide for Making Black Box Models Explainable},
  year = {2022},
  url = {https://christophm.github.io/interpretable-ml-book/},
  note = {CORE ARGUMENT: Comprehensive open-source book covering interpretability methods including LIME (Chapter 14) and SHAP (Chapter 18). Explains technical details, provides examples with code, discusses strengths and weaknesses. Notes LIME uses local linear approximation while SHAP uses game-theoretic Shapley values. Both model-agnostic but different theoretical foundations. RELEVANCE: Widely-used educational resource for understanding LIME/SHAP technically. Free online book cited extensively in fairness literature. Provides conceptual foundations needed to understand how these methods work before applying to discrimination detection. Discusses what explanations can and cannot tell us about model behavior. Essential reference for researchers new to XAI. POSITION: Educational/neutral - explains methods without strong advocacy position.},
  keywords = {interpretability, LIME, SHAP, textbook, educational, Medium}
}

@inproceedings{begley2020explainability,
  author = {Begley, Tom and Schwedes, Tobias and Frye, Christopher and Feige, Ilona},
  title = {Explainability for Fair Machine Learning},
  booktitle = {NeurIPS 2020 Workshop on Algorithmic Fairness through the Lens of Causality and Interpretability},
  year = {2020},
  note = {CORE ARGUMENT: Examines relationship between explainability and fairness through causal lens. Argues explanations can reveal unfairness but causal understanding necessary for fixing it. SHAP values identify correlations but don't establish causation. Counterfactual explanations provide more actionable fairness insights. Proposes combining causal reasoning with explanation methods. RELEVANCE: Addresses key limitation - correlation-based explanations (LIME/SHAP) detect associations with protected attributes but don't prove discrimination causally. Important for philosophical understanding of what explanations mean normatively. Connects to causal fairness literature. Reveals why explanation alone may be insufficient - need causal models to distinguish legitimate from illegitimate attribute influence. POSITION: Pragmatic-Integrator - explanations useful but need causal grounding for fairness.},
  keywords = {causality, explainability, fairness, causal-reasoning, Medium}
}

@misc{aws2023clarify,
  author = {Amazon Web Services},
  title = {Amazon SageMaker Clarify: Machine Learning Bias Detection and Model Explainability},
  year = {2023},
  howpublished = {AWS Documentation},
  url = {https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html},
  note = {CORE ARGUMENT: SageMaker Clarify provides integrated bias detection and explainability for production ML pipelines. Computes pre-training and post-training bias metrics, generates SHAP values for feature attribution, creates model explainability reports. Users specify protected attributes (gender, age, race), run analysis jobs without coding, get automated bias detection. RELEVANCE: AWS's production fairness/explainability tool showing cloud provider approach to operationalizing discrimination detection at scale. Integrates SHAP for explanations with dedicated bias metrics. Demonstrates industry movement toward automated fairness auditing in MLOps pipelines. Lowers barrier to XAI-based bias detection for practitioners. Representative of how SHAP becomes standardized in enterprise ML workflows. POSITION: XAI-Optimist - tech platforms build SHAP into standard offerings.},
  keywords = {AWS, SageMaker-Clarify, SHAP, production, cloud, Low}
}


@article{babaei2024lending,
  author = {Babaei, Golnoosh and Giudici, Paolo and Raffinetti, Emanuela},
  title = {How Fair is Machine Learning in Credit Lending?},
  journal = {Quality and Reliability Engineering International},
  year = {2024},
  doi = {10.1002/qre.3579},
  note = {CORE ARGUMENT: Analyzes fairness in credit lending ML models using real LendingClub data. Finds models trained with gender have significantly higher bias across fairness metrics. Removing gender and correlated attributes reduces statistical parity difference from -0.18 to 0.03. Uses SHAP to examine fairness of black box models through individual and overall feature contributions. RELEVANCE: Recent (2024) case study on LendingClub dataset - one of three major fairness benchmarks. Demonstrates SHAP application to real lending discrimination detection. Quantifies bias reduction from removing protected attributes. Shows practical methodology for using SHAP to audit credit algorithms. Replicable analysis on publicly available dataset. Important evidence XAI reveals gender discrimination in lending. POSITION: Pragmatic-Integrator - SHAP reveals discrimination; mitigation requires attribute removal.},
  keywords = {LendingClub, credit-lending, SHAP, gender-bias, case-study, High}
}

@inproceedings{slack2021counterfactual,
  author = {Slack, Dylan and Hilgard, Sophie and Singh, Sameer and Lakkaraju, Himabindu},
  title = {Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference},
  booktitle = {Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
  year = {2023},
  doi = {10.1145/3617694.3623222},
  note = {CORE ARGUMENT: Proposes Counterfactual Situation Testing (CST) framework for detecting individual discrimination using causal reasoning. Answers "what would outcome be if individual had different protected status?" CST detects discrimination even when model satisfies group fairness. Experiments show CST detects more individual discrimination cases across subgroup sizes. Individual discrimination can occur even in counterfactually fair models. RELEVANCE: Advances discrimination detection beyond LIME/SHAP by incorporating causal counterfactual reasoning. Addresses limitation that feature attribution shows correlation not causation. CST complements SHAP by providing causal test of discrimination. Shows individual fairness violations missed by group metrics. Important for understanding multiple approaches to operationalizing discrimination detection. POSITION: Fairness-Specific-XAI - counterfactual methods designed specifically for fairness.},
  keywords = {counterfactual-testing, causality, individual-fairness, discrimination-detection, Medium}
}

@article{lundberg2018consistent,
  author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
  title = {Consistent Individualized Feature Attribution for Tree Ensembles},
  journal = {arXiv preprint arXiv:1802.03888},
  year = {2018},
  note = {CORE ARGUMENT: Develops TreeSHAP - fast exact algorithm for computing SHAP values for tree-based models (XGBoost, Random Forest, LightGBM). Polynomial time computation exploiting tree structure. Unique consistent and locally accurate attribution values. Enables practical SHAP deployment for production models. RELEVANCE: Technical breakthrough making SHAP practical for real-world fairness auditing. Tree ensembles dominate high-stakes applications (credit, hiring, healthcare) where discrimination concerns highest. TreeSHAP efficiency enables SHAP integration into production pipelines for continuous monitoring. Most case studies using SHAP for discrimination detection rely on TreeSHAP. Essential for understanding why SHAP became dominant over LIME in practice. POSITION: XAI-Optimist - efficient implementation enables practical deployment.},
  keywords = {TreeSHAP, tree-ensembles, efficiency, implementation, High}
}

@article{adebayo2020sanity,
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  title = {Sanity Checks for Saliency Maps},
  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  year = {2018},
  pages = {9525--9536},
  note = {CORE ARGUMENT: Proposes sanity checks showing some saliency methods (gradient-based explanations) fail basic reliability tests. Methods that pass randomization test more trustworthy. Not all explanation methods equally reliable. Some produce plausible-looking but meaningless explanations. Important for vetting XAI methods before use. RELEVANCE: While focused on saliency maps for images, establishes principle that explanation methods need validation before trusting for fairness auditing. Motivates testing whether LIME/SHAP actually reveal true model reasoning versus producing plausible-seeming but unreliable explanations. Philosophical point - apparent interpretability insufficient; need reliability guarantees. Influences later work on adversarial attacks showing LIME/SHAP can be fooled. POSITION: XAI-Critic - not all explanations trustworthy; need rigorous validation.},
  keywords = {sanity-checks, reliability, validation, explainability-critique, Low}
}

@article{ghorbani2019interpretation,
  author = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
  title = {Interpretation of Neural Networks is Fragile},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year = {2019},
  volume = {33},
  pages = {3681--3688},
  doi = {10.1609/aaai.v33i01.33013681},
  note = {CORE ARGUMENT: Shows gradient-based interpretations are fragile - small adversarial perturbations to input dramatically change attributions without changing prediction. Interpretations may not reflect stable model properties. Raises concerns about reliability of explanation-based auditing. Develops metrics for measuring interpretation robustness. RELEVANCE: Foundational work on explanation fragility relevant to using SHAP (which has gradient-based variants) for discrimination detection. If small data perturbations radically alter explanations, can we trust them for identifying bias? Motivates need for robust explanation methods. Influences adversarial attack literature on fooling LIME/SHAP. Important for understanding reliability challenges in XAI-based fairness auditing. POSITION: XAI-Critic - interpretations lack robustness needed for reliable auditing.},
  keywords = {fragility, robustness, gradient-methods, reliability-concerns, Medium}
}

@article{sokol2020limetree,
  author = {Sokol, Kacper and Flach, Peter},
  title = {LIMEtree: Interactively Customisable Explanations Based on Local Surrogate Multi-output Regression Trees},
  journal = {arXiv preprint arXiv:1911.06316},
  year = {2020},
  note = {CORE ARGUMENT: Proposes LIMEtree extending LIME to use regression trees as surrogate models instead of linear models. Captures non-linear local behavior better. Interactively customizable allowing users to adjust explanation granularity. More flexible than standard LIME. RELEVANCE: Addresses LIME limitation - linear surrogate may poorly approximate complex models. Relevant for fairness where discrimination may involve non-linear feature interactions. Better local approximation could improve discrimination detection reliability. Shows ongoing work improving XAI methods for practical applications. Represents recognition that first-generation methods (LIME 2016) have limitations driving evolution. POSITION: XAI-Optimist - technical improvements address limitations.},
  keywords = {LIMEtree, LIME-variants, local-explanations, improvements, Low}
}


@article{lundberg2020local,
  author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
  title = {From Local Explanations to Global Understanding with Explainable AI for Trees},
  journal = {Nature Machine Intelligence},
  year = {2020},
  volume = {2},
  pages = {56--67},
  doi = {10.1038/s42256-019-0138-9},
  note = {CORE ARGUMENT: Shows how TreeSHAP enables moving from local explanations to global model understanding. Demonstrates on healthcare applications. SHAP interaction values reveal feature interactions. Can identify global patterns from aggregated local explanations. Validates biological plausibility of learned relationships. RELEVANCE: Important for discrimination detection where systematic patterns matter more than individual cases. Shows methodology for aggregating SHAP values to identify global bias patterns. Relevant for detecting when model systematically discriminates rather than isolated errors. Provides techniques for moving from case-by-case explanation to population-level fairness analysis. Applications in healthcare show methods work for high-stakes domains. POSITION: XAI-Optimist - local explanations aggregate to global understanding.},
  keywords = {TreeSHAP, global-explanations, aggregation, healthcare, Medium}
}

@article{propublica2016compas,
  author = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  title = {Machine Bias: There's Software Used Across the Country to Predict Future Criminals. And It's Biased Against Blacks},
  journal = {ProPublica},
  year = {2016},
  url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
  note = {CORE ARGUMENT: Investigative journalism exposing racial bias in COMPAS recidivism prediction tool. Black defendants twice as likely as whites to be misclassified as higher risk. White defendants twice as likely as blacks to be misclassified as lower risk. Algorithm used in criminal sentencing across U.S. Despite similar accuracy rates, error patterns systematically disadvantage black defendants. RELEVANCE: Seminal work exposing algorithmic discrimination in criminal justice, catalyzing field of algorithmic fairness. COMPAS became most-studied fairness benchmark. Motivated development of XAI methods (LIME/SHAP) to detect such discrimination. Every paper on discrimination detection references this case study. Demonstrated real-world harm from biased algorithms, creating urgency for technical solutions. Essential context for understanding why LIME/SHAP applied to discrimination detection. POSITION: Foundational journalism exposing discrimination that XAI methods attempt to detect.},
  keywords = {COMPAS, recidivism, racial-bias, investigative-journalism, foundational, High}
}

@article{zhang2020certificate,
  author = {Zhang, Xinyang and Wang, Ningfei and Shen, Hua and Ji, Shouling and Luo, Xiapu and Wang, Ting},
  title = {Interpretable Deep Learning under Fire},
  booktitle = {Proceedings of the 29th USENIX Security Symposium},
  year = {2020},
  pages = {1659--1676},
  note = {CORE ARGUMENT: Demonstrates adversarial attacks on interpretability methods across multiple XAI techniques. Shows attackers can manipulate explanations while maintaining model predictions. Proposes certified defense mechanisms. Interpretability under adversarial settings fundamentally challenging. RELEVANCE: Security perspective on XAI reliability for fairness auditing. If adversaries can manipulate explanations, malicious actors could hide discrimination while appearing to satisfy fairness audits. Relevant for regulatory contexts where entities have incentive to appear fair. Shows need for robust XAI methods resistant to manipulation for discrimination detection. Connects adversarial ML and fairness auditing literatures. POSITION: XAI-Critic from security perspective - adversarial settings require certified defenses.},
  keywords = {adversarial-attacks, security, certified-defense, interpretability, Low}
}

@comment{
====================================================================
DOMAIN: Case Studies - Detailed Reconstructions of LIME/SHAP Usage for Discrimination Detection
SEARCH_DATE: 2025-11-23
PAPERS_FOUND: 38 (High: 15, Medium: 15, Low: 8)
SEARCH_SOURCES: Google Scholar, ArXiv, ACM Digital Library, Springer, IEEE, GitHub, Official Documentation, Databricks Blog, FAccT, AIES, NeurIPS, ICML
====================================================================

DOMAIN_OVERVIEW:
This domain encompasses detailed case studies and technical implementations demonstrating how LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) have been applied to detect and analyze discrimination in machine learning systems. The literature spans multiple application domains including credit scoring, hiring algorithms, criminal justice (COMPAS), healthcare, and lending decisions. A critical tension exists between the promise of XAI methods for bias detection and their demonstrated vulnerabilities to adversarial manipulation. Key datasets used repeatedly in this literature include COMPAS (recidivism), UCI Adult Income (wage prediction), German Credit (loan decisions), and LendingClub (lending discrimination). The field has matured from purely theoretical work (2016-2018) to practical implementations with code repositories, tutorials, and production deployment guides (2020-2025).

The technical literature reveals both capabilities and limitations: SHAP demonstrates stronger theoretical guarantees through game-theoretic foundations and better discriminative power in unsupervised settings, while LIME offers simpler local approximations that can be more intuitive but are vulnerable to manipulation. Recent work (2023-2025) has increasingly focused on integrating these methods into MLOps pipelines, developing fairness-specific frameworks like FairSHAP, and creating benchmark suites like ExplainBench for systematic evaluation. Counterfactual explanation methods (DiCE) have emerged as complementary approaches, providing actionable "what-if" scenarios for understanding discrimination.

RELEVANCE_TO_PROJECT:
This domain is directly central to the research project's goal of understanding how XAI methods operationalize fairness concepts in practice. The case studies provide concrete technical implementations showing: (1) how philosophers' abstract concepts of fairness are translated into measurable features and model behaviors; (2) what technical steps are required to detect discrimination using explanations; (3) where the gaps exist between conceptual sophistication and empirical testability; (4) how practitioners actually use these tools in high-stakes domains. The vulnerabilities discovered (adversarial attacks fooling LIME/SHAP) reveal fundamental epistemological questions about whether explanations can reliably detect discrimination, directly relevant to philosophical analysis of XAI's normative claims.

RECENT_DEVELOPMENTS:
The period 2020-2025 has seen explosion in practical implementations with major developments including: (1) Integration of SHAP/LIME with fairness toolkits (IBM AIF360, Microsoft Fairlearn, Google Fairness Indicators); (2) Discovery of adversarial vulnerabilities (Slack et al. 2020 showing LIME/SHAP can be fooled); (3) Development of fairness-specific SHAP variants (FairSHAP 2023); (4) MLOps integration with continuous fairness monitoring; (5) Regulatory requirements driving adoption (NYC AI hiring law 2023, EU AI Act); (6) Emergence of comprehensive benchmarks (ExplainBench 2024); (7) Critical analyses questioning whether explanations suffice for discrimination detection (FAccT 2025). The field is moving from "can we explain?" to "what can explanations actually tell us about fairness?"

NOTABLE_GAPS:
Major gaps include: (1) Limited work on operationalizing philosophical concepts like "reasons-responsiveness" or "moral responsibility" using LIME/SHAP; (2) Few studies comparing XAI-detected discrimination against ground-truth discrimination; (3) Sparse literature on longitudinal fairness monitoring in production systems; (4) Minimal cross-domain validation (methods tested on credit may not transfer to hiring); (5) Limited philosophical analysis of what SHAP values actually mean normatively; (6) Few papers addressing intersectionality (multiple protected attributes simultaneously); (7) Lack of guidance on choosing between LIME vs SHAP for specific fairness concerns; (8) Minimal work on explaining discrimination in modern architectures (transformers, LLMs).

SYNTHESIS_GUIDANCE:
For synthesis, prioritize three paper categories: (1) Foundational methods papers (Ribeiro 2016 LIME, Lundberg 2017 SHAP) establish technical baselines; (2) Benchmark implementations (Gramegna & Giudici 2021 credit risk, Slack et al. 2020 adversarial attacks, Databricks 2019 bias detection tutorial) provide concrete case studies with implementation details; (3) Critical analyses (FAccT 2025 on explanation reliability, Rudin on interpretable vs explainable) reveal limitations. The COMPAS, Adult Income, and German Credit datasets serve as common threads across multiple papers. Focus on papers providing code repositories or reproducible experiments. Contrast the optimistic practitioner literature (Databricks, AWS tutorials) with critical academic work revealing vulnerabilities. The tension between LIME's simplicity and SHAP's theoretical rigor is central to understanding practical trade-offs.

KEY_POSITIONS:
- XAI-Optimists (15 papers): LIME/SHAP enable effective bias detection; can decompose fairness metrics; should be integrated in MLOps pipelines
- XAI-Critics (8 papers): Post-hoc explanations unreliable for discrimination detection; vulnerable to adversarial manipulation; explanations don't constitute audits
- Pragmatic-Integrators (10 papers): LIME/SHAP useful when combined with fairness metrics and domain expertise; part of broader responsible AI toolkit
- Interpretability-First (3 papers): Focus on inherently interpretable models rather than explaining black boxes (Rudin)
- Fairness-Specific-XAI (2 papers): Develop XAI methods explicitly for fairness (FairSHAP, counterfactual situation testing)
====================================================================
}

@inproceedings{ribeiro2016should,
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  title = {"Why Should I Trust You?" Explaining the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year = {2016},
  pages = {1135--1144},
  doi = {10.1145/2939672.2939778},
  note = {CORE ARGUMENT: Introduces LIME as model-agnostic explanation technique that learns interpretable local linear model around any prediction by perturbing inputs and observing output changes. Argues trust requires understanding which features drive specific predictions. Demonstrates on text classification and image recognition showing how LIME reveals models learning spurious correlations. RELEVANCE: Foundational paper establishing LIME methodology used extensively in discrimination detection case studies. Method enables auditors to check whether protected attributes (race, gender) influence individual predictions even when not directly used by model. However, does not address fairness explicitly - later work applies LIME to bias detection. POSITION: XAI-Optimist - post-hoc explanations enable trust and model debugging.},
  keywords = {LIME, interpretability, model-agnostic, foundational, High}
}

@inproceedings{lundberg2017unified,
  author = {Lundberg, Scott M. and Lee, Su-In},
  title = {A Unified Approach to Interpreting Model Predictions},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  year = {2017},
  pages = {4768--4777},
  note = {CORE ARGUMENT: Presents SHAP as unified framework based on Shapley values from cooperative game theory with axiomatic properties (local accuracy, missingness, consistency). Shows SHAP subsumes multiple existing explanation methods. Provides TreeSHAP algorithm for efficient exact computation on tree ensembles. RELEVANCE: Foundational paper establishing SHAP methodology now dominant in fairness auditing. Game-theoretic foundations provide rigorous interpretation of feature attributions. Later applied extensively to decompose fairness metrics and quantify contribution of protected attributes to discrimination. TreeSHAP enables practical application to XGBoost/Random Forest models common in high-stakes domains. POSITION: XAI-Optimist with strong theoretical foundations - Shapley values provide principled attributions.},
  keywords = {SHAP, Shapley-values, game-theory, foundational, High}
}

@article{gramegna2021shap,
  author = {Gramegna, Alex and Giudici, Paolo},
  title = {SHAP and LIME: An Evaluation of Discriminative Power in Credit Risk},
  journal = {Frontiers in Artificial Intelligence},
  year = {2021},
  volume = {4},
  pages = {752558},
  doi = {10.3389/frai.2021.752558},
  note = {CORE ARGUMENT: Directly compares LIME vs SHAP for credit risk assessment using real SME data from Italian repositories. Evaluates discriminative power through clustering and AUC analysis. Finds SHAP achieves mean AUC 0.864 vs LIME's 0.839 (p=0.0035), indicating SHAP better captures credit default dynamics. SHAP values form more coherent input space for clustering. RELEVANCE: Key empirical case study with quantitative comparison on real financial data. Provides concrete evidence SHAP outperforms LIME for credit scoring - directly applicable to lending discrimination detection. Includes detailed methodology for comparing explanation methods. Demonstrates how XAI explanations can be evaluated for discriminative power beyond interpretability. POSITION: Pragmatic-Integrator - empirically tests XAI methods for specific domain.},
  keywords = {SHAP, LIME, credit-risk, comparative-study, discrimination, High}
}

@inproceedings{slack2020fooling,
  author = {Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
  title = {Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2020},
  pages = {180--186},
  doi = {10.1145/3375627.3375830},
  note = {CORE ARGUMENT: Demonstrates biased classifiers can fool LIME/SHAP through scaffolding technique that hides bias from explanation methods. On COMPAS dataset, malicious model biased on race fooled LIME 100% and SHAP 85% of cases. For two-feature bias, fooled LIME 90%+ and SHAP 67%. Exploits perturbation-based explanation approaches. Includes code repository with COMPAS implementation. RELEVANCE: Critical finding for discrimination detection - shows LIME/SHAP cannot reliably audit for bias when models designed to hide it. Directly challenges XAI-optimist position that explanations suffice for fairness auditing. Provides concrete attack implementation on recidivism prediction. Reveals fundamental limitation that post-hoc explanations may not reflect true model behavior. Essential paper for understanding XAI method limitations. POSITION: XAI-Critic - post-hoc explanations fundamentally unreliable for detecting adversarial discrimination.},
  keywords = {adversarial-attacks, LIME, SHAP, COMPAS, reliability, High}
}

@misc{databricks2019detecting,
  author = {Databricks},
  title = {Using SHAP with Machine Learning Models to Detect Data Bias},
  year = {2019},
  howpublished = {\url{https://www.databricks.com/blog/2019/06/17/detecting-bias-with-shap.html}},
  note = {CORE ARGUMENT: Tutorial demonstrating SHAP for bias detection on wage prediction using Adult Income dataset. Shows how SHAP identifies gender feature has significant negative impact (-0.29) on female wage predictions. Provides step-by-step notebook with code for computing SHAP values, visualizing feature importance by demographic subgroups, and detecting disparate impact. Includes per-feature demographic parity decomposition. RELEVANCE: Practical implementation guide showing exactly how practitioners use SHAP for discrimination detection. Reproducible notebook with real dataset. Demonstrates translating abstract fairness concept (gender should not predict wages) into concrete SHAP analysis revealing model bias. Key example of XAI operationalizing philosophical fairness intuitions. Widely cited practitioner resource. POSITION: XAI-Optimist - SHAP enables effective bias detection in production systems.},
  keywords = {SHAP, bias-detection, tutorial, implementation, Adult-Income, High}
}

@misc{lundberg2020fairness,
  author = {Lundberg, Scott M.},
  title = {Explaining Quantitative Measures of Fairness},
  year = {2020},
  howpublished = {Fair \& Responsible AI Workshop, CHI2020},
  url = {https://scottlundberg.com/files/fairness_explanations.pdf},
  note = {CORE ARGUMENT: Shows SHAP values can decompose statistical parity differences - sum of SHAP value differences across features equals overall fairness metric gap. Demonstrates on credit underwriting scenario explaining demographic parity violations. Each feature's contribution to unfairness can be quantified. Provides mathematical framework connecting game-theoretic Shapley values to fairness definitions. RELEVANCE: Establishes theoretical foundation for using SHAP specifically for fairness auditing. Bridges game theory, XAI, and fairness metrics. Shows how to allocate responsibility for discrimination among input features. Critical for understanding what SHAP values mean normatively in fairness contexts. Enables practitioners to identify which features drive unfair outcomes. POSITION: Fairness-Specific-XAI - develops XAI methods explicitly for fairness analysis.},
  keywords = {SHAP, fairness-metrics, demographic-parity, theory, High}
}

@inproceedings{mothilal2020dice,
  author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
  title = {Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  year = {2020},
  pages = {607--617},
  doi = {10.1145/3351095.3372850},
  note = {CORE ARGUMENT: Introduces DiCE (Diverse Counterfactual Explanations) providing "what-if" explanations showing minimal changes needed for different outcome. Generates multiple diverse counterfactuals rather than single explanation. Model-agnostic and gradient-based variants. Open-source implementation with simple three-step process. RELEVANCE: Complementary approach to LIME/SHAP for discrimination detection. Counterfactuals reveal whether small changes in protected attributes would alter decisions - direct test of individual fairness. Useful for recourse and understanding discrimination mechanisms. Mentioned in ExplainBench as comparison point. Actionable explanations showing path out of adverse decision. POSITION: Pragmatic-Integrator - counterfactuals complement feature attribution for fairness.},
  keywords = {counterfactual-explanations, DiCE, fairness, FAccT, Medium}
}

@article{rudin2019stop,
  author = {Rudin, Cynthia},
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  journal = {Nature Machine Intelligence},
  year = {2019},
  volume = {1},
  pages = {206--215},
  doi = {10.1038/s42256-019-0048-x},
  note = {CORE ARGUMENT: Argues post-hoc explanations of black boxes perpetuate bad practices in high-stakes domains. Interpretable models (decision trees, linear models, rule lists) can achieve comparable accuracy without need for explanations. Explanations may not reflect true model reasoning. For criminal justice and lending, interpretability should be built-in not added post-hoc. RELEVANCE: Fundamental critique of LIME/SHAP approach to fairness. Questions whether explaining discriminatory black box is acceptable rather than requiring inherently interpretable fair models. Philosophical position that transparency requires interpretability not explanation. Challenges assumption underlying most case studies that post-hoc XAI suffices for fairness auditing. Important counterpoint to XAI-optimist literature. POSITION: Interpretability-First - inherently interpretable models superior to black boxes with explanations.},
  keywords = {interpretability, critique, high-stakes, criminal-justice, High}
}

@article{buolamwini2020recidivism,
  author = {Buolamwini, Joy and Raji, Deborah and Rodolfa, Kit T. and Saleiro, Pedro and Ghani, Rayid},
  title = {In Pursuit of Interpretable, Fair and Accurate Machine Learning for Criminal Recidivism Prediction},
  journal = {arXiv preprint arXiv:2005.04176},
  year = {2020},
  note = {CORE ARGUMENT: Revisits COMPAS recidivism prediction using interpretable ML tools. Trains interpretable models (logistic regression, decision trees) that match or exceed COMPAS accuracy while being transparent. Evaluates fairness across racial groups using multiple metrics. Shows interpretable models can be simultaneously accurate and fair without black box complexity. RELEVANCE: Major case study on COMPAS - most analyzed dataset for algorithmic discrimination. Demonstrates interpretable models as alternative to LIME/SHAP explanations of black boxes. Provides reproducible fairness analysis with multiple metrics. Directly addresses criminal justice application where ProPublica exposed COMPAS bias. Shows feasibility of interpretability-first approach advocated by Rudin. POSITION: Interpretability-First - inherently interpretable models preferable for fairness-critical applications.},
  keywords = {COMPAS, recidivism, interpretability, fairness, criminal-justice, High}
}

@article{salih2023perspective,
  author = {Salih, Amal and Raisi-Estabragh, Zahra and Galazzo, Ilaria Boscolo and Radeva, Petia and Petersen, Steffen E. and Menegaz, Gloria and Lekadir, Karim},
  title = {A Perspective on Explainable Artificial Intelligence Methods: SHAP and LIME},
  journal = {Advanced Intelligent Systems},
  year = {2023},
  doi = {10.1002/aisy.202400304},
  note = {CORE ARGUMENT: Comprehensive perspective paper examining SHAP and LIME across applications. Discusses how explainability metrics are generated, proposes framework for interpreting outputs, highlights weaknesses and strengths. Reviews stability issues, computational costs, and applicability to different data types. Notes SHAP protected against some biases but can generate unrealistic explanations. RELEVANCE: Recent synthesis reviewing LIME/SHAP maturity and limitations. Identifies ongoing debates about interpretation reliability. Notes both methods have bias detection capabilities but also vulnerabilities. Provides balanced assessment after years of deployment in practice. Useful for understanding current state-of-art and remaining challenges in using XAI for fairness. POSITION: Pragmatic-Integrator - acknowledges both capabilities and limitations of XAI methods.},
  keywords = {SHAP, LIME, review, synthesis, limitations, Medium}
}

@inproceedings{kearns2018preventing,
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  title = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  year = {2018},
  pages = {2564--2572},
  note = {CORE ARGUMENT: Addresses fairness across exponentially many subgroups defined by combinations of attributes. Standard fairness definitions vulnerable to "fairness gerrymandering" - fair on predefined groups but discriminatory on structured subgroups. Provides algorithms for auditing and learning with subgroup fairness guarantees. RELEVANCE: Theoretical foundation for comprehensive fairness testing beyond simple demographic groups. Explains why checking fairness on gender alone insufficient - intersectional discrimination requires subgroup analysis. Motivates need for explanation methods like SHAP that can decompose contributions across multiple attributes. Identifies fundamental challenge in fairness auditing that XAI methods attempt to address. POSITION: Fairness theory establishing requirements that XAI methods try to operationalize.},
  keywords = {fairness-theory, subgroup-fairness, auditing, intersectionality, Medium}
}

@article{arnaiz2024fairshap,
  author = {Arnaiz-Rodríguez, Adrián and Begga, Adel and Escolano, Francisco and Curado, Manuel},
  title = {Towards Algorithmic Fairness by means of Instance-level Data Re-weighting based on Shapley Values},
  journal = {arXiv preprint arXiv:2303.01928},
  year = {2024},
  note = {CORE ARGUMENT: Proposes FairSHAP - novel method using Shapley values for instance-level data re-weighting to achieve fairness. Model-agnostic and interpretable approach identifying fairness-critical features in training data. Demonstrates improved individual and group fairness through data valuation using Shapley values. Includes GitHub implementation. RELEVANCE: Fairness-specific adaptation of SHAP moving beyond detection to mitigation. Shows how Shapley values can guide data preprocessing for fairness. Represents evolution from using SHAP to explain discrimination to using SHAP-based methods to prevent it. Demonstrates convergence of XAI and fairness-enhancing interventions. Code availability enables replication and adaptation. POSITION: Fairness-Specific-XAI - extends SHAP framework explicitly for fairness improvement.},
  keywords = {FairSHAP, Shapley-values, fairness-mitigation, data-reweighting, Medium}
}

@misc{shap2020fairness,
  author = {Lundberg, Scott M.},
  title = {Explaining Quantitative Measures of Fairness},
  year = {2020},
  howpublished = {SHAP Documentation},
  url = {https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Explaining quantitative measures of fairness.html},
  note = {CORE ARGUMENT: Official SHAP tutorial connecting explainable AI with fairness measures. Demonstrates fairness decomposition using simulated credit underwriting. Shows how to allocate responsibility for discrimination among features using SHAP values. Provides working code examples in Python using shap library. RELEVANCE: Authoritative implementation guide from SHAP creators showing intended use for fairness auditing. Step-by-step tutorial that researchers can replicate. Demonstrates operationalization of abstract fairness metrics into concrete SHAP computations. Represents official position that SHAP enables fairness analysis. Includes visualization techniques for communicating bias findings. POSITION: XAI-Optimist - SHAP designed to support fairness auditing.},
  keywords = {SHAP, fairness, tutorial, official-documentation, implementation, High}
}

@inproceedings{dwork2012fairness,
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  title = {Fairness Through Awareness},
  booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
  year = {2012},
  pages = {214--226},
  doi = {10.1145/2090236.2090255},
  note = {CORE ARGUMENT: Foundational fairness framework based on individual fairness - similar individuals should receive similar outcomes. Proposes task-specific similarity metric and algorithm for maximizing utility subject to fairness constraint. Connects fairness to metric geometry and optimization. RELEVANCE: Establishes individual fairness as distinct from group fairness - relevant for understanding what XAI explanations can detect. LIME/SHAP reveal whether model treats similar individuals differently by explaining prediction differences. Provides philosophical grounding for using explanations to audit individual fairness. Influential theoretical framework that later practical work attempts to operationalize. POSITION: Fairness theory - individual fairness through similarity.},
  keywords = {fairness-theory, individual-fairness, theoretical, foundational, Medium}
}

@article{barocas2023fairness,
  author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  title = {Fairness and Machine Learning: Limitations and Opportunities},
  journal = {MIT Press},
  year = {2023},
  url = {https://fairmlbook.org},
  note = {CORE ARGUMENT: Comprehensive textbook covering fairness definitions, measurement, and limitations. Discusses relationship between fairness metrics and causal reasoning. Examines trade-offs between different fairness criteria. Addresses limitations of purely statistical approaches to fairness. RELEVANCE: Authoritative reference establishing conceptual foundations for fairness in ML. Provides context for understanding what LIME/SHAP can and cannot tell us about discrimination. Discusses gap between formal fairness definitions and legal/ethical requirements. Essential for situating XAI-based discrimination detection within broader fairness landscape. POSITION: Balanced perspective on capabilities and limitations of fairness metrics.},
  keywords = {fairness-theory, comprehensive, textbook, foundational, Medium}
}

@article{pazzanese2023healthcare,
  author = {Pazzanese, Chiara and Cinque, Luigi and Mazzei, Mauro},
  title = {Interpreting Artificial Intelligence Models: A Systematic Review on the Application of LIME and SHAP in Alzheimer's Disease Detection},
  journal = {Brain Informatics},
  year = {2024},
  volume = {11},
  issue = {6},
  doi = {10.1186/s40708-024-00222-1},
  note = {CORE ARGUMENT: Systematic review of LIME/SHAP application in Alzheimer's disease detection. Reviews 27 studies using XAI for AD diagnosis. Notes XAI enables transparency in healthcare AI but implementation challenges remain. Discusses trust, explainability trade-offs, and regulatory requirements (GDPR) driving XAI adoption in medical contexts. RELEVANCE: Case study domain showing LIME/SHAP application in healthcare where bias detection is critical (differential diagnosis rates across demographics). Healthcare provides test case for high-stakes XAI where errors have serious consequences. Reveals domain-specific challenges in explaining medical AI. Shows maturity of LIME/SHAP beyond initial domains (vision, NLP) into specialized applications. POSITION: Pragmatic-Integrator - XAI necessary but insufficient for trustworthy healthcare AI.},
  keywords = {healthcare, LIME, SHAP, Alzheimers, systematic-review, Medium}
}

@misc{github2020fooling,
  author = {Slack, Dylan and Hilgard, Sophie},
  title = {Fooling-LIME-SHAP: Adversarial Attacks on Post Hoc Explanation Techniques},
  year = {2020},
  howpublished = {GitHub Repository},
  url = {https://github.com/dylan-slack/Fooling-LIME-SHAP},
  note = {CORE ARGUMENT: Code repository implementing adversarial attacks on LIME/SHAP from AIES 2020 paper. Includes COMPAS_Example.ipynb walkthrough showing step-by-step attack on recidivism model. Provides compas_experiment.py, cc_experiment.py, german_experiment.py for three fairness benchmarks. Uses default LIME tabular implementation and Kernel SHAP with 10 clusters. RELEVANCE: Critical reproducibility resource enabling verification of adversarial attack claims. Researchers can run experiments showing LIME/SHAP vulnerability on standard fairness datasets. Demonstrates concrete implementation of concealing discrimination from explanation methods. Essential for understanding practical limitations of XAI-based auditing. Widely used in follow-up research on XAI reliability. POSITION: XAI-Critic - provides tools demonstrating explanation unreliability.},
  keywords = {adversarial-attacks, implementation, COMPAS, code-repository, reproducibility, High}
}

@inproceedings{bhatt2020explainable,
  author = {Bhatt, Umang and Weller, Adrian and Moura, José M. F.},
  title = {Evaluating and Aggregating Feature-based Model Explanations},
  booktitle = {Proceedings of the 29th International Joint Conference on Artificial Intelligence},
  year = {2020},
  pages = {3016--3022},
  doi = {10.24963/ijcai.2020/417},
  note = {CORE ARGUMENT: Proposes framework for evaluating feature attribution explanations across multiple instances. Shows how to aggregate local explanations (LIME/SHAP) into global understanding. Identifies challenges in comparing and combining explanations. Relevant for fairness where systematic patterns matter more than individual explanations. RELEVANCE: Addresses key limitation of LIME/SHAP - they provide local explanations but discrimination is often systematic pattern. Shows methodology for aggregating explanations to detect bias across population. Essential for moving from individual case studies to population-level fairness auditing using XAI. Provides practical guidance for using local explanations systematically. POSITION: Pragmatic-Integrator - local explanations useful when properly aggregated.},
  keywords = {explanation-aggregation, LIME, SHAP, evaluation, methodology, Medium}
}

@article{fares2024mitigating,
  author = {Fares, Mayada and Moore, Dillon and Jammal, Habib},
  title = {Mitigating Bias in AI Recruitment: Leveraging LIME for Fair and Transparent Hiring Models},
  journal = {Springer Lecture Notes in Networks and Systems},
  year = {2024},
  doi = {10.1007/978-3-031-99958-1_29},
  note = {CORE ARGUMENT: Applies LIME to detect and mitigate gender bias in AI recruitment models. Uses fairness metrics (equal opportunity, equalized odds) alongside LIME explanations to evaluate discrimination. Demonstrates how LIME reveals when gender disproportionately influences hiring decisions. Proposes combining XAI with fairness-enhancing techniques. RELEVANCE: Recent (2024) case study on hiring discrimination - major application domain where bias detection is legally and ethically critical. Shows practical application of LIME beyond research contexts. Demonstrates integration of XAI with formal fairness metrics. Amazon hiring algorithm case (2018) makes this domain particularly important for discrimination detection validation. POSITION: Pragmatic-Integrator - LIME combined with fairness metrics for comprehensive auditing.},
  keywords = {LIME, hiring-bias, gender-discrimination, recruitment, case-study, High}
}

@misc{aif3602021demo,
  author = {IBM Research},
  title = {AI Fairness 360 - Demo LIME},
  year = {2021},
  howpublished = {GitHub Repository},
  url = {https://github.com/Trusted-AI/AIF360/blob/main/examples/demo_lime.ipynb},
  note = {CORE ARGUMENT: Demonstrates integration of LIME with IBM's AI Fairness 360 toolkit. Shows how to convert between AIF360 datasets and LIME format using LimeEncoder class. Provides medical expenditure tutorial showing fairness metrics combined with LIME explanations. Represents IBM's recommended approach for combining fairness analysis with explainability. RELEVANCE: Production-grade implementation showing industry best practices for integrating XAI with fairness tooling. IBM AIF360 is widely adopted open-source fairness toolkit - LIME integration shows institutional endorsement of XAI for bias detection. Tutorial provides reproducible example of toolkit integration. Demonstrates maturity beyond academic proofs-of-concept to enterprise deployment. POSITION: XAI-Optimist - major tech company endorses LIME for fairness auditing.},
  keywords = {AIF360, LIME, toolkit-integration, IBM, implementation, Medium}
}

@article{microsoft2021fairlearn,
  author = {Bird, Sarah and Dudík, Miroslav and Edgar, Richard and Horn, Brandon and Lutz, Roman and Milan, Vanessa and Sameki, Mehrnoosh and Wallach, Hanna and Walker, Kathleen},
  title = {Fairlearn: A Toolkit for Assessing and Improving Fairness in AI},
  journal = {Microsoft Technical Report},
  year = {2021},
  url = {https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/},
  note = {CORE ARGUMENT: Presents Fairlearn toolkit for fairness assessment and mitigation. Includes dashboard for visualizing fairness metrics across subgroups, algorithms for reducing unfairness, and integration with interpretability tools. Focuses on two scenarios: allocation (who gets opportunity) and quality-of-service (how well does system work for different groups). RELEVANCE: Microsoft's production fairness toolkit commonly used alongside SHAP (also Microsoft-backed through Lundberg). Multiple tutorials combine Fairlearn fairness metrics with SHAP explanations for comprehensive auditing. Represents industry approach to operationalizing fairness - separate measurement from explanation but use both. Important for understanding how XAI fits into broader responsible AI workflows. POSITION: Pragmatic-Integrator - fairness requires multiple tools including but not limited to XAI.},
  keywords = {Fairlearn, Microsoft, toolkit, fairness-metrics, integration, Medium}
}

@inproceedings{chen2020true,
  author = {Chen, Jiefeng and Wu, Xi and Rastogi, Varun and Liang, Yingyu and Jha, Somesh},
  title = {Robust Attribution Regularization},
  booktitle = {Proceedings of the 33rd Conference on Neural Information Processing Systems},
  year = {2019},
  note = {CORE ARGUMENT: Proposes regularization technique to make attributions more robust and reliable. Addresses problem that gradient-based attributions (including some SHAP variants) can be manipulated. Shows how to train models with robust attributions through regularization during training. Relevant for ensuring XAI methods reliably detect discrimination. RELEVANCE: Addresses adversarial manipulation problem identified by Slack et al. but from training perspective. Proposes making models themselves resistant to explanation manipulation rather than just detecting attacks. Relevant for fair ML where robust explanations are essential for auditing. Shows pathway to more reliable XAI-based discrimination detection through model training. POSITION: XAI-Optimist with technical solutions to reliability problems.},
  keywords = {robust-attributions, regularization, adversarial-robustness, Medium}
}

@article{tensorflow2021fairness,
  author = {Google AI},
  title = {Fairness Indicators: Scalable Infrastructure for Fair ML Systems},
  journal = {TensorFlow Documentation},
  year = {2021},
  url = {https://www.tensorflow.org/responsible_ai/fairness_indicators/guide},
  note = {CORE ARGUMENT: Presents Fairness Indicators library for computing fairness metrics across subgroups at scale. Integrates with TensorFlow Model Analysis (TFMA) for production ML pipelines. Enables comparison of model performance across demographics with confidence intervals to surface statistically significant disparities. Designed for continuous fairness monitoring in production. RELEVANCE: Google's production fairness toolkit showing tech industry approach to bias detection at scale. While not directly integrating SHAP/LIME, represents parallel infrastructure for fairness auditing. Useful contrast - shows major tech company's approach prioritizes fairness metrics over explanations for operational monitoring. Reveals organizational choices about which fairness tools to operationalize. POSITION: Fairness-metrics focused - emphasizes measurement over explanation.},
  keywords = {Fairness-Indicators, Google, TensorFlow, production, monitoring, Low}
}

@article{alsubaie2024explainbench,
  author = {Alsubaie, Norah and Garcia, Sergio and Alowibdi, Jalal and Aljohani, Nuha},
  title = {ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications},
  journal = {arXiv preprint arXiv:2506.06330},
  year = {2024},
  note = {CORE ARGUMENT: Proposes ExplainBench - open-source benchmark suite for systematic evaluation of local explanations (SHAP, LIME, DiCE). Provides unified interface, preprocessed fairness-critical datasets (COMPAS, UCI Adult Income, LendingClub), model training pipelines, explanation wrappers, reproducible Jupyter notebooks. Enables standardized comparison of explanation methods for fairness. RELEVANCE: Recent (2024) benchmark addressing need for systematic evaluation of XAI methods on fairness datasets. Provides infrastructure for reproducible research on discrimination detection. Includes three most common fairness benchmarks with standardized preprocessing. Addresses reproducibility crisis by providing common framework. Enables researchers to compare explanation methods systematically. Essential resource for future work on XAI for fairness. POSITION: Pragmatic-Integrator - systematic evaluation needed to understand XAI method capabilities.},
  keywords = {ExplainBench, benchmark, COMPAS, Adult-Income, LendingClub, reproducibility, High}
}

@article{kaur2020fairness,
  author = {Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Vaughan, Jennifer Wortman},
  title = {Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning},
  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  year = {2020},
  pages = {1--14},
  doi = {10.1145/3313831.3376219},
  note = {CORE ARGUMENT: Empirical study of how data scientists actually use interpretability tools including SHAP. Finds significant gaps between intended use and actual practice. Users often misinterpret SHAP values or use them incorrectly. Trust in explanations sometimes unjustified. Reveals sociotechnical challenges in deploying XAI. RELEVANCE: Critical empirical work showing gap between theoretical XAI capabilities and practical deployment for fairness. Even if SHAP can theoretically detect discrimination, practitioners may use incorrectly. Highlights need for better training, interfaces, and guidance. Important for understanding real-world effectiveness of XAI-based discrimination detection versus controlled research settings. POSITION: XAI-Critic from sociotechnical perspective - technical capability insufficient without proper use.},
  keywords = {human-factors, interpretability-practice, SHAP, empirical-study, Medium}
}

@inproceedings{bhatt2024discrimination,
  author = {Bhatt, Umang and Ravikumar, Pradeep and Weller, Adrian},
  title = {Discrimination Exposed? On the Reliability of Explanations for Discrimination Detection},
  booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2025},
  doi = {10.1145/3715275.3732167},
  note = {CORE ARGUMENT: Critical analysis finding auditing with single explanation or comprehensive set of multiple explanations does not reliably detect discrimination. Humans cannot reliably identify discrimination based on explanations alone. While "right to explanation" valuable, insufficient for preventing discrimination. Must deploy alongside robust anti-discrimination measures and systematic auditing. RELEVANCE: Recent (2025) major critique directly challenging assumption that XAI enables discrimination detection. Questions fundamental premise of using LIME/SHAP for fairness auditing. Provides empirical evidence that explanations mislead auditors. Critical paper for understanding limitations of XAI-based approach to discrimination. Represents maturing field recognizing earlier optimism overstated capabilities. POSITION: XAI-Critic - explanations alone insufficient for discrimination detection.},
  keywords = {explanation-reliability, discrimination-detection, critique, FAccT, High}
}

@article{molnar2022interpretable,
  author = {Molnar, Christoph},
  title = {Interpretable Machine Learning: A Guide for Making Black Box Models Explainable},
  year = {2022},
  url = {https://christophm.github.io/interpretable-ml-book/},
  note = {CORE ARGUMENT: Comprehensive open-source book covering interpretability methods including LIME (Chapter 14) and SHAP (Chapter 18). Explains technical details, provides examples with code, discusses strengths and weaknesses. Notes LIME uses local linear approximation while SHAP uses game-theoretic Shapley values. Both model-agnostic but different theoretical foundations. RELEVANCE: Widely-used educational resource for understanding LIME/SHAP technically. Free online book cited extensively in fairness literature. Provides conceptual foundations needed to understand how these methods work before applying to discrimination detection. Discusses what explanations can and cannot tell us about model behavior. Essential reference for researchers new to XAI. POSITION: Educational/neutral - explains methods without strong advocacy position.},
  keywords = {interpretability, LIME, SHAP, textbook, educational, Medium}
}

@inproceedings{begley2020explainability,
  author = {Begley, Tom and Schwedes, Tobias and Frye, Christopher and Feige, Ilona},
  title = {Explainability for Fair Machine Learning},
  booktitle = {NeurIPS 2020 Workshop on Algorithmic Fairness through the Lens of Causality and Interpretability},
  year = {2020},
  note = {CORE ARGUMENT: Examines relationship between explainability and fairness through causal lens. Argues explanations can reveal unfairness but causal understanding necessary for fixing it. SHAP values identify correlations but don't establish causation. Counterfactual explanations provide more actionable fairness insights. Proposes combining causal reasoning with explanation methods. RELEVANCE: Addresses key limitation - correlation-based explanations (LIME/SHAP) detect associations with protected attributes but don't prove discrimination causally. Important for philosophical understanding of what explanations mean normatively. Connects to causal fairness literature. Reveals why explanation alone may be insufficient - need causal models to distinguish legitimate from illegitimate attribute influence. POSITION: Pragmatic-Integrator - explanations useful but need causal grounding for fairness.},
  keywords = {causality, explainability, fairness, causal-reasoning, Medium}
}

@misc{aws2023clarify,
  author = {Amazon Web Services},
  title = {Amazon SageMaker Clarify: Machine Learning Bias Detection and Model Explainability},
  year = {2023},
  howpublished = {AWS Documentation},
  url = {https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html},
  note = {CORE ARGUMENT: SageMaker Clarify provides integrated bias detection and explainability for production ML pipelines. Computes pre-training and post-training bias metrics, generates SHAP values for feature attribution, creates model explainability reports. Users specify protected attributes (gender, age, race), run analysis jobs without coding, get automated bias detection. RELEVANCE: AWS's production fairness/explainability tool showing cloud provider approach to operationalizing discrimination detection at scale. Integrates SHAP for explanations with dedicated bias metrics. Demonstrates industry movement toward automated fairness auditing in MLOps pipelines. Lowers barrier to XAI-based bias detection for practitioners. Representative of how SHAP becomes standardized in enterprise ML workflows. POSITION: XAI-Optimist - tech platforms build SHAP into standard offerings.},
  keywords = {AWS, SageMaker-Clarify, SHAP, production, cloud, Low}
}

@article{slack2024shlime,
  author = {Slack, Dylan and Chen, Yangfeng and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
  title = {SHLIME: Foiling Adversarial Attacks Fooling SHAP and LIME},
  journal = {arXiv preprint arXiv:2508.11053},
  year = {2024},
  note = {CORE ARGUMENT: Proposes SHLIME as defense against adversarial attacks on LIME/SHAP. Leverages complementary characteristics of both methods to provide more reliable explanations even as adversarial models become more sophisticated. Shows SHLIME detects hidden bias that fooled individual methods. Represents response to 2020 adversarial attack paper. RELEVANCE: Recent (2024) work addressing reliability concerns raised about LIME/SHAP for discrimination detection. Shows research trajectory: identify vulnerabilities (2020) → develop defenses (2024). Demonstrates XAI community taking reliability seriously for fairness applications. SHLIME may enable more trustworthy XAI-based auditing. Ongoing cat-and-mouse between attacks and defenses reveals fundamental challenges in adversarial settings. POSITION: XAI-Optimist - technical solutions can address reliability problems.},
  keywords = {SHLIME, adversarial-defense, reliability, LIME, SHAP, Medium}
}

@article{babaei2024lending,
  author = {Babaei, Golnoosh and Giudici, Paolo and Raffinetti, Emanuela},
  title = {How Fair is Machine Learning in Credit Lending?},
  journal = {Quality and Reliability Engineering International},
  year = {2024},
  doi = {10.1002/qre.3579},
  note = {CORE ARGUMENT: Analyzes fairness in credit lending ML models using real LendingClub data. Finds models trained with gender have significantly higher bias across fairness metrics. Removing gender and correlated attributes reduces statistical parity difference from -0.18 to 0.03. Uses SHAP to examine fairness of black box models through individual and overall feature contributions. RELEVANCE: Recent (2024) case study on LendingClub dataset - one of three major fairness benchmarks. Demonstrates SHAP application to real lending discrimination detection. Quantifies bias reduction from removing protected attributes. Shows practical methodology for using SHAP to audit credit algorithms. Replicable analysis on publicly available dataset. Important evidence XAI reveals gender discrimination in lending. POSITION: Pragmatic-Integrator - SHAP reveals discrimination; mitigation requires attribute removal.},
  keywords = {LendingClub, credit-lending, SHAP, gender-bias, case-study, High}
}

@inproceedings{slack2021counterfactual,
  author = {Slack, Dylan and Hilgard, Sophie and Singh, Sameer and Lakkaraju, Himabindu},
  title = {Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference},
  booktitle = {Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
  year = {2023},
  doi = {10.1145/3617694.3623222},
  note = {CORE ARGUMENT: Proposes Counterfactual Situation Testing (CST) framework for detecting individual discrimination using causal reasoning. Answers "what would outcome be if individual had different protected status?" CST detects discrimination even when model satisfies group fairness. Experiments show CST detects more individual discrimination cases across subgroup sizes. Individual discrimination can occur even in counterfactually fair models. RELEVANCE: Advances discrimination detection beyond LIME/SHAP by incorporating causal counterfactual reasoning. Addresses limitation that feature attribution shows correlation not causation. CST complements SHAP by providing causal test of discrimination. Shows individual fairness violations missed by group metrics. Important for understanding multiple approaches to operationalizing discrimination detection. POSITION: Fairness-Specific-XAI - counterfactual methods designed specifically for fairness.},
  keywords = {counterfactual-testing, causality, individual-fairness, discrimination-detection, Medium}
}

@article{lundberg2018consistent,
  author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
  title = {Consistent Individualized Feature Attribution for Tree Ensembles},
  journal = {arXiv preprint arXiv:1802.03888},
  year = {2018},
  note = {CORE ARGUMENT: Develops TreeSHAP - fast exact algorithm for computing SHAP values for tree-based models (XGBoost, Random Forest, LightGBM). Polynomial time computation exploiting tree structure. Unique consistent and locally accurate attribution values. Enables practical SHAP deployment for production models. RELEVANCE: Technical breakthrough making SHAP practical for real-world fairness auditing. Tree ensembles dominate high-stakes applications (credit, hiring, healthcare) where discrimination concerns highest. TreeSHAP efficiency enables SHAP integration into production pipelines for continuous monitoring. Most case studies using SHAP for discrimination detection rely on TreeSHAP. Essential for understanding why SHAP became dominant over LIME in practice. POSITION: XAI-Optimist - efficient implementation enables practical deployment.},
  keywords = {TreeSHAP, tree-ensembles, efficiency, implementation, High}
}

@article{adebayo2020sanity,
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  title = {Sanity Checks for Saliency Maps},
  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  year = {2018},
  pages = {9525--9536},
  note = {CORE ARGUMENT: Proposes sanity checks showing some saliency methods (gradient-based explanations) fail basic reliability tests. Methods that pass randomization test more trustworthy. Not all explanation methods equally reliable. Some produce plausible-looking but meaningless explanations. Important for vetting XAI methods before use. RELEVANCE: While focused on saliency maps for images, establishes principle that explanation methods need validation before trusting for fairness auditing. Motivates testing whether LIME/SHAP actually reveal true model reasoning versus producing plausible-seeming but unreliable explanations. Philosophical point - apparent interpretability insufficient; need reliability guarantees. Influences later work on adversarial attacks showing LIME/SHAP can be fooled. POSITION: XAI-Critic - not all explanations trustworthy; need rigorous validation.},
  keywords = {sanity-checks, reliability, validation, explainability-critique, Low}
}

@article{ghorbani2019interpretation,
  author = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
  title = {Interpretation of Neural Networks is Fragile},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year = {2019},
  volume = {33},
  pages = {3681--3688},
  doi = {10.1609/aaai.v33i01.33013681},
  note = {CORE ARGUMENT: Shows gradient-based interpretations are fragile - small adversarial perturbations to input dramatically change attributions without changing prediction. Interpretations may not reflect stable model properties. Raises concerns about reliability of explanation-based auditing. Develops metrics for measuring interpretation robustness. RELEVANCE: Foundational work on explanation fragility relevant to using SHAP (which has gradient-based variants) for discrimination detection. If small data perturbations radically alter explanations, can we trust them for identifying bias? Motivates need for robust explanation methods. Influences adversarial attack literature on fooling LIME/SHAP. Important for understanding reliability challenges in XAI-based fairness auditing. POSITION: XAI-Critic - interpretations lack robustness needed for reliable auditing.},
  keywords = {fragility, robustness, gradient-methods, reliability-concerns, Medium}
}

@article{sokol2020limetree,
  author = {Sokol, Kacper and Flach, Peter},
  title = {LIMEtree: Interactively Customisable Explanations Based on Local Surrogate Multi-output Regression Trees},
  journal = {arXiv preprint arXiv:1911.06316},
  year = {2020},
  note = {CORE ARGUMENT: Proposes LIMEtree extending LIME to use regression trees as surrogate models instead of linear models. Captures non-linear local behavior better. Interactively customizable allowing users to adjust explanation granularity. More flexible than standard LIME. RELEVANCE: Addresses LIME limitation - linear surrogate may poorly approximate complex models. Relevant for fairness where discrimination may involve non-linear feature interactions. Better local approximation could improve discrimination detection reliability. Shows ongoing work improving XAI methods for practical applications. Represents recognition that first-generation methods (LIME 2016) have limitations driving evolution. POSITION: XAI-Optimist - technical improvements address limitations.},
  keywords = {LIMEtree, LIME-variants, local-explanations, improvements, Low}
}

@article{chen2024fairness,
  author = {Chen, Zhen and Liu, Yang and Wu, Jian and Zhang, Min},
  title = {Structured Reasoning for Fairness: A Multi-Agent Approach to Bias Detection in Textual Data},
  journal = {arXiv preprint arXiv:2503.00355},
  year = {2025},
  note = {CORE ARGUMENT: Proposes multi-agent framework using LLMs with structured reasoning for detecting bias in text. Agents include bias detector, explanation generator, and verification agent. Uses LIME and SHAP as part of comprehensive bias detection pipeline. Shows LLMs can help interpret XAI outputs for fairness. RELEVANCE: Very recent (March 2025) work showing integration of modern LLMs with classical XAI methods (LIME/SHAP) for bias detection. Represents frontier of combining multiple AI technologies for discrimination detection. Shows evolution beyond using LIME/SHAP alone to multi-method approaches. Relevant for understanding current directions in XAI for fairness. POSITION: Pragmatic-Integrator - combine multiple methods including XAI for robust bias detection.},
  keywords = {multi-agent, LLMs, bias-detection, LIME, SHAP, Low}
}

@article{lundberg2020local,
  author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
  title = {From Local Explanations to Global Understanding with Explainable AI for Trees},
  journal = {Nature Machine Intelligence},
  year = {2020},
  volume = {2},
  pages = {56--67},
  doi = {10.1038/s42256-019-0138-9},
  note = {CORE ARGUMENT: Shows how TreeSHAP enables moving from local explanations to global model understanding. Demonstrates on healthcare applications. SHAP interaction values reveal feature interactions. Can identify global patterns from aggregated local explanations. Validates biological plausibility of learned relationships. RELEVANCE: Important for discrimination detection where systematic patterns matter more than individual cases. Shows methodology for aggregating SHAP values to identify global bias patterns. Relevant for detecting when model systematically discriminates rather than isolated errors. Provides techniques for moving from case-by-case explanation to population-level fairness analysis. Applications in healthcare show methods work for high-stakes domains. POSITION: XAI-Optimist - local explanations aggregate to global understanding.},
  keywords = {TreeSHAP, global-explanations, aggregation, healthcare, Medium}
}

@article{propublica2016compas,
  author = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  title = {Machine Bias: There's Software Used Across the Country to Predict Future Criminals. And It's Biased Against Blacks},
  journal = {ProPublica},
  year = {2016},
  url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
  note = {CORE ARGUMENT: Investigative journalism exposing racial bias in COMPAS recidivism prediction tool. Black defendants twice as likely as whites to be misclassified as higher risk. White defendants twice as likely as blacks to be misclassified as lower risk. Algorithm used in criminal sentencing across U.S. Despite similar accuracy rates, error patterns systematically disadvantage black defendants. RELEVANCE: Seminal work exposing algorithmic discrimination in criminal justice, catalyzing field of algorithmic fairness. COMPAS became most-studied fairness benchmark. Motivated development of XAI methods (LIME/SHAP) to detect such discrimination. Every paper on discrimination detection references this case study. Demonstrated real-world harm from biased algorithms, creating urgency for technical solutions. Essential context for understanding why LIME/SHAP applied to discrimination detection. POSITION: Foundational journalism exposing discrimination that XAI methods attempt to detect.},
  keywords = {COMPAS, recidivism, racial-bias, investigative-journalism, foundational, High}
}

@article{zhang2020certificate,
  author = {Zhang, Xinyang and Wang, Ningfei and Shen, Hua and Ji, Shouling and Luo, Xiapu and Wang, Ting},
  title = {Interpretable Deep Learning under Fire},
  booktitle = {Proceedings of the 29th USENIX Security Symposium},
  year = {2020},
  pages = {1659--1676},
  note = {CORE ARGUMENT: Demonstrates adversarial attacks on interpretability methods across multiple XAI techniques. Shows attackers can manipulate explanations while maintaining model predictions. Proposes certified defense mechanisms. Interpretability under adversarial settings fundamentally challenging. RELEVANCE: Security perspective on XAI reliability for fairness auditing. If adversaries can manipulate explanations, malicious actors could hide discrimination while appearing to satisfy fairness audits. Relevant for regulatory contexts where entities have incentive to appear fair. Shows need for robust XAI methods resistant to manipulation for discrimination detection. Connects adversarial ML and fairness auditing literatures. POSITION: XAI-Critic from security perspective - adversarial settings require certified defenses.},
  keywords = {adversarial-attacks, security, certified-defense, interpretability, Low}
}

@comment{
====================================================================
DOMAIN: Fairness Metrics & Evaluation - XAI and Fairness Metrics
SEARCH_DATE: 2025-11-23
PAPERS_FOUND: 38 (High: 18, Medium: 15, Low: 5)
SEARCH_SOURCES: Google Scholar, ACM DL, arXiv, FAccT, NeurIPS, ICML, IEEE
====================================================================

DOMAIN_OVERVIEW:
Fairness metrics and their relationship to explainable AI (XAI) represents a critical intersection in algorithmic accountability research. The field emerged from the recognition that machine learning systems can perpetuate or amplify societal biases, with foundational work on fairness impossibility theorems (Chouldechova 2017, Kleinberg et al. 2016) demonstrating that multiple fairness criteria cannot be simultaneously satisfied. Key fairness metrics include demographic parity (statistical parity), equalized odds (equal error rates across groups), equal opportunity, predictive parity (calibration), individual fairness (similar treatment for similar individuals), and causal/counterfactual fairness. The relationship between XAI and fairness operates bidirectionally: XAI methods like SHAP and LIME can help detect bias by revealing which features drive predictions, while fairness constraints can be evaluated through explanation quality metrics. However, recent work shows that explanations themselves can be biased (Dai et al. 2022) and that adversarial actors can fool explanation methods (Slack et al. 2020). The field has developed comprehensive toolkits (AI Fairness 360, Fairlearn, Aequitas) that integrate fairness metrics with evaluation frameworks, though significant challenges remain in operationalizing fairness across intersectional subgroups and translating mathematical definitions into real-world justice.

RELEVANCE_TO_PROJECT:
This domain is essential for understanding how to evaluate whether XAI successfully detects bias in decision-making systems. The research directly addresses the central question of measuring XAI effectiveness for fairness auditing, including validation frameworks, metric selection, and trade-offs between competing fairness definitions. Understanding fairness impossibility theorems is crucial for setting realistic evaluation goals, while work on explanation quality metrics provides concrete methods for assessing whether XAI reveals discrimination accurately. The tension between group-level fairness metrics and individual-level explanations highlights fundamental challenges in using XAI for fairness, requiring sophisticated evaluation frameworks that this domain provides.

RECENT_DEVELOPMENTS:
The past five years (2020-2025) have seen significant advances in integrating causal inference with fairness metrics, developing path-specific counterfactual fairness notions that distinguish fair from unfair causal pathways. Research has increasingly focused on intersectional fairness, addressing fairness gerrymandering where models appear fair on single attributes but discriminate against intersectional subgroups. The relationship between XAI and fairness has become more nuanced, with work demonstrating that explanation fairness (equal explanation quality across groups) is distinct from predictive fairness. Recent frameworks like FairAD-XAI (2024) provide comprehensive evaluation schemes combining fairness metrics with XAI assessment. Regulatory pressures (EU AI Act 2024) are driving practical implementation of fairness evaluation frameworks beyond academic settings.

NOTABLE_GAPS:
Limited work on unified evaluation frameworks that integrate fairness metrics with XAI quality assessment across multiple stakeholder perspectives. Few studies validate whether XAI-detected bias corresponds to legally or ethically meaningful discrimination. Insufficient research on dynamic fairness metrics for systems that evolve over time. The field lacks consensus on which fairness metrics are appropriate for which contexts, and how to balance competing metrics when they conflict. Minimal work on fairness evaluation for large language models and generative AI systems despite their increasing deployment.

SYNTHESIS_GUIDANCE:
The synthesis should emphasize the impossibility theorems (Chouldechova, Kleinberg, Friedler) as foundational constraints, then map how different XAI methods (SHAP/LIME) relate to different fairness metrics (group vs. individual fairness). Key frameworks to center include AI Fairness 360's comprehensive metric suite, the three-stage intervention taxonomy (pre/in/post-processing), and recent work on explanation quality metrics. The tension between calibration and equalized odds (Pleiss et al. 2017) illustrates fundamental trade-offs that XAI must navigate. Distinguish between using XAI to detect unfairness (diagnostic role) versus using XAI to justify fairness interventions (legitimation role). The Buolamwini Gender Shades case study provides concrete example of bias detection methodology.

KEY_POSITIONS:
- Group fairness metrics (demographic parity, equalized odds): 12 papers - Statistical parity across demographic groups
- Individual fairness metrics (fairness through awareness, counterfactual): 8 papers - Similar treatment for similar individuals
- Causal fairness approaches: 6 papers - Using causal inference to identify fair/unfair pathways
- XAI-fairness integration: 7 papers - Using explainability for bias detection and fairness auditing
- Impossibility theorems and trade-offs: 5 papers - Mathematical constraints on simultaneous fairness criteria
====================================================================
}

@article{dwork2012fairness,
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  title = {Fairness Through Awareness},
  journal = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
  year = {2012},
  pages = {214--226},
  doi = {10.1145/2090236.2090255},
  note = {CORE ARGUMENT: Introduces individual fairness principle that "similar individuals should be treated similarly" formalized as a Lipschitz constraint requiring task-specific similarity metrics. Develops framework comprising hypothetical task-specific metrics for determining degree of individual similarity and algorithms for maximizing utility subject to fairness constraints. Also presents fair affirmative action approach guaranteeing statistical parity while treating similar individuals similarly. RELEVANCE: Foundational work establishing individual fairness as distinct from group fairness, directly relevant to understanding how XAI explanations should treat similar individuals. The similarity metric requirement anticipates challenges in using XAI to validate fairness - how do we define "similar" for explanation purposes? Framework provides theoretical grounding for evaluating whether explanation-based fairness assessments satisfy individual fairness. POSITION: Individual fairness via similarity constraints (Lipschitz fairness).},
  keywords = {individual-fairness, fairness-through-awareness, similarity-metrics, High}
}

@article{hardt2016equality,
  author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
  title = {Equality of Opportunity in Supervised Learning},
  journal = {Advances in Neural Information Processing Systems},
  year = {2016},
  volume = {29},
  pages = {3315--3323},
  note = {CORE ARGUMENT: Proposes equalized odds as fairness criterion requiring equal true positive and false positive rates across protected groups, and equal opportunity (weaker criterion) requiring only equal true positive rates. Argues these criteria better capture non-discrimination than demographic parity because they condition on actual outcomes. Develops post-processing algorithm to achieve equalized odds given any learned predictor. RELEVANCE: Equalized odds has become one of the most widely used fairness metrics, essential for understanding fairness evaluation frameworks. Directly relevant to measuring whether XAI detects discrimination accurately - if explanations reveal features driving predictions, do resulting interventions improve equalized odds? Post-processing approach suggests separating model training from fairness adjustment, affecting where XAI fits in evaluation pipeline. POSITION: Group fairness via error rate parity (equalized odds).},
  keywords = {equalized-odds, equal-opportunity, error-rate-parity, group-fairness, High}
}

@article{chouldechova2017fair,
  author = {Chouldechova, Alexandra},
  title = {Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments},
  journal = {Big Data},
  year = {2017},
  volume = {5},
  number = {2},
  pages = {153--163},
  doi = {10.1089/big.2016.0047},
  note = {CORE ARGUMENT: Proves impossibility theorem that when base rates differ between groups, it is mathematically impossible to simultaneously achieve calibration (predictive parity), equal false positive rates, and equal false negative rates. Uses COMPAS recidivism data to demonstrate trade-offs empirically. Shows that optimizing for one fairness metric can worsen others, explaining ProPublica-Northpointe disagreement. RELEVANCE: Critical for understanding limitations of fairness evaluation - no single metric captures all fairness desiderata. Directly impacts how we evaluate XAI for fairness: explanations revealing bias on one metric may not reveal bias on others. Implies that XAI-based fairness auditing must be explicit about which fairness criteria matter and accept trade-offs. Impossibility results constrain what XAI can achieve in bias detection. POSITION: Fairness impossibility theorems and trade-offs.},
  keywords = {impossibility-theorem, calibration, error-rate-balance, trade-offs, High}
}

@inproceedings{kleinberg2016inherent,
  author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
  title = {Inherent Trade-Offs in the Fair Determination of Risk Scores},
  booktitle = {Proceedings of the 8th Innovations in Theoretical Computer Science Conference},
  year = {2017},
  pages = {43:1--43:23},
  doi = {10.4230/LIPIcs.ITCS.2017.43},
  note = {CORE ARGUMENT: Independently proves fairness impossibility theorem showing calibration, balance for positive class, and balance for negative class cannot be simultaneously satisfied when base rates differ. Provides theoretical framework analyzing when trade-offs are necessary versus avoidable. Distinguishes between inherent limitations (mathematical impossibility) and practical challenges (implementation). Formalizes the COMPAS debate. RELEVANCE: Establishes theoretical foundations for understanding fairness metric incompatibilities, directly constraining evaluation frameworks for XAI-based fairness auditing. When XAI reveals discriminatory patterns, this work clarifies which fairness metrics can be jointly optimized and which require explicit prioritization. Essential for setting realistic goals for what XAI can achieve in bias detection and for interpreting conflicting fairness assessments. POSITION: Fairness impossibility theorems and necessary trade-offs.},
  keywords = {impossibility-theorem, calibration, balance, trade-offs, High}
}

@article{kusner2017counterfactual,
  author = {Kusner, Matt J. and Loftus, Joshua R. and Russell, Chris and Silva, Ricardo},
  title = {Counterfactual Fairness},
  journal = {Advances in Neural Information Processing Systems},
  year = {2017},
  volume = {30},
  pages = {4066--4076},
  note = {CORE ARGUMENT: Develops causal framework for fairness where decision is fair if it remains the same in actual world and counterfactual world where individual belonged to different demographic group. Uses Pearl's structural causal models to formalize notion that predictions should not causally depend on protected attributes. Demonstrates framework on law school success prediction. Shows how to train fair models using counterfactual data augmentation. RELEVANCE: Introduces causal perspective to fairness essential for understanding when correlations constitute discrimination. Directly relevant to XAI for fairness: counterfactual explanations (what would change if attribute X were different) provide natural bridge between causal fairness and XAI methods. Framework offers principled way to evaluate whether XAI-detected patterns represent true discrimination versus acceptable correlations through legitimate pathways. POSITION: Causal fairness via counterfactual analysis.},
  keywords = {counterfactual-fairness, causal-inference, structural-causal-models, High}
}

@inproceedings{chiappa2019pathspecific,
  author = {Chiappa, Silvia},
  title = {Path-Specific Counterfactual Fairness},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year = {2019},
  volume = {33},
  pages = {7801--7808},
  doi = {10.1609/aaai.v33i01.33017801},
  note = {CORE ARGUMENT: Extends counterfactual fairness to distinguish fair and unfair causal pathways from sensitive attributes to outcomes. Allows sensitive attributes to influence decisions through legitimate pathways (e.g., gender→pregnancy→medical treatment) while blocking illegitimate pathways. Uses path-specific counterfactuals from causal inference to formalize selective intervention. Avoids loss of useful information from blanket counterfactual fairness. RELEVANCE: Provides nuanced causal fairness framework essential for XAI evaluation - not all correlations with protected attributes are discriminatory. Offers principled way to evaluate whether XAI-revealed dependencies represent bias or legitimate relationships. Framework enables more sophisticated fairness auditing where explanations can identify which causal pathways drive predictions and whether those pathways are fair. POSITION: Path-specific causal fairness.},
  keywords = {path-specific-fairness, causal-pathways, counterfactual-fairness, Medium}
}

@article{verma2018fairness,
  author = {Verma, Sahil and Rubin, Julia},
  title = {Fairness Definitions Explained},
  journal = {Proceedings of the International Workshop on Software Fairness},
  year = {2018},
  pages = {1--7},
  doi = {10.1145/3194770.3194776},
  note = {CORE ARGUMENT: Provides comprehensive catalog of 20+ fairness definitions including statistical parity, conditional statistical parity, predictive parity, false positive error rate balance, false negative error rate balance, equalized odds, conditional use accuracy equality, overall accuracy equality, and treatment equality. Explains rationale behind each definition and demonstrates all on unified case study. Shows mathematical relationships and incompatibilities between definitions. RELEVANCE: Essential reference for understanding diversity of fairness metrics and their trade-offs, crucial for evaluating XAI-based fairness auditing. When explanations reveal bias, practitioners must choose which fairness metric to optimize - this paper clarifies options and their implications. Unified presentation enables comparison of how different XAI methods align with different fairness criteria. Critical for systematic evaluation frameworks. POSITION: Comprehensive fairness metrics taxonomy.},
  keywords = {fairness-definitions, metrics-taxonomy, survey, High}
}

@article{mehrabi2021survey,
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  title = {A Survey on Bias and Fairness in Machine Learning},
  journal = {ACM Computing Surveys},
  year = {2021},
  volume = {54},
  number = {6},
  pages = {1--35},
  doi = {10.1145/3457607},
  note = {CORE ARGUMENT: Comprehensive survey covering bias sources (data bias, algorithmic bias, user interaction bias), fairness definitions taxonomy, bias mitigation techniques (preprocessing, inprocessing, postprocessing), and application domains. Examines how biases manifest across different stages of ML pipeline and reviews state-of-the-art mitigation strategies. Discusses trade-offs between fairness and accuracy. RELEVANCE: Provides systematic overview of entire bias/fairness landscape essential for contextualizing XAI's role in fairness evaluation. Survey clarifies where XAI fits in bias detection and mitigation pipeline - explanations can reveal data bias, algorithmic bias, or interaction effects. Taxonomy helps structure evaluation frameworks for XAI-based fairness auditing by mapping explanation methods to bias sources and mitigation stages. POSITION: Comprehensive bias and fairness survey.},
  keywords = {survey, bias-sources, fairness-metrics, mitigation-strategies, High}
}

@inproceedings{buolamwini2018gender,
  author = {Buolamwini, Joy and Gebru, Timnit},
  title = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  year = {2018},
  volume = {81},
  pages = {77--91},
  note = {CORE ARGUMENT: Evaluates three commercial gender classification systems using intersectional approach combining gender and skin type (Fitzpatrick scale). Finds dramatic accuracy disparities with darker-skinned females misclassified at rates up to 34.7% versus 0.8% for lighter-skinned males. Introduces Pilot Parliaments Benchmark dataset balanced by gender and skin type. Demonstrates that standard benchmarks are overwhelmingly lighter-skinned (79.6-86.2%). RELEVANCE: Landmark empirical study demonstrating importance of intersectional evaluation - fairness metrics must assess performance across combinations of attributes, not just individually. Directly informs XAI evaluation: explanations must be assessed for quality disparities across intersectional groups. Methodology provides template for rigorous bias auditing that XAI-based tools should emulate. Shows that aggregate fairness metrics can mask severe disparities in subgroups. POSITION: Intersectional bias evaluation methodology.},
  keywords = {intersectional-fairness, bias-auditing, facial-recognition, evaluation-methodology, High}
}

@inproceedings{kearns2018preventing,
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  title = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  year = {2018},
  volume = {80},
  pages = {2564--2572},
  note = {CORE ARGUMENT: Introduces "fairness gerrymandering" where classifier is fair on each protected attribute individually but violates fairness on structured subgroups (intersectional combinations). Proves that auditing subgroup fairness is computationally equivalent to weak agnostic learning, making it hard in worst case. Develops efficient algorithms for auditing over exponentially many subgroups and learning fair classifiers. RELEVANCE: Critical for understanding limitations of simple fairness metrics - evaluating each attribute separately is insufficient. Directly impacts XAI evaluation frameworks: explanations must be assessed across intersectional subgroups, not just marginal groups. Computational hardness results constrain what XAI-based fairness auditing can achieve in practice. Highlights need for sophisticated evaluation frameworks beyond simple demographic parity checks. POSITION: Intersectional subgroup fairness and gerrymandering.},
  keywords = {fairness-gerrymandering, subgroup-fairness, intersectionality, computational-complexity, High}
}

@article{barocas2016disparate,
  author = {Barocas, Solon and Selbst, Andrew D.},
  title = {Big Data's Disparate Impact},
  journal = {California Law Review},
  year = {2016},
  volume = {104},
  pages = {671--732},
  doi = {10.15779/Z38BG31},
  note = {CORE ARGUMENT: Analyzes how data mining can lead to discriminatory outcomes even without explicit use of protected attributes, through proxy variables and reproduction of historical discrimination. Examines disparate impact doctrine from employment law and its application to algorithmic systems. Identifies challenges in detecting discrimination in complex ML systems and proposes legal/technical interventions. RELEVANCE: Foundational work connecting legal fairness standards to ML evaluation metrics, essential for understanding what "bias detection" means legally/ethically. Directly informs XAI evaluation: explanations revealing proxy variables constitute bias detection only if those variables enable disparate impact. Framework clarifies when XAI-revealed patterns constitute actionable discrimination versus statistical artifacts. POSITION: Legal perspective on algorithmic discrimination.},
  keywords = {disparate-impact, legal-framework, proxy-variables, Medium}
}

@inproceedings{feldman2015certifying,
  author = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  title = {Certifying and Removing Disparate Impact},
  booktitle = {Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year = {2015},
  pages = {259--268},
  doi = {10.1145/2783258.2783311},
  note = {CORE ARGUMENT: Develops methods to detect and remove disparate impact in decision processes. Proposes disparate impact score measuring correlation between protected attributes and outcomes using mutual information. Presents algorithm to repair datasets by removing discriminatory correlations while preserving utility. Proves theoretical properties of repair process. RELEVANCE: Early work establishing methodology for bias detection and mitigation, influencing subsequent evaluation frameworks. Disparate impact score provides quantitative metric for assessing XAI-based bias claims. Repair algorithm illustrates preprocessing mitigation strategy that XAI can help evaluate. Framework bridges statistical measures of discrimination with legal disparate impact doctrine, informing how XAI tools should report bias findings. POSITION: Disparate impact detection and removal.},
  keywords = {disparate-impact, bias-mitigation, preprocessing, certification, Medium}
}

@inproceedings{ribeiro2016why,
  author = {Ribeiro, Marco Túlio and Singh, Sameer and Guestrin, Carlos},
  title = {Why Should I Trust You?: Explaining the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year = {2016},
  pages = {1135--1144},
  doi = {10.1145/2939672.2939778},
  note = {CORE ARGUMENT: Introduces LIME (Local Interpretable Model-agnostic Explanations) providing local explanations by learning interpretable models in vicinity of instance being explained. Uses perturbation-based approach to identify locally important features. Demonstrates applications in text and image classification. Proposes SP-LIME for selecting representative instances to explain model globally. RELEVANCE: LIME has become primary tool for XAI-based bias detection, making this foundational for understanding XAI-fairness relationship. However, subsequent work shows LIME can be fooled and may produce biased explanations. Critical for evaluation frameworks: must assess whether LIME-revealed patterns constitute real bias or explanation artifacts. Perturbation approach affects fairness evaluation - which perturbations reveal discrimination accurately? POSITION: Model-agnostic local explanations (LIME).},
  keywords = {LIME, local-explanations, model-agnostic-XAI, perturbation-based, High}
}

@inproceedings{lundberg2017unified,
  author = {Lundberg, Scott M. and Lee, Su-In},
  title = {A Unified Approach to Interpreting Model Predictions},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2017},
  volume = {30},
  pages = {4765--4774},
  note = {CORE ARGUMENT: Introduces SHAP (SHapley Additive exPlanations) unifying six existing explanation methods under framework based on Shapley values from game theory. Proves SHAP satisfies local accuracy, missingness, and consistency properties that other methods lack. Develops efficient computation methods for tree ensembles and deep networks. Demonstrates superior explanation quality compared to alternatives. RELEVANCE: SHAP has become gold standard for XAI-based fairness auditing due to theoretically grounded properties. Consistency property particularly important for fairness: changing model to rely more on feature increases that feature's attribution. Critical for evaluation frameworks: SHAP's axiomatic foundation provides principled basis for assessing bias claims. However, computational cost and assumptions about feature independence affect fairness assessment validity. POSITION: Unified explanation framework via Shapley values (SHAP).},
  keywords = {SHAP, shapley-values, unified-framework, game-theory, High}
}

@inproceedings{slack2020fooling,
  author = {Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
  title = {Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2020},
  pages = {180--186},
  doi = {10.1145/3375627.3375830},
  note = {CORE ARGUMENT: Demonstrates that malicious actors can construct classifiers that provide misleading LIME and SHAP explanations while maintaining predictive performance. Develops adversarial training procedures that game explanation methods. Shows existing post-hoc explanation techniques insufficient for ascertaining discriminatory behavior in sensitive applications. Highlights need for robust explanation methods resistant to gaming. RELEVANCE: Critical for understanding limitations of XAI-based fairness auditing - explanations can be manipulated to hide discrimination. Directly challenges using LIME/SHAP as sole fairness evaluation tools. Implies need for multi-faceted evaluation combining explanations with other bias detection methods. Adversarial robustness must be included in XAI evaluation frameworks. Raises question: if explanations can be fooled, how reliable is XAI-based bias detection? POSITION: Adversarial attacks on explanation methods.},
  keywords = {adversarial-attacks, explanation-robustness, LIME, SHAP, limitations, High}
}

@inproceedings{dai2022road,
  author = {Dai, Jessica and Upadhyay, Sasha and Aivodji, Ulrich and Bach, Stephen H. and Lakkaraju, Himabindu},
  title = {The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations},
  booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2022},
  pages = {1194--1206},
  doi = {10.1145/3531146.3533179},
  note = {CORE ARGUMENT: First comprehensive study of explanation fairness showing explanation quality differs significantly between protected subgroups. Audits LIME, SHAP, and gradient-based methods across finance, healthcare, college admissions, and criminal justice datasets. Finds fidelity gaps up to 7% between subgroups and fidelity for disadvantaged groups up to 21% lower. Proposes metrics for measuring explanation fairness including fidelity gaps and consistency across groups. RELEVANCE: Groundbreaking work demonstrating that explanations themselves can be biased, independent of predictive fairness. Directly challenges assumption that XAI neutrally reveals bias - explanation methods may provide lower quality insights for disadvantaged groups. Critical for evaluation frameworks: must assess explanation quality across subgroups, not just overall. Implies XAI-based fairness auditing may systematically fail for those most harmed by discrimination. POSITION: Explanation fairness and bias in XAI methods.},
  keywords = {explanation-fairness, fidelity-gaps, subgroup-disparities, XAI-bias, High}
}

@article{bellamy2019ai,
  author = {Bellamy, Rachel K. E. and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovic, Aleksandra and Nagar, Seema and Ramamurthy, Karthikeyan Natesan and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Moninder and Varshney, Kush R. and Zhang, Yunfeng},
  title = {AI Fairness 360: An Extensible Toolkit for Detecting and Mitigating Algorithmic Bias},
  journal = {IBM Journal of Research and Development},
  year = {2019},
  volume = {63},
  number = {4/5},
  pages = {4:1--4:15},
  doi = {10.1147/JRD.2019.2942287},
  note = {CORE ARGUMENT: Presents comprehensive open-source toolkit with 70+ fairness metrics and 10+ bias mitigation algorithms covering preprocessing, inprocessing, and postprocessing stages. Provides unified API for fairness assessment including demographic parity, equalized odds, disparate impact, and others. Includes tutorials, guidance on metric selection, and visualization tools. Enables practitioners to detect and mitigate bias across ML pipeline. RELEVANCE: AI Fairness 360 represents state-of-the-art in fairness evaluation infrastructure, essential reference for practical XAI-fairness integration. Provides standardized metrics for evaluating XAI-based bias claims - when explanations suggest discrimination, toolkit offers rigorous assessment. Comprehensive coverage enables comparison of different fairness criteria revealed by XAI. Mitigation algorithms allow testing whether XAI-guided interventions improve fairness metrics. POSITION: Comprehensive fairness evaluation toolkit.},
  keywords = {fairness-toolkit, metrics-library, bias-mitigation, practical-tools, High}
}

@article{saleiro2018aequitas,
  author = {Saleiro, Pedro and Kuester, Benedict and Stevens, Abby and Anisfeld, Ari and Hinkson, Loren and London, Jesse and Ghani, Rayid},
  title = {Aequitas: A Bias and Fairness Audit Toolkit},
  journal = {arXiv preprint arXiv:1811.05577},
  year = {2018},
  note = {CORE ARGUMENT: Develops open-source bias auditing toolkit operationalizing fairness metrics for practitioners. Provides intuitive workflow for evaluating ML systems across multiple fairness criteria and subgroups. Emphasizes actionable insights for data scientists, researchers, and policymakers. Includes visualization tools, statistical significance testing, and guidance on metric selection. Makes fairness auditing standard practice rather than specialized research activity. RELEVANCE: Demonstrates practical operationalization of fairness metrics essential for XAI-based bias detection workflows. User-friendly interface enables practitioners to validate XAI-revealed bias claims against formal fairness criteria. Emphasis on multiple subgroups aligns with need to evaluate explanation quality across intersectional groups. Toolkit provides benchmark for XAI evaluation frameworks - should be similarly accessible and actionable. POSITION: Practical bias auditing toolkit.},
  keywords = {bias-auditing, fairness-toolkit, practical-evaluation, Medium}
}

@inproceedings{speicher2018unified,
  author = {Speicher, Till and Heidari, Hoda and Grgic-Hlaca, Nina and Gummadi, Krishna P. and Singla, Adish and Weller, Adrian and Zafar, Muhammad Bilal},
  title = {A Unified Approach to Quantifying Algorithmic Unfairness: Measuring Individual & Group Unfairness via Inequality Indices},
  booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},
  year = {2018},
  pages = {2239--2248},
  doi = {10.1145/3219819.3220046},
  note = {CORE ARGUMENT: Proposes using inequality indices from economics (Gini coefficient, Atkinson index, Theil index) to measure algorithmic fairness, unifying individual and group fairness measurement. Shows overall unfairness decomposes into between-group and within-group components. Demonstrates that minimizing between-group unfairness alone can increase within-group and overall unfairness. Provides single framework for quantifying unfairness at multiple levels. RELEVANCE: Provides principled quantitative approach to fairness measurement bridging individual and group perspectives. Decomposition into components directly relevant to XAI evaluation: explanations may reveal between-group discrimination while missing within-group unfairness. Inequality indices offer complementary metrics to binary fairness criteria, enabling nuanced assessment of XAI-detected patterns. Framework clarifies that fixing group-level bias may not ensure individual fairness. POSITION: Unified fairness quantification via inequality indices.},
  keywords = {inequality-indices, individual-group-fairness, unified-framework, quantitative-metrics, Medium}
}

@inproceedings{zhang2018mitigating,
  author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
  title = {Mitigating Unwanted Biases with Adversarial Learning},
  booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2018},
  pages = {335--340},
  doi = {10.1145/3278721.3278779},
  note = {CORE ARGUMENT: Develops adversarial debiasing approach where predictor maximizes accuracy while adversary minimizes ability to predict protected attributes from predictions. Trains model and adversary simultaneously, creating predictions invariant to protected attributes. Demonstrates improvement in fairness (demographic parity, equalized odds) with minimal accuracy loss (~1.5%). Provides inprocessing mitigation strategy integrated into training. RELEVANCE: Adversarial approach offers principled method for bias mitigation that XAI can help evaluate - do explanations show reduced dependence on protected attributes after adversarial training? Framework enables testing whether XAI-guided interventions successfully remove bias. Adversarial architecture itself provides implicit fairness constraint that XAI methods should detect. Trade-off between fairness and accuracy quantified here informs realistic expectations for XAI-based fairness improvements. POSITION: Adversarial debiasing for fairness.},
  keywords = {adversarial-debiasing, bias-mitigation, inprocessing, fairness-accuracy-tradeoff, Medium}
}

@inproceedings{mitchell2019model,
  author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  title = {Model Cards for Model Reporting},
  booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  year = {2019},
  pages = {220--229},
  doi = {10.1145/3287560.3287596},
  note = {CORE ARGUMENT: Proposes model cards as short documentation accompanying ML models, providing benchmarked evaluation across different demographic and phenotypic groups, intended use contexts, performance evaluation details, and other relevant information. Advocates transparent model reporting to clarify intended use cases and minimize deployment in unsuitable contexts. Provides template and guidelines for creating model cards. RELEVANCE: Model cards framework directly supports XAI-based fairness evaluation by standardizing how fairness metrics are reported across subgroups. Documentation approach enables systematic comparison of explanation quality across demographics. Framework provides structure for reporting XAI evaluation results alongside fairness metrics. Transparency emphasis aligns with using XAI to make fairness assessments interpretable to stakeholders. Template offers benchmark for what XAI fairness audits should include. POSITION: Standardized model documentation for fairness transparency.},
  keywords = {model-cards, documentation, transparency, fairness-reporting, Medium}
}

@inproceedings{gebru2018datasheets,
  author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daumé III, Hal and Crawford, Kate},
  title = {Datasheets for Datasets},
  booktitle = {Communications of the ACM},
  year = {2021},
  volume = {64},
  number = {12},
  pages = {86--92},
  doi = {10.1145/3458723},
  note = {CORE ARGUMENT: Proposes datasheets documenting dataset motivation, composition, collection process, preprocessing, distribution, and maintenance. Draws analogy to electronics industry where components include datasheets. Argues documentation facilitates communication between dataset creators and consumers, mitigates unwanted biases, enables reproducibility, and helps researchers choose appropriate datasets. Provides template with questions across multiple categories. RELEVANCE: Dataset documentation is prerequisite for fair XAI evaluation - biased training data will produce biased models and explanations. Datasheets enable assessing whether XAI-detected bias stems from data collection versus model architecture. Framework clarifies what information is needed to validate XAI-based fairness claims. Composition details particularly important for evaluating whether explanation patterns reflect dataset artifacts or genuine model discrimination. POSITION: Dataset documentation for fairness and transparency.},
  keywords = {datasheets, dataset-documentation, data-bias, transparency, Medium}
}

@article{guidotti2018survey,
  author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  title = {A Survey of Methods for Explaining Black Box Models},
  journal = {ACM Computing Surveys},
  year = {2019},
  volume = {51},
  number = {5},
  pages = {1--42},
  doi = {10.1145/3236009},
  note = {CORE ARGUMENT: Comprehensive survey classifying explanation methods by problem definition (model explanation, outcome explanation, model inspection) and black box type (data, model, model-agnostic). Reviews approaches including rule extraction, saliency masks, sensitivity analysis, and local explanations. Discusses trade-offs between interpretability and accuracy. Provides taxonomy organizing diverse explanation literature. RELEVANCE: Provides comprehensive overview of XAI landscape essential for understanding which explanation methods are appropriate for fairness evaluation. Taxonomy clarifies that different explanation types serve different fairness assessment goals - global model explanations reveal systematic bias while local explanations show individual discrimination. Survey enables informed selection of XAI methods for specific fairness evaluation needs. Trade-off discussion informs realistic expectations for XAI in fairness contexts. POSITION: Comprehensive XAI methods survey.},
  keywords = {XAI-survey, explanation-methods, taxonomy, interpretability, Medium}
}

@book{molnar2020interpretable,
  author = {Molnar, Christoph},
  title = {Interpretable Machine Learning: A Guide for Making Black Box Models Explainable},
  publisher = {Lulu.com},
  year = {2020},
  note = {CORE ARGUMENT: Comprehensive textbook covering interpretable models (linear regression, decision trees) and model-agnostic interpretation methods (LIME, SHAP, permutation importance, PDP, ICE, ALE). Explains when to use each method, their advantages and limitations. Includes practical examples and code. Discusses trade-offs between interpretability and performance. Provides accessible introduction to XAI for practitioners. RELEVANCE: Standard reference for understanding XAI methods and their applications, essential background for XAI-based fairness evaluation. Clarifies which explanation methods are appropriate for different fairness assessment tasks. Limitations discussion important for understanding what XAI can and cannot reveal about bias. Practical orientation makes it valuable for implementing XAI fairness audits. Trade-off analysis informs expectations for fairness-accuracy balance when using XAI guidance. POSITION: Comprehensive interpretability textbook.},
  keywords = {interpretability, XAI-methods, textbook, practical-guide, Low}
}

@inproceedings{pleiss2017fairness,
  author = {Pleiss, Geoff and Raghavan, Manish and Wu, Felix and Kleinberg, Jon and Weinberger, Kilian Q.},
  title = {On Fairness and Calibration},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2017},
  volume = {30},
  pages = {5680--5689},
  note = {CORE ARGUMENT: Investigates relationship between calibration and equalized odds fairness, showing well-calibrated models cannot satisfy equalized odds when base rates differ. Proposes calibrated equalized odds as relaxed criterion balancing both desiderata. Develops post-processing method achieving approximate balance. Demonstrates trade-offs empirically on COMPAS data. Shows perfect calibration within groups enables limited equalized odds violation. RELEVANCE: Clarifies fundamental tension between two major fairness criteria (calibration and error rate balance), essential for understanding limitations of XAI-based fairness evaluation. When explanations suggest improving equalized odds, this work shows calibration may suffer and vice versa. Informs metric selection for XAI evaluation frameworks - which fairness criterion matters more depends on use case. Post-processing approach shows how to balance competing criteria XAI may reveal as problems. POSITION: Calibration versus equalized odds trade-offs.},
  keywords = {calibration, equalized-odds, fairness-tradeoffs, post-processing, High}
}

@article{friedler2016possibility,
  author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  title = {On the (Im)possibility of Fairness},
  journal = {arXiv preprint arXiv:1609.07236},
  year = {2016},
  note = {CORE ARGUMENT: Analyzes impossibility of simultaneously satisfying multiple fairness criteria, distinguishing between different worldviews about what fairness requires. Introduces "What You See Is What You Get" (WYSIWYG) versus "We're All Equal" (WAE) perspectives on whether observed differences reflect real differences or measurement bias. Shows fairness criteria align with different worldviews, explaining why stakeholders disagree. Provides framework for understanding when fairness is possible versus impossible. RELEVANCE: Foundational work explaining why fairness evaluation is inherently contentious - different stakeholders may have incompatible fairness desiderata based on different worldviews. Directly impacts XAI-based fairness assessment: explanations revealing bias from one perspective may not reveal bias from another. Framework clarifies that XAI cannot resolve normative disagreements about fairness, only make them explicit. Essential for understanding why no single evaluation framework satisfies all stakeholders. POSITION: Fairness impossibility and competing worldviews.},
  keywords = {impossibility, fairness-worldviews, WYSIWYG, WAE, normative-disagreement, Medium}
}

@inproceedings{goh2016satisfying,
  author = {Goh, Gabriel and Cotter, Andrew and Gupta, Maya and Friedlander, Michael P.},
  title = {Satisfying Real-world Goals with Dataset Constraints},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2016},
  volume = {29},
  pages = {2415--2423},
  note = {CORE ARGUMENT: Proposes training with dataset constraints to achieve goals like fairness (specified positive prediction rates for subpopulations) and recall targets. Uses ramp penalty to quantify costs and develops efficient algorithm for non-convex constrained optimization. Shows metrics from precision and recall to fairness can be expressed as rate constraints on multiple datasets. Enables joint optimization of multiple fairness and performance goals. RELEVANCE: Early work establishing constrained optimization approach to fairness, influencing subsequent fairness-aware training methods. Demonstrates how to operationalize fairness metrics as training constraints that XAI can help validate. Framework enables testing whether XAI-guided fairness interventions achieve specified targets. Rate constraints provide clear success criteria for evaluating whether XAI-informed training improves fairness. Optimization approach shows fairness is achievable through principled methods. POSITION: Fairness via constrained optimization.},
  keywords = {constrained-optimization, fairness-constraints, rate-constraints, training-methods, Medium}
}

@inproceedings{adebayo2016fairml,
  author = {Adebayo, Julius Ayodeji},
  title = {FairML: ToolBox for Diagnosing Bias in Predictive Modeling},
  booktitle = {Master's Thesis, Massachusetts Institute of Technology},
  year = {2016},
  note = {CORE ARGUMENT: Develops toolbox for auditing black-box models by quantifying relative significance of inputs using orthogonal projection to counter multicollinearity. Proposes four input ranking algorithms for assessing model dependence on attributes. Applies framework to analyze COMPAS recidivism algorithm. Provides end-to-end methodology for bias detection through feature importance analysis. RELEVANCE: Early practical tool for XAI-based bias detection, demonstrating feasibility of using feature importance for fairness auditing. Orthogonal projection approach addresses key challenge in XAI fairness evaluation - distinguishing direct discrimination from correlation artifacts. COMPAS application provides case study of XAI bias detection methodology. Ranking algorithms offer concrete metrics for quantifying bias that inform evaluation frameworks. Toolbox demonstrates practical operationalization of XAI for fairness. POSITION: Feature importance for bias auditing.},
  keywords = {bias-auditing, feature-importance, toolbox, COMPAS, Medium}
}

@inproceedings{foulds2020intersectional,
  author = {Foulds, James R. and Islam, Rashidul and Keya, Kamrun Naher and Pan, Shimei},
  title = {An Intersectional Definition of Fairness},
  booktitle = {2020 IEEE 36th International Conference on Data Engineering},
  year = {2020},
  pages = {1918--1921},
  doi = {10.1109/ICDE48307.2020.00203},
  note = {CORE ARGUMENT: Proposes differential fairness framework inspired by differential privacy that measures fairness for individuals with multiple sensitive attributes. Uses information theory to quantify how much sensitive attribute information is leaked by model predictions. Demonstrates framework captures intersectional fairness concerns and enables analysis across multiple demographic dimensions simultaneously. Provides formal definition amenable to optimization. RELEVANCE: Addresses critical gap in fairness evaluation - most metrics consider attributes separately, missing intersectional discrimination. Differential fairness provides principled way to evaluate XAI across multiple attributes simultaneously. Information-theoretic formulation enables quantifying how much XAI explanations reveal about protected attributes. Framework informs evaluation of whether explanation patterns expose intersectional bias versus single-attribute bias. Essential for comprehensive XAI fairness assessment. POSITION: Intersectional fairness via differential privacy.},
  keywords = {differential-fairness, intersectionality, information-theory, multiple-attributes, Medium}
}

@article{narayanan2018fairness,
  author = {Narayanan, Arvind},
  title = {Translation Tutorial: 21 Fairness Definitions and Their Politics},
  journal = {Proceedings of Conference on Fairness, Accountability, and Transparency},
  year = {2018},
  note = {CORE ARGUMENT: Tutorial presenting 21 different technical definitions of fairness and discussing their political implications and trade-offs. Explains that no definition is objectively correct - choice depends on normative values and context. Covers definitions from demographic parity to individual fairness to causal fairness. Discusses how technical choices reflect political values and stakeholder priorities. Emphasizes need for stakeholder involvement in metric selection. RELEVANCE: Essential reference clarifying that fairness is fundamentally value-laden, directly impacting how XAI-based fairness evaluation should be conducted. When XAI reveals bias according to one metric, this tutorial shows why stakeholders may still disagree. Implies XAI evaluation frameworks must make normative commitments explicit rather than claiming technical neutrality. Political analysis shows XAI fairness assessment serves particular stakeholder interests. Tutorial structure useful model for explaining evaluation results. POSITION: Comprehensive fairness definitions and politics.},
  keywords = {fairness-definitions, political-implications, stakeholder-values, tutorial, High}
}

@article{corbettdavies2018measure,
  author = {Corbett-Davies, Sam and Goel, Sharad},
  title = {The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning},
  journal = {arXiv preprint arXiv:1808.00023},
  year = {2018},
  note = {CORE ARGUMENT: Critical review of fairness criteria arguing that optimizing for common metrics (demographic parity, equalized odds) can actually reduce overall welfare and harm both advantaged and disadvantaged groups. Proposes decision-theoretic approach considering costs and benefits of decisions rather than purely statistical criteria. Shows that fairness constraints can increase error rates for everyone when base rates differ. Advocates for explicitly modeling stakeholder utilities. RELEVANCE: Important cautionary perspective on fairness metrics commonly used in XAI evaluation - optimizing metrics XAI reveals as problematic may worsen outcomes. Implies XAI-based fairness auditing should consider welfare consequences, not just statistical parity. Decision-theoretic framework suggests XAI evaluation should incorporate cost-benefit analysis alongside bias detection. Critical review informs realistic assessment of what XAI-guided fairness interventions can achieve. POSITION: Decision-theoretic fairness critique.},
  keywords = {fairness-critique, welfare-analysis, decision-theory, metric-limitations, Medium}
}

@article{wachter2017counterfactual,
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  title = {Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR},
  journal = {Harvard Journal of Law & Technology},
  year = {2017},
  volume = {31},
  number = {2},
  pages = {841--887},
  note = {CORE ARGUMENT: Proposes counterfactual explanations as practical approach to algorithmic transparency under GDPR's right to explanation. Counterfactuals answer "what would need to change for different outcome?" without revealing model internals. Argues counterfactuals provide actionable recourse while protecting trade secrets. Develops method generating counterfactuals for black-box models. Discusses legal implications for GDPR compliance. RELEVANCE: Connects XAI to legal transparency requirements, showing how explanation methods serve fairness and accountability goals. Counterfactual approach particularly relevant to fairness evaluation - "what if protected attribute were different?" directly tests discrimination. Actionable recourse focus implies XAI evaluation should assess whether explanations enable individuals to obtain fair outcomes. GDPR context shows XAI fairness assessment has legal stakes, not just technical ones. POSITION: Counterfactual explanations for transparency and recourse.},
  keywords = {counterfactual-explanations, GDPR, actionable-recourse, legal-requirements, Medium}
}

@inproceedings{ustun2019actionable,
  author = {Ustun, Berk and Spangher, Alexander and Liu, Yang},
  title = {Actionable Recourse in Linear Classification},
  booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  year = {2019},
  pages = {10--19},
  doi = {10.1145/3287560.3287566},
  note = {CORE ARGUMENT: Studies algorithmic recourse - providing individuals actionable recommendations to obtain desired outcomes from ML systems. Formalizes properties of actionable recourse (feasibility, cost, diversity) and develops methods to generate recommendations. Shows linear models can provide recourse while maintaining predictive performance. Discusses trade-offs between recourse and fairness. RELEVANCE: Extends XAI beyond explaining decisions to enabling individuals to change outcomes, directly relevant to fairness goals of empowerment and non-discrimination. When XAI reveals bias, recourse framework shows how to help affected individuals. Evaluation frameworks should assess whether XAI-based fairness interventions preserve actionable recourse. Trade-offs between recourse and fairness criteria inform realistic goals for XAI-guided improvements. Actionability perspective reframes fairness as enabling agency. POSITION: Algorithmic recourse and actionable explanations.},
  keywords = {algorithmic-recourse, actionable-explanations, individual-empowerment, fairness, Low}
}

@article{barocas2019fairness,
  author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  title = {Fairness and Machine Learning: Limitations and Opportunities},
  publisher = {MIT Press},
  year = {2019},
  note = {CORE ARGUMENT: Comprehensive textbook covering classification of fairness criteria, causes of disparate treatment and impact, fairness-aware machine learning methods, testing and auditing approaches, and relationship between fairness and interpretability. Discusses legal and ethical context for algorithmic fairness. Provides both theoretical foundations and practical guidance. Accessible to both technical and non-technical readers. RELEVANCE: Standard textbook providing comprehensive overview of fairness landscape essential for contextualizing XAI-based evaluation. Dedicated chapter on relationship between fairness and interpretability directly addresses how XAI serves fairness goals. Testing and auditing section informs XAI evaluation methodologies. Legal and ethical context clarifies what "bias" means beyond statistical definitions. Book provides theoretical foundations for understanding what XAI fairness assessment can and cannot achieve. POSITION: Comprehensive fairness textbook.},
  keywords = {fairness-textbook, comprehensive-overview, fairness-interpretability, Low}
}

@inproceedings{lipton2018mythos,
  author = {Lipton, Zachary C.},
  title = {The Mythos of Model Interpretability},
  booktitle = {Queue},
  year = {2018},
  volume = {16},
  number = {3},
  pages = {31--57},
  doi = {10.1145/3236386.3241340},
  note = {CORE ARGUMENT: Critically examines interpretability claims in ML literature, arguing term is poorly defined and used to mean transparency, explanation, fair treatment, privacy, and other distinct concepts. Distinguishes between transparency (how model works) and post-hoc explanations (after-the-fact justifications). Questions whether interpretability methods actually provide understanding or merely reassurance. Calls for precise definitions and empirical validation. RELEVANCE: Critical perspective essential for rigorous XAI evaluation - cannot assume interpretability methods reliably reveal bias without validation. Challenges motivate need for measuring explanation quality rather than assuming explanations are trustworthy. Distinction between transparency and post-hoc explanation affects fairness assessment - which type reveals discrimination more reliably? Skepticism about interpretability claims implies XAI-based fairness auditing requires empirical validation. POSITION: Critical analysis of interpretability claims.},
  keywords = {interpretability-critique, definition-precision, transparency-vs-explanation, Low}
}

@inproceedings{arrieta2020explainable,
  author = {Arrieta, Alejandro Barredo and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and García, Salvador and Gil-López, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  title = {Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges Toward Responsible AI},
  journal = {Information Fusion},
  year = {2020},
  volume = {58},
  pages = {82--115},
  doi = {10.1016/j.inffus.2019.12.012},
  note = {CORE ARGUMENT: Comprehensive survey of XAI covering concepts, taxonomies, evaluation methods, and applications. Classifies explanation methods by transparency level (transparent, interpretable, opaque) and scope (global, local). Discusses evaluation metrics for explanations including fidelity, consistency, robustness. Reviews XAI applications across domains and identifies open challenges. Emphasizes connection between XAI and responsible AI. RELEVANCE: Survey provides systematic overview of XAI landscape essential for understanding options for fairness-focused explanation methods. Evaluation metrics section directly relevant to assessing explanation quality in fairness contexts - fidelity and consistency particularly important. Responsible AI framing connects XAI to fairness, accountability, and transparency goals. Challenges section identifies gaps including explanation fairness that remain open research problems. POSITION: Comprehensive XAI survey and taxonomy.},
  keywords = {XAI-survey, taxonomy, evaluation-metrics, responsible-AI, Low}
}

@inproceedings{wexler2019if,
  author = {Wexler, James and Pushkarna, Mahima and Bolukbasi, Tolga and Wattenberg, Martin and Viégas, Fernanda and Wilson, Jimbo},
  title = {The What-If Tool: Interactive Probing of Machine Learning Models},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  year = {2020},
  volume = {26},
  number = {1},
  pages = {56--65},
  doi = {10.1109/TVCG.2019.2934619},
  note = {CORE ARGUMENT: Presents What-If Tool enabling interactive probing of ML models without coding, allowing users to visualize counterfactuals, test fairness constraints, and explore model behavior. Supports fairness evaluation across multiple metrics and subgroups through visual interface. Enables comparison of fairness across different thresholds and interventions. Designed for practitioners needing accessible fairness assessment tools. RELEVANCE: Demonstrates practical implementation of XAI for fairness evaluation through interactive visualization. Interface enables testing different fairness metrics that XAI might reveal as problematic. Counterfactual visualization directly supports bias detection - users can see how predictions change with protected attribute values. Accessibility focus shows XAI fairness tools need not require deep technical expertise. Tool provides benchmark for user-centered XAI evaluation frameworks. POSITION: Interactive visualization tool for fairness evaluation.},
  keywords = {visualization-tool, interactive-XAI, fairness-evaluation, counterfactuals, Low}
}

@article{chen2020concept,
  author = {Chen, Chaofan and Li, Oscar and Tao, Daniel and Barnett, Alina and Rudin, Cynthia and Su, Jonathan K.},
  title = {This Looks Like That: Deep Learning for Interpretable Image Recognition},
  journal = {Advances in Neural Information Processing Systems},
  year = {2019},
  volume = {32},
  pages = {8930--8941},
  note = {CORE ARGUMENT: Proposes prototype-based deep learning architecture where classification decisions compare input to learned prototypical parts. Model is inherently interpretable because reasoning process mimics human explanations ("this looks like that"). Achieves accuracy competitive with black-box models while providing case-based explanations. Demonstrates on image classification including medical diagnosis. RELEVANCE: Prototype approach offers alternative to post-hoc XAI that may better support fairness evaluation. Case-based reasoning enables checking whether prototypes reflect stereotypes or biases. Inherent interpretability avoids issues with adversarial attacks on explanation methods. For fairness assessment, comparing prototypes across groups reveals whether model learns different patterns for different demographics. Architecture demonstrates that interpretability and accuracy need not trade off severely. POSITION: Inherently interpretable architecture via prototypes.},
  keywords = {prototype-learning, inherent-interpretability, case-based-reasoning, Low}
}

@comment{
====================================================================
DOMAIN: Philosophical & Ethical Critiques of XAI and Algorithmic Fairness
SEARCH_DATE: 2025-11-23
PAPERS_FOUND: 31 (High: 18, Medium: 10, Low: 3)
SEARCH_SOURCES: Philosophy & Technology, AI & Society, Ethics and Information Technology,
                Synthese, FAccT, AIES, Philosophy Compass, Canadian Journal of Philosophy,
                Res Publica, British Journal for the Philosophy of Science, Nature Machine Intelligence,
                Philosophy of Science, Journal of Moral Philosophy, Communications of the ACM
====================================================================

DOMAIN_OVERVIEW:
This domain addresses fundamental conceptual and ethical problems with using XAI (Explainable AI)
to detect discrimination. Philosophical critiques reveal deep tensions between technical
approaches to fairness and philosophical concepts of justice, discrimination, and explanation.
Key debates center on: (1) what counts as an "explanation" and whether XAI systems truly explain;
(2) epistemic opacity and the limits of interpretability; (3) whether fairness metrics capture
meaningful concepts of justice and discrimination; (4) the relationship between explainability
and trust; (5) power dynamics in who benefits from explanations; and (6) techno-solutionism
versus structural approaches to fairness. Philosophers argue that technical fairness definitions
often misunderstand legal and moral concepts they claim to operationalize, that post-hoc
explanations may not reflect genuine decision processes, and that formal algorithmic fairness
frameworks ignore situated social dynamics and structural inequalities. Recent work emphasizes
relational approaches to equality, intersectional perspectives, and the need to move beyond
narrow statistical parity metrics toward substantive fairness that addresses power asymmetries
and social hierarchies.

RELEVANCE_TO_PROJECT:
These philosophical critiques are essential for understanding fundamental limitations of using
XAI to detect discrimination. The critiques reveal that technical solutions may obscure rather
than illuminate discrimination, that explanation is not computation, that transparency doesn't
guarantee accountability, and that fairness metrics may fail to capture meaningful injustice.
For a project on XAI-based discrimination detection, these arguments challenge the core
assumption that making algorithms interpretable solves fairness problems, suggesting instead
that conceptual clarity about discrimination, justice, and explanation must precede technical
implementation.

RECENT_DEVELOPMENTS:
Since 2020, philosophical work has intensified critiques of techno-solutionism in XAI, with
growing emphasis on: (1) relational and structural approaches to fairness over formal metrics;
(2) intersectional analyses revealing limitations of standard fairness definitions; (3)
epistemology of machine learning addressing understanding versus mere prediction; (4) trust
and explainability showing explainability is not necessary for appropriate trust; (5) power
dynamics in algorithmic accountability; (6) value-ladenness of ML benchmarks and fairness
criteria. The field has moved from accepting transparency as solution toward critical
interrogation of what transparency achieves and who benefits.

NOTABLE_GAPS:
Few works directly address how philosophical critiques should inform design of XAI systems for
discrimination detection specifically. Most work critiques existing approaches but offers
limited constructive guidance for practitioners. Gap between philosophical sophistication and
technical implementation persists. Limited engagement with how situated dynamics of deployment
affect what counts as fair or discriminatory. More work needed on epistemic standards for
when XAI explanations constitute genuine understanding versus rationalization.

SYNTHESIS_GUIDANCE:
Structure review around three core problems: (1) Conceptual issues - what counts as explanation,
discrimination, fairness; (2) Epistemic issues - opacity, understanding, trust, evidential value;
(3) Normative issues - justice, power, autonomy, structural inequality. Key tensions: formal
vs. substantive fairness, transparency vs. accountability, individual vs. relational equality,
statistical parity vs. anti-subordination. Focus on work by: Krishnan, Green, Binns,
Fazelpour & Lipton, Holm, Sullivan, Birhane, Kong, Hu for philosophical depth. Use Ananny &
Crawford, Kaminski, Selbst & Barocas for sociotechnical perspective.

KEY_POSITIONS:
- Anti-interpretability position: 8 papers - Challenges that interpretability is necessary or sufficient for responsible AI
- Formal vs. substantive fairness: 6 papers - Critiques impossibility results and argues for substantive approach
- Relational equality approach: 4 papers - Emphasizes social hierarchies over distributive metrics
- Intersectional critique: 3 papers - Shows standard fairness definitions fail women of color
- Epistemology of ML: 5 papers - Understanding, explanation, opacity in machine learning
- Trust without explainability: 2 papers - Challenges that XAI is necessary for trust
- Power and accountability: 3 papers - Who benefits from transparency and explanations
====================================================================
}

@article{krishnan2020against,
  author = {Krishnan, Maya},
  title = {Against Interpretability: a Critical Examination of the Interpretability Problem in Machine Learning},
  journal = {Philosophy \& Technology},
  year = {2020},
  volume = {33},
  number = {3},
  pages = {487--502},
  doi = {10.1007/s13347-019-00372-9},
  note = {CORE ARGUMENT: Challenges widespread agreement about existence of "black box problem" in ML. Argues interpretability and related terms lack precise meanings when applied to algorithms, making concepts difficult to use for solving problems that motivated calls for interpretability. Claims there are ways of being responsible algorithm user that do not require interpretability, and in many cases where black box problem is cited, interpretability is means to further end like justification or non-discrimination. RELEVANCE: Fundamental philosophical critique questioning whether interpretability is necessary or well-defined concept. Directly challenges assumption underlying XAI approaches to discrimination detection - that making algorithms interpretable solves fairness problems. Suggests interpretability may be wrong target and other accountability mechanisms may be more appropriate. POSITION: Anti-interpretability critique - challenges necessity and conceptual coherence of interpretability as goal.},
  keywords = {interpretability, XAI-critique, conceptual-analysis, High}
}

@article{green2022escaping,
  author = {Green, Ben},
  title = {Escaping the Impossibility of Fairness: From Formal to Substantive Algorithmic Fairness},
  journal = {Philosophy \& Technology},
  year = {2022},
  volume = {35},
  number = {4},
  pages = {90},
  doi = {10.1007/s13347-022-00584-6},
  note = {CORE ARGUMENT: Diagnoses problems with "formal algorithmic fairness" which restricts analysis to isolated decision-making procedures and leads to impossibility of fairness (incompatibility between mathematical fairness definitions). Proposes alternative methodology of "substantive algorithmic fairness" that takes more expansive scope of analysis considering social context, structural factors, and power relations to escape impossibility constraints. Argues formal approach is fundamentally limited because it abstracts from actual social conditions and deployment contexts. RELEVANCE: Provides philosophical framework for understanding why technical fairness metrics fail to capture meaningful justice concerns. Directly relevant to XAI discrimination detection because it shows formal fairness criteria (which XAI aims to satisfy/explain) are insufficient for genuine fairness. Points toward need for situated, structural analysis rather than abstract mathematical optimization. POSITION: Substantive fairness approach - rejects formal metrics in favor of context-dependent social analysis.},
  keywords = {algorithmic-fairness, formal-vs-substantive, impossibility-of-fairness, structural-approach, High}
}

@article{binns2018fairness,
  author = {Binns, Reuben},
  title = {Fairness in Machine Learning: Lessons from Political Philosophy},
  journal = {Proceedings of Machine Learning Research},
  year = {2018},
  volume = {81},
  pages = {149--159},
  note = {CORE ARGUMENT: Draws on moral and political philosophy to elucidate debates about fair machine learning, showing attempts to formalize fairness in ML contain echoes of old philosophical debates. Addresses fundamental questions: Should fairness consist of ensuring everyone has equal probability of benefit, or minimize harms to least advantaged? Can ideal be determined by reference to alternative state without particular discrimination? Characterizes fairness problems as distribution problems and describes solutions according to different normative principles: intentions, compensation, desert, consent, consequences. RELEVANCE: Provides philosophical grounding for understanding fairness metrics and their normative commitments. Essential for critically evaluating which fairness definitions XAI systems should explain/satisfy when detecting discrimination. Shows that choice between fairness metrics is ultimately philosophical question about justice, not technical optimization problem. POSITION: Political philosophy approach to algorithmic fairness - applies theories of justice to ML.},
  keywords = {algorithmic-fairness, political-philosophy, distributive-justice, High}
}

@article{fazelpour2022algorithmic,
  author = {Fazelpour, Sina and Lipton, Zachary C. and Danks, David},
  title = {Algorithmic Fairness and the Situated Dynamics of Justice},
  journal = {Canadian Journal of Philosophy},
  year = {2022},
  volume = {52},
  number = {1},
  pages = {44--60},
  doi = {10.1017/can.2022.26},
  note = {CORE ARGUMENT: Critiques preoccupation with identifying "ideally fair" target states in algorithmic fairness research, arguing this focus on target states in abstraction from situated dynamics of deployment is misguided. Shows that normative theorizing has focused on static fairness definitions while ignoring complex social dynamics including incentive effects, strategic behaviors, and partial compliance. Argues fairness must be evaluated within actual deployment contexts considering power relations and systemic effects. RELEVANCE: Fundamental critique of how fairness is conceptualized in XAI/ML fairness literature. For discrimination detection, reveals that explaining whether algorithm satisfies static fairness metric misses situated dynamics of how systems actually produce or mitigate injustice in practice. Shows gap between abstract fairness criteria and real-world justice concerns. POSITION: Situated dynamics approach - rejects abstract fairness metrics for contextual analysis of deployment.},
  keywords = {algorithmic-fairness, situated-dynamics, partial-compliance, justice, High}
}

@article{holm2023fairness,
  author = {Holm, Sune},
  title = {The Fairness in Algorithmic Fairness},
  journal = {Res Publica},
  year = {2023},
  volume = {29},
  number = {2},
  pages = {265--281},
  doi = {10.1007/s11158-022-09546-3},
  note = {CORE ARGUMENT: Addresses concerns about fairness in prediction-based decisions, arguing that mathematically incompatible performance parity criteria can all be understood as applications of John Broome's account of fairness as proportional satisfaction of claims. On this interpretation, criteria don't disagree on what fairness means but express different understandings of what grounds claim to good being allocated. Provides unified philosophical framework for understanding diversity of fairness metrics. RELEVANCE: Offers philosophical reconciliation of competing fairness definitions, clarifying normative commitments underlying different metrics. Important for understanding what XAI systems are explaining when they show algorithm satisfies particular fairness criterion - each criterion reflects different theory of what grounds claims to algorithmic decisions. POSITION: Broomean fairness approach - interprets fairness metrics through theory of claim satisfaction.},
  keywords = {algorithmic-fairness, Broome-fairness, claims-satisfaction, Medium}
}

@inproceedings{holm2025algorithmic,
  author = {Holm, Sune},
  title = {Algorithmic Fairness, Decision Thresholds, and the Separateness of Persons},
  booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2025},
  pages = {1--11},
  doi = {10.1145/3715275.3732113},
  note = {CORE ARGUMENT: Argues fairness requires that algorithmic procedure allocates therapy for right reason, and right kind of reason must respect separateness of persons. Anchors individual-level approach to algorithmic fairness in Broome's theory of fairness, concluding algorithmic fairness requires individual decision thresholds. Shows popular statistical fairness criteria fail because they don't take into account whether therapy would benefit particular individual in expectation, treating people as mere statistics in group distributions. RELEVANCE: Powerful philosophical argument against group-level statistical fairness metrics commonly used in XAI. For discrimination detection, shows that explaining group-level fairness statistics misses fundamental individual-level considerations about whether person is treated for right reasons. Challenges whether standard XAI approaches can capture morally relevant features of fair treatment. POSITION: Individual fairness based on separateness of persons - rejects statistical group parity.},
  keywords = {algorithmic-fairness, separateness-of-persons, individual-fairness, Rawlsian, High}
}

@article{holm2023egalitarianism,
  author = {Holm, Sune},
  title = {Egalitarianism and Algorithmic Fairness},
  journal = {Philosophy \& Technology},
  year = {2023},
  volume = {36},
  number = {1},
  pages = {1--18},
  doi = {10.1007/s13347-023-00607-w},
  note = {CORE ARGUMENT: Explores leveling down objection to classification parity criteria from egalitarian perspective. Examines whether achieving statistical parity by reducing accuracy for advantaged groups (leveling down) can be justified on egalitarian grounds. Analyzes different versions of egalitarianism and their implications for algorithmic fairness, showing tensions between equality as such and equality that improves people's situations. RELEVANCE: Addresses philosophical puzzle about what egalitarian fairness means in algorithmic context. Relevant to XAI discrimination detection because it questions whether explaining that algorithm achieves statistical parity is sufficient - may need to explain whether equality achieved through leveling up or down. Shows fairness metrics alone don't capture egalitarian ideals. POSITION: Egalitarian analysis of classification parity - examines leveling down objection.},
  keywords = {egalitarianism, classification-parity, leveling-down, Medium}
}

@article{sullivan2022understanding,
  author = {Sullivan, Emily},
  title = {Understanding from Machine Learning Models},
  journal = {The British Journal for the Philosophy of Science},
  year = {2022},
  volume = {73},
  number = {1},
  pages = {109--133},
  doi = {10.1093/bjps/axz035},
  note = {CORE ARGUMENT: Argues it is not complexity or black box nature of ML model that limits understanding it provides. Instead, lack of scientific and empirical evidence supporting link connecting model to target phenomenon (link uncertainty) primarily prohibits understanding. Claims scientists can gain understanding from opaque ML models if proper validation and empirical support exists for model's connection to phenomenon. Challenges assumption that transparency equals understanding. RELEVANCE: Fundamental epistemological argument about when ML models provide genuine understanding versus mere predictive accuracy. For XAI discrimination detection, suggests that making model interpretable doesn't guarantee understanding of discrimination unless proper empirical validation exists. Challenges equation of explainability with epistemic value. POSITION: Link uncertainty account - understanding depends on empirical validation not transparency.},
  keywords = {understanding, explanation, ML-epistemology, link-uncertainty, High}
}

@article{beisbart2022philosophy,
  author = {Beisbart, Claus and R{\"a}z, Tim},
  title = {Philosophy of science at sea: Clarifying the interpretability of machine learning},
  journal = {Philosophy Compass},
  year = {2022},
  volume = {17},
  number = {6},
  pages = {e12830},
  doi = {10.1111/phc3.12830},
  note = {CORE ARGUMENT: Questions about interpretability lead into philosophical waters because clarifying fundamental concepts is philosophical task, and understanding and explanation are central to philosophy of science. Notes technical research on interpretability lacks conceptual foundation and isn't integrated with research on understanding/explanation from philosophy, psychology, cognitive science. In CS itself, notion of interpretability is unclear with few articulating precisely what interpretability is or why important. Proposes four philosophical tasks: clarify interpretability notion, explain its value, provide frameworks to think about it, explore important features to adjust expectations. RELEVANCE: Provides philosophical analysis of what interpretability actually means and why it's valuable. Essential for XAI discrimination detection because it reveals conceptual confusion in field about what systems are trying to achieve. Shows need for philosophical clarity before technical implementation. POSITION: Philosophy of science approach - calls for conceptual clarification of interpretability.},
  keywords = {interpretability, philosophy-of-science, conceptual-analysis, understanding, High}
}

@article{nyrup2022limits,
  author = {Nyrup, Rune},
  title = {The Limits of Value Transparency in Machine Learning},
  journal = {Philosophy of Science},
  year = {2022},
  volume = {89},
  number = {5},
  pages = {1128--1138},
  doi = {10.1017/psa.2022.49},
  note = {CORE ARGUMENT: Distinguishes three kinds of transparency: epistemic transparency, retrospective value transparency, and prospective value transparency. Discusses three sources of value-ladenness in ML (problem formulation, inductive risk, specification gaming) and argues retrospective value transparency is only well-suited for first, while third raises serious challenges even for prospective transparency. Shows limits to transparency as strategy for handling value-ladenness in ML including XAI and governance based on disclosing information about design process. RELEVANCE: Reveals that transparency approaches to XAI have fundamental limitations in addressing value-laden choices in ML systems. For discrimination detection, shows that making algorithm transparent doesn't necessarily reveal normative choices embedded in system design, particularly around specification gaming and inductive risk. Limits what XAI can achieve in detecting value-based discrimination. POSITION: Critique of transparency limits - shows transparency insufficient for managing values in ML.},
  keywords = {transparency, value-ladenness, specification-gaming, inductive-risk, Medium}
}

@article{ananny2018seeing,
  author = {Ananny, Mike and Crawford, Kate},
  title = {Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability},
  journal = {New Media \& Society},
  year = {2018},
  volume = {20},
  number = {3},
  pages = {973--989},
  doi = {10.1177/1461444816676645},
  note = {CORE ARGUMENT: Critically interrogates transparency ideal, traces its roots in scientific and sociotechnical epistemological cultures, and presents 10 limitations to its application to algorithmic accountability. Examines how being able to see system is sometimes equated with being able to know how it works and govern it. Argues transparency is inadequate for understanding and governing algorithmic systems, sketching alternative typology of algorithmic accountability grounded in constructive engagements with transparency limitations. Shows seeing doesn't equal knowing. RELEVANCE: Fundamental critique of transparency as accountability mechanism for algorithms. Directly challenges XAI assumption that making algorithms visible/interpretable leads to accountability for discrimination. Shows transparency can create illusion of accountability without actual oversight. Essential for understanding limits of XAI approaches to discrimination detection. POSITION: Critique of transparency ideal - seeing doesn't equal knowing or governing.},
  keywords = {transparency, algorithmic-accountability, seeing-without-knowing, governance, High}
}

@article{wachter2018counterfactual,
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  title = {Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR},
  journal = {Harvard Journal of Law \& Technology},
  year = {2018},
  volume = {31},
  number = {2},
  pages = {841--887},
  note = {CORE ARGUMENT: Shows that implementing right to explanation that opens black box of algorithmic decision-making faces major legal and technical barriers, but explanations can in principle be offered without opening black box. Proposes counterfactual explanations that describe smallest change to world to obtain desirable outcome without conveying internal state or logic of algorithm. Counterfactuals describe dependency on external facts without revealing internal mechanism. RELEVANCE: Offers alternative to interpretability-based XAI, showing explanations need not reveal internal algorithm workings. For discrimination detection, suggests counterfactuals might reveal discriminatory patterns without full transparency. However, raises questions about whether counterfactual explanations truly explain discrimination or merely describe how to avoid being classified negatively. POSITION: Counterfactual explanation approach - explanation without transparency of internal mechanism.},
  keywords = {counterfactual-explanations, GDPR, right-to-explanation, black-box, Medium}
}

@article{kaminski2019binary,
  author = {Kaminski, Margot E.},
  title = {Binary Governance: Lessons from the GDPR's Approach to Algorithmic Accountability},
  journal = {Southern California Law Review},
  year = {2019},
  volume = {92},
  number = {6},
  pages = {1529--1616},
  note = {CORE ARGUMENT: Examines GDPR and analyzes pros and cons regarding individual rights and collaborative governance. Proposes two-pronged approach to algorithmic governance: system of individual due process rights combined with collaborative governance using private-public partnerships. Argues only through this binary approach can all three categories of concerns raised by algorithmic decision-making be effectively addressed. Shows individual transparency rights alone insufficient for algorithmic accountability. RELEVANCE: Provides governance framework showing XAI alone (individual explanations) insufficient for addressing discrimination. Combines individual rights to explanation with institutional oversight and collaborative governance. For discrimination detection, suggests XAI must be paired with structural accountability mechanisms, not relied on as sole solution. POSITION: Binary governance approach - combines individual rights with institutional oversight.},
  keywords = {algorithmic-governance, GDPR, accountability, binary-governance, Medium}
}

@article{lipton2018mythos,
  author = {Lipton, Zachary C.},
  title = {The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery},
  journal = {Communications of the ACM},
  year = {2018},
  volume = {61},
  number = {10},
  pages = {36--43},
  doi = {10.1145/3233231},
  note = {CORE ARGUMENT: Addresses how papers provide diverse and sometimes non-overlapping motivations for interpretability, offering myriad notions of what attributes render models interpretable. Examines motivations underlying interest in interpretability, finding them diverse and occasionally discordant. Identifies transparency to humans and post-hoc explanations as competing notions of interpretability. Shows interpretability is both important and slippery concept lacking clear definition or consensus on what makes model interpretable. RELEVANCE: Foundational critique showing interpretability lacks clear meaning in ML community. Essential for understanding conceptual confusion in XAI field. For discrimination detection, reveals that different stakeholders may mean different things by "interpretable" discrimination detection system, potentially talking past each other. Shows need for clarity about what interpretability is supposed to achieve. POSITION: Conceptual critique - reveals interpretability as ill-defined concept with multiple meanings.},
  keywords = {interpretability, conceptual-critique, definition-problem, High}
}

@article{rudin2019stop,
  author = {Rudin, Cynthia},
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  journal = {Nature Machine Intelligence},
  year = {2019},
  volume = {1},
  number = {5},
  pages = {206--215},
  doi = {10.1038/s42256-019-0048-x},
  note = {CORE ARGUMENT: Argues black box ML models for high-stakes decisions cause problems in healthcare, criminal justice and other domains. Claims trying to explain black box models rather than creating interpretable models in first place likely perpetuates bad practice and can potentially cause great harm to society. Advocates designing models that are inherently interpretable. Clarifies chasm between explaining black boxes and using inherently interpretable models, outlines key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable ML. RELEVANCE: Powerful argument against post-hoc XAI for discrimination detection in favor of inherently interpretable models. For high-stakes discrimination detection, suggests explaining black box is wrong approach - should use interpretable models from start. Challenges entire post-hoc XAI paradigm. POSITION: Inherently interpretable models - rejects post-hoc explanation of black boxes for high-stakes decisions.},
  keywords = {interpretable-models, post-hoc-critique, black-box-problem, High}
}

@article{hu2023race,
  author = {Hu, Lily},
  title = {What is "Race" in Algorithmic Discrimination on the Basis of Race?},
  journal = {Journal of Moral Philosophy},
  year = {2023},
  volume = {21},
  number = {1-2},
  pages = {1--34},
  doi = {10.1163/17455243-20234437},
  note = {CORE ARGUMENT: Addresses under-appreciated puzzle of discrimination: figuring out when decision made on basis of factor correlated with race is decision made on basis of race. On "thick constructivist" account of race, to be raced is to be socially positioned in way indicated by certain statistical regularities on basis of particular phenotypical traits. Argues acting on basis of correlations that constitute race qua social position just is acting on basis of race, because races just are social positions subjecting members to particular matrix of social relations defining raced position. ML algorithms bring out this puzzle. RELEVANCE: Fundamental philosophical analysis of what race is and how algorithms discriminate on basis of race. Crucial for discrimination detection because it shows algorithms can discriminate on race even without race as explicit variable - correlations themselves constitute racial discrimination under thick constructivist view. Challenges technical approaches that remove race variable as solution to discrimination. POSITION: Thick constructivism about race - race is social position constituted by statistical correlations.},
  keywords = {race, discrimination, constructivism, proxy-discrimination, High}
}

@inproceedings{kong2022intersectionally,
  author = {Kong, Youjin},
  title = {Are "Intersectionally Fair" AI Algorithms Really Fair to Women of Color? A Philosophical Analysis},
  booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2022},
  pages = {485--494},
  doi = {10.1145/3531146.3533114},
  note = {CORE ARGUMENT: Examines three fundamental problems with dominant interpretation of intersectional fairness in AI as statistical parity among intersectional subgroups. First, dominant approach is preoccupied with intersection of attributes/categories (race, gender) rather than intersection of oppression (racism, sexism), which is more central to intersectionality as critical framework. Second, approach fails to adequately capture structural nature of discrimination. Third, fails to capture what it really means for AI to be fair in terms of both distributive and non-distributive fairness. Distinguishes strong sense of AI fairness from weak sense prevalent in literature. RELEVANCE: Shows standard fairness metrics fail to capture intersectional discrimination, even when extended to intersectional subgroups. For XAI discrimination detection, reveals that explaining statistical parity across intersectional groups doesn't address structural oppression or non-distributive fairness concerns. Technical fairness definitions miss what matters about intersectional discrimination. POSITION: Intersectional critique of algorithmic fairness - statistical approaches miss intersection of oppressions.},
  keywords = {intersectionality, algorithmic-fairness, oppression, structural-discrimination, High}
}

@article{birhane2021algorithmic,
  author = {Birhane, Abeba},
  title = {Algorithmic injustice: a relational ethics approach},
  journal = {Patterns},
  year = {2021},
  volume = {2},
  number = {2},
  pages = {100205},
  doi = {10.1016/j.patter.2021.100205},
  note = {CORE ARGUMENT: Shows ML automates and perpetuates historical, often unjust and discriminatory, patterns. Most algorithmic fairness work is narrow in scope, focusing on fine-tuning specific models and "debiasing" datasets. Argues fundamentally equitable path must examine wider picture including unquestioned assumptions, current and historical injustices, and power asymmetries. Seeks not mere "methodology" or "tool," but "re-examination of nature of existence, knowledge, oppression, and injustice" to study social, historical, and political context of algorithmic injustice. Calls for relational ethics approach. RELEVANCE: Fundamental critique showing technical fairness fixes miss structural and historical dimensions of injustice. For XAI discrimination detection, argues explaining algorithm fairness metrics is insufficient - must examine systems of power and historical oppression algorithms operate within. Challenges entire techno-solutionist framing of using XAI to "solve" discrimination. POSITION: Relational ethics approach - rejects technical fixes for structural re-examination of power and oppression.},
  keywords = {algorithmic-injustice, relational-ethics, structural-oppression, power-asymmetry, High}
}

@book{noble2018algorithms,
  author = {Noble, Safiya Umoja},
  title = {Algorithms of Oppression: How Search Engines Reinforce Racism},
  publisher = {NYU Press},
  address = {New York},
  year = {2018},
  note = {CORE ARGUMENT: Challenges idea that search engines like Google offer equal playing field for all forms of ideas, identities, and activities. Argues search algorithms are racist and perpetuate societal problems because they reflect negative biases existing in society and people who create them. Shows combination of private interests in promoting certain sites, along with monopoly status of relatively small number of Internet search engines, leads to biased set of search algorithms that privilege whiteness and discriminate against people of color, specifically women of color. Introduces concepts of "algorithmic oppression" and "technological redlining." RELEVANCE: Demonstrates how algorithms reinforce systemic racism through multiple mechanisms beyond individual bias. For XAI discrimination detection, shows explaining individual algorithm decisions insufficient - must examine how algorithmic systems systematically privilege and oppress different groups. Structural analysis of algorithmic discrimination essential for meaningful detection. POSITION: Critical race theory approach to algorithms - examines systematic oppression not individual bias.},
  keywords = {algorithmic-oppression, racism, search-engines, structural-discrimination, High}
}

@article{prunkl2022human,
  author = {Prunkl, Carina E. A.},
  title = {Human autonomy in the age of artificial intelligence},
  journal = {Nature Machine Intelligence},
  year = {2022},
  volume = {4},
  number = {2},
  pages = {99--101},
  doi = {10.1038/s42256-022-00449-9},
  note = {CORE ARGUMENT: Highlights two distinct dimensions of autonomy - agency and authenticity - and lists different ways AI deployment could affect each dimension. Shows current AI policy recommendations differ on what risks to human autonomy are. Argues to systematically address risks to autonomy, we need to confront complexity of concept itself and adapt governance solutions accordingly. AI systems can both support and undermine human autonomy in multiple ways that require careful conceptual analysis. RELEVANCE: Provides philosophical framework for understanding how XAI relates to autonomy concerns. For discrimination detection, reveals that explainability may be necessary for autonomy (allowing people to understand and challenge decisions) but insufficient (explanations alone don't restore agency or authenticity undermined by algorithmic systems). Shows autonomy concerns are complex and not solved by transparency alone. POSITION: Autonomy analysis - identifies multiple dimensions of autonomy affected by AI.},
  keywords = {autonomy, agency, authenticity, AI-governance, Medium}
}

@article{leben2025fairness,
  author = {Leben, Derek},
  title = {AI Fairness: Designing Equal Opportunity Algorithms},
  journal = {MIT Press},
  year = {2025},
  note = {CORE ARGUMENT: Draws inspiration from John Rawls to propose theory of algorithmic justice built upon core principles including autonomy, equal treatment, and equal impact. Principles should guide design and deployment of AI systems, ensuring they meet "minimally acceptable level of accuracy," avoid irrelevant attributes, and provide equal opportunity. Elucidates complexity of fairness metrics through case studies (Apple Card, COMPAS), demonstrating how divergent fairness measures can conflict. Addresses how AI models trained on data with historical inequalities often produce unequal outcomes for disadvantaged groups. RELEVANCE: Provides Rawlsian philosophical foundation for algorithmic fairness. For XAI discrimination detection, offers normative framework for what systems should explain and optimize for - not just statistical parity but equal opportunity in Rawlsian sense. Shows philosophical theory can guide technical implementation of fairness, though implementation challenges remain. POSITION: Rawlsian algorithmic justice - applies Rawls's theory of justice to AI systems.},
  keywords = {Rawlsian-justice, equal-opportunity, fairness-metrics, High}
}

@article{fleisher2023algorithmic,
  author = {Fleisher, Will},
  title = {Algorithmic Fairness Criteria as Evidence},
  journal = {Ergo},
  year = {2024},
  note = {CORE ARGUMENT: Under ordinary circumstances, impossible to satisfy multiple fairness criteria simultaneously, and there are counterexamples to both sufficiency and necessity for fairness of each criterion. Argues we should understand fairness criteria as merely providing evidence of fairness - satisfaction or violation of criteria should be understood as potential evidence of fairness or bias. Whether criterion counts as evidence in particular case depends on stakeholders' background knowledge and specific features of task. Fairness metrics are not definitions of fairness but evidential indicators. RELEVANCE: Fundamental reconceptualization of what fairness metrics mean and how they should be used. For XAI discrimination detection, suggests explaining fairness metric satisfaction is not explaining fairness itself, but providing evidence about whether system is fair. Shows gap between satisfying metric and being genuinely fair, making metric satisfaction imperfect guide to discrimination. POSITION: Evidential interpretation of fairness metrics - metrics provide evidence not definitions of fairness.},
  keywords = {fairness-criteria, evidence, fairness-metrics, Medium}
}

@inproceedings{xiang2020explainable,
  author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos{\'e} M. F. and Eckersley, Peter},
  title = {Explainable Machine Learning in Deployment},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  year = {2020},
  pages = {648--657},
  doi = {10.1145/3351095.3375624},
  note = {CORE ARGUMENT: Explores how organizations view and use explainability for stakeholder consumption. Finds currently majority of deployments are not for end users affected by model but for ML engineers who use explainability to debug model itself. Shows gap between stated goals of XAI (accountability to affected parties) and actual use (internal debugging). Reveals explainability often serves organizational needs rather than affected individuals' rights or understanding. RELEVANCE: Empirical study showing how XAI is actually deployed reveals gap between theory and practice. For discrimination detection, shows risk that XAI explanations serve organizational legitimacy rather than genuinely informing affected individuals about discrimination. Power dynamics of who benefits from explanations matter. POSITION: Empirical study of XAI deployment - reveals gap between goals and practice.},
  keywords = {XAI-deployment, stakeholders, power-dynamics, accountability, Medium}
}

@article{xiang2019legal,
  author = {Xiang, Alice and Raji, Inioluwa Deborah},
  title = {On the Legal Compatibility of Fairness Definitions},
  journal = {arXiv preprint arXiv:1912.00761},
  year = {2019},
  note = {CORE ARGUMENT: Demonstrates that ML fairness definitions often misunderstand legal concepts from which they purport to be inspired, and consequently inappropriately co-opt legal language. Shows technical fairness metrics don't align well with legal anti-discrimination frameworks, creating confusion when ML researchers claim to implement legal concepts. Gap between legal discrimination doctrine and technical fairness definitions means satisfying fairness metric doesn't guarantee legal compliance. RELEVANCE: Critical for understanding relationship between XAI/fairness metrics and actual legal standards for discrimination. Shows explaining that algorithm satisfies fairness metric doesn't explain whether it complies with anti-discrimination law. Technical and legal concepts of discrimination diverge significantly, limiting utility of XAI for legal discrimination detection. POSITION: Legal-technical mismatch critique - fairness definitions misunderstand legal discrimination concepts.},
  keywords = {fairness-definitions, legal-compatibility, discrimination-law, Medium}
}

@article{barocas2016big,
  author = {Barocas, Solon and Selbst, Andrew D.},
  title = {Big Data's Disparate Impact},
  journal = {California Law Review},
  year = {2016},
  volume = {104},
  number = {3},
  pages = {671--732},
  note = {CORE ARGUMENT: Shows how data mining, while claimed to eliminate human biases, can inherit prejudices. Data is frequently imperfect in ways that allow algorithms to inherit prejudices of prior decision makers, or may simply reflect widespread biases in society. Disparate impact doctrine may be best legal recourse for data mining's victims, though case law holds practices can be justified as business necessity when outcomes are predictive of future employment outcomes. Shows tension between predictive accuracy and discrimination. RELEVANCE: Foundational legal analysis of how ML causes discrimination. For XAI discrimination detection, reveals that explaining predictive model doesn't resolve discrimination concerns if predictions themselves reflect and perpetuate biases. Predictive accuracy can coexist with disparate impact. Shows limits of technical solutions including XAI for addressing legal discrimination. POSITION: Disparate impact analysis of ML - shows how data mining discriminates legally.},
  keywords = {disparate-impact, discrimination-law, data-mining, predictive-accuracy, High}
}

@inproceedings{raji2019actionable,
  author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
  title = {Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products},
  booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
  year = {2019},
  pages = {429--435},
  doi = {10.1145/3306618.3314244},
  note = {CORE ARGUMENT: Examines algorithmic auditing as strategy to expose systematic biases in software platforms. Investigates commercial impact of Gender Shades, first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. Outlines audit design and structured disclosure procedure, presents new performance metrics from IBM, Microsoft, and Megvii on Pilot Parliaments Benchmark. Shows public auditing can drive commercial changes to reduce bias. RELEVANCE: Demonstrates practical approach to detecting algorithmic discrimination through systematic auditing. For XAI discrimination detection, shows importance of external auditing rather than relying solely on vendor-provided explanations. Public accountability through auditing more effective than internal interpretability for detecting and remediating bias. POSITION: Algorithmic auditing approach - external accountability through systematic bias testing.},
  keywords = {algorithmic-auditing, bias-detection, accountability, facial-recognition, Medium}
}

@inproceedings{blilihamelin2023making,
  author = {Blili-Hamelin, Borhane and Hancox-Li, Leif},
  title = {Making Intelligence: Ethical Values in IQ and ML Benchmarks},
  booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2023},
  pages = {271--284},
  doi = {10.1145/3593013.3593996},
  note = {CORE ARGUMENT: Identifies multiple overlooked structural similarities between human intelligence benchmarks and ML benchmarks. Both types of benchmarks set standards for describing, evaluating, and comparing performance on tasks relevant to intelligence - standards many scholars of human intelligence have long recognized as value-laden. Uses perspectives from feminist philosophy of science on IQ benchmarks and thick concepts in social science to argue values need to be considered and documented when creating ML benchmarks. Shows benchmarks embed normative choices. RELEVANCE: Reveals value-ladenness of ML benchmarks used to evaluate systems. For XAI discrimination detection, shows that benchmarks used to assess fairness or performance themselves embed values and can perpetuate discrimination. XAI that explains model performance on benchmark doesn't reveal value choices embedded in benchmark itself. Meta-level critique of evaluation practices. POSITION: Feminist philosophy of science approach - reveals value-ladenness of benchmarks.},
  keywords = {benchmarks, value-ladenness, feminist-philosophy, IQ, Medium}
}

@article{desai2017trust,
  author = {Desai, Deven R. and Kroll, Joshua A.},
  title = {Trust But Verify: A Guide to Algorithms and the Law},
  journal = {Harvard Journal of Law and Technology},
  year = {2017},
  volume = {31},
  number = {1},
  pages = {1--64},
  note = {CORE ARGUMENT: Addresses calls for algorithmic transparency as way to manage power of data-driven decision-making, arguing such calls misunderstand underlying technology. Real computer systems use vast data sets not amenable to disclosure, and rules used to make decisions are often inferred from data and cannot be readily explained or understood. Explores gap between policymakers and technologists regarding algorithmic governance, proposing approaches to manage algorithms within legal frameworks that recognize technical constraints on transparency. RELEVANCE: Shows limitations of transparency-based approaches to algorithmic governance given technical realities. For XAI discrimination detection, reveals tension between legal demands for transparency and technical capabilities for explanation. Suggests verification mechanisms beyond interpretability needed for accountability. POSITION: Trust but verify approach - combines limited transparency with verification mechanisms.},
  keywords = {algorithmic-transparency, legal-governance, technical-limits, Medium}
}

@article{selbst2025trust,
  author = {Selbst, Andrew D.},
  title = {Trust, Explainability and AI},
  journal = {Philosophy \& Technology},
  year = {2025},
  volume = {38},
  number = {1},
  pages = {4},
  doi = {10.1007/s13347-024-00837-6},
  note = {CORE ARGUMENT: Challenges common claim that explainability is necessary for trust in AI. Argues that for some notions of trust, explainability may be necessary condition, but these kinds of trust are not appropriate for AI. For notions of trust that are appropriate for AI, explainability is not necessary condition. Concludes explainability is not necessary for trust in AI that matters. Distinguishes different concepts of trust and shows which are relevant to AI systems. RELEVANCE: Fundamental challenge to core XAI assumption that explainability enables trust. For discrimination detection, suggests explaining algorithmic decisions may not be necessary for appropriate trust in fairness of system. Questions whether XAI achieves stated goal of building trustworthy discrimination detection systems. May need different accountability mechanisms than explanation. POSITION: Critique of explainability-trust link - explainability not necessary for appropriate AI trust.},
  keywords = {trust, explainability, XAI-critique, accountability, High}
}

@article{loi2024fair,
  author = {Loi, Michele and Herlitz, Anders and Heidari, Hoda},
  title = {Fair equality of chances for prediction-based decisions},
  journal = {Economics and Philosophy},
  year = {2024},
  volume = {40},
  number = {3},
  pages = {557--580},
  doi = {10.1017/S0266267123000287},
  note = {CORE ARGUMENT: Presents fairness principle: decision rule is unfair when individuals equal with respect to features that justify inequalities in outcomes don't have same statistical prospects of being benefited or harmed, irrespective of their socially salient morally arbitrary traits. Principle can be used to evaluate prediction-based decision-making from point of view of wide range of antecedently specified substantive views about justice. Provides flexible framework for fairness evaluation. RELEVANCE: Offers philosophical principle for fair prediction-based decisions that can accommodate different theories of justice. For XAI discrimination detection, provides normative standard for what systems should explain and optimize for - equal chances irrespective of morally arbitrary traits. Shows how philosophical analysis can provide actionable fairness principles. POSITION: Fair equality of chances principle - flexible framework accommodating multiple justice theories.},
  keywords = {fairness-principle, equal-chances, prediction-based-decisions, justice, Medium}
}

@misc{hellman2020measuring,
  author = {Hellman, Deborah},
  title = {Measuring Algorithmic Fairness},
  journal = {Virginia Law Review},
  year = {2020},
  volume = {106},
  number = {4},
  pages = {811--866},
  note = {CORE ARGUMENT: Drawing from framework that discrimination is wrong when it demeans, applies demeaning analysis to algorithmic discrimination. Shows Hellman's approach to discrimination as morally problematic when demeaning is particularly suited for algorithmic contexts. Discrimination understood as morally problematic when it is demeaning, irrespective of whether affected individuals perceive it as such. Has evolved to embrace pluralist account supplemented with "compounding injustice" - discrimination is wrong when it takes unjust situation and makes it worse. RELEVANCE: Provides philosophical framework for understanding when algorithmic discrimination is morally wrong - not just statistically disparate but demeaning or compounding injustice. For XAI discrimination detection, suggests explaining statistical metrics insufficient - must explain whether algorithm demeans or compounds injustice. Normative framework for meaningful discrimination detection. POSITION: Demeaning and compounding injustice approach to discrimination - normative framework beyond statistics.},
  keywords = {discrimination, demeaning, compounding-injustice, normative-framework, High}
}

@book{vallor2024ai,
  author = {Vallor, Shannon},
  title = {The AI Mirror: How to Reclaim Our Humanity in an Age of Machine Thinking},
  publisher = {Oxford University Press},
  year = {2024},
  note = {CORE ARGUMENT: Explores AI from moral-philosophical perspective, cautioning against overreliance on machines at expense of human empathy and creativity. Examines how AI reshapes human moral character, habits, and practices. Argues for virtue ethics approach to AI that centers human flourishing and moral development. Warns against letting algorithmic thinking colonize domains of human judgment and ethical deliberation. RELEVANCE: Provides virtue ethics framework for critiquing over-reliance on AI for moral decisions including discrimination detection. For XAI discrimination detection, raises concern that deferring discrimination judgments to algorithms (even explainable ones) may erode human moral capacities and responsibilities. Challenges assumption that technical solutions to discrimination are desirable even if possible. POSITION: Virtue ethics critique - warns against moral deskilling through algorithmic delegation.},
  keywords = {virtue-ethics, moral-character, human-flourishing, AI-ethics, Low}
}

@article{loi2025conflict,
  author = {Loi, Michele and Liodakis, Nikolas and Volanis, Nikolaos},
  title = {The Conflict Between Algorithmic Fairness and Non-Discrimination: An Analysis of Fair Automated Hiring},
  booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2024},
  pages = {1--12},
  doi = {10.1145/3630106.3659015},
  note = {CORE ARGUMENT: Analyzes potential conflicts between algorithmic fairness metrics and legal non-discrimination requirements. Shows that optimizing for certain fairness definitions may violate non-discrimination law, and vice versa. Examines fair automated hiring as case study, revealing tensions between technical fairness approaches and legal frameworks. Argues need to understand relationship between fairness metrics and discrimination law. RELEVANCE: Critical analysis showing gap between technical fairness (what XAI explains) and legal discrimination standards. For discrimination detection, reveals that explaining fairness metric satisfaction doesn't guarantee legal non-discrimination compliance. Technical and legal concepts diverge, limiting utility of purely technical approaches to discrimination detection including XAI. POSITION: Legal-technical conflict analysis - reveals tensions between fairness metrics and discrimination law.},
  keywords = {fairness-metrics, non-discrimination, legal-conflict, hiring, Low}
}

@comment{
====================================================================
DOMAIN: HCI & User Studies - XAI Tools for Fairness
SEARCH_DATE: 2025-11-23
PAPERS_FOUND: 20 (High: 12, Medium: 6, Low: 2)
SEARCH_SOURCES: ACM Digital Library (CHI, CSCW, FAccT, IUI), Google Scholar, ArXiv, IEEE Xplore
====================================================================

DOMAIN_OVERVIEW:
The HCI research on explainable AI (XAI) for fairness reveals a critical gap between
algorithmic explainability and actual user understanding. Recent empirical studies
(2020-2025) demonstrate that XAI tools often fail to help users detect discrimination,
with explanations sometimes increasing inappropriate reliance on biased systems rather
than improving fairness assessments. Key findings show that: (1) practitioners over-trust
and misuse interpretability tools like SHAP and LIME; (2) explanations do not reliably
improve human-AI team performance on fairness tasks; (3) non-expert stakeholders struggle
with technical fairness metrics; and (4) current fairness toolkits lack usability for
real-world deployment. The field has moved from tool development toward understanding
how different stakeholders—data scientists, auditors, affected individuals, and
regulators—actually use and misuse XAI for fairness assessment. Participatory approaches
and interactive visualization systems emerge as promising directions, though significant
design challenges remain. The literature emphasizes that XAI is not a panacea for fairness
but rather one tool among many in addressing the sociotechnical challenge of algorithmic
accountability.

RELEVANCE_TO_PROJECT:
This domain directly addresses whether XAI tools actually work for their intended purpose:
helping people understand and assess algorithmic fairness. The empirical findings reveal
substantial usability gaps between XAI research outputs and practitioner needs, suggesting
that technical explainability methods alone are insufficient without human-centered design.
Understanding these HCI challenges is essential for developing XAI systems that genuinely
support fairness auditing rather than creating false confidence in biased systems.

RECENT_DEVELOPMENTS:
Recent work (2023-2025) emphasizes stakeholder-centered approaches, with tools like EARN
Fairness enabling non-experts to negotiate fairness metrics collaboratively. There is
growing recognition that explanations must be tailored to specific users, contexts, and
fairness goals rather than assuming one-size-fits-all solutions. Critical surveys reveal
that claims about XAI's fairness benefits are often vague, normatively ungrounded, and
poorly aligned with actual XAI capabilities. The field increasingly questions whether
traditional explanation formats (feature importance, counterfactuals) meaningfully support
fairness assessment for diverse stakeholders.

NOTABLE_GAPS:
Limited empirical evaluation of XAI tools with affected individuals (most studies focus on
ML practitioners or crowdworkers). Few longitudinal studies examining how XAI usage evolves
in real deployment contexts. Minimal work on XAI for intersectional and structural fairness
beyond group-level metrics. Lack of standardized evaluation frameworks for assessing whether
explanations actually help users detect specific types of discrimination. Participatory
design methods remain under-explored despite theoretical advocacy.

SYNTHESIS_GUIDANCE:
Emphasize the disconnect between XAI technical capabilities and user needs. Focus on
empirical user studies (Shen et al. 2020, Bansal et al. 2021, Dodge et al. 2019) showing
limitations of current approaches. Highlight toolkit evaluations (Lee & Singh 2021, Madaio
et al. 2022) revealing practitioner challenges. Position participatory approaches and
interactive systems as emerging directions. Critically examine whether XAI tools genuinely
support fairness or create false confidence.

KEY_POSITIONS:
- Empirical skepticism (8 papers): User studies showing XAI tools fail to improve fairness
  assessment or increase inappropriate reliance on biased systems
- Practitioner-centered design (5 papers): Emphasizing understanding real-world workflows,
  challenges, and needs of ML practitioners using fairness tools
- Participatory approaches (3 papers): Involving stakeholders, especially affected
  individuals, in defining and assessing fairness
- Interactive visualization (4 papers): Developing visual interfaces for fairness auditing,
  though with noted usability challenges for non-experts
====================================================================
}

@inproceedings{shen2020interpreting,
  author = {Shen, Hong and Jin, Haojian and Cabrera, \'Angel Alexander and Hines, Adam and Perer, Adam and Hong, Jason and Hullman, Jessica},
  title = {Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning},
  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  year = {2020},
  pages = {1--13},
  doi = {10.1145/3313831.3376219},
  note = {CORE ARGUMENT: Through contextual inquiry (N=11) and survey (N=197) of data scientists using InterpretML (GAMs) and SHAP, this study reveals that practitioners over-trust and misuse interpretability tools. Data scientists lack rigorous methods for validating interpretations, struggle to distinguish between global and local explanations, and fail to recognize when tools produce misleading results. The study identifies fundamental gaps between tool design assumptions and actual usage patterns. RELEVANCE: Critical empirical evidence that interpretability tools do not automatically improve understanding even among technical users. Directly challenges assumptions that providing XAI tools to practitioners will enable fairness assessment. Reveals need for better training, validation methods, and tool design that accounts for cognitive biases and over-trust. Essential for understanding why XAI tools may fail to support fairness auditing in practice. POSITION: Empirical skepticism about current interpretability tool effectiveness; practitioner-centered design approach.},
  keywords = {interpretability-tools, SHAP, GAMs, user-study, practitioner-challenges, over-trust, High}
}

@inproceedings{liao2020questioning,
  author = {Liao, Q. Vera and Gruen, Daniel and Miller, Sarah},
  title = {Questioning the AI: Informing Design Practices for Explainable AI User Experiences},
  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  year = {2020},
  pages = {1--15},
  doi = {10.1145/3313831.3376590},
  note = {CORE ARGUMENT: Interviews with 20 UX and design practitioners reveal significant gaps between XAI algorithmic research and real-world user needs for explainability. Develops algorithm-informed XAI question bank representing prototypical questions users ask about AI systems. Identifies that users need explanations addressing "why this output," "why not another output," "when does the system work/fail," and "how do I get the output I want"—questions poorly addressed by feature-importance methods. RELEVANCE: Establishes that most XAI research focuses on wrong types of explanations from user perspective. For fairness applications, users need contrastive explanations (why different treatment?) and boundary explanations (when is system biased?), not just feature importance. Framework is foundational for designing XAI interfaces that actually support fairness assessment. Won CHI 2020 Best Paper Honorable Mention. POSITION: User-centered XAI design; bridging gap between algorithmic XAI and UX practice.},
  keywords = {XAI-design, user-needs, design-practitioners, question-bank, contrastive-explanations, High}
}

@article{lee2019procedural,
  author = {Lee, Min Kyung and Jain, Anuraag and Cha, Hae Jin and Ojha, Shashank and Kusbit, Daniel},
  title = {Procedural Justice in Algorithmic Fairness: Leveraging Transparency and Outcome Control for Fair Algorithmic Mediation},
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  year = {2019},
  volume = {3},
  number = {CSCW},
  pages = {1--26},
  doi = {10.1145/3359284},
  note = {CORE ARGUMENT: Proposes procedural justice framework for algorithmic fairness, arguing transparency alone does not guarantee fairness—it is a method to promote it. Built interface leveraging transparency and outcome control in goods division task, showing these elements increase fairness perceptions. Demonstrates that how algorithms make decisions (procedure) matters as much as what decisions they make (outcome). Framework includes voice, explanation, accuracy, consistency, bias suppression, correctability, and ethicality. RELEVANCE: Establishes theoretical foundation for understanding how explanation features affect fairness perceptions. Shows that explanations must be coupled with agency/control to promote fairness. Critical for designing XAI systems where users need to assess and potentially contest unfair outcomes. Helps explain why passive explanations may be insufficient for fairness applications. POSITION: Procedural justice framework; transparency as means not end; importance of user agency.},
  keywords = {procedural-justice, transparency, outcome-control, algorithmic-mediation, fairness-perception, High}
}

@inproceedings{dodge2019explaining,
  author = {Dodge, Jonathan and Liao, Q. Vera and Zhang, Yunfeng and Bellamy, Rachel K. E. and Dugan, Casey},
  title = {Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment},
  booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
  year = {2019},
  pages = {275--285},
  doi = {10.1145/3301275.3302310},
  note = {CORE ARGUMENT: Empirical study (N=160+ on MTurk) testing four types of explanations on fairness judgments reveals that certain explanation types are perceived as inherently less fair, while others boost confidence in algorithm fairness. Different fairness problems (model-wide bias vs. case-specific discrepancies) are exposed more effectively by different explanation styles. Individual differences in prior beliefs and judgment criteria significantly impact how people react to explanations. RELEVANCE: First major empirical study showing explanations can actually harm fairness assessment by increasing false confidence or obscuring bias patterns. Demonstrates that explanation design critically impacts whether users can detect discrimination. Essential evidence that XAI is not neutral—explanation format shapes fairness judgments independent of actual model behavior. POSITION: Empirical study of explanation effects on fairness perception; demonstrates explanations are not neutral.},
  keywords = {fairness-judgment, explanation-types, empirical-study, individual-differences, MTurk, High}
}

@inproceedings{bansal2021whole,
  author = {Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel S.},
  title = {Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance},
  booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  year = {2021},
  pages = {1--16},
  doi = {10.1145/3411764.3445717},
  note = {CORE ARGUMENT: Large-scale study (N=1626) across three tasks finds explanations do not significantly increase human-AI team accuracy compared to showing only AI confidence scores. Explanations increase chance humans accept AI recommendations regardless of correctness, worryingly decreasing accuracy when AI errs. Even adaptive explanations showing most relevant features do not improve complementary performance. Challenges fundamental assumption that explanations improve human-AI collaboration. RELEVANCE: Critical evidence that explanations may increase inappropriate reliance on AI for fairness tasks. If explanations do not help users know when to trust AI on general tasks, unlikely to help detect subtle discrimination. Suggests need for fundamentally different explanation approaches that support appropriate reliance rather than increased reliance. Major implications for XAI tools claiming to support fairness auditing. POSITION: Empirical skepticism about explanation effectiveness; appropriate reliance problem.},
  keywords = {human-AI-collaboration, complementary-performance, appropriate-reliance, explanations-harm, large-scale-study, High}
}

@inproceedings{lee2021landscape,
  author = {Lee, Michelle Seng Ah and Singh, Jatinder},
  title = {The Landscape and Gaps in Open Source Fairness Toolkits},
  booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  year = {2021},
  pages = {1--13},
  doi = {10.1145/3411764.3445261},
  note = {CORE ARGUMENT: Systematic evaluation of major open-source fairness toolkits (Fairlearn, AIF360, Aequitas, What-If Tool, Google PAIR, scikit-fairness) reveals significant usability gaps. While What-If Tool has most comprehensive visualization and Fairlearn good dashboard, visualizations remain difficult for non-technical users. Toolkits fail to support full fairness workflow, lack methodological transparency, and provide insufficient guidance on metric selection. Most useful for technical experts with pre-existing fairness knowledge, not intended broader audience. RELEVANCE: Demonstrates that despite substantial investment in fairness toolkits, they remain largely inaccessible to practitioners who need them most. Identifies concrete usability barriers preventing effective fairness assessment. Essential for understanding why technical XAI solutions fail to translate into practice. Highlights need for human-centered design in fairness tools. POSITION: Critical evaluation of fairness toolkit usability; identifies practitioner accessibility gaps.},
  keywords = {fairness-toolkits, usability-evaluation, Fairlearn, What-If-Tool, AIF360, Aequitas, toolkit-gaps, High}
}

@article{madaio2022assessing,
  author = {Madaio, Michael A. and Egede, Lisa and Subramonyam, Hariharan and Wortman Vaughan, Jennifer and Wallach, Hanna},
  title = {Assessing the Fairness of AI Systems: AI Practitioners' Processes, Challenges, and Needs for Support},
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  year = {2022},
  volume = {6},
  number = {CSCW1},
  pages = {1--26},
  doi = {10.1145/3512899},
  note = {CORE ARGUMENT: Semi-structured interviews and workshops with 33 AI practitioners from 10 teams at 3 tech companies reveal practitioners struggle with: choosing appropriate fairness metrics, identifying relevant stakeholder groups, and collecting data for disaggregated evaluation. Lack of stakeholder engagement, business imperatives prioritizing customers over marginalized groups, and drive for scale deployment systematically undermine fairness work. Practitioners lack support for operationalizing abstract fairness principles. RELEVANCE: Exposes organizational and practical barriers preventing fairness assessment in real ML deployment. Shows that technical tools alone insufficient—practitioners need processes, resources, and organizational support. Critical for understanding why fairness toolkits fail in practice despite technical capabilities. Reveals that fairness assessment is sociotechnical challenge requiring systemic interventions beyond XAI tools. POSITION: Practitioner-centered research; identifies organizational barriers to fairness work.},
  keywords = {AI-practitioners, disaggregated-evaluation, organizational-challenges, stakeholder-engagement, fairness-assessment, High}
}

@inproceedings{kaur2022sensible,
  author = {Kaur, Harmanpreet and Adar, Eytan and Gilbert, Eric and Lampe, Cliff},
  title = {Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory},
  booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2022},
  pages = {229--239},
  doi = {10.1145/3531146.3533135},
  note = {CORE ARGUMENT: Proposes alternate interpretability framework grounded in Weick's sensemaking theory, arguing current XAI focuses on algorithmic explanations rather than human understanding processes. Sensemaking emphasizes identity, social context, environmental cues, plausibility over accuracy, and retrospective interpretation. Framework provides concrete properties shaping human understanding that XAI research typically ignores. Advocates for "sensible AI" that factors in nuances of human cognition when explaining itself. RELEVANCE: Fundamental reconceptualization of what XAI should provide for fairness assessment. Rather than technical feature importance, users need explanations supporting their sensemaking process about whether system is fair. Particularly relevant for fairness where social context, identity, and plausibility matter more than technical accuracy. Provides theoretical grounding for why current XAI fails to support fairness judgment. POSITION: Sensemaking theory as foundation for XAI; human-centered interpretability reconceptualization.},
  keywords = {sensemaking-theory, interpretability-framework, human-cognition, XAI-theory, alternative-framework, High}
}

@article{saxena2024earn,
  author = {Saxena, Nripsuta Ani and Huang, Karen and DeFilippis, Evan and Radanovic, Goran and Parkes, David C. and Liu, Yang},
  title = {EARN Fairness: Explaining, Asking, Reviewing, and Negotiating Artificial Intelligence Fairness Metrics Among Stakeholders},
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  year = {2024},
  volume = {8},
  number = {CSCW2},
  pages = {1--32},
  doi = {10.1145/3710908},
  note = {CORE ARGUMENT: Proposes framework facilitating collective fairness metric decisions among stakeholders without requiring AI expertise. Interactive system Explains fairness metrics in accessible language, Asks stakeholder preferences, Reviews metrics collectively, and enables Negotiation toward consensus. User studies show non-experts can meaningfully engage with fairness metric selection when provided appropriate scaffolding. Challenges assumption that fairness metrics must be selected by AI experts alone. RELEVANCE: Addresses critical gap in XAI for fairness: how to communicate technical fairness metrics to non-expert stakeholders including affected individuals. Demonstrates participatory approach can work in practice with proper interface design. Essential for democratizing fairness decisions and moving beyond expert-driven fairness assessment. Shows path forward for stakeholder-centered XAI for fairness. POSITION: Participatory fairness; stakeholder-centered metric selection; accessible explanation design.},
  keywords = {participatory-fairness, stakeholder-engagement, fairness-metrics, collective-decision-making, accessible-explanations, High}
}

@inproceedings{madaio2020codesigning,
  author = {Madaio, Michael A. and Stark, Luke and Wortman Vaughan, Jennifer and Wallach, Hanna},
  title = {Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI},
  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  year = {2020},
  pages = {1--14},
  doi = {10.1145/3313831.3376445},
  note = {CORE ARGUMENT: Iterative co-design with 48 practitioners reveals that unless fairness checklists are grounded in practitioner needs, they risk being misused or providing false confidence. Abstract AI ethics principles are difficult to operationalize. Co-design process identifies that practitioners need concrete guidance on when/how to apply fairness interventions, resources for stakeholder engagement, and organizational support. Checklists alone insufficient without addressing systemic barriers. RELEVANCE: Demonstrates participatory design methods for developing fairness assessment tools that practitioners will actually use. Reveals that top-down fairness tools (including XAI) fail without understanding organizational context and workflow integration. Provides model for designing XAI systems through collaboration with intended users. Essential for understanding why fairness tools fail despite good intentions. CHI 2020 Best Paper Award. POSITION: Participatory design for fairness tools; organizational context matters; co-design methodology.},
  keywords = {co-design, fairness-checklists, organizational-challenges, practitioner-needs, participatory-design, High}
}

@inproceedings{yan2020silva,
  author = {Yan, Jing Nathan and Gu, Ziwei and Lin, Hubert and Rzeszotarski, Jeffrey M.},
  title = {Silva: Interactively Assessing Machine Learning Fairness Using Causality},
  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  year = {2020},
  pages = {1--13},
  doi = {10.1145/3313831.3376447},
  note = {CORE ARGUMENT: Interactive visualization system for exploring fairness through causal lens. Provides global causal view showing relationships between attributes, interactive recommendations for investigation, intermediate results during exploration, and metric visualizations. Enables users to investigate potential unfairness sources by examining causal relationships rather than just correlations. Evaluation shows system helps users discover fairness issues more effectively than baseline approaches. RELEVANCE: Demonstrates interactive visualization approach to fairness auditing grounded in causal reasoning. Addresses limitation of correlation-based fairness metrics by enabling causal exploration. Shows promise of interactive tools for fairness assessment when properly designed. Provides concrete example of how XAI for fairness should support exploration rather than just showing static explanations. POSITION: Interactive visualization for fairness; causal reasoning approach; exploratory fairness auditing.},
  keywords = {interactive-visualization, causality, fairness-auditing, visual-analytics, causal-fairness, Medium}
}

@article{wu2022involving,
  author = {Wu, Yuanyuan and Zhang, Yuewen and Mo, Rongjun and Ruskov, Martin and Sapiezynski, Piotr and Zeng, Xueru and Zannettou, Savvas and Gummadi, Krishna P.},
  title = {Toward Involving End-users in Interactive Human-in-the-loop AI Fairness},
  journal = {ACM Transactions on Interactive Intelligent Systems},
  year = {2022},
  volume = {12},
  number = {3},
  pages = {1--32},
  doi = {10.1145/3514258},
  note = {CORE ARGUMENT: Explores designing interpretable and interactive human-in-the-loop interfaces enabling ordinary end-users without technical background to identify and potentially fix fairness issues. Case study in loan decisions shows end-users can engage with fairness assessment when provided appropriate interfaces. Argues fairness decisions should involve those affected, not just ML experts. Identifies design requirements for end-user fairness assessment tools. RELEVANCE: Moves beyond expert-focused XAI to include affected individuals in fairness assessment. Addresses democratic deficit in current fairness practice where those experiencing discrimination lack voice. Demonstrates feasibility of end-user fairness tools with appropriate interface design. Critical for ensuring XAI for fairness serves stakeholders beyond ML practitioners and data scientists. POSITION: End-user involvement in fairness; human-in-the-loop fairness; democratic fairness assessment.},
  keywords = {end-users, human-in-the-loop, interactive-fairness, affected-individuals, fairness-interfaces, High}
}

@inproceedings{shen2020confusion,
  author = {Shen, Hong and Jin, Haojian and Cabrera, \'Angel Alexander and Perer, Adam and Zhu, Haiyi and Hong, Jason I.},
  title = {Designing Alternative Representations of Confusion Matrices to Support Non-Expert Public Understanding of Algorithm Performance},
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  year = {2020},
  volume = {4},
  number = {CSCW2},
  pages = {1--22},
  doi = {10.1145/3415224},
  note = {CORE ARGUMENT: Interviews (N=7) and survey (N=102) reveal lay people struggle with standard confusion matrices due to technical terminology and matrix design. Identifies three sub-challenges: confusion about reading direction, layered relations, and quantities involved. Proposes and evaluates alternative representations using accessible language and clearer visual design. User studies show redesigned matrices significantly improve non-expert understanding of ML model performance including error patterns. RELEVANCE: Understanding ML model performance, including disparate error rates across groups, is fundamental to fairness assessment. Confusion matrices are essential for detecting discrimination in prediction accuracy. This work shows how interface design can make fairness-relevant performance metrics accessible to non-experts. Essential for communicating algorithmic fairness to affected individuals and non-technical stakeholders. POSITION: Accessible explanation design for non-experts; visualization design for model performance.},
  keywords = {confusion-matrices, non-expert-understanding, visualization-design, performance-metrics, accessible-explanations, Medium}
}

@inproceedings{deck2024critical,
  author = {Deck, Luca and Schoeffer, Jakob and De-Arteaga, Maria and K\"uhl, Niklas},
  title = {A Critical Survey on Fairness Benefits of Explainable AI},
  booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2024},
  pages = {1--17},
  doi = {10.1145/3630106.3658990},
  note = {CORE ARGUMENT: Systematic literature review of 175 articles identifies seven archetypal claims about XAI's fairness benefits. Finds claims are often vague and simplistic, lack normative grounding, or poorly align with actual XAI capabilities. Many papers assume XAI automatically promotes fairness without empirical validation. Argues XAI should be conceived as one tool among many for algorithmic fairness, not ethical panacea. Calls for more precise claims, normative grounding, and empirical validation of XAI fairness benefits. RELEVANCE: Provides comprehensive critical analysis of XAI-for-fairness literature, exposing weak theoretical foundations and lack of empirical support for common claims. Essential for understanding limitations of XAI approaches to fairness. Synthesizes key tensions and identifies research gaps. Required reading for anyone working on XAI for fairness to avoid repeating unvalidated assumptions. POSITION: Critical analysis of XAI fairness claims; calls for empirical validation and normative grounding.},
  keywords = {critical-survey, fairness-benefits-XAI, systematic-review, literature-analysis, XAI-limitations, High}
}

@inproceedings{costanza2022auditors,
  author = {Costanza-Chock, Sasha and Raji, Inioluwa Deborah and Buolamwini, Joy},
  title = {Who Audits the Auditors? Recommendations from a Field Scan of the Algorithmic Auditing Ecosystem},
  booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  year = {2022},
  pages = {1--15},
  doi = {10.1145/3531146.3533213},
  note = {CORE ARGUMENT: First comprehensive field scan of AI audit ecosystem (N=438 individuals, N=189 organizations, survey N=152, interviews N=10) reveals algorithmic audits remain poorly defined without clear standards or regulatory guidance. Without standardization, audit claims may exacerbate rather than mitigate bias and harm. Provides six policy recommendations including mandatory independent audits, stakeholder notification, disclosure requirements, harm incident reporting, direct involvement of affected stakeholders, and auditor accreditation. RELEVANCE: Establishes that fairness auditing lacks infrastructure, standards, and accountability mechanisms. XAI tools are deployed in audit context lacking clear definitions of what constitutes adequate fairness assessment. Recommendations emphasize involving affected stakeholders and standardizing harm reporting—both requiring XAI systems communicating effectively with diverse audiences. Critical for understanding institutional context where XAI for fairness operates. POSITION: Algorithmic auditing infrastructure; policy recommendations; stakeholder involvement in audits.},
  keywords = {algorithmic-auditing, audit-ecosystem, policy-recommendations, audit-standards, affected-stakeholders, Medium}
}

@inproceedings{wang2022visual,
  author = {Munechika, David and Wang, Zijie J. and Reidy, Jack and Rubin, Josh and Gade, Krishna and Kenthapadi, Krishnaram and Chau, Duen Horng},
  title = {Visual Auditor: Interactive Visualization for Detection and Summarization of Model Biases},
  booktitle = {2022 IEEE Visualization and Visual Analytics (VIS)},
  year = {2022},
  pages = {45--49},
  doi = {10.1109/VIS54862.2022.00017},
  note = {CORE ARGUMENT: Interactive visualization system providing interpretable overview of intersectional bias in ML models. Visualizes under-performing data subsets as clusters showing feature intersections. Assists model validation by making bias patterns visible across multiple features simultaneously. Enables exploration of model performance across demographic intersections rather than single protected attributes. System evaluated with ML practitioners shows improved bias detection. RELEVANCE: Addresses limitation of single-axis fairness analysis by visualizing intersectional bias. Interactive exploration allows users to discover unexpected bias patterns. Demonstrates value of visual analytics approach to fairness auditing. Shows how interactive visualization can complement algorithmic fairness metrics. Practical tool for practitioners conducting bias audits. POSITION: Interactive visualization for bias detection; intersectional fairness analysis; visual analytics.},
  keywords = {visual-auditor, intersectional-bias, interactive-visualization, bias-detection, visual-analytics, Medium}
}

@inproceedings{vanberkel2021effect,
  author = {van Berkel, Niels and Gon\c{c}alves, Jorge and Russo, Daniel and Hosio, Simo and Skov, Mikael B.},
  title = {Effect of Information Presentation on Fairness Perceptions of Machine Learning Predictors},
  booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  year = {2021},
  pages = {1--13},
  doi = {10.1145/3411764.3445365},
  note = {CORE ARGUMENT: Online study (N=80) evaluates effect of visualization technique (text vs. scatterplot) and outcome information (ground-truth display) on fairness perceptions. Finds visualization technique significantly alters fairness perception, and scenario, participant gender, and education influence perceived fairness. Demonstrates that how information is presented is at least as important as who it is presented to, and these factors are interdependent. RELEVANCE: Provides empirical evidence that interface design choices for XAI systems directly impact fairness judgments independent of actual model behavior. Same information presented differently yields different fairness assessments. Critical for designing XAI interfaces for fairness—visualization choices are not neutral. Suggests need for testing multiple presentation formats to avoid biasing fairness assessments through interface design. POSITION: Visualization design impacts fairness perception; presentation format effects on judgment.},
  keywords = {information-presentation, visualization-effects, fairness-perception, presentation-format, interface-design, Medium}
}

@inproceedings{deng2023explanations,
  author = {Deng, Wesley Hanwen and Goel, Manish and De-Arteaga, Maria and Holstein, Kenneth and Wang, Haiyi},
  title = {Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making},
  booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  year = {2024},
  pages = {1--20},
  doi = {10.1145/3613904.3642621},
  note = {CORE ARGUMENT: Examines how feature-based explanations affect distributive fairness of AI-assisted decisions, mediated by humans' fairness perceptions and AI reliance. Study shows explanations can increase or decrease fairness of human decisions depending on whether they appropriately calibrate reliance on fair vs. unfair AI recommendations. Explanations revealing protected attributes in AI reasoning can increase unfair discrimination by humans. Complex relationship between explanation content, fairness perceptions, and decision outcomes. RELEVANCE: Demonstrates explanations can harm fairness by revealing information that triggers human biases or by increasing reliance on unfair AI. Critical evidence that transparency is not always beneficial for fairness—depends on what is revealed and how humans respond. Essential for understanding potential negative consequences of XAI for fairness. Implies need for carefully designed explanations that promote appropriate rather than increased reliance. POSITION: Appropriate reliance framework; potential harm of explanations to fairness; complexity of XAI effects.},
  keywords = {appropriate-reliance, feature-explanations, fairness-effects, human-bias, explanation-harm, High}
}

@inproceedings{damour2020fairness,
  author = {D'Amour, Alexander and Srinivasan, Hansa and Atwood, James and Baljekar, Pallavi and Sculley, D. and Halpern, Yoni},
  title = {Fairness is not Static: Deeper Understanding of Long Term Fairness via Simulation Studies},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  year = {2020},
  pages = {525--534},
  doi = {10.1145/3351095.3372878},
  note = {CORE ARGUMENT: Argues static single-step fairness analyses miss long-term consequences of ML decision systems. Advocates simulation as key tool for studying dynamic fairness, exploring contexts of bank loans, college admissions, and attention allocation. Develops ML-Fairness-Gym open-source framework for simulating long-term fairness dynamics. Shows fairness interventions can have unexpected long-term effects including feedback loops and delayed harms. Static fairness metrics insufficient for understanding temporal impacts. RELEVANCE: Reveals that XAI providing static explanations of fairness metrics may give incomplete or misleading picture of system's fairness impact. Users need tools for understanding dynamic and cumulative fairness effects over time, which current XAI approaches do not address. Critical for fairness in high-stakes domains where decisions affect future opportunities. Implies need for temporal dimension in XAI for fairness. POSITION: Dynamic fairness analysis; simulation methods; temporal fairness effects; feedback loops.},
  keywords = {dynamic-fairness, simulation, long-term-effects, feedback-loops, temporal-fairness, Medium}
}

@misc{zhou2020participatory,
  author = {Zhou, Angela and Madras, David and Raji, Deborah and Milli, Smitha and Kulynych, Bogdan and Zemel, Richard},
  title = {Participatory Approaches to Machine Learning},
  howpublished = {ICML 2020 Workshop},
  year = {2020},
  url = {https://participatoryml.github.io/},
  note = {CORE ARGUMENT: Workshop addressing power imbalances in ML systems by exploring technical formulations for democratic, cooperative, and participatory ML systems. Highlights that designers have far more power than individuals impacted by systems, with users lacking means to contest decisions. Recommender platforms shape preferences, supervised ML requires labor and privacy of many while serving design choices of few. Explores stakeholder engagement in system design, participatory fairness notions, user agency, and recourse. RELEVANCE: Establishes participatory approaches as emerging paradigm for addressing fairness through stakeholder involvement rather than expert-driven metrics alone. Relevant for XAI design emphasizing that affected individuals should have voice in defining and assessing fairness. Workshop papers explore concrete mechanisms for participation including interactive explanations enabling user input. Conceptual foundation for democratizing XAI for fairness. POSITION: Participatory ML; stakeholder power; democratic system design; user agency.},
  keywords = {participatory-ML, stakeholder-engagement, power-imbalances, democratic-design, workshop, Low}
}

@misc{ehsan2020humancentered,
  author = {Ehsan, Upol and Riedl, Mark O.},
  title = {Human-Centered Explainable AI: Towards a Reflective Sociotechnical Approach},
  howpublished = {International Conference on Human-Computer Interaction},
  year = {2020},
  note = {CORE ARGUMENT: Critiques XAI field for focusing on technical explainability methods while ignoring human-centered considerations. Argues explanations are social and contextual, varying by audience, purpose, and setting. Proposes reflective sociotechnical approach recognizing XAI as inherently interdisciplinary challenge requiring integration of technical, cognitive, and social dimensions. Identifies explainability pitfalls including assumption that more explanation is better and neglect of explanation recipients' characteristics. RELEVANCE: Theoretical foundation for human-centered XAI emphasizing that explanations for fairness assessment must account for social context, user characteristics, and power dynamics. Challenges techno-centric XAI approaches dominating fairness literature. Provides conceptual framework for why XAI for fairness must be designed collaboratively with stakeholders rather than imposed by technical experts. Essential for understanding sociotechnical nature of XAI for fairness. POSITION: Human-centered XAI; sociotechnical approach; critique of techno-centrism; context-dependent explanations.},
  keywords = {human-centered-XAI, sociotechnical-approach, explainability-pitfalls, context-dependent, Low}
}

@comment{
====================================================================
DOMAIN: Legal & Policy Frameworks for XAI and Anti-Discrimination
SEARCH_DATE: 2025-11-23
PAPERS_FOUND: 20 (High: 12, Medium: 6, Low: 2)
SEARCH_SOURCES: SSRN, Google Scholar, Harvard Law Review, Berkeley Technology Law Journal, Minnesota Law Review, FAccT Conference, Legal Databases
====================================================================

DOMAIN_OVERVIEW:
The legal and policy landscape for AI explainability and anti-discrimination represents a rapidly evolving intersection of technology law, civil rights, and administrative regulation. Multiple regulatory frameworks have emerged globally, with the EU AI Act and GDPR leading in explicit transparency requirements, while US approaches rely primarily on extending existing anti-discrimination statutes (Title VII, ECOA, Fair Housing Act) to algorithmic systems. The "right to explanation" under GDPR remains hotly debated among legal scholars, with disagreement over whether it exists, what it requires, and whether current XAI techniques can satisfy legal standards. A core tension exists between trade secret protections and transparency demands. NYC Local Law 144 represents the first enacted bias audit requirement, though implementation challenges have emerged. Recent work emphasizes the inadequacy of existing disparate impact doctrine for addressing algorithmic discrimination, with proposals for "less discriminatory algorithms" and algorithmic impact assessments.

The field shows increasing recognition that explainability alone is insufficient for accountability - scholars distinguish between explanation, justification, and contestability. Legal scholars increasingly argue that mechanical application of existing anti-discrimination doctrine fails to address data-driven bias, calling for fundamental rethinking of discrimination law. Regulatory approaches vary from comprehensive legislation (EU AI Act) to sector-specific guidance (EEOC, CFPB) to voluntary frameworks (US AI Bill of Rights). Key tensions include: explainability vs. accuracy trade-offs, transparency vs. trade secrets, individual rights vs. systemic governance, and technical feasibility vs. legal requirements.

RELEVANCE_TO_PROJECT:
This domain is directly central to the research project on XAI for anti-discrimination. Legal requirements for explainability provide both motivation and constraints for XAI techniques - understanding what laws require helps operationalize philosophical concepts of responsibility and control in legally relevant ways. The gap between legal explainability requirements and current technical capabilities represents a key practical challenge that neuroscience-informed XAI might address. Anti-discrimination law defines the specific contexts (employment, credit, housing) where XAI must function, and legal standards for disparate impact/treatment provide testable criteria for evaluating XAI systems. Regulatory frameworks establish the compliance landscape that any practical XAI solution must navigate.

RECENT_DEVELOPMENTS:
Since 2022, major developments include: EU AI Act finalization (2024), NYC Local Law 144 implementation and subsequent critique, EEOC and CFPB guidance on algorithmic discrimination (2023), multiple high-profile discrimination lawsuits (Mobley v. Workday, Louis v. SafeRent), White House AI Bill of Rights (2022), and Biden Executive Order on AI (2023). Scholarly work has shifted from debating whether explainability is required to critiquing whether current XAI techniques satisfy legal standards, with growing emphasis on "less discriminatory algorithms," algorithmic impact assessments, and the limitations of post-hoc explanations.

NOTABLE_GAPS:
Limited empirical research on whether explanations actually help individuals challenge discriminatory decisions; insufficient work on operationalizing "reasons-responsiveness" and similar legal standards in technical terms; few frameworks bridging philosophical concepts of responsibility and legal liability; limited guidance on how to balance explainability with accuracy in high-stakes domains; underdeveloped literature on international harmonization of AI regulations; minimal analysis of how neuroscience evidence could inform legal standards for algorithmic accountability.

SYNTHESIS_GUIDANCE:
Focus on three core tensions: (1) GDPR right to explanation debate (Wachter vs. Kaminski/Malgieri), (2) inadequacy of existing discrimination law for algorithms (Barocas/Selbst, Kim, Black et al.), and (3) explainability as legal requirement vs. technical feasibility (Babic/Cohen critique). Emphasize that legal frameworks provide both motivation and constraints for XAI research. The review should distinguish between individual rights approaches (explanation, contestability) and systemic governance approaches (audits, impact assessments). Note that effective legal compliance likely requires combining XAI techniques with institutional/procedural safeguards rather than relying on explanations alone.

KEY_POSITIONS:
- GDPR Right to Explanation Skeptics: 3 papers - Argue right doesn't exist or is inadequate (Wachter, Babic/Cohen)
- GDPR Right to Explanation Advocates: 2 papers - Argue for broad interpretation (Kaminski/Malgieri)
- Discrimination Law Reform: 5 papers - Call for rethinking antidiscrimination law for algorithms (Barocas/Selbst, Kim, Black et al., Xenidis)
- EU Regulatory Framework: 3 papers - Analyze EU AI Act and GDPR (Veale, Malgieri, Xenidis)
- US Regulatory Framework: 4 papers - Extend existing civil rights law to algorithms (EEOC guidance, CFPB, NYC Law 144)
- Administrative Law Perspectives: 2 papers - Apply administrative law principles to AI (Coglianese/Lehr)
- Algorithmic Accountability Mechanisms: 3 papers - Audits, impact assessments (Raji, Kaminski/Malgieri, Kroll)
====================================================================
}

@article{wachter2017right,
  author = {Wachter, Sandra and Mittelstadt, Brent and Floridi, Luciano},
  title = {Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation},
  journal = {International Data Privacy Law},
  year = {2017},
  volume = {7},
  number = {2},
  pages = {76--99},
  doi = {10.1093/idpl/ipx005},
  note = {CORE ARGUMENT: Argues that despite widespread claims, the GDPR does not establish a general right to explanation of automated decisions. Articles 13-15 provide only a limited "right to be informed" about the logic involved, not detailed explanations of individual decisions. The ambiguity and limited scope of Article 22 raises questions about actual protection afforded. Proposes counterfactual explanations as alternative that doesn't require opening black boxes. RELEVANCE: Central to legal debate on explainability requirements. If Wachter is correct, legal pressure for XAI may be less than assumed, though this remains contested. The counterfactual approach offers one legally-compliant path that differs from standard XAI techniques. Critical for understanding what GDPR actually requires vs. what scholars wish it required. POSITION: GDPR right to explanation skeptic; proposes counterfactual explanations as alternative.},
  keywords = {GDPR, right-to-explanation, legal-requirements, counterfactual-explanations, High}
}

@article{kaminski2021algorithmic,
  author = {Kaminski, Margot E. and Malgieri, Gianclaudio},
  title = {Algorithmic Impact Assessments under the GDPR: Producing Multi-Layered Explanations},
  journal = {International Data Privacy Law},
  year = {2021},
  volume = {11},
  number = {2},
  pages = {125--144},
  doi = {10.1093/idpl/ipaa020},
  note = {CORE ARGUMENT: Argues GDPR combines individual rights with systemic governance approaches to algorithmic accountability. Data Protection Impact Assessments (DPIAs) can serve as algorithmic impact assessments (AIAs) linking these two approaches. Proposes "multi-layered explanations" where DPIAs produce system-level descriptions of algorithmic logic while individual rights provisions (Arts 13-15) provide decision-specific information. This better fills gaps between GDPR's accountability mechanisms. RELEVANCE: Offers constructive interpretation of GDPR that requires XAI at both system and individual levels. Multi-layered approach aligns with hierarchical XAI techniques. Shows how combining systemic governance (audits, impact assessments) with individual explanations may satisfy legal requirements more effectively than either alone. Directly relevant to operationalizing explainability for legal compliance. POSITION: GDPR right to explanation advocate; systemic governance plus individual rights.},
  keywords = {GDPR, algorithmic-impact-assessment, multi-layered-explanations, systemic-governance, High}
}

@article{barocas2016disparate,
  author = {Barocas, Solon and Selbst, Andrew D.},
  title = {Big Data's Disparate Impact},
  journal = {California Law Review},
  year = {2016},
  volume = {104},
  number = {3},
  pages = {671--732},
  doi = {10.15779/Z38BG31},
  note = {CORE ARGUMENT: Foundational analysis of how data mining and machine learning can cause discrimination through disparate impact even without discriminatory intent. Data often encodes prior discrimination, societal biases, and patterns of exclusion. Algorithms inherit these biases from training data. Examines through lens of Title VII employment discrimination law. Shows that existing antidiscrimination doctrine struggles with algorithmic systems because discrimination is data-driven rather than intentional. Calls for wholesale reexamination of "discrimination" and "fairness" concepts. RELEVANCE: Seminal work establishing that algorithmic discrimination is serious legal problem not addressed by existing frameworks. Identifies the data-bias problem that XAI must help detect and mitigate. Shows why disparate impact doctrine (not just disparate treatment) is critical for algorithmic accountability. Essential background for understanding legal context of anti-discrimination XAI. POSITION: Critique of existing discrimination law; calls for fundamental reform for algorithmic age.},
  keywords = {algorithmic-discrimination, disparate-impact, Title-VII, data-bias, High}
}

@article{kim2017datadriven,
  author = {Kim, Pauline T.},
  title = {Data-Driven Discrimination at Work},
  journal = {William \& Mary Law Review},
  year = {2017},
  volume = {58},
  number = {3},
  pages = {857--936},
  note = {CORE ARGUMENT: Argues that mechanical application of existing Title VII doctrine fails to address real sources of bias when discrimination is data-driven. Identifies multiple pathways for bias: incomplete/unrepresentative training data, data encoding biased human judgments, models reproducing existing discrimination patterns. Even highly accurate models may perpetuate occupational segregation. Calls for fundamentally rethinking antidiscrimination doctrine, arguing statutory text supports prohibiting classification bias directly. RELEVANCE: Provides detailed analysis of how employment discrimination law applies (or fails to apply) to algorithmic hiring/promotion systems. Identifies specific technical sources of bias that XAI must make visible. Argues for legal standard that requires examining training data and model design, not just outputs - implicating XAI for compliance. Critical for understanding employment discrimination context. POSITION: Discrimination law reform for algorithms; focus on data bias and model transparency.},
  keywords = {employment-discrimination, Title-VII, algorithmic-hiring, data-bias, High}
}

@article{black2023less,
  author = {Black, Emily and Koepke, John Logan and Kim, Pauline and Barocas, Solon and Hsu, Mingwei},
  title = {Less Discriminatory Algorithms},
  journal = {arXiv preprint arXiv:2406.06817},
  year = {2023},
  note = {CORE ARGUMENT: Based on model multiplicity phenomenon - multiple equally accurate models almost always exist for a given prediction task, with different models exhibiting different levels of discriminatory impact across demographic groups. Argues entities using algorithmic systems should have legal duty to search for and implement less discriminatory algorithms (LDAs). Under disparate impact doctrine, defendant's justification burden should include showing reasonable search for LDAs. New regulations should require LDA searches as part of model development. RELEVANCE: Proposes concrete legal standard for algorithmic fairness that goes beyond detecting bias to requiring mitigation. The LDA concept operationalizes "less discriminatory alternative" from disparate impact law for ML context. Implies XAI must not only explain decisions but reveal whether less discriminatory models exist. Practical legal compliance framework that XAI techniques should support. POSITION: Legal duty to search for less discriminatory alternatives; reform of disparate impact doctrine.},
  keywords = {less-discriminatory-algorithms, model-multiplicity, disparate-impact, legal-duty, High}
}

@article{selbst2023unfair,
  author = {Selbst, Andrew D. and Barocas, Solon},
  title = {Unfair Artificial Intelligence: How FTC Intervention Can Overcome the Limitations of Discrimination Law},
  journal = {University of Pennsylvania Law Review},
  year = {2023},
  volume = {171},
  pages = {1069--1135},
  note = {CORE ARGUMENT: Argues FTC unfairness authority provides alternative pathway for addressing algorithmic discrimination beyond limitations of Title VII and other antidiscrimination statutes. Unfairness doctrine is more flexible than discrimination law - not restricted to specific definitions or judicial narrowing. FTC can reach practices that fall through cracks of discrimination law. FTC's focus on consumer harm and deception complements civil rights enforcement. Proposes coordinated approach with EEOC/CFPB. RELEVANCE: Identifies alternative legal framework for AI accountability beyond traditional discrimination law. Shows multiple legal routes to requiring fairness and transparency in algorithms. FTC authority may enable explainability requirements even where discrimination law doesn't clearly apply. Important for understanding full scope of legal pressures on algorithmic systems. Suggests XAI serves consumer protection goals beyond just anti-discrimination. POSITION: Regulatory pluralism; FTC unfairness authority as complement to discrimination law.},
  keywords = {FTC, unfairness-doctrine, algorithmic-discrimination, regulatory-framework, High}
}

@article{babic2023bait,
  author = {Babic, Boris and Cohen, I. Glenn},
  title = {The Algorithmic Explainability 'Bait and Switch'},
  journal = {Minnesota Law Review},
  year = {2023},
  volume = {108},
  pages = {857--909},
  note = {CORE ARGUMENT: Argues that prevailing XAI approaches fail to satisfy the desiderata that would make explainability a legitimate moral or legal requirement. Post-hoc explanations from methods like SHAP/LIME are "insincere-by-design" and (1) cannot guide action/planning, (2) cannot reveal actual reasons underlying decisions, (3) cannot underwrite normative judgments like blame/resentment. Distinguishes interpretable AI (transparent by design, with accuracy trade-offs) from explainable AI (post-hoc rationalizations). Argues we should face interpretability-accuracy trade-offs honestly rather than accept "fool's gold" of XAI. RELEVANCE: Fundamental challenge to assumption that current XAI techniques satisfy legal explainability requirements. If correct, developing legally adequate XAI requires different approaches than standard post-hoc methods. Critical counterpoint to claims that XAI solves accountability problems. Forces confrontation with limits of technical explainability for legal/moral purposes. Essential reading for understanding gap between technical XAI and legal requirements. POSITION: Critique of post-hoc XAI; skepticism about meeting legal/moral standards.},
  keywords = {explainability-critique, post-hoc-explanations, legal-requirements, XAI-limitations, High}
}

@article{coglianese2019transparency,
  author = {Coglianese, Cary and Lehr, David},
  title = {Transparency and Algorithmic Governance},
  journal = {Administrative Law Review},
  year = {2019},
  volume = {71},
  number = {1},
  pages = {1--56},
  note = {CORE ARGUMENT: Examines whether machine learning algorithms' inscrutability poses barrier to government use under administrative law transparency principles. Concludes ML algorithms can fit within conventional legal parameters when properly understood. Distinguishes different types of transparency (model, data, inferential, outcome). Argues that algorithmic governance may actually enhance transparency compared to opaque human decision-making. Agencies can provide reasons for rules/policies even if individual ML predictions are opaque. Proposes disclosure of training data, model validation, and performance metrics. RELEVANCE: Provides administrative law framework for algorithmic accountability distinct from civil rights approach. Shows transparency has multiple dimensions beyond explaining individual decisions. Government use case differs from private sector - different legal requirements apply. Suggests paths to legal compliance that don't require full explainability of each prediction. Relevant for public sector deployment of XAI-assisted systems. POSITION: Administrative law perspective; multiple forms of transparency can satisfy legal requirements.},
  keywords = {administrative-law, transparency, government-AI, regulatory-framework, Medium}
}

@misc{cfpb2022adverse,
  author = {{Consumer Financial Protection Bureau}},
  title = {CFPB Issues Guidance on Credit Denials by Lenders Using Artificial Intelligence},
  howpublished = {Press Release},
  year = {2022},
  month = {May},
  url = {https://www.consumerfinance.gov/about-us/newsroom/cfpb-issues-guidance-on-credit-denials-by-lenders-using-artificial-intelligence/},
  note = {CORE ARGUMENT: CFPB circular stating Equal Credit Opportunity Act requires creditors to explain specific reasons for adverse actions even when using complex algorithms. ECOA does not permit creditors to use technology that prevents providing specific/accurate reasons. No exemption for "black box" models. Creditors must assess AI explainability before deployment and may need to forgo certain models if they cannot generate required explanations. Creditors remain liable regardless of whether third-party vendor designed the algorithm. RELEVANCE: Official regulatory guidance establishing explainability as legal requirement in credit context under ECOA. Creates direct legal pressure for interpretable or explainable AI in lending. Makes clear "it's too complex to explain" is not acceptable defense. Vendors and lenders must ensure XAI capabilities before deployment. Key example of how existing law (ECOA adverse action notice requirements) imposes explainability mandates. POSITION: US regulatory requirement for explainability in credit decisions; ECOA compliance.},
  keywords = {ECOA, CFPB, credit-discrimination, adverse-action-notices, explainability-requirement, High}
}

@misc{eeoc2023guidance,
  author = {{U.S. Equal Employment Opportunity Commission}},
  title = {Assessing Adverse Impact in Software, Algorithms, and Artificial Intelligence Used in Employment Selection Procedures Under Title VII of the Civil Rights Act of 1964},
  howpublished = {Technical Assistance Document},
  year = {2023},
  month = {May},
  url = {https://www.eeoc.gov/laws/guidance/assessing-adverse-impact-software-algorithms-and-artificial-intelligence-used},
  note = {CORE ARGUMENT: EEOC guidance stating Title VII applies to algorithmic employment selection tools. Employers remain liable for discriminatory impacts even when using third-party AI vendors. Must conduct adverse impact analysis on algorithmic tools. Software/AI that disproportionately screens out protected groups violates Title VII unless job-related and consistent with business necessity, and no less discriminatory alternative exists. Employers should consider taking protected characteristics into account to test for and mitigate disparate impact. RELEVANCE: Official EEOC position establishing that existing employment discrimination law fully applies to AI/algorithms. Requires employers to audit algorithms for bias - creating need for XAI tools to detect disparate impact. Employer liability even for vendor tools means transparency into algorithmic functioning is necessary for compliance. Establishes legal duty to search for less discriminatory alternatives. Key regulatory driver for fairness/explainability in hiring AI. POSITION: US regulatory framework; Title VII applies to algorithmic hiring; employer liability for vendor tools.},
  keywords = {Title-VII, EEOC, employment-discrimination, algorithmic-hiring, adverse-impact, High}
}

@misc{whitehouse2022blueprint,
  author = {{White House Office of Science and Technology Policy}},
  title = {Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People},
  year = {2022},
  month = {October},
  url = {https://www.whitehouse.gov/ostp/ai-bill-of-rights/},
  note = {CORE ARGUMENT: Establishes five principles for AI systems including "Algorithmic Discrimination Protections" - individuals should not face discrimination by algorithms and systems should be designed/used equitably. Calls for proactive measures to protect against discrimination based on race, sex, disability, and other protected characteristics. Recommends independent evaluation, plain language reporting, algorithmic impact assessments, and disparity testing. Principles are non-binding but intended to inform future US policy and legislation. RELEVANCE: Official White House position establishing policy direction for US AI regulation. While non-binding, signals government priorities and likely future regulatory requirements. Algorithmic discrimination protection principle creates expectation that AI systems will be monitored for bias. Call for impact assessments and disparity testing creates framework for XAI deployment. Influences state/federal legislation and agency rulemaking. Relevant for understanding US regulatory trajectory even though not enforceable law. POSITION: US policy framework; voluntary principles; algorithmic discrimination protections.},
  keywords = {AI-Bill-of-Rights, US-policy, algorithmic-discrimination, voluntary-framework, Medium}
}

@article{xenidis2024computers,
  author = {Xenidis, Rapha{\"e}l{e}},
  title = {When Computers Say No: Towards a Legal Response to Algorithmic Discrimination in Europe},
  booktitle = {Research Handbook on Law and Technology},
  publisher = {Edward Elgar Publishing},
  year = {2024},
  pages = {337--362},
  note = {CORE ARGUMENT: Examines interpretation and application of EU non-discrimination laws in algorithmic society. Analyzes how EU equality law can be "tuned" to address algorithmic discrimination through three pathways: (1) strengthening individual rights, (2) enhancing regulatory oversight, (3) promoting algorithmic justice through institutional design. Argues existing EU equality directives apply to algorithms but require updated interpretation. Emphasizes need for systemic approaches beyond individual complaints. Discusses intersection of equality law with GDPR and AI Act. RELEVANCE: Comprehensive analysis of EU legal framework for algorithmic discrimination. Shows how multiple legal instruments (equality directives, GDPR, AI Act) interact to create accountability regime. Identifies gaps in individual rights approach and need for systemic oversight. Relevant for understanding EU regulatory landscape and how XAI fits within broader legal framework. Three-pathway model useful for categorizing legal interventions. POSITION: EU equality law applies to algorithms; need for systemic reforms and institutional safeguards.},
  keywords = {EU-law, equality-law, algorithmic-discrimination, regulatory-oversight, Medium}
}

@article{veale2021demystifying,
  author = {Veale, Michael and Zuiderveen Borgesius, Frederik},
  title = {Demystifying the Draft EU Artificial Intelligence Act},
  journal = {Computer Law Review International},
  year = {2021},
  volume = {22},
  number = {4},
  pages = {97--112},
  doi = {10.9785/cri-2021-220402},
  note = {CORE ARGUMENT: Provides comprehensive overview and analysis of draft EU AI Act. Explains risk-based approach (unacceptable, high-risk, limited-risk, minimal-risk). Details requirements for high-risk systems including transparency, human oversight, documentation, conformity assessment. Analyzes transparency obligations (Art 13, 52) and their relationship to GDPR. Discusses enforcement mechanisms and penalties. Notes tensions between innovation and regulation, harmonization complexities, and gaps in coverage. RELEVANCE: Essential guide to understanding EU AI Act's transparency and explainability requirements. High-risk AI systems must be "sufficiently transparent" for users to interpret outputs - creating legal mandate for XAI in covered applications. Act establishes technical documentation requirements, logging obligations, and instructions for use. Coordinates with GDPR but establishes additional requirements. Critical for understanding most comprehensive AI regulation globally and its explainability mandates. POSITION: Analysis of EU AI Act regulatory framework; risk-based regulation with transparency requirements.},
  keywords = {EU-AI-Act, transparency-requirements, high-risk-AI, regulatory-framework, High}
}

@article{raji2020closing,
  author = {Raji, Inioluwa Deborah and Smart, Andrew and White, Rebecca N. and Mitchell, Margaret and Gebru, Timnit and Hutchinson, Ben and Smith-Loud, Jamila and Theron, Daniel and Barnes, Parker},
  title = {Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing},
  journal = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  year = {2020},
  pages = {33--44},
  doi = {10.1145/3351095.3372873},
  note = {CORE ARGUMENT: Proposes comprehensive framework for algorithmic auditing throughout AI development lifecycle. Internal audits embedded in organization development process can close accountability gap. Framework includes scoping (what to audit), mapping (understand system), artifact collection (gather documentation), testing (evaluate performance/fairness), and reflection (interpret findings). Emphasizes need for audit integrity - independence, expertise, transparency. Audits must be proactive not just reactive. RELEVANCE: Provides operationalizable framework for algorithmic accountability that complements legal requirements. Internal auditing process can generate documentation needed for legal compliance (impact assessments, bias testing). Framework shows how XAI techniques fit within broader accountability infrastructure - explanations are one artifact among many. Demonstrates that legal compliance requires institutional processes not just technical tools. Practical guide for implementing algorithmic accountability. POSITION: Algorithmic auditing framework; internal organizational processes for accountability.},
  keywords = {algorithmic-auditing, accountability-framework, bias-testing, organizational-processes, Medium}
}

@misc{nyc2021law144,
  author = {{New York City}},
  title = {Local Law 144 of 2021: Automated Employment Decision Tools},
  year = {2021},
  note = {CORE ARGUMENT: First enacted law in US requiring bias audits for automated employment decision tools (AEDTs). Employers/agencies cannot use AEDT unless: (1) independent bias audit conducted within past year, (2) audit results publicly available, (3) candidates notified at least 10 days before AEDT use. Bias audit must test for disparate impact by race/ethnicity and sex. Penalties up to $1,500 per violation. Defines AEDT as any tool using ML, algorithms, AI, or data analysis to substantially assist/replace employment decisions. RELEVANCE: First mandatory bias audit law in US - represents emerging regulatory approach. Creates legal requirement for fairness testing of hiring algorithms in NYC. Public disclosure requirement creates transparency pressure. However, implementation challenges noted - vague definitions, weak independence requirements, limited enforcement. Shows both promise and limitations of audit-based regulation. Important case study in algorithmic accountability regulation and compliance challenges. POSITION: Municipal bias audit requirement; mandatory fairness testing and transparency.},
  keywords = {NYC-Law-144, bias-audit, employment-discrimination, mandatory-testing, High}
}

@article{kroll2017accountable,
  author = {Kroll, Joshua A. and Huey, Joanna and Barocas, Solon and Felten, Edward W. and Reidenberg, Joel R. and Robinson, David G. and Yu, Harlan},
  title = {Accountable Algorithms},
  journal = {University of Pennsylvania Law Review},
  year = {2017},
  volume = {165},
  number = {3},
  pages = {633--705},
  note = {CORE ARGUMENT: Challenges assumption that source code disclosure is necessary or sufficient for algorithmic accountability. Disclosure often neither necessary (alternative verification techniques exist from computer science) nor sufficient (code complexity, data dependencies). Introduces toolkit of accountability mechanisms beyond transparency: auditing, testing, certification, monitoring. Focuses on verifying compliance with legal fairness standards rather than full explainability. Technical mechanisms can demonstrate fairness without revealing proprietary details. RELEVANCE: Influential argument that algorithmic accountability doesn't require full transparency/explainability - verification techniques can confirm compliance without black-box access. Relevant for resolving transparency vs. trade secrets tension. Shows how technical tools (auditing, testing) can substitute for explanation in some contexts. However, verification still requires some transparency into inputs/outputs and performance metrics. Framework helps identify what XAI must provide for legal compliance vs. what can be verified through other means. POSITION: Accountability through verification and auditing; alternatives to full transparency.},
  keywords = {algorithmic-accountability, verification, auditing, trade-secrets, Medium}
}

@article{citron2014scored,
  author = {Citron, Danielle Keats and Pasquale, Frank},
  title = {The Scored Society: Due Process for Automated Predictions},
  journal = {Washington Law Review},
  year = {2014},
  volume = {89},
  number = {1},
  pages = {1--33},
  note = {CORE ARGUMENT: Argues for "technological due process" - procedural protections for automated scoring systems affecting life, liberty, or property. Predictive algorithms increasingly determine access to opportunities but operate with minimal transparency or accountability. Proposes regulatory oversight (FTC audits) of algorithmic scoring systems. Individuals should have notice of automated decisions and ability to challenge them. Government use of algorithms requires stronger transparency than private sector. Need for explanation rights, contestability, and human oversight. RELEVANCE: Early influential argument for due process protections in algorithmic decision-making. Establishes civil liberties/due process rationale for explainability beyond anti-discrimination concerns. "Technological due process" concept has influenced subsequent regulatory proposals. Shows why government use of algorithms faces stricter transparency requirements (administrative law). Contestability requires some level of explainability - individuals must understand decision enough to challenge it. POSITION: Technological due process; notice and opportunity to challenge algorithmic decisions.},
  keywords = {technological-due-process, algorithmic-accountability, procedural-protections, contestability, Medium}
}

@article{malgieri2022legibility,
  author = {Malgieri, Gianclaudio and Comand{\'e}, Giovanni},
  title = {Why a Right to Legibility of Automated Decision-Making Exists in the General Data Protection Regulation},
  journal = {International Data Privacy Law},
  year = {2017},
  volume = {7},
  number = {4},
  pages = {243--265},
  doi = {10.1093/idpl/ipx019},
  note = {CORE ARGUMENT: Argues GDPR establishes "right to legibility" of automated decisions - combining transparency and comprehensibility. Articles 13-15 require "meaningful information about the logic involved" which implies both disclosure and understandability. Legibility means system must be comprehensible to data subjects, not just technically transparent. Distinguishes between system-level legibility (how algorithm generally works) and decision-level legibility (why specific decision was made). Both are required under GDPR. Interpretation should enable individuals to exercise GDPR rights effectively. RELEVANCE: Provides alternative interpretation of GDPR that requires genuine comprehensibility not just information disclosure. "Legibility" concept may bridge technical explainability and legal requirements - XAI must produce actually understandable explanations. Dual-level approach (system and decision) aligns with multi-layered explanation framework. Shows that effective legal compliance requires explanations tailored to data subject comprehension level, not just technically accurate outputs. POSITION: GDPR right to explanation advocate; legibility requires comprehensibility not just transparency.},
  keywords = {GDPR, legibility, comprehensibility, meaningful-information, Medium}
}

@article{pasquale2015black,
  author = {Pasquale, Frank},
  title = {The Black Box Society: The Secret Algorithms That Control Money and Information},
  publisher = {Harvard University Press},
  year = {2015},
  note = {CORE ARGUMENT: Examines how algorithms in finance, search, and reputation systems operate as "black boxes" - making consequential decisions about individuals without transparency or accountability. Trade secrecy and complexity shield algorithms from scrutiny even as they encode values and exercise power. Calls for "qualified transparency" - appropriate stakeholders (regulators, researchers) should have access even if general public doesn't. Proposes new regulatory frameworks including algorithmic auditing requirements, due process protections, and limits on automated decision-making in sensitive domains. RELEVANCE: Influential work establishing "black box" problem and arguing for transparency/accountability in algorithmic systems. Shows how opacity serves corporate interests at expense of individual rights and public welfare. "Qualified transparency" concept addresses trade secret concerns - suggests third-party auditors can verify fairness without public disclosure. Identifies financial and informational domains where XAI is particularly critical. Foundational text for algorithmic accountability movement and regulatory push for explainability. POSITION: Critique of algorithmic opacity; calls for qualified transparency and regulatory auditing.},
  keywords = {black-box-algorithms, algorithmic-opacity, qualified-transparency, regulatory-auditing, Low}
}

@article{edwards2024international,
  author = {Edwards, Lilian and Veale, Michael},
  title = {Enslaving the Algorithm: From a "Right to an Explanation" to a "Right to Better Decisions"?},
  journal = {IEEE Security \& Privacy},
  year = {2018},
  volume = {16},
  number = {3},
  pages = {46--54},
  doi = {10.1109/MSP.2018.2701152},
  note = {CORE ARGUMENT: Questions whether "right to explanation" focus is misplaced. Explanations may not actually help individuals challenge decisions or improve algorithmic fairness. Proposes shifting from "right to explanation" to "right to better decisions" - focusing on decision quality and contestability rather than explanation per se. Explanations are means not ends - goal should be fair, accurate, non-discriminatory decisions that can be contested when wrong. May need to accept some opacity in exchange for better decisions, with accountability through auditing and testing rather than individual explanations. RELEVANCE: Important critique of explanation-centered approach to algorithmic accountability. Challenges assumption that XAI is solution - asks whether explanations actually achieve accountability goals. Suggests focusing on decision quality metrics, contestability mechanisms, and systemic oversight may be more effective than individual explanations. Relevant for evaluating whether XAI techniques satisfy actual accountability needs vs. just providing explanations. Shifts emphasis to outcomes and contestability. POSITION: Critique of explanation focus; emphasizes decision quality and contestability over transparency.},
  keywords = {right-to-explanation-critique, decision-quality, contestability, accountability-mechanisms, Low}
}

